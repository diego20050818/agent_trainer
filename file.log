>>> 2025-08-16 21:24:49,821 - INFO - 开始进行训练
>>> 2025-08-16 21:26:15,103 - INFO - 开始进行训练
>>> 2025-08-16 21:29:13,259 - INFO - 开始进行训练
>>> 2025-08-16 23:11:39,316 - INFO - 当前环境：/home/liangshuqiao/anaconda3/bin/python
>>> 2025-08-16 23:11:42,597 - INFO - 开始进行训练
>>> 2025-08-16 23:15:38,382 - INFO - 当前环境：/bin/python
>>> 2025-08-16 23:15:40,819 - INFO - 开始进行训练
>>> 2025-08-16 23:16:11,544 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-16 23:16:15,907 - INFO - 开始进行训练
>>> 2025-08-16 23:17:10,779 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-16 23:17:15,142 - INFO - 开始进行训练
>>> 2025-08-16 23:17:21,573 - INFO - 导入包完成
>>> 2025-08-16 23:17:23,743 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-16 23:17:26,902 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-16 23:17:31,143 - INFO - tokenizer读取完成
>>> 2025-08-16 23:17:35,370 - INFO - model dtype:torch.float16
>>> 2025-08-16 23:17:38,108 - INFO - 模型导入完成
>>> 2025-08-16 23:17:41,062 - INFO - 读取数据集成功
>>> 2025-08-16 23:17:43,389 - INFO - 数据处理成功
>>> 2025-08-16 23:17:47,799 - INFO - None
>>> 2025-08-16 23:18:00,028 - INFO - 开始训练！
>>> 2025-08-16 23:19:17,983 - INFO - 训练成功！
>>> 2025-08-16 23:19:20,149 - INFO - 模型存放位置：./output/DeepSeek202508162317
>>> 2025-08-16 23:32:46,181 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-16 23:32:50,544 - INFO - 开始进行训练
>>> 2025-08-16 23:32:56,994 - INFO - 导入包完成
>>> 2025-08-16 23:32:59,163 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-16 23:33:02,323 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-16 23:33:06,594 - INFO - tokenizer读取完成
>>> 2025-08-16 23:33:10,925 - INFO - model dtype:torch.float16
>>> 2025-08-16 23:33:13,663 - INFO - 模型导入完成
>>> 2025-08-16 23:33:16,590 - INFO - 读取数据集成功
>>> 2025-08-16 23:33:18,938 - INFO - 数据处理成功
>>> 2025-08-16 23:33:23,321 - INFO - None
>>> 2025-08-16 23:33:35,484 - INFO - 开始训练！
>>> 2025-08-16 23:34:53,557 - INFO - 训练成功！
>>> 2025-08-16 23:34:55,723 - INFO - 模型存放位置：./output/DeepSeek202508162333
>>> 2025-08-16 23:35:33,925 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-16 23:35:38,286 - INFO - 开始进行训练
>>> 2025-08-16 23:35:44,608 - INFO - 导入包完成
>>> 2025-08-16 23:35:46,775 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-16 23:35:49,933 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-16 23:35:54,179 - INFO - tokenizer读取完成
>>> 2025-08-16 23:35:58,384 - INFO - model dtype:torch.float16
>>> 2025-08-16 23:36:01,123 - INFO - 模型导入完成
>>> 2025-08-16 23:36:04,047 - INFO - 读取数据集成功
>>> 2025-08-16 23:36:06,387 - INFO - 数据处理成功
>>> 2025-08-16 23:36:10,679 - INFO - None
>>> 2025-08-16 23:36:18,227 - INFO - 开始训练！
>>> 2025-08-16 23:37:51,533 - INFO - 训练成功！
>>> 2025-08-16 23:37:53,699 - INFO - 模型存放位置：./output/DeepSeek202508162336
>>> 2025-08-16 23:43:51,958 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-16 23:43:56,319 - INFO - 开始进行训练
>>> 2025-08-16 23:44:02,638 - INFO - 导入包完成
>>> 2025-08-16 23:44:04,806 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-16 23:44:07,965 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-16 23:44:12,209 - INFO - tokenizer读取完成
>>> 2025-08-16 23:44:16,520 - INFO - model dtype:torch.float16
>>> 2025-08-16 23:44:19,259 - INFO - 模型导入完成
>>> 2025-08-16 23:44:23,572 - INFO - 读取数据集成功
>>> 2025-08-16 23:44:25,910 - INFO - 数据处理成功
>>> 2025-08-16 23:44:30,253 - INFO - None
>>> 2025-08-16 23:44:41,806 - INFO - 开始训练！
>>> 2025-08-16 23:46:02,028 - INFO - 训练成功！
>>> 2025-08-16 23:46:04,195 - INFO - 模型存放位置：./output/DeepSeek202508162344
>>> 2025-08-16 23:47:26,207 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-16 23:47:30,569 - INFO - 开始进行训练
>>> 2025-08-16 23:47:36,926 - INFO - 导入包完成
>>> 2025-08-16 23:47:39,095 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-16 23:47:42,254 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-16 23:47:46,495 - INFO - tokenizer读取完成
>>> 2025-08-16 23:47:50,861 - INFO - model dtype:torch.float16
>>> 2025-08-16 23:47:53,599 - INFO - 模型导入完成
>>> 2025-08-16 23:47:56,511 - INFO - 读取数据集成功
>>> 2025-08-16 23:47:58,854 - INFO - 数据处理成功
>>> 2025-08-16 23:48:03,152 - INFO - None
>>> 2025-08-16 23:48:14,790 - INFO - 开始训练！
>>> 2025-08-16 23:49:46,396 - INFO - 训练成功！
>>> 2025-08-16 23:49:48,562 - INFO - 模型存放位置：./output/DeepSeek202508162348
>>> 2025-08-16 23:52:13,360 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-16 23:52:17,723 - INFO - 开始进行训练
>>> 2025-08-16 23:52:24,021 - INFO - 导入包完成
>>> 2025-08-16 23:52:26,189 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-16 23:52:29,348 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-16 23:52:33,580 - INFO - tokenizer读取完成
>>> 2025-08-16 23:52:37,852 - INFO - model dtype:torch.float16
>>> 2025-08-16 23:52:40,590 - INFO - 模型导入完成
>>> 2025-08-16 23:52:43,571 - INFO - 读取数据集成功
>>> 2025-08-16 23:52:46,011 - INFO - 数据处理成功
>>> 2025-08-16 23:52:50,351 - INFO - None
>>> 2025-08-16 23:53:01,685 - INFO - 开始训练！
>>> 2025-08-16 23:54:26,014 - INFO - 训练成功！
>>> 2025-08-16 23:54:28,180 - INFO - 模型存放位置：./output/DeepSeek202508162352
>>> 2025-08-16 23:56:42,025 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-16 23:56:46,387 - INFO - 开始进行训练
>>> 2025-08-16 23:56:52,699 - INFO - 导入包完成
>>> 2025-08-16 23:56:54,868 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-16 23:56:58,026 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-16 23:57:02,276 - INFO - tokenizer读取完成
>>> 2025-08-16 23:57:06,682 - INFO - model dtype:torch.float16
>>> 2025-08-16 23:57:09,420 - INFO - 模型导入完成
>>> 2025-08-16 23:57:12,635 - INFO - 读取数据集成功
>>> 2025-08-16 23:57:14,974 - INFO - 数据处理成功
>>> 2025-08-16 23:57:19,249 - INFO - None
>>> 2025-08-16 23:57:32,304 - INFO - 开始训练！
>>> 2025-08-16 23:59:03,038 - INFO - 训练成功！
>>> 2025-08-16 23:59:05,204 - INFO - 模型存放位置：./output/DeepSeek202508162357
>>> 2025-08-16 23:59:55,952 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 00:00:00,975 - INFO - 开始进行训练
>>> 2025-08-17 00:00:25,670 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 00:00:30,033 - INFO - 开始进行训练
>>> 2025-08-17 00:00:36,331 - INFO - 导入包完成
>>> 2025-08-17 00:00:38,497 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 00:00:41,656 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 00:00:45,888 - INFO - tokenizer读取完成
>>> 2025-08-17 00:00:50,156 - INFO - model dtype:torch.float16
>>> 2025-08-17 00:00:52,894 - INFO - 模型导入完成
>>> 2025-08-17 00:00:55,828 - INFO - 读取数据集成功
>>> 2025-08-17 00:00:58,168 - INFO - 数据处理成功
>>> 2025-08-17 00:01:02,583 - INFO - None
>>> 2025-08-17 00:01:14,047 - INFO - 开始训练！
>>> 2025-08-17 00:05:54,119 - INFO - 训练成功！
>>> 2025-08-17 00:05:56,285 - INFO - 模型存放位置：./output/DeepSeek202508170001
>>> 2025-08-17 08:49:10,321 - INFO - 当前环境：/bin/python
>>> 2025-08-17 08:49:12,758 - INFO - 开始进行训练
>>> 2025-08-17 08:49:42,594 - INFO - 当前环境：/home/liangshuqiao/anaconda3/bin/python
>>> 2025-08-17 08:49:45,874 - INFO - 开始进行训练
>>> 2025-08-17 08:50:06,023 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 08:50:10,386 - INFO - 开始进行训练
>>> 2025-08-17 08:50:16,731 - INFO - 导入包完成
>>> 2025-08-17 08:50:18,900 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 08:50:22,059 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 08:50:26,301 - INFO - tokenizer读取完成
>>> 2025-08-17 09:03:18,983 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:03:23,345 - INFO - 开始进行训练
>>> 2025-08-17 09:03:26,060 - ERROR - 没有找到 checkpoint 文件夹!
>>> 2025-08-17 09:09:00,616 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:09:04,978 - INFO - 开始进行模型测试
>>> 2025-08-17 09:09:12,622 - INFO - 已选择模型文件夹: DeepSeek202508170001
>>> 2025-08-17 09:12:36,319 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:12:40,681 - INFO - 开始进行模型测试
>>> 2025-08-17 09:12:46,729 - INFO - 已选择模型文件夹: DeepSeek202508170001
>>> 2025-08-17 09:12:49,982 - INFO - 最新的 LoRA checkpoint 路径:output/DeepSeek202508170001/checkpoint-120
>>> 2025-08-17 09:22:21,964 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:22:26,325 - INFO - 开始进行模型测试
>>> 2025-08-17 09:22:33,503 - INFO - 已选择模型文件夹: Qwen2.5_instruct_lora
>>> 2025-08-17 09:22:36,783 - INFO - 最新的 LoRA checkpoint 路径:output/Qwen2.5_instruct_lora/checkpoint-180
>>> 2025-08-17 09:23:28,094 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:23:32,456 - INFO - 开始进行模型测试
>>> 2025-08-17 09:23:43,805 - INFO - 已选择模型文件夹: Qwen2.5_instruct_lora
>>> 2025-08-17 09:23:47,084 - ERROR - 你选了一个不是deepseek的模型，接下来可能会报错
>>> 2025-08-17 09:24:13,180 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:24:17,542 - INFO - 开始进行模型测试
>>> 2025-08-17 09:24:23,395 - INFO - 已选择模型文件夹: DeepSeek202508170001
>>> 2025-08-17 09:24:26,646 - ERROR - 你选了一个不是deepseek的模型，接下来可能会报错
>>> 2025-08-17 09:25:47,713 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:25:52,074 - INFO - 开始进行模型测试
>>> 2025-08-17 09:25:59,992 - INFO - 已选择模型文件夹: DeepSeek202508170001
>>> 2025-08-17 09:26:03,242 - INFO - 最新的 LoRA checkpoint 路径:output/DeepSeek202508170001/checkpoint-120
>>> 2025-08-17 09:26:47,763 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:26:52,125 - INFO - 开始进行模型测试
>>> 2025-08-17 09:26:57,913 - INFO - 已选择模型文件夹: DeepSeek202508170001
>>> 2025-08-17 09:27:01,165 - INFO - 最新的 LoRA checkpoint 路径:output/DeepSeek202508170001/checkpoint-120
>>> 2025-08-17 09:29:34,633 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:29:38,994 - INFO - 开始进行模型测试
>>> 2025-08-17 09:29:44,700 - INFO - 已选择模型文件夹: DeepSeek202508170001
>>> 2025-08-17 09:29:47,951 - INFO - 最新的 LoRA checkpoint 路径:output/DeepSeek202508170001/checkpoint-120
>>> 2025-08-17 09:41:59,400 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:42:03,763 - INFO - 开始进行训练
>>> 2025-08-17 09:42:10,245 - INFO - 导入包完成
>>> 2025-08-17 09:42:12,414 - INFO - 配置文件读取完成
>>> 2025-08-17 09:42:14,641 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 09:42:17,801 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 09:42:22,038 - INFO - tokenizer读取完成
>>> 2025-08-17 09:42:26,368 - INFO - model dtype:torch.float16
>>> 2025-08-17 09:42:29,107 - INFO - 模型导入完成
>>> 2025-08-17 09:42:32,048 - INFO - 读取数据集成功
>>> 2025-08-17 09:42:34,494 - INFO - 数据处理成功
>>> 2025-08-17 09:42:38,767 - INFO - None
>>> 2025-08-17 09:42:51,184 - INFO - 开始训练！
>>> 2025-08-17 09:43:16,535 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 4.79 GiB. GPU 0 has a total capacity of 31.73 GiB of which 2.50 GiB is free. Including non-PyTorch memory, this process has 29.23 GiB memory in use. Of the allocated memory 25.76 GiB is allocated by PyTorch, and 3.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-08-17 09:44:11,057 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:44:15,421 - INFO - 开始进行训练
>>> 2025-08-17 09:44:21,877 - INFO - 导入包完成
>>> 2025-08-17 09:44:24,046 - INFO - 配置文件读取完成
>>> 2025-08-17 09:44:26,272 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 09:44:29,431 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 09:44:33,663 - INFO - tokenizer读取完成
>>> 2025-08-17 09:44:38,043 - INFO - model dtype:torch.float16
>>> 2025-08-17 09:44:40,781 - INFO - 模型导入完成
>>> 2025-08-17 09:44:43,720 - INFO - 读取数据集成功
>>> 2025-08-17 09:44:46,050 - INFO - 数据处理成功
>>> 2025-08-17 09:44:50,375 - INFO - None
>>> 2025-08-17 09:45:02,548 - INFO - 开始训练！
>>> 2025-08-17 09:45:27,823 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 4.79 GiB. GPU 0 has a total capacity of 31.73 GiB of which 2.50 GiB is free. Including non-PyTorch memory, this process has 29.23 GiB memory in use. Of the allocated memory 25.76 GiB is allocated by PyTorch, and 3.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-08-17 09:48:12,379 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:48:16,743 - INFO - 开始进行训练
>>> 2025-08-17 09:48:23,109 - INFO - 导入包完成
>>> 2025-08-17 09:48:25,275 - INFO - 配置文件读取完成
>>> 2025-08-17 09:48:27,501 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 09:48:30,660 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 09:48:34,900 - INFO - tokenizer读取完成
>>> 2025-08-17 09:48:39,211 - INFO - model dtype:torch.float16
>>> 2025-08-17 09:48:41,950 - INFO - 模型导入完成
>>> 2025-08-17 09:48:44,886 - INFO - 读取数据集成功
>>> 2025-08-17 09:48:47,326 - INFO - 数据处理成功
>>> 2025-08-17 09:48:51,566 - INFO - None
>>> 2025-08-17 09:49:03,312 - INFO - 开始训练！
>>> 2025-08-17 09:57:42,407 - INFO - 训练成功！
>>> 2025-08-17 09:57:44,573 - INFO - 模型存放位置：./output/DeepSeek202508170948
>>> 2025-08-17 10:11:12,934 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 10:11:17,296 - INFO - 开始进行模型测试
>>> 2025-08-17 10:11:23,862 - INFO - 已选择模型文件夹: DeepSeek202508170948
>>> 2025-08-17 10:11:27,113 - INFO - 最新的 LoRA checkpoint 路径:output/DeepSeek202508170948/checkpoint-240

>>> 2025-08-17 10:30:14,046 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 10:30:18,409 - INFO - 开始进行训练
>>> 2025-08-17 10:30:22,789 - INFO - 导入包完成
>>> 2025-08-17 10:30:24,957 - INFO - 配置文件读取完成
>>> 2025-08-17 10:30:27,183 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 10:31:47,927 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 10:31:52,289 - INFO - 开始进行训练
>>> 2025-08-17 10:31:56,675 - INFO - 导入包完成
>>> 2025-08-17 10:31:58,843 - INFO - 配置文件读取完成
>>> 2025-08-17 10:32:01,069 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 10:32:04,229 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 10:32:08,468 - INFO - tokenizer读取完成
>>> 2025-08-17 10:32:12,798 - INFO - model dtype:torch.float16
>>> 2025-08-17 10:32:15,536 - INFO - 模型导入完成
>>> 2025-08-17 10:32:18,477 - INFO - 读取数据集成功
>>> 2025-08-17 10:32:20,913 - INFO - 数据处理成功
>>> 2025-08-17 10:32:36,036 - INFO - 开始训练！
>>> 2025-08-17 10:32:38,203 - INFO - 批次大小  : 4
>>> 2025-08-17 10:32:40,489 - INFO - 训练轮数  : 20
>>> 2025-08-17 10:32:42,806 - INFO - 学习率    : 0.0001
>>> 2025-08-17 10:32:45,273 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 10:32:48,493 - INFO - 模型路径  : /home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 10:32:54,748 - INFO - 训练日志: >>> 训练进度: Epoch 0.08 | Loss 4.0652 | LR 1.00e-04 | Grad Norm 1.6210
>>> 2025-08-17 10:33:01,713 - INFO - 训练日志: >>> 训练进度: Epoch 0.40 | Loss 4.0982 | LR 9.99e-05 | Grad Norm 2.2548
>>> 2025-08-17 10:33:12,593 - INFO - 训练日志: >>> 训练进度: Epoch 0.80 | Loss 3.7997 | LR 9.97e-05 | Grad Norm 1.3718
>>> 2025-08-17 10:33:21,621 - INFO - 训练日志: >>> 训练进度: Epoch 1.16 | Loss 3.0882 | LR 9.92e-05 | Grad Norm 2.6920
>>> 2025-08-17 10:33:33,631 - INFO - 训练日志: >>> 训练进度: Epoch 1.56 | Loss 2.7578 | LR 9.85e-05 | Grad Norm 1.4040
>>> 2025-08-17 10:34:32,384 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 10:34:36,746 - INFO - 开始进行训练
>>> 2025-08-17 10:34:41,126 - INFO - 导入包完成
>>> 2025-08-17 10:34:43,293 - INFO - 配置文件读取完成
>>> 2025-08-17 10:34:45,520 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 10:34:48,679 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 10:34:52,922 - INFO - tokenizer读取完成
>>> 2025-08-17 10:34:57,172 - INFO - model dtype:torch.float16
>>> 2025-08-17 10:34:59,910 - INFO - 模型导入完成
>>> 2025-08-17 10:35:02,837 - INFO - 读取数据集成功
>>> 2025-08-17 10:35:05,176 - INFO - 数据处理成功
>>> 2025-08-17 10:35:19,356 - INFO - 开始训练！
>>> 2025-08-17 10:35:21,522 - INFO - 批次大小  : 4
>>> 2025-08-17 10:35:23,808 - INFO - 训练轮数  : 20
>>> 2025-08-17 10:35:26,125 - INFO - 学习率    : 0.0001
>>> 2025-08-17 10:35:28,592 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 10:35:31,811 - INFO - 模型路径  : /home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 10:35:38,061 - INFO - >>> {'loss': 4.0652, 'grad_norm': 1.623105764389038, 'learning_rate': 0.0001, 'epoch': 0.08}
>>> 2025-08-17 10:35:45,074 - INFO - >>> {'loss': 4.095, 'grad_norm': 2.280607223510742, 'learning_rate': 9.99314767377287e-05, 'epoch': 0.4}
>>> 2025-08-17 10:35:56,024 - INFO - >>> {'loss': 3.791, 'grad_norm': 1.3859643936157227, 'learning_rate': 9.965342284774632e-05, 'epoch': 0.8}
>>> 2025-08-17 10:36:05,150 - INFO - >>> {'loss': 3.076, 'grad_norm': 2.666529655456543, 'learning_rate': 9.916274537819775e-05, 'epoch': 1.16}
>>> 2025-08-17 10:36:17,242 - INFO - >>> {'loss': 2.7519, 'grad_norm': 1.4053139686584473, 'learning_rate': 9.846154548533773e-05, 'epoch': 1.56}
>>> 2025-08-17 10:36:28,735 - INFO - >>> {'loss': 2.5726, 'grad_norm': 1.5842623710632324, 'learning_rate': 9.755282581475769e-05, 'epoch': 1.96}
>>> 2025-08-17 10:36:40,598 - INFO - >>> {'loss': 2.5449, 'grad_norm': 1.4177361726760864, 'learning_rate': 9.644047764359622e-05, 'epoch': 2.32}
>>> 2025-08-17 10:36:54,027 - INFO - >>> {'loss': 2.2919, 'grad_norm': 1.8708523511886597, 'learning_rate': 9.512926421749304e-05, 'epoch': 2.7199999999999998}
>>> 2025-08-17 10:37:07,275 - INFO - >>> {'loss': 2.027, 'grad_norm': 1.7385317087173462, 'learning_rate': 9.362480035363986e-05, 'epoch': 3.08}
>>> 2025-08-17 10:37:20,961 - INFO - >>> {'loss': 2.1722, 'grad_norm': 2.201190233230591, 'learning_rate': 9.193352839727121e-05, 'epoch': 3.48}
>>> 2025-08-17 10:37:32,603 - INFO - >>> {'loss': 1.589, 'grad_norm': 3.2426958084106445, 'learning_rate': 9.006269063455304e-05, 'epoch': 3.88}
>>> 2025-08-17 10:37:41,779 - INFO - >>> {'loss': 1.7189, 'grad_norm': 2.098285436630249, 'learning_rate': 8.802029828000156e-05, 'epoch': 4.24}
>>> 2025-08-17 10:37:55,646 - INFO - >>> {'loss': 1.5384, 'grad_norm': 3.510235071182251, 'learning_rate': 8.581509717123273e-05, 'epoch': 4.64}
>>> 2025-08-17 10:38:05,240 - INFO - >>> {'loss': 1.5435, 'grad_norm': 0.0, 'learning_rate': 8.345653031794292e-05, 'epoch': 5.0}
>>> 2025-08-17 10:38:17,276 - INFO - >>> {'loss': 1.192, 'grad_norm': 3.0777924060821533, 'learning_rate': 8.095469746549172e-05, 'epoch': 5.4}
>>> 2025-08-17 10:38:27,191 - INFO - >>> {'loss': 1.0794, 'grad_norm': 2.4594388008117676, 'learning_rate': 7.832031184624164e-05, 'epoch': 5.8}
>>> 2025-08-17 10:38:38,493 - INFO - >>> {'loss': 1.0162, 'grad_norm': 2.8182291984558105, 'learning_rate': 7.55646543038526e-05, 'epoch': 6.16}
>>> 2025-08-17 10:38:48,447 - INFO - >>> {'loss': 0.676, 'grad_norm': 2.8934922218322754, 'learning_rate': 7.269952498697734e-05, 'epoch': 6.5600000000000005}
>>> 2025-08-17 10:39:00,369 - INFO - >>> {'loss': 0.7846, 'grad_norm': 3.3559560775756836, 'learning_rate': 6.973719281921335e-05, 'epoch': 6.96}
>>> 2025-08-17 10:39:09,603 - INFO - >>> {'loss': 0.5525, 'grad_norm': 3.6404635906219482, 'learning_rate': 6.669034296168855e-05, 'epoch': 7.32}
>>> 2025-08-17 10:39:20,181 - INFO - >>> {'loss': 0.5257, 'grad_norm': 2.0878188610076904, 'learning_rate': 6.357202249325371e-05, 'epoch': 7.72}
>>> 2025-08-17 10:39:32,492 - INFO - >>> {'loss': 0.3458, 'grad_norm': 2.4022514820098877, 'learning_rate': 6.0395584540887963e-05, 'epoch': 8.08}
>>> 2025-08-17 10:39:44,515 - INFO - >>> {'loss': 0.2044, 'grad_norm': 2.1720547676086426, 'learning_rate': 5.717463109955896e-05, 'epoch': 8.48}
>>> 2025-08-17 10:39:56,749 - INFO - >>> {'loss': 0.1671, 'grad_norm': 2.999216318130493, 'learning_rate': 5.392295478639225e-05, 'epoch': 8.88}
>>> 2025-08-17 10:40:04,620 - INFO - >>> {'loss': 0.0774, 'grad_norm': 0.7733047604560852, 'learning_rate': 5.0654479778567223e-05, 'epoch': 9.24}
>>> 2025-08-17 10:40:17,045 - INFO - >>> {'loss': 0.0715, 'grad_norm': 1.7389665842056274, 'learning_rate': 4.738320218785281e-05, 'epoch': 9.64}
>>> 2025-08-17 10:40:27,072 - INFO - >>> {'loss': 0.0986, 'grad_norm': 2.620321750640869, 'learning_rate': 4.412313012710813e-05, 'epoch': 10.0}
>>> 2025-08-17 10:40:39,587 - INFO - >>> {'loss': 0.0262, 'grad_norm': 0.8122404217720032, 'learning_rate': 4.088822372539263e-05, 'epoch': 10.4}
>>> 2025-08-17 10:40:48,475 - INFO - >>> {'loss': 0.0196, 'grad_norm': 1.4553475379943848, 'learning_rate': 3.769233534855035e-05, 'epoch': 10.8}
>>> 2025-08-17 10:40:59,455 - INFO - >>> {'loss': 0.037, 'grad_norm': 0.4439069330692291, 'learning_rate': 3.4549150281252636e-05, 'epoch': 11.16}
>>> 2025-08-17 10:41:12,589 - INFO - >>> {'loss': 0.0121, 'grad_norm': 0.48753395676612854, 'learning_rate': 3.147212812450819e-05, 'epoch': 11.56}
>>> 2025-08-17 10:41:24,864 - INFO - >>> {'loss': 0.0232, 'grad_norm': 0.6335101127624512, 'learning_rate': 2.8474445159585235e-05, 'epoch': 11.96}
>>> 2025-08-17 10:41:35,979 - INFO - >>> {'loss': 0.0089, 'grad_norm': 0.3817635476589203, 'learning_rate': 2.556893792515227e-05, 'epoch': 12.32}
>>> 2025-08-17 10:41:47,951 - INFO - >>> {'loss': 0.014, 'grad_norm': 0.6407415866851807, 'learning_rate': 2.2768048249248648e-05, 'epoch': 12.72}
>>> 2025-08-17 10:41:58,048 - INFO - >>> {'loss': 0.0118, 'grad_norm': 0.6664864420890808, 'learning_rate': 2.008376997146705e-05, 'epoch': 13.08}
>>> 2025-08-17 10:42:08,586 - INFO - >>> {'loss': 0.0095, 'grad_norm': 0.6174389719963074, 'learning_rate': 1.7527597583490822e-05, 'epoch': 13.48}
>>> 2025-08-17 10:42:19,124 - INFO - >>> {'loss': 0.0127, 'grad_norm': 2.5650634765625, 'learning_rate': 1.5110477007916001e-05, 'epoch': 13.88}
>>> 2025-08-17 10:42:29,739 - INFO - >>> {'loss': 0.0079, 'grad_norm': 0.717056393623352, 'learning_rate': 1.2842758726130283e-05, 'epoch': 14.24}
>>> 2025-08-17 10:42:40,906 - INFO - >>> {'loss': 0.0068, 'grad_norm': 0.5853997468948364, 'learning_rate': 1.0734153455962765e-05, 'epoch': 14.64}
>>> 2025-08-17 10:42:50,856 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.578515350818634, 'learning_rate': 8.793690568899216e-06, 'epoch': 15.0}
>>> 2025-08-17 10:43:00,883 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.37233641743659973, 'learning_rate': 7.029679424927365e-06, 'epoch': 15.4}
>>> 2025-08-17 10:43:12,820 - INFO - >>> {'loss': 0.0075, 'grad_norm': 0.3451235592365265, 'learning_rate': 5.449673790581611e-06, 'epoch': 15.8}
>>> 2025-08-17 10:43:25,111 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.4977540075778961, 'learning_rate': 4.06043949255509e-06, 'epoch': 16.16}
>>> 2025-08-17 10:43:34,587 - INFO - >>> {'loss': 0.0077, 'grad_norm': 0.5045515894889832, 'learning_rate': 2.8679254453910785e-06, 'epoch': 16.56}
>>> 2025-08-17 10:43:45,497 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.3286968171596527, 'learning_rate': 1.8772381773176417e-06, 'epoch': 16.96}
>>> 2025-08-17 10:43:54,554 - INFO - >>> {'loss': 0.0065, 'grad_norm': 0.4072408974170685, 'learning_rate': 1.0926199633097157e-06, 'epoch': 17.32}
>>> 2025-08-17 10:44:05,669 - INFO - >>> {'loss': 0.0078, 'grad_norm': 0.6165383458137512, 'learning_rate': 5.174306590164879e-07, 'epoch': 17.72}
>>> 2025-08-17 10:44:14,102 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.6236782073974609, 'learning_rate': 1.5413331334360182e-07, 'epoch': 18.08}
>>> 2025-08-17 10:44:26,304 - INFO - >>> {'loss': 0.0061, 'grad_norm': 0.3075663447380066, 'learning_rate': 4.2836212996499865e-09, 'epoch': 18.48}
>>> 2025-08-17 10:44:26,825 - INFO - >>> {'train_runtime': 530.8602, 'train_samples_per_second': 3.767, 'train_steps_per_second': 0.452, 'train_loss': 0.8072815732176726, 'epoch': 18.48}
>>> 2025-08-17 10:44:26,826 - INFO - 训练成功！
>>> 2025-08-17 10:44:28,993 - INFO - 模型存放位置：./output/DeepSeek202508171035
>>> 2025-08-17 10:45:35,235 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 10:45:39,597 - INFO - 开始进行模型测试
>>> 2025-08-17 10:45:51,838 - INFO - 已选择模型文件夹: DeepSeek202508171035
>>> 2025-08-17 10:45:55,090 - INFO - 最新的 LoRA checkpoint 路径:output/DeepSeek202508171035/checkpoint-240
>>> 2025-08-17 10:58:35,684 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 10:58:39,926 - INFO - 开始进行训练
>>> 2025-08-17 10:58:44,215 - INFO - 导入包完成
>>> 2025-08-17 10:58:46,262 - INFO - 配置文件读取完成
>>> 2025-08-17 10:58:48,369 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 10:58:51,410 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-14b
>>> 2025-08-17 10:58:55,074 - INFO - tokenizer读取完成
>>> 2025-08-17 10:58:57,585 - ERROR - 模型导入失败：FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100), actual = `7.0`
>>> 2025-08-17 11:02:14,556 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 11:02:18,798 - INFO - 开始进行模型测试
>>> 2025-08-17 11:02:50,678 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 11:02:54,920 - INFO - 开始进行训练
>>> 2025-08-17 11:02:59,207 - INFO - 导入包完成
>>> 2025-08-17 11:03:01,255 - INFO - 配置文件读取完成
>>> 2025-08-17 11:03:03,361 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 11:03:06,400 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2.5-14B-Instruct
>>> 2025-08-17 11:03:10,308 - INFO - tokenizer读取完成
>>> 2025-08-17 11:05:40,344 - INFO - model dtype:torch.float16
>>> 2025-08-17 11:05:42,963 - INFO - 模型导入完成
>>> 2025-08-17 11:05:45,763 - INFO - 读取数据集成功
>>> 2025-08-17 11:05:51,969 - INFO - 数据处理成功
>>> 2025-08-17 11:06:14,391 - INFO - 开始训练！
>>> 2025-08-17 11:06:16,438 - INFO - 批次大小  : 4
>>> 2025-08-17 11:06:18,604 - INFO - 训练轮数  : 20
>>> 2025-08-17 11:06:20,801 - INFO - 学习率    : 0.0001
>>> 2025-08-17 11:06:23,148 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 11:06:26,247 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2.5-14B-Instruct
>>> 2025-08-17 11:06:33,836 - INFO - >>> {'loss': 2.9513, 'grad_norm': 3.123476028442383, 'learning_rate': 0.0001, 'epoch': 0.08}
>>> 2025-08-17 11:06:36,321 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 788.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 780.19 MiB is free. Including non-PyTorch memory, this process has 30.97 GiB memory in use. Of the allocated memory 30.37 GiB is allocated by PyTorch, and 230.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-08-17 11:09:27,173 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 11:09:31,415 - INFO - 开始进行训练
>>> 2025-08-17 11:09:37,367 - INFO - 导入包完成
>>> 2025-08-17 11:09:39,417 - INFO - 配置文件读取完成
>>> 2025-08-17 11:09:41,522 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 11:09:44,562 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2.5-14B-Instruct
>>> 2025-08-17 11:09:48,443 - INFO - tokenizer读取完成
>>> 2025-08-17 11:10:05,651 - INFO - model dtype:torch.float16
>>> 2025-08-17 11:10:08,269 - INFO - 模型导入完成
>>> 2025-08-17 11:10:11,106 - INFO - 读取数据集成功
>>> 2025-08-17 11:10:17,298 - INFO - 数据处理成功
>>> 2025-08-17 11:10:43,953 - INFO - 开始训练！
>>> 2025-08-17 11:10:45,999 - INFO - 批次大小  : 4
>>> 2025-08-17 11:10:48,165 - INFO - 训练轮数  : 20
>>> 2025-08-17 11:10:50,362 - INFO - 学习率    : 0.0001
>>> 2025-08-17 11:10:52,708 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 11:10:55,807 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2.5-14B-Instruct
>>> 2025-08-17 11:11:07,322 - INFO - >>> {'loss': 3.2679, 'grad_norm': 2.452756404876709, 'learning_rate': 0.0001, 'epoch': 0.16}
>>> 2025-08-17 11:11:11,763 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 830.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 722.19 MiB is free. Including non-PyTorch memory, this process has 31.02 GiB memory in use. Of the allocated memory 30.09 GiB is allocated by PyTorch, and 569.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-08-17 11:13:06,830 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 11:13:11,072 - INFO - 开始进行训练
>>> 2025-08-17 11:13:15,336 - INFO - 导入包完成
>>> 2025-08-17 11:13:17,383 - INFO - 配置文件读取完成
>>> 2025-08-17 11:13:19,490 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 11:13:22,529 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 11:13:26,046 - INFO - tokenizer读取完成
>>> 2025-08-17 11:15:02,201 - INFO - model dtype:torch.float16
>>> 2025-08-17 11:15:04,819 - INFO - 模型导入完成
>>> 2025-08-17 11:15:07,654 - INFO - 读取数据集成功
>>> 2025-08-17 11:15:13,782 - INFO - 数据处理成功
>>> 2025-08-17 11:15:29,124 - INFO - 开始训练！
>>> 2025-08-17 11:15:31,170 - INFO - 批次大小  : 4
>>> 2025-08-17 11:15:33,337 - INFO - 训练轮数  : 20
>>> 2025-08-17 11:15:35,534 - INFO - 学习率    : 0.0001
>>> 2025-08-17 11:15:37,881 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 11:15:40,981 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 11:15:48,603 - INFO - >>> {'loss': 2.7863, 'grad_norm': 1.015097975730896, 'learning_rate': 0.0001, 'epoch': 0.16}
>>> 2025-08-17 11:16:05,036 - INFO - >>> {'loss': 2.4808, 'grad_norm': 0.9193146228790283, 'learning_rate': 9.972609476841367e-05, 'epoch': 0.8}
>>> 2025-08-17 11:16:24,456 - INFO - >>> {'loss': 2.1814, 'grad_norm': 0.6623067855834961, 'learning_rate': 9.861849601988383e-05, 'epoch': 1.48}
>>> 2025-08-17 11:16:44,300 - INFO - >>> {'loss': 1.9734, 'grad_norm': 0.7996106743812561, 'learning_rate': 9.667902132486009e-05, 'epoch': 2.16}
>>> 2025-08-17 11:17:08,241 - INFO - >>> {'loss': 1.7759, 'grad_norm': 1.227807879447937, 'learning_rate': 9.394085563309827e-05, 'epoch': 2.96}
>>> 2025-08-17 11:17:32,189 - INFO - >>> {'loss': 1.6904, 'grad_norm': 1.161568284034729, 'learning_rate': 9.045084971874738e-05, 'epoch': 3.64}
>>> 2025-08-17 11:17:48,152 - INFO - >>> {'loss': 1.4271, 'grad_norm': 1.9831335544586182, 'learning_rate': 8.626871855061438e-05, 'epoch': 4.32}
>>> 2025-08-17 11:18:06,142 - INFO - >>> {'loss': 1.3286, 'grad_norm': 0.0, 'learning_rate': 8.146601955249188e-05, 'epoch': 5.0}
>>> 2025-08-17 11:18:26,440 - INFO - >>> {'loss': 1.1546, 'grad_norm': 2.1271982192993164, 'learning_rate': 7.612492823579745e-05, 'epoch': 5.8}
>>> 2025-08-17 11:18:43,488 - INFO - >>> {'loss': 0.9996, 'grad_norm': 4.529637813568115, 'learning_rate': 7.033683215379002e-05, 'epoch': 6.48}
>>> 2025-08-17 11:19:01,730 - INFO - >>> {'loss': 0.908, 'grad_norm': 2.806048631668091, 'learning_rate': 6.420076723519614e-05, 'epoch': 7.16}
>>> 2025-08-17 11:19:22,643 - INFO - >>> {'loss': 0.7948, 'grad_norm': 2.7288081645965576, 'learning_rate': 5.782172325201155e-05, 'epoch': 7.96}
>>> 2025-08-17 11:19:42,042 - INFO - >>> {'loss': 0.5652, 'grad_norm': 3.690415143966675, 'learning_rate': 5.1308847415393666e-05, 'epoch': 8.64}
>>> 2025-08-17 11:19:58,409 - INFO - >>> {'loss': 0.4398, 'grad_norm': 2.947415351867676, 'learning_rate': 4.477357683661734e-05, 'epoch': 9.32}
>>> 2025-08-17 11:20:17,318 - INFO - >>> {'loss': 0.4094, 'grad_norm': 4.265871524810791, 'learning_rate': 3.832773180720475e-05, 'epoch': 10.0}
>>> 2025-08-17 11:20:37,226 - INFO - >>> {'loss': 0.2616, 'grad_norm': 3.2889950275421143, 'learning_rate': 3.2081602522734986e-05, 'epoch': 10.8}
>>> 2025-08-17 11:20:57,404 - INFO - >>> {'loss': 0.2502, 'grad_norm': 3.36405873298645, 'learning_rate': 2.6142061987019577e-05, 'epoch': 11.48}
>>> 2025-08-17 11:21:16,184 - INFO - >>> {'loss': 0.174, 'grad_norm': 3.382486581802368, 'learning_rate': 2.061073738537635e-05, 'epoch': 12.16}
>>> 2025-08-17 11:21:37,839 - INFO - >>> {'loss': 0.1346, 'grad_norm': 3.4515035152435303, 'learning_rate': 1.5582271215312294e-05, 'epoch': 12.96}
>>> 2025-08-17 11:21:53,854 - INFO - >>> {'loss': 0.1304, 'grad_norm': 3.5875556468963623, 'learning_rate': 1.1142701927151456e-05, 'epoch': 13.64}
>>> 2025-08-17 11:22:13,057 - INFO - >>> {'loss': 0.1618, 'grad_norm': 5.078430652618408, 'learning_rate': 7.367991782295391e-06, 'epoch': 14.32}
>>> 2025-08-17 11:22:32,043 - INFO - >>> {'loss': 0.1208, 'grad_norm': 6.18014669418335, 'learning_rate': 4.322727117869951e-06, 'epoch': 15.0}
>>> 2025-08-17 11:22:52,665 - INFO - >>> {'loss': 0.1081, 'grad_norm': 4.762761116027832, 'learning_rate': 2.0590132565903476e-06, 'epoch': 15.8}
>>> 2025-08-17 11:23:11,515 - INFO - >>> {'loss': 0.1142, 'grad_norm': 4.100128650665283, 'learning_rate': 6.15582970243117e-07, 'epoch': 16.48}
>>> 2025-08-17 11:23:27,943 - INFO - >>> {'loss': 0.1126, 'grad_norm': 3.707332134246826, 'learning_rate': 1.7133751222137007e-08, 'epoch': 17.16}
>>> 2025-08-17 11:23:28,867 - INFO - >>> {'train_runtime': 464.4559, 'train_samples_per_second': 4.306, 'train_steps_per_second': 0.258, 'train_loss': 0.823268016676108, 'epoch': 17.16}
>>> 2025-08-17 11:23:28,869 - INFO - 训练成功！
>>> 2025-08-17 11:23:30,914 - INFO - 模型存放位置：./output/qwen202508171115
>>> 2025-08-17 11:24:07,657 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 11:24:11,899 - INFO - 开始进行模型测试
>>> 2025-08-17 11:24:18,524 - INFO - 已选择模型文件夹: qwen202508171115
>>> 2025-08-17 11:24:21,535 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508171115/checkpoint-120
>>> 2025-08-17 17:32:17,809 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 17:32:22,051 - INFO - 开始进行训练
>>> 2025-08-17 17:32:26,368 - INFO - 导入包完成
>>> 2025-08-17 17:32:28,416 - INFO - 配置文件读取完成
>>> 2025-08-17 17:32:30,521 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 17:32:33,561 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 17:32:36,878 - INFO - tokenizer读取完成
>>> 2025-08-17 17:32:41,028 - INFO - model dtype:torch.float16
>>> 2025-08-17 17:32:43,645 - INFO - 模型导入完成
>>> 2025-08-17 17:32:46,490 - INFO - 读取数据集成功
>>> 2025-08-17 17:32:52,712 - INFO - 数据处理成功
>>> 2025-08-17 17:33:07,801 - INFO - 开始训练！
>>> 2025-08-17 17:33:09,848 - INFO - 批次大小  : 4
>>> 2025-08-17 17:33:12,014 - INFO - 训练轮数  : 40
>>> 2025-08-17 17:33:14,211 - INFO - 学习率    : 0.0001
>>> 2025-08-17 17:33:16,558 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 17:33:19,657 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 17:33:27,369 - INFO - >>> {'loss': 2.7863, 'grad_norm': 1.0297833681106567, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-17 17:33:43,862 - INFO - >>> {'loss': 2.6032, 'grad_norm': 1.007665991783142, 'learning_rate': 9.996112860009688e-05, 'epoch': 0.8}
>>> 2025-08-17 17:34:03,297 - INFO - >>> {'loss': 2.2361, 'grad_norm': 0.7062309384346008, 'learning_rate': 9.972379999624936e-05, 'epoch': 1.48}
>>> 2025-08-17 17:34:23,132 - INFO - >>> {'loss': 2.0122, 'grad_norm': 0.855418860912323, 'learning_rate': 9.927176156353899e-05, 'epoch': 2.16}
>>> 2025-08-17 17:34:45,265 - INFO - >>> {'loss': 1.8093, 'grad_norm': 1.1970126628875732, 'learning_rate': 9.860696522628639e-05, 'epoch': 2.96}
>>> 2025-08-17 17:35:11,130 - INFO - >>> {'loss': 1.7271, 'grad_norm': 1.1044740676879883, 'learning_rate': 9.773228160797188e-05, 'epoch': 3.64}
>>> 2025-08-17 17:35:30,988 - INFO - >>> {'loss': 1.4416, 'grad_norm': 2.0451571941375732, 'learning_rate': 9.665148763574123e-05, 'epoch': 4.32}
>>> 2025-08-17 17:35:53,208 - INFO - >>> {'loss': 1.3632, 'grad_norm': 0.0, 'learning_rate': 9.536925023144742e-05, 'epoch': 5.0}
>>> 2025-08-17 17:36:16,678 - INFO - >>> {'loss': 1.1481, 'grad_norm': 1.9663816690444946, 'learning_rate': 9.389110615965102e-05, 'epoch': 5.8}
>>> 2025-08-17 17:36:36,478 - INFO - >>> {'loss': 0.9855, 'grad_norm': 4.434081077575684, 'learning_rate': 9.222343811959693e-05, 'epoch': 6.48}
>>> 2025-08-17 17:36:57,678 - INFO - >>> {'loss': 0.8584, 'grad_norm': 2.818603515625, 'learning_rate': 9.037344718440322e-05, 'epoch': 7.16}
>>> 2025-08-17 17:37:21,501 - INFO - >>> {'loss': 0.7118, 'grad_norm': 3.204150438308716, 'learning_rate': 8.834912170647101e-05, 'epoch': 7.96}
>>> 2025-08-17 17:37:42,988 - INFO - >>> {'loss': 0.4359, 'grad_norm': 3.5836267471313477, 'learning_rate': 8.615920282338355e-05, 'epoch': 8.64}
>>> 2025-08-17 17:38:01,351 - INFO - >>> {'loss': 0.3304, 'grad_norm': 3.0895211696624756, 'learning_rate': 8.381314671324159e-05, 'epoch': 9.32}
>>> 2025-08-17 17:38:22,576 - INFO - >>> {'loss': 0.2429, 'grad_norm': 8.262640953063965, 'learning_rate': 8.132108376241849e-05, 'epoch': 10.0}
>>> 2025-08-17 17:38:45,026 - INFO - >>> {'loss': 0.1474, 'grad_norm': 4.9767279624938965, 'learning_rate': 7.869377482205042e-05, 'epoch': 10.8}
>>> 2025-08-17 17:39:07,686 - INFO - >>> {'loss': 0.1373, 'grad_norm': 4.9953436851501465, 'learning_rate': 7.594256474214882e-05, 'epoch': 11.48}
>>> 2025-08-17 17:39:28,659 - INFO - >>> {'loss': 0.1547, 'grad_norm': 6.998010158538818, 'learning_rate': 7.307933338397667e-05, 'epoch': 12.16}
>>> 2025-08-17 17:39:52,970 - INFO - >>> {'loss': 0.1148, 'grad_norm': 4.62699031829834, 'learning_rate': 7.011644432221958e-05, 'epoch': 12.96}
>>> 2025-08-17 17:40:10,973 - INFO - >>> {'loss': 0.0812, 'grad_norm': 3.5343194007873535, 'learning_rate': 6.706669145845863e-05, 'epoch': 13.64}
>>> 2025-08-17 17:40:32,127 - INFO - >>> {'loss': 0.0683, 'grad_norm': 3.118391752243042, 'learning_rate': 6.394324377647028e-05, 'epoch': 14.32}
>>> 2025-08-17 17:40:52,714 - INFO - >>> {'loss': 0.0548, 'grad_norm': 6.904941558837891, 'learning_rate': 6.075958847790262e-05, 'epoch': 15.0}
>>> 2025-08-17 17:41:15,038 - INFO - >>> {'loss': 0.0261, 'grad_norm': 2.253222703933716, 'learning_rate': 5.752947274387147e-05, 'epoch': 15.8}
>>> 2025-08-17 17:41:35,176 - INFO - >>> {'loss': 0.0197, 'grad_norm': 1.5420961380004883, 'learning_rate': 5.426684437395196e-05, 'epoch': 16.48}
>>> 2025-08-17 17:41:52,705 - INFO - >>> {'loss': 0.0317, 'grad_norm': 3.0907204151153564, 'learning_rate': 5.0985791558889785e-05, 'epoch': 17.16}
>>> 2025-08-17 17:42:15,659 - INFO - >>> {'loss': 0.018, 'grad_norm': 1.6252354383468628, 'learning_rate': 4.770048204709648e-05, 'epoch': 17.96}
>>> 2025-08-17 17:42:34,742 - INFO - >>> {'loss': 0.01, 'grad_norm': 1.2538872957229614, 'learning_rate': 4.4425101967610674e-05, 'epoch': 18.64}
>>> 2025-08-17 17:42:56,398 - INFO - >>> {'loss': 0.016, 'grad_norm': 1.392032504081726, 'learning_rate': 4.1173794573690996e-05, 'epoch': 19.32}
>>> 2025-08-17 17:43:14,572 - INFO - >>> {'loss': 0.0125, 'grad_norm': 1.11179518699646, 'learning_rate': 3.7960599171548574e-05, 'epoch': 20.0}
>>> 2025-08-17 17:43:37,662 - INFO - >>> {'loss': 0.0122, 'grad_norm': 2.1364760398864746, 'learning_rate': 3.479939049792817e-05, 'epoch': 20.8}
>>> 2025-08-17 17:43:56,085 - INFO - >>> {'loss': 0.0083, 'grad_norm': 0.9926274418830872, 'learning_rate': 3.1703818808308324e-05, 'epoch': 21.48}
>>> 2025-08-17 17:44:17,251 - INFO - >>> {'loss': 0.0074, 'grad_norm': 0.9881607890129089, 'learning_rate': 2.8687250934422772e-05, 'epoch': 22.16}
>>> 2025-08-17 17:44:37,564 - INFO - >>> {'loss': 0.0079, 'grad_norm': 1.23251211643219, 'learning_rate': 2.5762712565619528e-05, 'epoch': 22.96}
>>> 2025-08-17 17:44:58,518 - INFO - >>> {'loss': 0.0061, 'grad_norm': 0.9311179518699646, 'learning_rate': 2.2942832003289823e-05, 'epoch': 23.64}
>>> 2025-08-17 17:45:17,342 - INFO - >>> {'loss': 0.0076, 'grad_norm': 0.9657589197158813, 'learning_rate': 2.0239785631237705e-05, 'epoch': 24.32}
>>> 2025-08-17 17:45:37,457 - INFO - >>> {'loss': 0.007, 'grad_norm': 0.0, 'learning_rate': 1.7665245337452368e-05, 'epoch': 25.0}
>>> 2025-08-17 17:45:59,530 - INFO - >>> {'loss': 0.0059, 'grad_norm': 1.1263786554336548, 'learning_rate': 1.5230328114318127e-05, 'epoch': 25.8}
>>> 2025-08-17 17:46:16,172 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.9226062297821045, 'learning_rate': 1.2945548054891321e-05, 'epoch': 26.48}
>>> 2025-08-17 17:46:37,122 - INFO - >>> {'loss': 0.0071, 'grad_norm': 1.0169086456298828, 'learning_rate': 1.0820770952526155e-05, 'epoch': 27.16}
>>> 2025-08-17 17:47:00,710 - INFO - >>> {'loss': 0.0071, 'grad_norm': 0.6976758241653442, 'learning_rate': 8.865171699890834e-06, 'epoch': 27.96}
>>> 2025-08-17 17:47:20,773 - INFO - >>> {'loss': 0.0059, 'grad_norm': 1.1627002954483032, 'learning_rate': 7.0871946713269856e-06, 'epoch': 28.64}
>>> 2025-08-17 17:47:42,419 - INFO - >>> {'loss': 0.0066, 'grad_norm': 1.3052295446395874, 'learning_rate': 5.494517259623477e-06, 'epoch': 29.32}
>>> 2025-08-17 17:48:00,382 - INFO - >>> {'loss': 0.006, 'grad_norm': 1.1731914281845093, 'learning_rate': 4.094016724654359e-06, 'epoch': 30.0}
>>> 2025-08-17 17:48:24,360 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.8093262910842896, 'learning_rate': 2.8917404970305097e-06, 'epoch': 30.8}
>>> 2025-08-17 17:48:43,746 - INFO - >>> {'loss': 0.0047, 'grad_norm': 1.0727428197860718, 'learning_rate': 1.892880064994934e-06, 'epoch': 31.48}
>>> 2025-08-17 17:49:00,572 - INFO - >>> {'loss': 0.0065, 'grad_norm': 1.2016245126724243, 'learning_rate': 1.101748557319715e-06, 'epoch': 32.16}
>>> 2025-08-17 17:49:23,950 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.9226978421211243, 'learning_rate': 5.217621190024779e-07, 'epoch': 32.96}
>>> 2025-08-17 17:49:41,459 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.9359661936759949, 'learning_rate': 1.554251601833201e-07, 'epoch': 33.64}
>>> 2025-08-17 17:50:02,842 - INFO - >>> {'loss': 0.0055, 'grad_norm': 1.1070713996887207, 'learning_rate': 4.319541977831909e-09, 'epoch': 34.32}
>>> 2025-08-17 17:50:03,681 - INFO - >>> {'train_runtime': 1000.5849, 'train_samples_per_second': 3.998, 'train_steps_per_second': 0.24, 'train_loss': 0.3951395073517536, 'epoch': 34.32}
>>> 2025-08-17 17:50:03,682 - INFO - 训练成功！
>>> 2025-08-17 17:50:05,728 - INFO - 模型存放位置：./output/qwen202508171732
>>> 2025-08-17 19:31:12,667 - INFO - 当前环境：/home/liangshuqiao/anaconda3/bin/python
>>> 2025-08-17 19:31:15,826 - INFO - 开始进行训练
>>> 2025-08-17 19:35:01,581 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 19:35:05,823 - INFO - 开始进行训练
>>> 2025-08-17 19:35:10,081 - INFO - 导入包完成
>>> 2025-08-17 19:35:12,129 - INFO - 配置文件读取完成
>>> 2025-08-17 19:35:14,236 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 19:35:17,275 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 19:35:20,581 - INFO - tokenizer读取完成
>>> 2025-08-17 19:35:24,675 - INFO - model dtype:torch.float16
>>> 2025-08-17 19:35:27,294 - INFO - 模型导入完成
>>> 2025-08-17 19:35:30,139 - INFO - 读取数据集成功
>>> 2025-08-17 19:35:36,413 - INFO - 数据处理成功
>>> 2025-08-17 19:35:51,976 - INFO - 开始训练！
>>> 2025-08-17 19:35:54,022 - INFO - 批次大小  : 4
>>> 2025-08-17 19:35:56,189 - INFO - 训练轮数  : 15
>>> 2025-08-17 19:35:58,387 - INFO - 学习率    : 0.0001
>>> 2025-08-17 19:36:00,734 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 19:36:03,833 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 19:36:11,575 - INFO - >>> {'loss': 2.7863, 'grad_norm': 1.0117459297180176, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-17 19:36:28,078 - INFO - >>> {'loss': 2.6094, 'grad_norm': 1.0005830526351929, 'learning_rate': 9.97199108003991e-05, 'epoch': 0.8}
>>> 2025-08-17 19:36:47,509 - INFO - >>> {'loss': 2.2424, 'grad_norm': 0.6899375915527344, 'learning_rate': 9.801960531718896e-05, 'epoch': 1.48}
>>> 2025-08-17 19:37:07,343 - INFO - >>> {'loss': 2.0214, 'grad_norm': 0.865118682384491, 'learning_rate': 9.482736218434143e-05, 'epoch': 2.16}
>>> 2025-08-17 19:37:30,029 - INFO - >>> {'loss': 1.8197, 'grad_norm': 1.1208182573318481, 'learning_rate': 9.024236230276629e-05, 'epoch': 2.96}
>>> 2025-08-17 19:37:57,012 - INFO - >>> {'loss': 1.7477, 'grad_norm': 1.0795193910598755, 'learning_rate': 8.440705861229344e-05, 'epoch': 3.64}
>>> 2025-08-17 19:38:17,921 - INFO - >>> {'loss': 1.4791, 'grad_norm': 1.6308071613311768, 'learning_rate': 7.750275017224207e-05, 'epoch': 4.32}
>>> 2025-08-17 19:38:40,822 - INFO - >>> {'loss': 1.4116, 'grad_norm': 0.0, 'learning_rate': 6.974394931852956e-05, 'epoch': 5.0}
>>> 2025-08-17 19:39:04,831 - INFO - >>> {'loss': 1.2849, 'grad_norm': 2.068526268005371, 'learning_rate': 6.137171690605533e-05, 'epoch': 5.8}
>>> 2025-08-17 19:39:23,869 - INFO - >>> {'loss': 1.1688, 'grad_norm': 3.6393239498138428, 'learning_rate': 5.2646172706008156e-05, 'epoch': 6.48}
>>> 2025-08-17 19:39:43,865 - INFO - >>> {'loss': 1.0738, 'grad_norm': 3.213078260421753, 'learning_rate': 4.383841365514208e-05, 'epoch': 7.16}
>>> 2025-08-17 19:40:06,562 - INFO - >>> {'loss': 1.0397, 'grad_norm': 2.7370388507843018, 'learning_rate': 3.52220910517158e-05, 'epoch': 7.96}
>>> 2025-08-17 19:40:27,332 - INFO - >>> {'loss': 0.8703, 'grad_norm': 2.4724602699279785, 'learning_rate': 2.7064908389095468e-05, 'epoch': 8.64}
>>> 2025-08-17 19:40:44,854 - INFO - >>> {'loss': 0.7392, 'grad_norm': 2.813042163848877, 'learning_rate': 1.962030398375506e-05, 'epoch': 9.32}
>>> 2025-08-17 19:41:05,039 - INFO - >>> {'loss': 0.7888, 'grad_norm': 3.85652232170105, 'learning_rate': 1.3119576812968892e-05, 'epoch': 10.0}
>>> 2025-08-17 19:41:26,465 - INFO - >>> {'loss': 0.6945, 'grad_norm': 3.477707862854004, 'learning_rate': 7.764700207255903e-06, 'epoch': 10.8}
>>> 2025-08-17 19:41:47,746 - INFO - >>> {'loss': 0.7131, 'grad_norm': 3.4374613761901855, 'learning_rate': 3.72204667143895e-06, 'epoch': 11.48}
>>> 2025-08-17 19:42:08,054 - INFO - >>> {'loss': 0.6819, 'grad_norm': 3.0345020294189453, 'learning_rate': 1.1172188000142802e-06, 'epoch': 12.16}
>>> 2025-08-17 19:42:31,119 - INFO - >>> {'loss': 0.637, 'grad_norm': 2.82218074798584, 'learning_rate': 3.1146886901090025e-08, 'epoch': 12.96}
>>> 2025-08-17 19:42:32,020 - INFO - >>> {'train_runtime': 384.7441, 'train_samples_per_second': 3.899, 'train_steps_per_second': 0.234, 'train_loss': 1.2810447560416327, 'epoch': 12.96}
>>> 2025-08-17 19:42:32,021 - INFO - 训练成功！
>>> 2025-08-17 19:42:34,067 - INFO - 模型存放位置：./output/qwen202508171935
>>> 2025-08-17 20:00:26,594 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 20:00:30,836 - INFO - 开始进行训练
>>> 2025-08-17 20:00:35,078 - INFO - 导入包完成
>>> 2025-08-17 20:00:37,127 - INFO - 配置文件读取完成
>>> 2025-08-17 20:00:39,233 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 20:00:42,271 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 20:00:45,588 - INFO - tokenizer读取完成
>>> 2025-08-17 20:00:49,643 - INFO - model dtype:torch.float16
>>> 2025-08-17 20:00:52,261 - INFO - 模型导入完成
>>> 2025-08-17 20:00:55,298 - INFO - 读取数据集成功
>>> 2025-08-17 20:01:01,042 - INFO - 数据处理成功
>>> 2025-08-17 20:01:11,473 - INFO - 开始训练！
>>> 2025-08-17 20:01:13,519 - INFO - 批次大小  : 4
>>> 2025-08-17 20:01:15,685 - INFO - 训练轮数  : 15
>>> 2025-08-17 20:01:17,882 - INFO - 学习率    : 0.0001
>>> 2025-08-17 20:01:20,229 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 20:01:23,328 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 20:01:30,922 - INFO - >>> {'loss': 2.7863, 'grad_norm': 1.0164543390274048, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-17 20:01:47,356 - INFO - >>> {'loss': 2.6038, 'grad_norm': 0.9784812331199646, 'learning_rate': 9.97199108003991e-05, 'epoch': 0.8}
>>> 2025-08-17 20:02:06,732 - INFO - >>> {'loss': 2.241, 'grad_norm': 0.6563040018081665, 'learning_rate': 9.801960531718896e-05, 'epoch': 1.48}
>>> 2025-08-17 20:02:26,555 - INFO - >>> {'loss': 2.0233, 'grad_norm': 0.820602297782898, 'learning_rate': 9.482736218434143e-05, 'epoch': 2.16}
>>> 2025-08-17 20:02:48,681 - INFO - >>> {'loss': 1.822, 'grad_norm': 1.1099566221237183, 'learning_rate': 9.024236230276629e-05, 'epoch': 2.96}
>>> 2025-08-17 20:03:15,264 - INFO - >>> {'loss': 1.7477, 'grad_norm': 1.0675932168960571, 'learning_rate': 8.440705861229344e-05, 'epoch': 3.64}
>>> 2025-08-17 20:03:35,332 - INFO - >>> {'loss': 1.4799, 'grad_norm': 1.7475078105926514, 'learning_rate': 7.750275017224207e-05, 'epoch': 4.32}
>>> 2025-08-17 20:07:23,989 - INFO - ========__main__  202508172007========
>>> 2025-08-17 20:07:26,967 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 20:07:31,209 - INFO - 开始进行训练
>>> 2025-08-17 20:07:35,509 - INFO - 导入包完成
>>> 2025-08-17 20:07:37,556 - INFO - 配置文件读取完成
>>> 2025-08-17 20:07:39,662 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 20:07:42,701 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 20:07:46,018 - INFO - tokenizer读取完成
>>> 2025-08-17 20:07:50,015 - INFO - model dtype:torch.float16
>>> 2025-08-17 20:07:52,634 - INFO - 模型导入完成
>>> 2025-08-17 20:07:55,740 - INFO - 读取数据集成功
>>> 2025-08-17 20:08:01,542 - INFO - 数据处理成功
>>> 2025-08-17 20:08:17,172 - INFO - 开始训练！
>>> 2025-08-17 20:08:19,218 - INFO - 批次大小  : 4
>>> 2025-08-17 20:08:21,386 - INFO - 训练轮数  : 15
>>> 2025-08-17 20:08:23,583 - INFO - 学习率    : 0.0001
>>> 2025-08-17 20:08:25,930 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 20:08:29,029 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 20:08:36,748 - INFO - >>> {'loss': 2.7863, 'grad_norm': 1.0209243297576904, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-17 20:08:53,242 - INFO - >>> {'loss': 2.6081, 'grad_norm': 1.0063488483428955, 'learning_rate': 9.97199108003991e-05, 'epoch': 0.8}
>>> 2025-08-17 20:09:12,693 - INFO - >>> {'loss': 2.2409, 'grad_norm': 0.695899486541748, 'learning_rate': 9.801960531718896e-05, 'epoch': 1.48}
>>> 2025-08-17 20:09:32,693 - INFO - >>> {'loss': 2.019, 'grad_norm': 0.8584234118461609, 'learning_rate': 9.482736218434143e-05, 'epoch': 2.16}
>>> 2025-08-17 20:09:58,186 - INFO - >>> {'loss': 1.8172, 'grad_norm': 1.120789885520935, 'learning_rate': 9.024236230276629e-05, 'epoch': 2.96}
>>> 2025-08-17 20:10:27,084 - INFO - >>> {'loss': 1.7432, 'grad_norm': 1.076008915901184, 'learning_rate': 8.440705861229344e-05, 'epoch': 3.64}
>>> 2025-08-17 20:10:47,774 - INFO - >>> {'loss': 1.4734, 'grad_norm': 1.842250943183899, 'learning_rate': 7.750275017224207e-05, 'epoch': 4.32}
>>> 2025-08-17 20:11:09,443 - INFO - >>> {'loss': 1.4016, 'grad_norm': 0.0, 'learning_rate': 6.974394931852956e-05, 'epoch': 5.0}
>>> 2025-08-17 20:11:31,718 - INFO - >>> {'loss': 1.2719, 'grad_norm': 2.347533941268921, 'learning_rate': 6.137171690605533e-05, 'epoch': 5.8}
>>> 2025-08-17 20:11:49,936 - INFO - >>> {'loss': 1.1532, 'grad_norm': 3.7989020347595215, 'learning_rate': 5.2646172706008156e-05, 'epoch': 6.48}
>>> 2025-08-17 20:12:09,398 - INFO - >>> {'loss': 1.0536, 'grad_norm': 3.1761789321899414, 'learning_rate': 4.383841365514208e-05, 'epoch': 7.16}
>>> 2025-08-17 20:12:31,671 - INFO - >>> {'loss': 1.0118, 'grad_norm': 2.4481639862060547, 'learning_rate': 3.52220910517158e-05, 'epoch': 7.96}
>>> 2025-08-17 20:12:51,999 - INFO - >>> {'loss': 0.851, 'grad_norm': 2.530978202819824, 'learning_rate': 2.7064908389095468e-05, 'epoch': 8.64}
>>> 2025-08-17 20:13:09,002 - INFO - >>> {'loss': 0.7189, 'grad_norm': 3.0225746631622314, 'learning_rate': 1.962030398375506e-05, 'epoch': 9.32}
>>> 2025-08-17 20:13:28,333 - INFO - >>> {'loss': 0.7655, 'grad_norm': 3.9357149600982666, 'learning_rate': 1.3119576812968892e-05, 'epoch': 10.0}
>>> 2025-08-17 20:13:49,006 - INFO - >>> {'loss': 0.6705, 'grad_norm': 3.4395463466644287, 'learning_rate': 7.764700207255903e-06, 'epoch': 10.8}
>>> 2025-08-17 20:14:09,569 - INFO - >>> {'loss': 0.6973, 'grad_norm': 3.356724977493286, 'learning_rate': 3.72204667143895e-06, 'epoch': 11.48}
>>> 2025-08-17 20:14:28,894 - INFO - >>> {'loss': 0.6574, 'grad_norm': 2.6476917266845703, 'learning_rate': 1.1172188000142802e-06, 'epoch': 12.16}
>>> 2025-08-17 20:14:51,169 - INFO - >>> {'loss': 0.6185, 'grad_norm': 2.7238802909851074, 'learning_rate': 3.1146886901090025e-08, 'epoch': 12.96}
>>> 2025-08-17 20:14:52,054 - INFO - >>> {'train_runtime': 379.5735, 'train_samples_per_second': 3.952, 'train_steps_per_second': 0.237, 'train_loss': 1.2671371777852376, 'epoch': 12.96}
>>> 2025-08-17 20:14:52,055 - INFO - 训练成功！
>>> 2025-08-17 20:14:54,101 - INFO - 模型存放位置：./output/qwen202508172008
>>> 2025-08-17 20:23:36,307 - INFO - ========__main__  202508172023========
>>> 2025-08-17 20:23:39,285 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-17 20:23:43,467 - INFO - 开始进行训练
>>> 2025-08-17 20:24:25,596 - INFO - 导入包完成
>>> 2025-08-17 20:24:27,645 - INFO - 配置文件读取完成
>>> 2025-08-17 20:24:29,751 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 20:24:32,789 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 20:24:36,540 - INFO - tokenizer读取完成
>>> 2025-08-17 20:24:44,016 - INFO - model dtype:torch.float16
>>> 2025-08-17 20:24:46,634 - INFO - 模型导入完成
>>> 2025-08-17 20:24:49,514 - INFO - 读取数据集成功
>>> 2025-08-17 20:24:56,336 - INFO - 数据处理成功
>>> 2025-08-17 20:25:13,189 - INFO - 开始训练！
>>> 2025-08-17 20:25:15,235 - INFO - 批次大小  : 4
>>> 2025-08-17 20:25:17,401 - INFO - 训练轮数  : 15
>>> 2025-08-17 20:25:19,598 - INFO - 学习率    : 0.0001
>>> 2025-08-17 20:25:21,944 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 20:25:25,044 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 20:25:32,715 - INFO - >>> {'loss': 2.7863, 'grad_norm': 1.01213538646698, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-17 20:54:16,095 - INFO - ========__main__  202508172054========
>>> 2025-08-17 20:54:19,073 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-17 20:54:23,255 - INFO - 开始进行训练
>>> 2025-08-17 20:54:27,748 - INFO - 导入包完成
>>> 2025-08-17 20:54:29,796 - INFO - 配置文件读取完成
>>> 2025-08-17 20:54:31,902 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 20:54:34,941 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 20:54:38,273 - INFO - tokenizer读取完成
>>> 2025-08-17 20:54:42,496 - INFO - model dtype:torch.float16
>>> 2025-08-17 20:54:45,114 - INFO - 模型导入完成
>>> 2025-08-17 20:54:49,852 - INFO - 读取数据集成功
>>> 2025-08-17 20:54:56,665 - INFO - 数据处理成功
>>> 2025-08-17 20:55:13,134 - INFO - 开始训练！
>>> 2025-08-17 20:55:15,180 - INFO - 批次大小  : 4
>>> 2025-08-17 20:55:17,348 - INFO - 训练轮数  : 100
>>> 2025-08-17 20:55:19,575 - INFO - 学习率    : 0.0001
>>> 2025-08-17 20:55:21,922 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 20:55:25,021 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 20:55:32,743 - INFO - >>> {'loss': 2.7863, 'grad_norm': 1.0173797607421875, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-17 20:55:36,698 - INFO - >>> {'loss': 2.6459, 'grad_norm': 1.290208101272583, 'learning_rate': 0.0001, 'epoch': 0.32}
>>> 2025-08-17 20:55:39,124 - INFO - >>> {'loss': 2.6612, 'grad_norm': 1.2508149147033691, 'learning_rate': 9.999931232202689e-05, 'epoch': 0.48}
>>> 2025-08-17 20:55:43,681 - INFO - >>> {'loss': 2.6739, 'grad_norm': 1.191503643989563, 'learning_rate': 9.999724930702356e-05, 'epoch': 0.64}
>>> 2025-08-17 20:55:49,234 - INFO - >>> {'loss': 2.4469, 'grad_norm': 1.0047636032104492, 'learning_rate': 9.999381101173764e-05, 'epoch': 0.8}
>>> 2025-08-17 20:55:53,832 - INFO - >>> {'loss': 2.3147, 'grad_norm': 1.0620521306991577, 'learning_rate': 9.998899753074669e-05, 'epoch': 0.96}
>>> 2025-08-17 20:55:55,013 - INFO - >>> {'loss': 2.487, 'grad_norm': 1.0654444694519043, 'learning_rate': 9.998280899645574e-05, 'epoch': 1.0}
>>> 2025-08-17 20:55:58,580 - INFO - >>> {'loss': 2.2683, 'grad_norm': 0.7157343029975891, 'learning_rate': 9.997524557909352e-05, 'epoch': 1.16}
>>> 2025-08-17 20:56:03,643 - INFO - >>> {'loss': 2.0667, 'grad_norm': 0.7022306323051453, 'learning_rate': 9.996630748670787e-05, 'epoch': 1.32}
>>> 2025-08-17 20:56:08,677 - INFO - >>> {'loss': 2.0492, 'grad_norm': 0.7264428734779358, 'learning_rate': 9.995599496515995e-05, 'epoch': 1.48}
>>> 2025-08-17 20:56:13,393 - INFO - >>> {'loss': 1.9002, 'grad_norm': 0.8291198015213013, 'learning_rate': 9.99443082981175e-05, 'epoch': 1.6400000000000001}
>>> 2025-08-17 20:56:18,397 - INFO - >>> {'loss': 2.1107, 'grad_norm': 0.7863041758537292, 'learning_rate': 9.993124780704707e-05, 'epoch': 1.8}
>>> 2025-08-17 20:56:22,322 - INFO - >>> {'loss': 1.9528, 'grad_norm': 1.0440808534622192, 'learning_rate': 9.991681385120515e-05, 'epoch': 1.96}
>>> 2025-08-17 20:56:23,505 - INFO - >>> {'loss': 2.1167, 'grad_norm': 1.5966235399246216, 'learning_rate': 9.990100682762828e-05, 'epoch': 2.0}
>>> 2025-08-17 20:56:28,524 - INFO - >>> {'loss': 1.9726, 'grad_norm': 0.8806989789009094, 'learning_rate': 9.988382717112213e-05, 'epoch': 2.16}
>>> 2025-08-17 20:56:32,521 - INFO - >>> {'loss': 1.8739, 'grad_norm': 1.026495337486267, 'learning_rate': 9.986527535424957e-05, 'epoch': 2.32}
>>> 2025-08-17 20:56:37,731 - INFO - >>> {'loss': 1.8457, 'grad_norm': 1.020254135131836, 'learning_rate': 9.984535188731759e-05, 'epoch': 2.48}
>>> 2025-08-17 20:56:42,256 - INFO - >>> {'loss': 1.7595, 'grad_norm': 1.1757465600967407, 'learning_rate': 9.982405731836342e-05, 'epoch': 2.64}
>>> 2025-08-17 20:56:46,308 - INFO - >>> {'loss': 1.8646, 'grad_norm': 1.3792724609375, 'learning_rate': 9.980139223313925e-05, 'epoch': 2.8}
>>> 2025-08-17 20:56:50,964 - INFO - >>> {'loss': 1.6911, 'grad_norm': 1.2141661643981934, 'learning_rate': 9.977735725509632e-05, 'epoch': 2.96}
>>> 2025-08-17 20:56:51,870 - INFO - >>> {'loss': 1.8056, 'grad_norm': 3.012831926345825, 'learning_rate': 9.97519530453676e-05, 'epoch': 3.0}
>>> 2025-08-17 20:56:58,813 - INFO - >>> {'loss': 1.8187, 'grad_norm': 1.2221587896347046, 'learning_rate': 9.972518030274971e-05, 'epoch': 3.16}
>>> 2025-08-17 20:57:05,587 - INFO - >>> {'loss': 1.829, 'grad_norm': 1.3938701152801514, 'learning_rate': 9.969703976368368e-05, 'epoch': 3.32}
>>> 2025-08-17 20:57:11,290 - INFO - >>> {'loss': 1.5846, 'grad_norm': 1.2890825271606445, 'learning_rate': 9.966753220223465e-05, 'epoch': 3.48}
>>> 2025-08-17 20:57:17,710 - INFO - >>> {'loss': 1.5832, 'grad_norm': 1.136777639389038, 'learning_rate': 9.963665843007064e-05, 'epoch': 3.64}
>>> 2025-08-17 20:57:23,094 - INFO - >>> {'loss': 1.3122, 'grad_norm': 1.6488423347473145, 'learning_rate': 9.960441929644017e-05, 'epoch': 3.8}
>>> 2025-08-17 20:57:27,094 - INFO - >>> {'loss': 1.5984, 'grad_norm': 2.1675922870635986, 'learning_rate': 9.95708156881489e-05, 'epoch': 3.96}
>>> 2025-08-17 20:57:28,032 - INFO - >>> {'loss': 1.3204, 'grad_norm': 3.1852738857269287, 'learning_rate': 9.953584852953529e-05, 'epoch': 4.0}
>>> 2025-08-17 20:57:33,071 - INFO - >>> {'loss': 1.6746, 'grad_norm': 2.1651525497436523, 'learning_rate': 9.949951878244515e-05, 'epoch': 4.16}
>>> 2025-08-17 20:57:40,680 - INFO - >>> {'loss': 1.2596, 'grad_norm': 1.912469744682312, 'learning_rate': 9.946182744620512e-05, 'epoch': 4.32}
>>> 2025-08-17 20:57:47,216 - INFO - >>> {'loss': 1.3666, 'grad_norm': 1.8060182332992554, 'learning_rate': 9.942277555759529e-05, 'epoch': 4.48}
>>> 2025-08-17 20:57:54,042 - INFO - >>> {'loss': 1.3421, 'grad_norm': 2.3263466358184814, 'learning_rate': 9.938236419082061e-05, 'epoch': 4.64}
>>> 2025-08-17 20:57:59,250 - INFO - >>> {'loss': 1.4773, 'grad_norm': 2.544668674468994, 'learning_rate': 9.934059445748134e-05, 'epoch': 4.8}
>>> 2025-08-17 20:58:04,970 - INFO - >>> {'loss': 1.4329, 'grad_norm': 2.158216953277588, 'learning_rate': 9.929746750654249e-05, 'epoch': 4.96}
>>> 2025-08-17 20:58:05,325 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.925298452430226e-05, 'epoch': 5.0}
>>> 2025-08-17 20:58:11,545 - INFO - >>> {'loss': 1.4524, 'grad_norm': 3.991312026977539, 'learning_rate': 9.92071467343593e-05, 'epoch': 5.16}
>>> 2025-08-17 20:58:17,205 - INFO - >>> {'loss': 1.0954, 'grad_norm': 3.485146999359131, 'learning_rate': 9.915995539757917e-05, 'epoch': 5.32}
>>> 2025-08-17 20:58:21,576 - INFO - >>> {'loss': 1.0042, 'grad_norm': 2.37394118309021, 'learning_rate': 9.911141181205958e-05, 'epoch': 5.48}
>>> 2025-08-17 20:58:24,560 - INFO - >>> {'loss': 1.0101, 'grad_norm': 2.986771583557129, 'learning_rate': 9.906151731309472e-05, 'epoch': 5.64}
>>> 2025-08-17 20:58:30,137 - INFO - >>> {'loss': 1.1786, 'grad_norm': 2.0024349689483643, 'learning_rate': 9.901027327313848e-05, 'epoch': 5.8}
>>> 2025-08-17 20:58:36,183 - INFO - >>> {'loss': 1.1703, 'grad_norm': 2.76664137840271, 'learning_rate': 9.895768110176678e-05, 'epoch': 5.96}
>>> 2025-08-17 20:58:37,329 - INFO - >>> {'loss': 1.2249, 'grad_norm': 5.462205410003662, 'learning_rate': 9.890374224563872e-05, 'epoch': 6.0}
>>> 2025-08-17 20:58:42,183 - INFO - >>> {'loss': 0.9084, 'grad_norm': 2.4526126384735107, 'learning_rate': 9.884845818845685e-05, 'epoch': 6.16}
>>> 2025-08-17 20:58:47,254 - INFO - >>> {'loss': 1.0056, 'grad_norm': 2.3017690181732178, 'learning_rate': 9.879183045092628e-05, 'epoch': 6.32}
>>> 2025-08-17 20:58:49,405 - INFO - >>> {'loss': 0.5658, 'grad_norm': 4.7032647132873535, 'learning_rate': 9.873386059071294e-05, 'epoch': 6.48}
>>> 2025-08-17 20:58:54,566 - INFO - >>> {'loss': 1.0249, 'grad_norm': 4.029311180114746, 'learning_rate': 9.867455020240069e-05, 'epoch': 6.64}
>>> 2025-08-17 20:58:58,521 - INFO - >>> {'loss': 0.8876, 'grad_norm': 4.303473949432373, 'learning_rate': 9.861390091744737e-05, 'epoch': 6.8}
>>> 2025-08-17 20:59:04,593 - INFO - >>> {'loss': 0.959, 'grad_norm': 4.377981662750244, 'learning_rate': 9.855191440414013e-05, 'epoch': 6.96}
>>> 2025-08-17 20:59:05,891 - INFO - >>> {'loss': 0.658, 'grad_norm': 4.439909934997559, 'learning_rate': 9.848859236754935e-05, 'epoch': 7.0}
>>> 2025-08-17 20:59:09,837 - INFO - >>> {'loss': 0.7305, 'grad_norm': 3.2496185302734375, 'learning_rate': 9.842393654948181e-05, 'epoch': 7.16}
>>> 2025-08-17 20:59:14,430 - INFO - >>> {'loss': 0.7731, 'grad_norm': 2.9710726737976074, 'learning_rate': 9.83579487284328e-05, 'epoch': 7.32}
>>> 2025-08-17 20:59:18,595 - INFO - >>> {'loss': 0.8792, 'grad_norm': 3.2282509803771973, 'learning_rate': 9.829063071953714e-05, 'epoch': 7.48}
>>> 2025-08-17 20:59:22,919 - INFO - >>> {'loss': 0.7678, 'grad_norm': 3.210278272628784, 'learning_rate': 9.822198437451932e-05, 'epoch': 7.64}
>>> 2025-08-17 20:59:29,466 - INFO - >>> {'loss': 0.577, 'grad_norm': 2.2897067070007324, 'learning_rate': 9.815201158164254e-05, 'epoch': 7.8}
>>> 2025-08-17 20:59:33,045 - INFO - >>> {'loss': 0.5184, 'grad_norm': 2.874008893966675, 'learning_rate': 9.808071426565671e-05, 'epoch': 7.96}
>>> 2025-08-17 20:59:34,291 - INFO - >>> {'loss': 0.3794, 'grad_norm': 6.452005386352539, 'learning_rate': 9.800809438774556e-05, 'epoch': 8.0}
>>> 2025-08-17 20:59:39,228 - INFO - >>> {'loss': 0.5158, 'grad_norm': 3.017939567565918, 'learning_rate': 9.793415394547274e-05, 'epoch': 8.16}
>>> 2025-08-17 20:59:45,219 - INFO - >>> {'loss': 0.463, 'grad_norm': 2.849076986312866, 'learning_rate': 9.785889497272677e-05, 'epoch': 8.32}
>>> 2025-08-17 20:59:49,703 - INFO - >>> {'loss': 0.3681, 'grad_norm': 3.2446975708007812, 'learning_rate': 9.778231953966519e-05, 'epoch': 8.48}
>>> 2025-08-17 20:59:54,164 - INFO - >>> {'loss': 0.3144, 'grad_norm': 3.3243820667266846, 'learning_rate': 9.770442975265752e-05, 'epoch': 8.64}
>>> 2025-08-17 20:59:58,985 - INFO - >>> {'loss': 0.3885, 'grad_norm': 3.4383037090301514, 'learning_rate': 9.762522775422741e-05, 'epoch': 8.8}
>>> 2025-08-17 21:00:04,404 - INFO - >>> {'loss': 0.3756, 'grad_norm': 4.245587348937988, 'learning_rate': 9.754471572299363e-05, 'epoch': 8.96}
>>> 2025-08-17 21:00:04,756 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.746289587361021e-05, 'epoch': 9.0}
>>> 2025-08-17 21:00:08,727 - INFO - >>> {'loss': 0.1697, 'grad_norm': 2.860780715942383, 'learning_rate': 9.737977045670548e-05, 'epoch': 9.16}
>>> 2025-08-17 21:00:11,813 - INFO - >>> {'loss': 0.2193, 'grad_norm': 2.5530498027801514, 'learning_rate': 9.729534175882016e-05, 'epoch': 9.32}
>>> 2025-08-17 21:00:16,301 - INFO - >>> {'loss': 0.2092, 'grad_norm': 2.986985683441162, 'learning_rate': 9.720961210234449e-05, 'epoch': 9.48}
>>> 2025-08-17 21:00:21,960 - INFO - >>> {'loss': 0.2067, 'grad_norm': 3.346466064453125, 'learning_rate': 9.712258384545432e-05, 'epoch': 9.64}
>>> 2025-08-17 21:00:26,859 - INFO - >>> {'loss': 0.2453, 'grad_norm': 3.290935754776001, 'learning_rate': 9.703425938204627e-05, 'epoch': 9.8}
>>> 2025-08-17 21:00:30,148 - INFO - >>> {'loss': 0.0965, 'grad_norm': 2.5676097869873047, 'learning_rate': 9.694464114167186e-05, 'epoch': 9.96}
>>> 2025-08-17 21:00:31,817 - INFO - >>> {'loss': 0.2732, 'grad_norm': 4.494732856750488, 'learning_rate': 9.685373158947067e-05, 'epoch': 10.0}
>>> 2025-08-17 21:00:36,130 - INFO - >>> {'loss': 0.1085, 'grad_norm': 2.64713978767395, 'learning_rate': 9.676153322610259e-05, 'epoch': 10.16}
>>> 2025-08-17 21:00:41,764 - INFO - >>> {'loss': 0.1008, 'grad_norm': 2.662109375, 'learning_rate': 9.666804858767894e-05, 'epoch': 10.32}
>>> 2025-08-17 21:00:46,751 - INFO - >>> {'loss': 0.0779, 'grad_norm': 3.140854835510254, 'learning_rate': 9.65732802456928e-05, 'epoch': 10.48}
>>> 2025-08-17 21:00:50,070 - INFO - >>> {'loss': 0.1278, 'grad_norm': 3.852860689163208, 'learning_rate': 9.647723080694821e-05, 'epoch': 10.64}
>>> 2025-08-17 21:00:52,856 - INFO - >>> {'loss': 0.0894, 'grad_norm': 4.641670227050781, 'learning_rate': 9.637990291348853e-05, 'epoch': 10.8}
>>> 2025-08-17 21:00:57,792 - INFO - >>> {'loss': 0.1736, 'grad_norm': 5.802214622497559, 'learning_rate': 9.628129924252369e-05, 'epoch': 10.96}
>>> 2025-08-17 21:00:59,671 - INFO - >>> {'loss': 0.1976, 'grad_norm': 10.585649490356445, 'learning_rate': 9.618142250635658e-05, 'epoch': 11.0}
>>> 2025-08-17 21:01:03,351 - INFO - >>> {'loss': 0.1392, 'grad_norm': 5.242000102996826, 'learning_rate': 9.608027545230847e-05, 'epoch': 11.16}
>>> 2025-08-17 21:01:09,125 - INFO - >>> {'loss': 0.0643, 'grad_norm': 3.950962543487549, 'learning_rate': 9.597786086264338e-05, 'epoch': 11.32}
>>> 2025-08-17 21:01:13,860 - INFO - >>> {'loss': 0.0983, 'grad_norm': 4.87555456161499, 'learning_rate': 9.587418155449167e-05, 'epoch': 11.48}
>>> 2025-08-17 21:01:18,386 - INFO - >>> {'loss': 0.1299, 'grad_norm': 4.934397220611572, 'learning_rate': 9.576924037977233e-05, 'epoch': 11.64}
>>> 2025-08-17 21:01:23,189 - INFO - >>> {'loss': 0.1483, 'grad_norm': 5.4744038581848145, 'learning_rate': 9.566304022511477e-05, 'epoch': 11.8}
>>> 2025-08-17 21:01:27,737 - INFO - >>> {'loss': 0.0961, 'grad_norm': 3.600605010986328, 'learning_rate': 9.555558401177926e-05, 'epoch': 11.96}
>>> 2025-08-17 21:01:28,544 - INFO - >>> {'loss': 0.0796, 'grad_norm': 5.543358325958252, 'learning_rate': 9.544687469557666e-05, 'epoch': 12.0}
>>> 2025-08-17 21:01:33,700 - INFO - >>> {'loss': 0.0522, 'grad_norm': 2.934706449508667, 'learning_rate': 9.533691526678705e-05, 'epoch': 12.16}
>>> 2025-08-17 21:01:38,507 - INFO - >>> {'loss': 0.0675, 'grad_norm': 3.883074998855591, 'learning_rate': 9.52257087500775e-05, 'epoch': 12.32}
>>> 2025-08-17 21:01:43,319 - INFO - >>> {'loss': 0.0646, 'grad_norm': 3.373567581176758, 'learning_rate': 9.51132582044189e-05, 'epoch': 12.48}
>>> 2025-08-17 21:01:47,903 - INFO - >>> {'loss': 0.0693, 'grad_norm': 3.4116883277893066, 'learning_rate': 9.499956672300178e-05, 'epoch': 12.64}
>>> 2025-08-17 21:01:51,815 - INFO - >>> {'loss': 0.0562, 'grad_norm': 2.266843557357788, 'learning_rate': 9.488463743315126e-05, 'epoch': 12.8}
>>> 2025-08-17 21:01:56,498 - INFO - >>> {'loss': 0.0896, 'grad_norm': 3.125359535217285, 'learning_rate': 9.476847349624097e-05, 'epoch': 12.96}
>>> 2025-08-17 21:01:57,657 - INFO - >>> {'loss': 0.0552, 'grad_norm': 2.631122350692749, 'learning_rate': 9.46510781076061e-05, 'epoch': 13.0}
>>> 2025-08-17 21:02:02,244 - INFO - >>> {'loss': 0.0415, 'grad_norm': 2.4663124084472656, 'learning_rate': 9.453245449645563e-05, 'epoch': 13.16}
>>> 2025-08-17 21:02:05,339 - INFO - >>> {'loss': 0.0347, 'grad_norm': 2.583225727081299, 'learning_rate': 9.441260592578329e-05, 'epoch': 13.32}
>>> 2025-08-17 21:02:10,618 - INFO - >>> {'loss': 0.0339, 'grad_norm': 2.2474465370178223, 'learning_rate': 9.4291535692278e-05, 'epoch': 13.48}
>>> 2025-08-17 21:02:13,377 - INFO - >>> {'loss': 0.07, 'grad_norm': 3.596053123474121, 'learning_rate': 9.416924712623305e-05, 'epoch': 13.64}
>>> 2025-08-17 21:02:19,146 - INFO - >>> {'loss': 0.0401, 'grad_norm': 2.656689405441284, 'learning_rate': 9.404574359145459e-05, 'epoch': 13.8}
>>> 2025-08-17 21:02:23,716 - INFO - >>> {'loss': 0.0617, 'grad_norm': 3.672759771347046, 'learning_rate': 9.392102848516901e-05, 'epoch': 13.96}
>>> 2025-08-17 21:02:24,763 - INFO - >>> {'loss': 0.1051, 'grad_norm': 6.739542007446289, 'learning_rate': 9.379510523792961e-05, 'epoch': 14.0}
>>> 2025-08-17 21:02:29,298 - INFO - >>> {'loss': 0.0465, 'grad_norm': 3.166274070739746, 'learning_rate': 9.366797731352209e-05, 'epoch': 14.16}
>>> 2025-08-17 21:02:33,386 - INFO - >>> {'loss': 0.0579, 'grad_norm': 3.4504430294036865, 'learning_rate': 9.353964820886938e-05, 'epoch': 14.32}
>>> 2025-08-17 21:02:38,443 - INFO - >>> {'loss': 0.0386, 'grad_norm': 2.862034559249878, 'learning_rate': 9.341012145393547e-05, 'epoch': 14.48}
>>> 2025-08-17 21:02:43,110 - INFO - >>> {'loss': 0.0579, 'grad_norm': 3.517169713973999, 'learning_rate': 9.327940061162817e-05, 'epoch': 14.64}
>>> 2025-08-17 21:02:47,498 - INFO - >>> {'loss': 0.0666, 'grad_norm': 3.5698351860046387, 'learning_rate': 9.314748927770125e-05, 'epoch': 14.8}
>>> 2025-08-17 21:02:50,841 - INFO - >>> {'loss': 0.066, 'grad_norm': 3.3121917247772217, 'learning_rate': 9.301439108065546e-05, 'epoch': 14.96}
>>> 2025-08-17 21:02:53,344 - INFO - >>> {'loss': 0.1123, 'grad_norm': 7.365694046020508, 'learning_rate': 9.288010968163872e-05, 'epoch': 15.0}
>>> 2025-08-17 21:02:57,130 - INFO - >>> {'loss': 0.0448, 'grad_norm': 3.034092426300049, 'learning_rate': 9.274464877434548e-05, 'epoch': 15.16}
>>> 2025-08-17 21:03:02,344 - INFO - >>> {'loss': 0.0526, 'grad_norm': 2.8314144611358643, 'learning_rate': 9.260801208491498e-05, 'epoch': 15.32}
>>> 2025-08-17 21:03:05,444 - INFO - >>> {'loss': 0.0273, 'grad_norm': 3.1524288654327393, 'learning_rate': 9.247020337182893e-05, 'epoch': 15.48}
>>> 2025-08-17 21:03:10,035 - INFO - >>> {'loss': 0.0372, 'grad_norm': 2.7801220417022705, 'learning_rate': 9.233122642580796e-05, 'epoch': 15.64}
>>> 2025-08-17 21:03:14,960 - INFO - >>> {'loss': 0.0329, 'grad_norm': 2.031961679458618, 'learning_rate': 9.219108506970746e-05, 'epoch': 15.8}
>>> 2025-08-17 21:03:19,639 - INFO - >>> {'loss': 0.024, 'grad_norm': 1.7320843935012817, 'learning_rate': 9.204978315841237e-05, 'epoch': 15.96}
>>> 2025-08-17 21:03:21,946 - INFO - >>> {'loss': 0.0167, 'grad_norm': 1.7324095964431763, 'learning_rate': 9.190732457873119e-05, 'epoch': 16.0}
>>> 2025-08-17 21:03:27,608 - INFO - >>> {'loss': 0.0162, 'grad_norm': 1.373884916305542, 'learning_rate': 9.176371324928899e-05, 'epoch': 16.16}
>>> 2025-08-17 21:03:31,408 - INFO - >>> {'loss': 0.0141, 'grad_norm': 1.2642136812210083, 'learning_rate': 9.161895312041971e-05, 'epoch': 16.32}
>>> 2025-08-17 21:03:34,760 - INFO - >>> {'loss': 0.0211, 'grad_norm': 1.4947547912597656, 'learning_rate': 9.14730481740574e-05, 'epoch': 16.48}
>>> 2025-08-17 21:03:39,643 - INFO - >>> {'loss': 0.0323, 'grad_norm': 2.3414814472198486, 'learning_rate': 9.132600242362681e-05, 'epoch': 16.64}
>>> 2025-08-17 21:03:44,593 - INFO - >>> {'loss': 0.0298, 'grad_norm': 2.465996265411377, 'learning_rate': 9.117781991393283e-05, 'epoch': 16.8}
>>> 2025-08-17 21:03:48,227 - INFO - >>> {'loss': 0.0383, 'grad_norm': 3.2072834968566895, 'learning_rate': 9.102850472104944e-05, 'epoch': 16.96}
>>> 2025-08-17 21:03:49,177 - INFO - >>> {'loss': 0.1518, 'grad_norm': 7.438885688781738, 'learning_rate': 9.087806095220739e-05, 'epoch': 17.0}
>>> 2025-08-17 21:03:52,070 - INFO - >>> {'loss': 0.0097, 'grad_norm': 1.311116099357605, 'learning_rate': 9.072649274568129e-05, 'epoch': 17.16}
>>> 2025-08-17 21:03:57,827 - INFO - >>> {'loss': 0.0192, 'grad_norm': 1.7304191589355469, 'learning_rate': 9.057380427067584e-05, 'epoch': 17.32}
>>> 2025-08-17 21:04:02,729 - INFO - >>> {'loss': 0.0361, 'grad_norm': 2.586108922958374, 'learning_rate': 9.041999972721109e-05, 'epoch': 17.48}
>>> 2025-08-17 21:04:07,304 - INFO - >>> {'loss': 0.0271, 'grad_norm': 1.8590465784072876, 'learning_rate': 9.02650833460069e-05, 'epoch': 17.64}
>>> 2025-08-17 21:04:11,409 - INFO - >>> {'loss': 0.0271, 'grad_norm': 2.1433210372924805, 'learning_rate': 9.010905938836661e-05, 'epoch': 17.8}
>>> 2025-08-17 21:04:14,454 - INFO - >>> {'loss': 0.0656, 'grad_norm': 3.521125555038452, 'learning_rate': 8.995193214605973e-05, 'epoch': 17.96}
>>> 2025-08-17 21:04:15,680 - INFO - >>> {'loss': 0.0274, 'grad_norm': 4.479522228240967, 'learning_rate': 8.979370594120402e-05, 'epoch': 18.0}
>>> 2025-08-17 21:04:20,201 - INFO - >>> {'loss': 0.0197, 'grad_norm': 1.4567850828170776, 'learning_rate': 8.963438512614655e-05, 'epoch': 18.16}
>>> 2025-08-17 21:04:26,682 - INFO - >>> {'loss': 0.0217, 'grad_norm': 1.8890377283096313, 'learning_rate': 8.947397408334391e-05, 'epoch': 18.32}
>>> 2025-08-17 21:04:30,158 - INFO - >>> {'loss': 0.017, 'grad_norm': 2.076076030731201, 'learning_rate': 8.931247722524169e-05, 'epoch': 18.48}
>>> 2025-08-17 21:04:33,514 - INFO - >>> {'loss': 0.0173, 'grad_norm': 2.1239280700683594, 'learning_rate': 8.914989899415323e-05, 'epoch': 18.64}
>>> 2025-08-17 21:04:36,691 - INFO - >>> {'loss': 0.0175, 'grad_norm': 2.582585573196411, 'learning_rate': 8.898624386213725e-05, 'epoch': 18.8}
>>> 2025-08-17 21:04:42,787 - INFO - >>> {'loss': 0.024, 'grad_norm': 1.7372187376022339, 'learning_rate': 8.88215163308749e-05, 'epoch': 18.96}
>>> 2025-08-17 21:04:44,153 - INFO - >>> {'loss': 0.0557, 'grad_norm': 4.235774517059326, 'learning_rate': 8.8655720931546e-05, 'epoch': 19.0}
>>> 2025-08-17 21:04:50,564 - INFO - >>> {'loss': 0.0254, 'grad_norm': 2.0951590538024902, 'learning_rate': 8.84888622247043e-05, 'epoch': 19.16}
>>> 2025-08-17 21:04:54,717 - INFO - >>> {'loss': 0.0082, 'grad_norm': 1.170570969581604, 'learning_rate': 8.83209448001521e-05, 'epoch': 19.32}
>>> 2025-08-17 21:04:59,585 - INFO - >>> {'loss': 0.0292, 'grad_norm': 2.645371198654175, 'learning_rate': 8.815197327681399e-05, 'epoch': 19.48}
>>> 2025-08-17 21:05:03,587 - INFO - >>> {'loss': 0.079, 'grad_norm': 4.706268310546875, 'learning_rate': 8.798195230260973e-05, 'epoch': 19.64}
>>> 2025-08-17 21:05:07,007 - INFO - >>> {'loss': 0.0215, 'grad_norm': 2.23045015335083, 'learning_rate': 8.781088655432648e-05, 'epoch': 19.8}
>>> 2025-08-17 21:05:11,655 - INFO - >>> {'loss': 0.0369, 'grad_norm': 2.5727646350860596, 'learning_rate': 8.763878073749012e-05, 'epoch': 19.96}
>>> 2025-08-17 21:05:12,583 - INFO - >>> {'loss': 0.0117, 'grad_norm': 1.5812387466430664, 'learning_rate': 8.746563958623584e-05, 'epoch': 20.0}
>>> 2025-08-17 21:05:15,633 - INFO - >>> {'loss': 0.0127, 'grad_norm': 2.362565755844116, 'learning_rate': 8.729146786317786e-05, 'epoch': 20.16}
>>> 2025-08-17 21:05:21,583 - INFO - >>> {'loss': 0.0109, 'grad_norm': 1.707464575767517, 'learning_rate': 8.711627035927847e-05, 'epoch': 20.32}
>>> 2025-08-17 21:05:26,667 - INFO - >>> {'loss': 0.0138, 'grad_norm': 1.8794419765472412, 'learning_rate': 8.694005189371627e-05, 'epoch': 20.48}
>>> 2025-08-17 21:05:30,718 - INFO - >>> {'loss': 0.0129, 'grad_norm': 1.56442391872406, 'learning_rate': 8.676281731375353e-05, 'epoch': 20.64}
>>> 2025-08-17 21:05:35,497 - INFO - >>> {'loss': 0.0154, 'grad_norm': 1.524240255355835, 'learning_rate': 8.658457149460295e-05, 'epoch': 20.8}
>>> 2025-08-17 21:05:39,513 - INFO - >>> {'loss': 0.0168, 'grad_norm': 1.028961181640625, 'learning_rate': 8.640531933929344e-05, 'epoch': 20.96}
>>> 2025-08-17 21:05:40,805 - INFO - >>> {'loss': 0.0054, 'grad_norm': 1.1576275825500488, 'learning_rate': 8.622506577853538e-05, 'epoch': 21.0}
>>> 2025-08-17 21:05:45,228 - INFO - >>> {'loss': 0.0077, 'grad_norm': 0.9163605570793152, 'learning_rate': 8.604381577058486e-05, 'epoch': 21.16}
>>> 2025-08-17 21:05:49,750 - INFO - >>> {'loss': 0.012, 'grad_norm': 2.4403693675994873, 'learning_rate': 8.586157430110747e-05, 'epoch': 21.32}
>>> 2025-08-17 21:05:53,498 - INFO - >>> {'loss': 0.017, 'grad_norm': 2.16485595703125, 'learning_rate': 8.56783463830409e-05, 'epoch': 21.48}
>>> 2025-08-17 21:05:58,443 - INFO - >>> {'loss': 0.0272, 'grad_norm': 2.9175631999969482, 'learning_rate': 8.549413705645737e-05, 'epoch': 21.64}
>>> 2025-08-17 21:06:02,899 - INFO - >>> {'loss': 0.0136, 'grad_norm': 2.039775848388672, 'learning_rate': 8.530895138842467e-05, 'epoch': 21.8}
>>> 2025-08-17 21:06:08,536 - INFO - >>> {'loss': 0.0237, 'grad_norm': 1.853742241859436, 'learning_rate': 8.512279447286703e-05, 'epoch': 21.96}
>>> 2025-08-17 21:06:09,850 - INFO - >>> {'loss': 0.0097, 'grad_norm': 2.4441630840301514, 'learning_rate': 8.493567143042485e-05, 'epoch': 22.0}
>>> 2025-08-17 21:06:14,488 - INFO - >>> {'loss': 0.0069, 'grad_norm': 0.7790737152099609, 'learning_rate': 8.47475874083139e-05, 'epoch': 22.16}
>>> 2025-08-17 21:06:19,347 - INFO - >>> {'loss': 0.0102, 'grad_norm': 1.4448556900024414, 'learning_rate': 8.455854758018376e-05, 'epoch': 22.32}
>>> 2025-08-17 21:06:23,155 - INFO - >>> {'loss': 0.0271, 'grad_norm': 3.594365119934082, 'learning_rate': 8.436855714597546e-05, 'epoch': 22.48}
>>> 2025-08-17 21:06:26,341 - INFO - >>> {'loss': 0.0085, 'grad_norm': 1.2823960781097412, 'learning_rate': 8.417762133177848e-05, 'epoch': 22.64}
>>> 2025-08-17 21:06:30,796 - INFO - >>> {'loss': 0.0142, 'grad_norm': 1.6589875221252441, 'learning_rate': 8.398574538968697e-05, 'epoch': 22.8}
>>> 2025-08-17 21:06:35,087 - INFO - >>> {'loss': 0.0265, 'grad_norm': 2.3302271366119385, 'learning_rate': 8.379293459765526e-05, 'epoch': 22.96}
>>> 2025-08-17 21:06:36,314 - INFO - >>> {'loss': 0.01, 'grad_norm': 1.2795532941818237, 'learning_rate': 8.359919425935275e-05, 'epoch': 23.0}
>>> 2025-08-17 21:06:40,772 - INFO - >>> {'loss': 0.0081, 'grad_norm': 0.9481780529022217, 'learning_rate': 8.340452970401797e-05, 'epoch': 23.16}
>>> 2025-08-17 21:06:44,997 - INFO - >>> {'loss': 0.0051, 'grad_norm': 1.141900897026062, 'learning_rate': 8.3208946286312e-05, 'epoch': 23.32}
>>> 2025-08-17 21:06:51,491 - INFO - >>> {'loss': 0.0148, 'grad_norm': 2.4153380393981934, 'learning_rate': 8.301244938617116e-05, 'epoch': 23.48}
>>> 2025-08-17 21:06:56,150 - INFO - >>> {'loss': 0.035, 'grad_norm': 1.5227601528167725, 'learning_rate': 8.281504440865905e-05, 'epoch': 23.64}
>>> 2025-08-17 21:06:59,763 - INFO - >>> {'loss': 0.0472, 'grad_norm': 2.4658052921295166, 'learning_rate': 8.261673678381786e-05, 'epoch': 23.8}
>>> 2025-08-17 21:07:03,728 - INFO - >>> {'loss': 0.0149, 'grad_norm': 1.370026707649231, 'learning_rate': 8.241753196651902e-05, 'epoch': 23.96}
>>> 2025-08-17 21:07:05,995 - INFO - >>> {'loss': 0.0215, 'grad_norm': 2.5703272819519043, 'learning_rate': 8.221743543631313e-05, 'epoch': 24.0}
>>> 2025-08-17 21:07:10,406 - INFO - >>> {'loss': 0.0181, 'grad_norm': 1.5673118829727173, 'learning_rate': 8.201645269727925e-05, 'epoch': 24.16}
>>> 2025-08-17 21:07:15,105 - INFO - >>> {'loss': 0.0091, 'grad_norm': 0.8069869875907898, 'learning_rate': 8.181458927787347e-05, 'epoch': 24.32}
>>> 2025-08-17 21:07:20,983 - INFO - >>> {'loss': 0.0167, 'grad_norm': 1.192679762840271, 'learning_rate': 8.161185073077686e-05, 'epoch': 24.48}
>>> 2025-08-17 21:07:26,111 - INFO - >>> {'loss': 0.0048, 'grad_norm': 1.0230449438095093, 'learning_rate': 8.140824263274279e-05, 'epoch': 24.64}
>>> 2025-08-17 21:07:31,523 - INFO - >>> {'loss': 0.0195, 'grad_norm': 1.3390306234359741, 'learning_rate': 8.120377058444336e-05, 'epoch': 24.8}
>>> 2025-08-17 21:07:35,044 - INFO - >>> {'loss': 0.0076, 'grad_norm': 1.1758581399917603, 'learning_rate': 8.09984402103156e-05, 'epoch': 24.96}
>>> 2025-08-17 21:07:35,398 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.079225715840646e-05, 'epoch': 25.0}
>>> 2025-08-17 21:07:39,538 - INFO - >>> {'loss': 0.0171, 'grad_norm': 1.8780646324157715, 'learning_rate': 8.058522710021772e-05, 'epoch': 25.16}
>>> 2025-08-17 21:07:44,537 - INFO - >>> {'loss': 0.0081, 'grad_norm': 0.9783213138580322, 'learning_rate': 8.037735573054979e-05, 'epoch': 25.32}
>>> 2025-08-17 21:07:48,027 - INFO - >>> {'loss': 0.007, 'grad_norm': 0.7963345646858215, 'learning_rate': 8.016864876734514e-05, 'epoch': 25.48}
>>> 2025-08-17 21:07:52,142 - INFO - >>> {'loss': 0.0128, 'grad_norm': 1.3200629949569702, 'learning_rate': 7.995911195153105e-05, 'epoch': 25.64}
>>> 2025-08-17 21:07:57,273 - INFO - >>> {'loss': 0.0232, 'grad_norm': 2.2831311225891113, 'learning_rate': 7.974875104686163e-05, 'epoch': 25.8}
>>> 2025-08-17 21:08:03,099 - INFO - >>> {'loss': 0.0149, 'grad_norm': 1.652957558631897, 'learning_rate': 7.95375718397593e-05, 'epoch': 25.96}
>>> 2025-08-17 21:08:03,450 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.932558013915562e-05, 'epoch': 26.0}
>>> 2025-08-17 21:08:06,055 - INFO - >>> {'loss': 0.012, 'grad_norm': 1.4806269407272339, 'learning_rate': 7.911278177633151e-05, 'epoch': 26.16}
>>> 2025-08-17 21:08:08,569 - INFO - >>> {'loss': 0.0076, 'grad_norm': 2.407878875732422, 'learning_rate': 7.889918260475685e-05, 'epoch': 26.32}
>>> 2025-08-17 21:08:13,917 - INFO - >>> {'loss': 0.0085, 'grad_norm': 1.3548510074615479, 'learning_rate': 7.868478849992945e-05, 'epoch': 26.48}
>>> 2025-08-17 21:08:19,181 - INFO - >>> {'loss': 0.0173, 'grad_norm': 1.309993863105774, 'learning_rate': 7.846960535921344e-05, 'epoch': 26.64}
>>> 2025-08-17 21:08:24,525 - INFO - >>> {'loss': 0.0098, 'grad_norm': 1.750665307044983, 'learning_rate': 7.825363910167708e-05, 'epoch': 26.8}
>>> 2025-08-17 21:08:30,058 - INFO - >>> {'loss': 0.0145, 'grad_norm': 1.1915336847305298, 'learning_rate': 7.803689566792989e-05, 'epoch': 26.96}
>>> 2025-08-17 21:08:30,855 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.8992814421653748, 'learning_rate': 7.781938101995927e-05, 'epoch': 27.0}
>>> 2025-08-17 21:08:34,934 - INFO - >>> {'loss': 0.0076, 'grad_norm': 1.0606049299240112, 'learning_rate': 7.76011011409665e-05, 'epoch': 27.16}
>>> 2025-08-17 21:08:39,886 - INFO - >>> {'loss': 0.0164, 'grad_norm': 1.5137531757354736, 'learning_rate': 7.738206203520222e-05, 'epoch': 27.32}
>>> 2025-08-17 21:08:44,240 - INFO - >>> {'loss': 0.0096, 'grad_norm': 1.3489478826522827, 'learning_rate': 7.716226972780112e-05, 'epoch': 27.48}
>>> 2025-08-17 21:08:50,904 - INFO - >>> {'loss': 0.0099, 'grad_norm': 1.3621671199798584, 'learning_rate': 7.694173026461634e-05, 'epoch': 27.64}
>>> 2025-08-17 21:08:55,262 - INFO - >>> {'loss': 0.0103, 'grad_norm': 2.354633331298828, 'learning_rate': 7.672044971205314e-05, 'epoch': 27.8}
>>> 2025-08-17 21:08:58,266 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.7150498628616333, 'learning_rate': 7.649843415690198e-05, 'epoch': 27.96}
>>> 2025-08-17 21:08:59,540 - INFO - >>> {'loss': 0.0062, 'grad_norm': 1.254191279411316, 'learning_rate': 7.627568970617113e-05, 'epoch': 28.0}
>>> 2025-08-17 21:09:04,873 - INFO - >>> {'loss': 0.0061, 'grad_norm': 1.0978652238845825, 'learning_rate': 7.605222248691872e-05, 'epoch': 28.16}
>>> 2025-08-17 21:09:08,473 - INFO - >>> {'loss': 0.0045, 'grad_norm': 0.9851598143577576, 'learning_rate': 7.582803864608411e-05, 'epoch': 28.32}
>>> 2025-08-17 21:09:13,345 - INFO - >>> {'loss': 0.0062, 'grad_norm': 1.1417886018753052, 'learning_rate': 7.560314435031885e-05, 'epoch': 28.48}
>>> 2025-08-17 21:09:18,342 - INFO - >>> {'loss': 0.0295, 'grad_norm': 2.268519639968872, 'learning_rate': 7.53775457858171e-05, 'epoch': 28.64}
>>> 2025-08-17 21:09:24,867 - INFO - >>> {'loss': 0.014, 'grad_norm': 1.645309567451477, 'learning_rate': 7.51512491581454e-05, 'epoch': 28.8}
>>> 2025-08-17 21:09:28,065 - INFO - >>> {'loss': 0.0166, 'grad_norm': 3.1022121906280518, 'learning_rate': 7.4924260692072e-05, 'epoch': 28.96}
>>> 2025-08-17 21:09:29,389 - INFO - >>> {'loss': 0.0062, 'grad_norm': 0.7484157681465149, 'learning_rate': 7.469658663139563e-05, 'epoch': 29.0}
>>> 2025-08-17 21:09:33,748 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.6447039246559143, 'learning_rate': 7.446823323877375e-05, 'epoch': 29.16}
>>> 2025-08-17 21:09:39,641 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.8156490325927734, 'learning_rate': 7.423920679555028e-05, 'epoch': 29.32}
>>> 2025-08-17 21:09:44,532 - INFO - >>> {'loss': 0.0054, 'grad_norm': 1.1648179292678833, 'learning_rate': 7.400951360158284e-05, 'epoch': 29.48}
>>> 2025-08-17 21:09:48,271 - INFO - >>> {'loss': 0.0144, 'grad_norm': 2.7978334426879883, 'learning_rate': 7.377915997506945e-05, 'epoch': 29.64}
>>> 2025-08-17 21:09:51,681 - INFO - >>> {'loss': 0.0191, 'grad_norm': 2.367030143737793, 'learning_rate': 7.354815225237468e-05, 'epoch': 29.8}
>>> 2025-08-17 21:09:56,477 - INFO - >>> {'loss': 0.0119, 'grad_norm': 1.4594712257385254, 'learning_rate': 7.331649678785546e-05, 'epoch': 29.96}
>>> 2025-08-17 21:09:57,305 - INFO - >>> {'loss': 0.027, 'grad_norm': 2.0073256492614746, 'learning_rate': 7.308419995368616e-05, 'epoch': 30.0}
>>> 2025-08-17 21:10:00,636 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.7153834700584412, 'learning_rate': 7.285126813968346e-05, 'epoch': 30.16}
>>> 2025-08-17 21:10:05,921 - INFO - >>> {'loss': 0.0091, 'grad_norm': 1.8414621353149414, 'learning_rate': 7.261770775313046e-05, 'epoch': 30.32}
>>> 2025-08-17 21:10:11,512 - INFO - >>> {'loss': 0.0041, 'grad_norm': 1.0855419635772705, 'learning_rate': 7.238352521860049e-05, 'epoch': 30.48}
>>> 2025-08-17 21:10:16,392 - INFO - >>> {'loss': 0.0183, 'grad_norm': 1.9059703350067139, 'learning_rate': 7.214872697778037e-05, 'epoch': 30.64}
>>> 2025-08-17 21:10:20,687 - INFO - >>> {'loss': 0.0063, 'grad_norm': 1.0402671098709106, 'learning_rate': 7.191331948929323e-05, 'epoch': 30.8}
>>> 2025-08-17 21:10:25,558 - INFO - >>> {'loss': 0.0145, 'grad_norm': 3.0978283882141113, 'learning_rate': 7.167730922852087e-05, 'epoch': 30.96}
>>> 2025-08-17 21:10:26,600 - INFO - >>> {'loss': 0.0031, 'grad_norm': 1.0754096508026123, 'learning_rate': 7.14407026874256e-05, 'epoch': 31.0}
>>> 2025-08-17 21:10:30,447 - INFO - >>> {'loss': 0.0075, 'grad_norm': 1.1924004554748535, 'learning_rate': 7.120350637437165e-05, 'epoch': 31.16}
>>> 2025-08-17 21:10:36,093 - INFO - >>> {'loss': 0.0101, 'grad_norm': 1.1130542755126953, 'learning_rate': 7.096572681394625e-05, 'epoch': 31.32}
>>> 2025-08-17 21:10:39,845 - INFO - >>> {'loss': 0.0095, 'grad_norm': 1.4943312406539917, 'learning_rate': 7.072737054678003e-05, 'epoch': 31.48}
>>> 2025-08-17 21:10:42,063 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.7229160070419312, 'learning_rate': 7.048844412936719e-05, 'epoch': 31.64}
>>> 2025-08-17 21:10:46,651 - INFO - >>> {'loss': 0.0192, 'grad_norm': 1.8435544967651367, 'learning_rate': 7.024895413388508e-05, 'epoch': 31.8}
>>> 2025-08-17 21:10:50,358 - INFO - >>> {'loss': 0.0172, 'grad_norm': 3.0233449935913086, 'learning_rate': 7.000890714801351e-05, 'epoch': 31.96}
>>> 2025-08-17 21:10:52,661 - INFO - >>> {'loss': 0.0115, 'grad_norm': 1.5615718364715576, 'learning_rate': 6.976830977475346e-05, 'epoch': 32.0}
>>> 2025-08-17 21:10:56,384 - INFO - >>> {'loss': 0.0128, 'grad_norm': 0.8489910364151001, 'learning_rate': 6.952716863224551e-05, 'epoch': 32.16}
>>> 2025-08-17 21:11:00,497 - INFO - >>> {'loss': 0.0119, 'grad_norm': 1.4952822923660278, 'learning_rate': 6.928549035358772e-05, 'epoch': 32.32}
>>> 2025-08-17 21:11:06,857 - INFO - >>> {'loss': 0.0089, 'grad_norm': 1.0704649686813354, 'learning_rate': 6.904328158665323e-05, 'epoch': 32.48}
>>> 2025-08-17 21:11:10,323 - INFO - >>> {'loss': 0.0501, 'grad_norm': 3.048722505569458, 'learning_rate': 6.880054899390744e-05, 'epoch': 32.64}
>>> 2025-08-17 21:11:16,106 - INFO - >>> {'loss': 0.0069, 'grad_norm': 1.3664151430130005, 'learning_rate': 6.855729925222462e-05, 'epoch': 32.8}
>>> 2025-08-17 21:11:19,408 - INFO - >>> {'loss': 0.0248, 'grad_norm': 2.2372350692749023, 'learning_rate': 6.831353905270434e-05, 'epoch': 32.96}
>>> 2025-08-17 21:11:19,759 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.806927510048738e-05, 'epoch': 33.0}
>>> 2025-08-17 21:11:24,064 - INFO - >>> {'loss': 0.0094, 'grad_norm': 1.2972642183303833, 'learning_rate': 6.782451411457137e-05, 'epoch': 33.16}
>>> 2025-08-17 21:11:28,559 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.973983645439148, 'learning_rate': 6.757926282762583e-05, 'epoch': 33.32}
>>> 2025-08-17 21:11:32,460 - INFO - >>> {'loss': 0.0066, 'grad_norm': 0.9776307940483093, 'learning_rate': 6.733352798580708e-05, 'epoch': 33.48}
>>> 2025-08-17 21:11:36,678 - INFO - >>> {'loss': 0.0103, 'grad_norm': 0.9161984920501709, 'learning_rate': 6.708731634857263e-05, 'epoch': 33.64}
>>> 2025-08-17 21:11:40,435 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.7971829771995544, 'learning_rate': 6.684063468849527e-05, 'epoch': 33.8}
>>> 2025-08-17 21:11:45,701 - INFO - >>> {'loss': 0.0067, 'grad_norm': 1.35382080078125, 'learning_rate': 6.659348979107679e-05, 'epoch': 33.96}
>>> 2025-08-17 21:11:48,352 - INFO - >>> {'loss': 0.0173, 'grad_norm': 1.5322333574295044, 'learning_rate': 6.634588845456123e-05, 'epoch': 34.0}
>>> 2025-08-17 21:11:54,019 - INFO - >>> {'loss': 0.012, 'grad_norm': 1.1205878257751465, 'learning_rate': 6.609783748974802e-05, 'epoch': 34.16}
>>> 2025-08-17 21:11:57,924 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.6310208439826965, 'learning_rate': 6.584934371980453e-05, 'epoch': 34.32}
>>> 2025-08-17 21:12:02,904 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.9723160266876221, 'learning_rate': 6.560041398007847e-05, 'epoch': 34.48}
>>> 2025-08-17 21:12:07,163 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.745212733745575, 'learning_rate': 6.53510551179098e-05, 'epoch': 34.64}
>>> 2025-08-17 21:12:10,496 - INFO - >>> {'loss': 0.0034, 'grad_norm': 1.2540435791015625, 'learning_rate': 6.510127399244234e-05, 'epoch': 34.8}
>>> 2025-08-17 21:12:14,762 - INFO - >>> {'loss': 0.0036, 'grad_norm': 1.6081446409225464, 'learning_rate': 6.485107747443528e-05, 'epoch': 34.96}
>>> 2025-08-17 21:12:15,528 - INFO - >>> {'loss': 0.0051, 'grad_norm': 1.3212448358535767, 'learning_rate': 6.460047244607397e-05, 'epoch': 35.0}
>>> 2025-08-17 21:12:20,245 - INFO - >>> {'loss': 0.0066, 'grad_norm': 0.8732432126998901, 'learning_rate': 6.434946580078072e-05, 'epoch': 35.16}
>>> 2025-08-17 21:12:25,662 - INFO - >>> {'loss': 0.0058, 'grad_norm': 1.0761480331420898, 'learning_rate': 6.409806444302518e-05, 'epoch': 35.32}
>>> 2025-08-17 21:12:30,136 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.6055358052253723, 'learning_rate': 6.38462752881344e-05, 'epoch': 35.48}
>>> 2025-08-17 21:12:35,173 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.8692769408226013, 'learning_rate': 6.359410526210258e-05, 'epoch': 35.64}
>>> 2025-08-17 21:12:39,443 - INFO - >>> {'loss': 0.0082, 'grad_norm': 1.4108455181121826, 'learning_rate': 6.334156130140068e-05, 'epoch': 35.8}
>>> 2025-08-17 21:12:43,309 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.5231736302375793, 'learning_rate': 6.30886503527854e-05, 'epoch': 35.96}
>>> 2025-08-17 21:12:44,296 - INFO - >>> {'loss': 0.0037, 'grad_norm': 1.0871553421020508, 'learning_rate': 6.283537937310828e-05, 'epoch': 36.0}
>>> 2025-08-17 21:12:47,889 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.866411566734314, 'learning_rate': 6.258175532912431e-05, 'epoch': 36.16}
>>> 2025-08-17 21:12:52,360 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.6033278703689575, 'learning_rate': 6.232778519730023e-05, 'epoch': 36.32}
>>> 2025-08-17 21:12:57,923 - INFO - >>> {'loss': 0.0062, 'grad_norm': 1.4613431692123413, 'learning_rate': 6.207347596362265e-05, 'epoch': 36.48}
>>> 2025-08-17 21:13:01,782 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.6857220530509949, 'learning_rate': 6.181883462340588e-05, 'epoch': 36.64}
>>> 2025-08-17 21:13:07,120 - INFO - >>> {'loss': 0.0242, 'grad_norm': 2.1261179447174072, 'learning_rate': 6.15638681810996e-05, 'epoch': 36.8}
>>> 2025-08-17 21:13:11,673 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.9280788898468018, 'learning_rate': 6.1308583650096e-05, 'epoch': 36.96}
>>> 2025-08-17 21:13:12,707 - INFO - >>> {'loss': 0.0088, 'grad_norm': 1.3152827024459839, 'learning_rate': 6.105298805253708e-05, 'epoch': 37.0}
>>> 2025-08-17 21:13:19,562 - INFO - >>> {'loss': 0.0079, 'grad_norm': 1.4838578701019287, 'learning_rate': 6.079708841912133e-05, 'epoch': 37.16}
>>> 2025-08-17 21:13:24,215 - INFO - >>> {'loss': 0.0124, 'grad_norm': 1.9778151512145996, 'learning_rate': 6.054089178891039e-05, 'epoch': 37.32}
>>> 2025-08-17 21:13:29,217 - INFO - >>> {'loss': 0.0049, 'grad_norm': 1.210724949836731, 'learning_rate': 6.028440520913544e-05, 'epoch': 37.48}
>>> 2025-08-17 21:13:33,795 - INFO - >>> {'loss': 0.0115, 'grad_norm': 1.7143999338150024, 'learning_rate': 6.0027635735003316e-05, 'epoch': 37.64}
>>> 2025-08-17 21:13:38,753 - INFO - >>> {'loss': 0.0096, 'grad_norm': 1.2449696063995361, 'learning_rate': 5.9770590429502516e-05, 'epoch': 37.8}
>>> 2025-08-17 21:13:42,105 - INFO - >>> {'loss': 0.0173, 'grad_norm': 2.677313804626465, 'learning_rate': 5.9513276363208784e-05, 'epoch': 37.96}
>>> 2025-08-17 21:13:42,455 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.925570061409077e-05, 'epoch': 38.0}
>>> 2025-08-17 21:13:46,461 - INFO - >>> {'loss': 0.0088, 'grad_norm': 1.680868148803711, 'learning_rate': 5.8997870267315234e-05, 'epoch': 38.16}
>>> 2025-08-17 21:13:51,219 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.8692765831947327, 'learning_rate': 5.873979241505218e-05, 'epoch': 38.32}
>>> 2025-08-17 21:13:54,188 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.6005377769470215, 'learning_rate': 5.84814741562798e-05, 'epoch': 38.48}
>>> 2025-08-17 21:13:59,117 - INFO - >>> {'loss': 0.0072, 'grad_norm': 1.8180360794067383, 'learning_rate': 5.822292259658914e-05, 'epoch': 38.64}
>>> 2025-08-17 21:14:05,327 - INFO - >>> {'loss': 0.0073, 'grad_norm': 1.1392555236816406, 'learning_rate': 5.79641448479887e-05, 'epoch': 38.8}
>>> 2025-08-17 21:14:09,178 - INFO - >>> {'loss': 0.0056, 'grad_norm': 2.48427414894104, 'learning_rate': 5.770514802870879e-05, 'epoch': 38.96}
>>> 2025-08-17 21:14:10,247 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.9060854315757751, 'learning_rate': 5.7445939263005734e-05, 'epoch': 39.0}
>>> 2025-08-17 21:14:14,594 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.5533854961395264, 'learning_rate': 5.718652568096585e-05, 'epoch': 39.16}
>>> 2025-08-17 21:14:19,074 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.6636012196540833, 'learning_rate': 5.692691441830941e-05, 'epoch': 39.32}
>>> 2025-08-17 21:14:22,012 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.6155008673667908, 'learning_rate': 5.666711261619428e-05, 'epoch': 39.48}
>>> 2025-08-17 21:14:26,051 - INFO - >>> {'loss': 0.0074, 'grad_norm': 0.9534177184104919, 'learning_rate': 5.6407127421019534e-05, 'epoch': 39.64}
>>> 2025-08-17 21:14:29,205 - INFO - >>> {'loss': 0.0052, 'grad_norm': 1.8087679147720337, 'learning_rate': 5.614696598422885e-05, 'epoch': 39.8}
>>> 2025-08-17 21:14:35,562 - INFO - >>> {'loss': 0.0071, 'grad_norm': 1.4274920225143433, 'learning_rate': 5.5886635462113804e-05, 'epoch': 39.96}
>>> 2025-08-17 21:14:36,628 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.46047982573509216, 'learning_rate': 5.562614301561704e-05, 'epoch': 40.0}
>>> 2025-08-17 21:14:41,622 - INFO - >>> {'loss': 0.0083, 'grad_norm': 1.2162624597549438, 'learning_rate': 5.536549581013525e-05, 'epoch': 40.16}
>>> 2025-08-17 21:14:46,072 - INFO - >>> {'loss': 0.0073, 'grad_norm': 1.0727481842041016, 'learning_rate': 5.5104701015322125e-05, 'epoch': 40.32}
>>> 2025-08-17 21:14:51,936 - INFO - >>> {'loss': 0.0125, 'grad_norm': 1.3253662586212158, 'learning_rate': 5.48437658048911e-05, 'epoch': 40.48}
>>> 2025-08-17 21:14:55,614 - INFO - >>> {'loss': 0.004, 'grad_norm': 1.319348692893982, 'learning_rate': 5.4582697356418034e-05, 'epoch': 40.64}
>>> 2025-08-17 21:14:58,677 - INFO - >>> {'loss': 0.0068, 'grad_norm': 2.5195086002349854, 'learning_rate': 5.432150285114378e-05, 'epoch': 40.8}
>>> 2025-08-17 21:15:02,158 - INFO - >>> {'loss': 0.0023, 'grad_norm': 1.1865192651748657, 'learning_rate': 5.4060189473776676e-05, 'epoch': 40.96}
>>> 2025-08-17 21:15:02,689 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.2702125906944275, 'learning_rate': 5.379876441229486e-05, 'epoch': 41.0}
>>> 2025-08-17 21:15:05,931 - INFO - >>> {'loss': 0.0026, 'grad_norm': 1.0353777408599854, 'learning_rate': 5.3537234857748584e-05, 'epoch': 41.16}
>>> 2025-08-17 21:15:10,856 - INFO - >>> {'loss': 0.004, 'grad_norm': 1.2623865604400635, 'learning_rate': 5.327560800406241e-05, 'epoch': 41.32}
>>> 2025-08-17 21:15:14,988 - INFO - >>> {'loss': 0.0137, 'grad_norm': 2.3149356842041016, 'learning_rate': 5.30138910478373e-05, 'epoch': 41.48}
>>> 2025-08-17 21:15:18,535 - INFO - >>> {'loss': 0.0057, 'grad_norm': 1.2343584299087524, 'learning_rate': 5.275209118815273e-05, 'epoch': 41.64}
>>> 2025-08-17 21:15:24,497 - INFO - >>> {'loss': 0.0169, 'grad_norm': 1.4609544277191162, 'learning_rate': 5.249021562636857e-05, 'epoch': 41.8}
>>> 2025-08-17 21:15:29,181 - INFO - >>> {'loss': 0.0046, 'grad_norm': 1.2148094177246094, 'learning_rate': 5.222827156592701e-05, 'epoch': 41.96}
>>> 2025-08-17 21:15:30,394 - INFO - >>> {'loss': 0.0192, 'grad_norm': 3.8533072471618652, 'learning_rate': 5.196626621215449e-05, 'epoch': 42.0}
>>> 2025-08-17 21:15:34,503 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.9030823707580566, 'learning_rate': 5.170420677206343e-05, 'epoch': 42.16}
>>> 2025-08-17 21:15:38,307 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.7241603136062622, 'learning_rate': 5.144210045415402e-05, 'epoch': 42.32}
>>> 2025-08-17 21:15:42,854 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.47551342844963074, 'learning_rate': 5.1179954468215915e-05, 'epoch': 42.48}
>>> 2025-08-17 21:15:47,527 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.8536728024482727, 'learning_rate': 5.0917776025129926e-05, 'epoch': 42.64}
>>> 2025-08-17 21:15:53,320 - INFO - >>> {'loss': 0.0068, 'grad_norm': 1.0095829963684082, 'learning_rate': 5.065557233666968e-05, 'epoch': 42.8}
>>> 2025-08-17 21:15:57,485 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.9984426498413086, 'learning_rate': 5.039335061530319e-05, 'epoch': 42.96}
>>> 2025-08-17 21:15:59,541 - INFO - >>> {'loss': 0.0104, 'grad_norm': 1.9479553699493408, 'learning_rate': 5.0131118073994556e-05, 'epoch': 43.0}
>>> 2025-08-17 21:16:02,907 - INFO - >>> {'loss': 0.0041, 'grad_norm': 1.0598315000534058, 'learning_rate': 4.986888192600546e-05, 'epoch': 43.16}
>>> 2025-08-17 21:16:08,680 - INFO - >>> {'loss': 0.0026, 'grad_norm': 1.0923937559127808, 'learning_rate': 4.9606649384696826e-05, 'epoch': 43.32}
>>> 2025-08-17 21:16:12,965 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.826282799243927, 'learning_rate': 4.934442766333034e-05, 'epoch': 43.48}
>>> 2025-08-17 21:16:15,700 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.31947028636932373, 'learning_rate': 4.9082223974870086e-05, 'epoch': 43.64}
>>> 2025-08-17 21:16:20,552 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.4616408348083496, 'learning_rate': 4.8820045531784096e-05, 'epoch': 43.8}
>>> 2025-08-17 21:16:25,488 - INFO - >>> {'loss': 0.0207, 'grad_norm': 1.6428930759429932, 'learning_rate': 4.8557899545846e-05, 'epoch': 43.96}
>>> 2025-08-17 21:16:26,272 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.44890278577804565, 'learning_rate': 4.829579322793659e-05, 'epoch': 44.0}
>>> 2025-08-17 21:16:31,988 - INFO - >>> {'loss': 0.0032, 'grad_norm': 1.3368526697158813, 'learning_rate': 4.8033733787845535e-05, 'epoch': 44.16}
>>> 2025-08-17 21:16:34,975 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.4994266629219055, 'learning_rate': 4.7771728434073e-05, 'epoch': 44.32}
>>> 2025-08-17 21:16:39,267 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.7380144000053406, 'learning_rate': 4.7509784373631444e-05, 'epoch': 44.48}
>>> 2025-08-17 21:16:43,360 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.5401855707168579, 'learning_rate': 4.724790881184727e-05, 'epoch': 44.64}
>>> 2025-08-17 21:16:47,207 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.6817562580108643, 'learning_rate': 4.6986108952162695e-05, 'epoch': 44.8}
>>> 2025-08-17 21:16:52,224 - INFO - >>> {'loss': 0.0099, 'grad_norm': 1.377633810043335, 'learning_rate': 4.6724391995937604e-05, 'epoch': 44.96}
>>> 2025-08-17 21:16:53,034 - INFO - >>> {'loss': 0.008, 'grad_norm': 3.38112211227417, 'learning_rate': 4.646276514225143e-05, 'epoch': 45.0}
>>> 2025-08-17 21:16:56,246 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.5445300340652466, 'learning_rate': 4.6201235587705154e-05, 'epoch': 45.16}
>>> 2025-08-17 21:16:59,936 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.7378478050231934, 'learning_rate': 4.5939810526223336e-05, 'epoch': 45.32}
>>> 2025-08-17 21:17:04,291 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.6580665707588196, 'learning_rate': 4.567849714885623e-05, 'epoch': 45.48}
>>> 2025-08-17 21:17:08,383 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.4603584110736847, 'learning_rate': 4.5417302643581985e-05, 'epoch': 45.64}
>>> 2025-08-17 21:17:13,698 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.4302370846271515, 'learning_rate': 4.5156234195108916e-05, 'epoch': 45.8}
>>> 2025-08-17 21:17:17,851 - INFO - >>> {'loss': 0.0144, 'grad_norm': 1.5240052938461304, 'learning_rate': 4.4895298984677886e-05, 'epoch': 45.96}
>>> 2025-08-17 21:17:19,636 - INFO - >>> {'loss': 0.0061, 'grad_norm': 1.0932419300079346, 'learning_rate': 4.4634504189864765e-05, 'epoch': 46.0}
>>> 2025-08-17 21:17:22,852 - INFO - >>> {'loss': 0.0065, 'grad_norm': 0.675493061542511, 'learning_rate': 4.4373856984382984e-05, 'epoch': 46.16}
>>> 2025-08-17 21:17:27,632 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.4269779920578003, 'learning_rate': 4.4113364537886215e-05, 'epoch': 46.32}
>>> 2025-08-17 21:17:32,059 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.5227886438369751, 'learning_rate': 4.385303401577118e-05, 'epoch': 46.48}
>>> 2025-08-17 21:17:35,324 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.7048608064651489, 'learning_rate': 4.359287257898049e-05, 'epoch': 46.64}
>>> 2025-08-17 21:17:39,788 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.35778915882110596, 'learning_rate': 4.333288738380573e-05, 'epoch': 46.8}
>>> 2025-08-17 21:17:44,082 - INFO - >>> {'loss': 0.0012, 'grad_norm': 1.6703506708145142, 'learning_rate': 4.3073085581690605e-05, 'epoch': 46.96}
>>> 2025-08-17 21:17:44,431 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.281347431903416e-05, 'epoch': 47.0}
>>> 2025-08-17 21:17:48,143 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.306733101606369, 'learning_rate': 4.2554060736994284e-05, 'epoch': 47.16}
>>> 2025-08-17 21:17:51,572 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.678516149520874, 'learning_rate': 4.229485197129122e-05, 'epoch': 47.32}
>>> 2025-08-17 21:17:57,379 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.36595964431762695, 'learning_rate': 4.203585515201131e-05, 'epoch': 47.48}
>>> 2025-08-17 21:18:01,228 - INFO - >>> {'loss': 0.0081, 'grad_norm': 0.866912305355072, 'learning_rate': 4.177707740341087e-05, 'epoch': 47.64}
>>> 2025-08-17 21:18:06,019 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.5142207145690918, 'learning_rate': 4.1518525843720216e-05, 'epoch': 47.8}
>>> 2025-08-17 21:18:11,006 - INFO - >>> {'loss': 0.0027, 'grad_norm': 1.2276042699813843, 'learning_rate': 4.126020758494782e-05, 'epoch': 47.96}
>>> 2025-08-17 21:18:11,358 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.100212973268478e-05, 'epoch': 48.0}
>>> 2025-08-17 21:18:16,705 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.5595277547836304, 'learning_rate': 4.074429938590924e-05, 'epoch': 48.16}
>>> 2025-08-17 21:18:21,424 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.3044368624687195, 'learning_rate': 4.0486723636791234e-05, 'epoch': 48.32}
>>> 2025-08-17 21:18:24,165 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.4707944691181183, 'learning_rate': 4.022940957049751e-05, 'epoch': 48.48}
>>> 2025-08-17 21:18:27,602 - INFO - >>> {'loss': 0.0139, 'grad_norm': 0.9070896506309509, 'learning_rate': 3.9972364264996696e-05, 'epoch': 48.64}
>>> 2025-08-17 21:18:30,819 - INFO - >>> {'loss': 0.0087, 'grad_norm': 0.6389329433441162, 'learning_rate': 3.9715594790864586e-05, 'epoch': 48.8}
>>> 2025-08-17 21:18:35,539 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.6341981887817383, 'learning_rate': 3.945910821108963e-05, 'epoch': 48.96}
>>> 2025-08-17 21:18:36,659 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.4818950891494751, 'learning_rate': 3.920291158087869e-05, 'epoch': 49.0}
>>> 2025-08-17 21:18:40,716 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.563865065574646, 'learning_rate': 3.894701194746291e-05, 'epoch': 49.16}
>>> 2025-08-17 21:18:46,371 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.3082135021686554, 'learning_rate': 3.869141634990399e-05, 'epoch': 49.32}
>>> 2025-08-17 21:18:51,049 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.33492982387542725, 'learning_rate': 3.8436131818900416e-05, 'epoch': 49.48}
>>> 2025-08-17 21:18:54,914 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.7305338382720947, 'learning_rate': 3.818116537659412e-05, 'epoch': 49.64}
>>> 2025-08-17 21:18:59,457 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.6297098994255066, 'learning_rate': 3.7926524036377364e-05, 'epoch': 49.8}
>>> 2025-08-17 21:19:02,375 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.3847680389881134, 'learning_rate': 3.767221480269978e-05, 'epoch': 49.96}
>>> 2025-08-17 21:19:03,133 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.8200938105583191, 'learning_rate': 3.741824467087569e-05, 'epoch': 50.0}
>>> 2025-08-17 21:19:07,719 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.7228386998176575, 'learning_rate': 3.716462062689172e-05, 'epoch': 50.16}
>>> 2025-08-17 21:19:12,485 - INFO - >>> {'loss': 0.0016, 'grad_norm': 1.0188071727752686, 'learning_rate': 3.691134964721462e-05, 'epoch': 50.32}
>>> 2025-08-17 21:19:16,747 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.3089901804924011, 'learning_rate': 3.665843869859934e-05, 'epoch': 50.48}
>>> 2025-08-17 21:19:21,198 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.740172266960144, 'learning_rate': 3.6405894737897414e-05, 'epoch': 50.64}
>>> 2025-08-17 21:19:25,434 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.23809053003787994, 'learning_rate': 3.615372471186562e-05, 'epoch': 50.8}
>>> 2025-08-17 21:19:27,853 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.6630768775939941, 'learning_rate': 3.5901935556974834e-05, 'epoch': 50.96}
>>> 2025-08-17 21:19:28,966 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.5688936114311218, 'learning_rate': 3.5650534199219296e-05, 'epoch': 51.0}
>>> 2025-08-17 21:19:32,176 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.4012785851955414, 'learning_rate': 3.539952755392605e-05, 'epoch': 51.16}
>>> 2025-08-17 21:19:36,171 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.9964006543159485, 'learning_rate': 3.514892252556474e-05, 'epoch': 51.32}
>>> 2025-08-17 21:19:40,669 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.45961993932724, 'learning_rate': 3.489872600755765e-05, 'epoch': 51.48}
>>> 2025-08-17 21:19:44,713 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.2151927500963211, 'learning_rate': 3.464894488209022e-05, 'epoch': 51.64}
>>> 2025-08-17 21:19:49,351 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.2938346862792969, 'learning_rate': 3.4399586019921534e-05, 'epoch': 51.8}
>>> 2025-08-17 21:19:53,339 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.47255682945251465, 'learning_rate': 3.415065628019547e-05, 'epoch': 51.96}
>>> 2025-08-17 21:19:54,196 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.15074051916599274, 'learning_rate': 3.3902162510252e-05, 'epoch': 52.0}
>>> 2025-08-17 21:19:58,982 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.32415953278541565, 'learning_rate': 3.365411154543878e-05, 'epoch': 52.16}
>>> 2025-08-17 21:20:02,586 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.4026709496974945, 'learning_rate': 3.3406510208923224e-05, 'epoch': 52.32}
>>> 2025-08-17 21:20:06,564 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.3492070436477661, 'learning_rate': 3.315936531150473e-05, 'epoch': 52.48}
>>> 2025-08-17 21:20:11,014 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.39531686902046204, 'learning_rate': 3.291268365142738e-05, 'epoch': 52.64}
>>> 2025-08-17 21:20:15,558 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.2763652205467224, 'learning_rate': 3.266647201419294e-05, 'epoch': 52.8}
>>> 2025-08-17 21:20:21,033 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.5182449221611023, 'learning_rate': 3.242073717237418e-05, 'epoch': 52.96}
>>> 2025-08-17 21:20:22,053 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.33522647619247437, 'learning_rate': 3.217548588542864e-05, 'epoch': 53.0}
>>> 2025-08-17 21:20:25,066 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.3119027316570282, 'learning_rate': 3.193072489951263e-05, 'epoch': 53.16}
>>> 2025-08-17 21:20:30,362 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.158472940325737, 'learning_rate': 3.1686460947295695e-05, 'epoch': 53.32}
>>> 2025-08-17 21:20:33,989 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.304156094789505, 'learning_rate': 3.1442700747775414e-05, 'epoch': 53.48}
>>> 2025-08-17 21:20:37,367 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.18697573244571686, 'learning_rate': 3.1199451006092584e-05, 'epoch': 53.64}
>>> 2025-08-17 21:20:42,604 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.4438560903072357, 'learning_rate': 3.095671841334678e-05, 'epoch': 53.8}
>>> 2025-08-17 21:20:45,424 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.41129711270332336, 'learning_rate': 3.0714509646412296e-05, 'epoch': 53.96}
>>> 2025-08-17 21:20:47,209 - INFO - >>> {'loss': 0.0103, 'grad_norm': 1.0863783359527588, 'learning_rate': 3.0472831367754494e-05, 'epoch': 54.0}
>>> 2025-08-17 21:20:49,329 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.3299413025379181, 'learning_rate': 3.0231690225246535e-05, 'epoch': 54.16}
>>> 2025-08-17 21:20:53,644 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.29573535919189453, 'learning_rate': 2.999109285198649e-05, 'epoch': 54.32}
>>> 2025-08-17 21:20:57,310 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.2928832769393921, 'learning_rate': 2.9751045866114922e-05, 'epoch': 54.48}
>>> 2025-08-17 21:21:01,662 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.44270220398902893, 'learning_rate': 2.9511555870632824e-05, 'epoch': 54.64}
>>> 2025-08-17 21:21:05,878 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.19284750521183014, 'learning_rate': 2.927262945321998e-05, 'epoch': 54.8}
>>> 2025-08-17 21:21:11,490 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.34578293561935425, 'learning_rate': 2.9034273186053755e-05, 'epoch': 54.96}
>>> 2025-08-17 21:21:12,929 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.3250220715999603, 'learning_rate': 2.8796493625628356e-05, 'epoch': 55.0}
>>> 2025-08-17 21:21:17,951 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.32891449332237244, 'learning_rate': 2.8559297312574417e-05, 'epoch': 55.16}
>>> 2025-08-17 21:21:22,676 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.5047342777252197, 'learning_rate': 2.832269077147913e-05, 'epoch': 55.32}
>>> 2025-08-17 21:21:26,761 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.3035094738006592, 'learning_rate': 2.8086680510706774e-05, 'epoch': 55.48}
>>> 2025-08-17 21:21:30,961 - INFO - >>> {'loss': 0.0065, 'grad_norm': 0.5967007875442505, 'learning_rate': 2.7851273022219644e-05, 'epoch': 55.64}
>>> 2025-08-17 21:21:34,586 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3656657636165619, 'learning_rate': 2.7616474781399526e-05, 'epoch': 55.8}
>>> 2025-08-17 21:21:39,301 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.36211806535720825, 'learning_rate': 2.7382292246869547e-05, 'epoch': 55.96}
>>> 2025-08-17 21:21:41,537 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.6400649547576904, 'learning_rate': 2.7148731860316546e-05, 'epoch': 56.0}
>>> 2025-08-17 21:21:44,934 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.2759789824485779, 'learning_rate': 2.6915800046313848e-05, 'epoch': 56.16}
>>> 2025-08-17 21:21:48,102 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.6662061810493469, 'learning_rate': 2.6683503212144563e-05, 'epoch': 56.32}
>>> 2025-08-17 21:21:52,393 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.37758195400238037, 'learning_rate': 2.645184774762533e-05, 'epoch': 56.48}
>>> 2025-08-17 21:21:57,677 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.32174891233444214, 'learning_rate': 2.622084002493056e-05, 'epoch': 56.64}
>>> 2025-08-17 21:22:01,398 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.303634911775589, 'learning_rate': 2.599048639841717e-05, 'epoch': 56.8}
>>> 2025-08-17 21:22:06,978 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.5297691226005554, 'learning_rate': 2.5760793204449735e-05, 'epoch': 56.96}
>>> 2025-08-17 21:22:07,843 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.31400367617607117, 'learning_rate': 2.5531766761226272e-05, 'epoch': 57.0}
>>> 2025-08-17 21:22:12,870 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.24325159192085266, 'learning_rate': 2.530341336860439e-05, 'epoch': 57.16}
>>> 2025-08-17 21:22:18,844 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3232419192790985, 'learning_rate': 2.5075739307928014e-05, 'epoch': 57.32}
>>> 2025-08-17 21:22:21,913 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.43053877353668213, 'learning_rate': 2.4848750841854616e-05, 'epoch': 57.48}
>>> 2025-08-17 21:22:26,637 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.3971806764602661, 'learning_rate': 2.4622454214182917e-05, 'epoch': 57.64}
>>> 2025-08-17 21:22:28,964 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.6526781320571899, 'learning_rate': 2.4396855649681166e-05, 'epoch': 57.8}
>>> 2025-08-17 21:22:32,864 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.1311187595129013, 'learning_rate': 2.417196135391591e-05, 'epoch': 57.96}
>>> 2025-08-17 21:22:33,858 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.3037683665752411, 'learning_rate': 2.3947777513081292e-05, 'epoch': 58.0}
>>> 2025-08-17 21:22:37,609 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.5531523823738098, 'learning_rate': 2.372431029382888e-05, 'epoch': 58.16}
>>> 2025-08-17 21:22:42,609 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.18134938180446625, 'learning_rate': 2.350156584309804e-05, 'epoch': 58.32}
>>> 2025-08-17 21:22:45,430 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.44128361344337463, 'learning_rate': 2.327955028794688e-05, 'epoch': 58.48}
>>> 2025-08-17 21:22:49,266 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.40385574102401733, 'learning_rate': 2.305826973538366e-05, 'epoch': 58.64}
>>> 2025-08-17 21:22:53,056 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.19510194659233093, 'learning_rate': 2.2837730272198888e-05, 'epoch': 58.8}
>>> 2025-08-17 21:22:58,882 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.19031380116939545, 'learning_rate': 2.2617937964797785e-05, 'epoch': 58.96}
>>> 2025-08-17 21:22:59,884 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.3273622393608093, 'learning_rate': 2.2398898859033494e-05, 'epoch': 59.0}
>>> 2025-08-17 21:23:04,703 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.2453145533800125, 'learning_rate': 2.2180618980040747e-05, 'epoch': 59.16}
>>> 2025-08-17 21:23:09,387 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.25175201892852783, 'learning_rate': 2.1963104332070127e-05, 'epoch': 59.32}
>>> 2025-08-17 21:23:13,100 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.20956936478614807, 'learning_rate': 2.1746360898322933e-05, 'epoch': 59.48}
>>> 2025-08-17 21:23:18,502 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.3251297175884247, 'learning_rate': 2.1530394640786567e-05, 'epoch': 59.64}
>>> 2025-08-17 21:23:23,436 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.31112316250801086, 'learning_rate': 2.1315211500070558e-05, 'epoch': 59.8}
>>> 2025-08-17 21:23:26,691 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.1318349540233612, 'learning_rate': 2.1100817395243157e-05, 'epoch': 59.96}
>>> 2025-08-17 21:23:27,822 - INFO - >>> {'loss': 0.0083, 'grad_norm': 0.8441581726074219, 'learning_rate': 2.088721822366849e-05, 'epoch': 60.0}
>>> 2025-08-17 21:23:30,952 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.293051153421402, 'learning_rate': 2.0674419860844384e-05, 'epoch': 60.16}
>>> 2025-08-17 21:23:33,592 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.45195290446281433, 'learning_rate': 2.046242816024071e-05, 'epoch': 60.32}
>>> 2025-08-17 21:23:37,797 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.5083240270614624, 'learning_rate': 2.0251248953138374e-05, 'epoch': 60.48}
>>> 2025-08-17 21:23:42,642 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.34483417868614197, 'learning_rate': 2.0040888048468954e-05, 'epoch': 60.64}
>>> 2025-08-17 21:23:47,957 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.3384074866771698, 'learning_rate': 1.9831351232654872e-05, 'epoch': 60.8}
>>> 2025-08-17 21:23:53,908 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.15531040728092194, 'learning_rate': 1.962264426945023e-05, 'epoch': 60.96}
>>> 2025-08-17 21:23:54,730 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.42457348108291626, 'learning_rate': 1.9414772899782276e-05, 'epoch': 61.0}
>>> 2025-08-17 21:23:59,031 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.2229946106672287, 'learning_rate': 1.920774284159353e-05, 'epoch': 61.16}
>>> 2025-08-17 21:24:02,928 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.4587106704711914, 'learning_rate': 1.9001559789684404e-05, 'epoch': 61.32}
>>> 2025-08-17 21:24:08,621 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.4049334228038788, 'learning_rate': 1.8796229415556628e-05, 'epoch': 61.48}
>>> 2025-08-17 21:24:12,849 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.2533396780490875, 'learning_rate': 1.859175736725724e-05, 'epoch': 61.64}
>>> 2025-08-17 21:24:18,270 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.39353951811790466, 'learning_rate': 1.8388149269223153e-05, 'epoch': 61.8}
>>> 2025-08-17 21:24:22,182 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.28583869338035583, 'learning_rate': 1.8185410722126556e-05, 'epoch': 61.96}
>>> 2025-08-17 21:24:23,872 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.24075384438037872, 'learning_rate': 1.798354730272077e-05, 'epoch': 62.0}
>>> 2025-08-17 21:24:26,412 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.3037663996219635, 'learning_rate': 1.7782564563686884e-05, 'epoch': 62.16}
>>> 2025-08-17 21:24:30,331 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3496171236038208, 'learning_rate': 1.7582468033480992e-05, 'epoch': 62.32}
>>> 2025-08-17 21:24:33,724 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.26031333208084106, 'learning_rate': 1.7383263216182157e-05, 'epoch': 62.48}
>>> 2025-08-17 21:24:38,817 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.22582334280014038, 'learning_rate': 1.7184955591340974e-05, 'epoch': 62.64}
>>> 2025-08-17 21:24:43,025 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.4832027852535248, 'learning_rate': 1.6987550613828862e-05, 'epoch': 62.8}
>>> 2025-08-17 21:24:48,001 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.3759043514728546, 'learning_rate': 1.679105371368802e-05, 'epoch': 62.96}
>>> 2025-08-17 21:24:49,595 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3774772584438324, 'learning_rate': 1.6595470295982045e-05, 'epoch': 63.0}
>>> 2025-08-17 21:24:52,241 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.44474563002586365, 'learning_rate': 1.6400805740647267e-05, 'epoch': 63.16}
>>> 2025-08-17 21:24:56,366 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.2855818569660187, 'learning_rate': 1.6207065402344747e-05, 'epoch': 63.32}
>>> 2025-08-17 21:25:02,010 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.23499058187007904, 'learning_rate': 1.6014254610313033e-05, 'epoch': 63.48}
>>> 2025-08-17 21:25:06,461 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.20770736038684845, 'learning_rate': 1.582237866822151e-05, 'epoch': 63.64}
>>> 2025-08-17 21:25:11,354 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.2855657935142517, 'learning_rate': 1.563144285402453e-05, 'epoch': 63.8}
>>> 2025-08-17 21:25:15,632 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.17562948167324066, 'learning_rate': 1.5441452419816237e-05, 'epoch': 63.96}
>>> 2025-08-17 21:25:16,281 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.1962798535823822, 'learning_rate': 1.5252412591686105e-05, 'epoch': 64.0}
>>> 2025-08-17 21:25:21,077 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.22240686416625977, 'learning_rate': 1.5064328569575165e-05, 'epoch': 64.16}
>>> 2025-08-17 21:25:25,151 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.37063249945640564, 'learning_rate': 1.4877205527132982e-05, 'epoch': 64.32}
>>> 2025-08-17 21:25:30,036 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.20925256609916687, 'learning_rate': 1.4691048611575337e-05, 'epoch': 64.48}
>>> 2025-08-17 21:25:33,547 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.30694806575775146, 'learning_rate': 1.4505862943542642e-05, 'epoch': 64.64}
>>> 2025-08-17 21:25:37,138 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.25528284907341003, 'learning_rate': 1.4321653616959097e-05, 'epoch': 64.8}
>>> 2025-08-17 21:25:42,270 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.26719239354133606, 'learning_rate': 1.4138425698892555e-05, 'epoch': 64.96}
>>> 2025-08-17 21:25:43,230 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.4302758276462555, 'learning_rate': 1.3956184229415148e-05, 'epoch': 65.0}
>>> 2025-08-17 21:25:46,604 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.1628751903772354, 'learning_rate': 1.3774934221464642e-05, 'epoch': 65.16}
>>> 2025-08-17 21:25:50,927 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.17398759722709656, 'learning_rate': 1.359468066070657e-05, 'epoch': 65.32}
>>> 2025-08-17 21:25:54,359 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.4978746473789215, 'learning_rate': 1.341542850539706e-05, 'epoch': 65.48}
>>> 2025-08-17 21:25:58,935 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.3128000795841217, 'learning_rate': 1.3237182686246468e-05, 'epoch': 65.64}
>>> 2025-08-17 21:26:02,199 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3444420099258423, 'learning_rate': 1.3059948106283725e-05, 'epoch': 65.8}
>>> 2025-08-17 21:26:08,531 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.18565772473812103, 'learning_rate': 1.2883729640721531e-05, 'epoch': 65.96}
>>> 2025-08-17 21:26:09,397 - INFO - >>> {'loss': 0.0061, 'grad_norm': 0.6479136943817139, 'learning_rate': 1.2708532136822155e-05, 'epoch': 66.0}
>>> 2025-08-17 21:26:13,060 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.20705607533454895, 'learning_rate': 1.2534360413764169e-05, 'epoch': 66.16}
>>> 2025-08-17 21:26:16,997 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.32825711369514465, 'learning_rate': 1.2361219262509883e-05, 'epoch': 66.32}
>>> 2025-08-17 21:26:21,474 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.5383176207542419, 'learning_rate': 1.2189113445673528e-05, 'epoch': 66.48}
>>> 2025-08-17 21:26:25,915 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.19427260756492615, 'learning_rate': 1.2018047697390279e-05, 'epoch': 66.64}
>>> 2025-08-17 21:26:31,020 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.4876630902290344, 'learning_rate': 1.1848026723186012e-05, 'epoch': 66.8}
>>> 2025-08-17 21:26:35,606 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.1216127797961235, 'learning_rate': 1.1679055199847893e-05, 'epoch': 66.96}
>>> 2025-08-17 21:26:36,908 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.4154532849788666, 'learning_rate': 1.1511137775295704e-05, 'epoch': 67.0}
>>> 2025-08-17 21:26:42,273 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.289681077003479, 'learning_rate': 1.1344279068454011e-05, 'epoch': 67.16}
>>> 2025-08-17 21:26:46,069 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.2614549696445465, 'learning_rate': 1.1178483669125112e-05, 'epoch': 67.32}
>>> 2025-08-17 21:26:49,808 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3874301016330719, 'learning_rate': 1.101375613786278e-05, 'epoch': 67.48}
>>> 2025-08-17 21:26:54,086 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.3392510414123535, 'learning_rate': 1.0850101005846786e-05, 'epoch': 67.64}
>>> 2025-08-17 21:26:57,358 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.37197086215019226, 'learning_rate': 1.0687522774758319e-05, 'epoch': 67.8}
>>> 2025-08-17 21:27:02,212 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.5446373224258423, 'learning_rate': 1.0526025916656119e-05, 'epoch': 67.96}
>>> 2025-08-17 21:27:03,325 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.4176971912384033, 'learning_rate': 1.0365614873853462e-05, 'epoch': 68.0}
>>> 2025-08-17 21:27:07,728 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.2263735830783844, 'learning_rate': 1.0206294058795973e-05, 'epoch': 68.16}
>>> 2025-08-17 21:27:11,927 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.45692864060401917, 'learning_rate': 1.0048067853940285e-05, 'epoch': 68.32}
>>> 2025-08-17 21:27:15,326 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.4368167519569397, 'learning_rate': 9.890940611633414e-06, 'epoch': 68.48}
>>> 2025-08-17 21:27:20,835 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.3362939953804016, 'learning_rate': 9.734916653993103e-06, 'epoch': 68.64}
>>> 2025-08-17 21:27:25,287 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.45249682664871216, 'learning_rate': 9.580000272788914e-06, 'epoch': 68.8}
>>> 2025-08-17 21:27:29,735 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.15380768477916718, 'learning_rate': 9.426195729324161e-06, 'epoch': 68.96}
>>> 2025-08-17 21:27:30,082 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.27350725431872e-06, 'epoch': 69.0}
>>> 2025-08-17 21:27:33,967 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.2568818926811218, 'learning_rate': 9.121939047792621e-06, 'epoch': 69.16}
>>> 2025-08-17 21:27:37,852 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.2821698784828186, 'learning_rate': 8.971495278950559e-06, 'epoch': 69.32}
>>> 2025-08-17 21:27:42,680 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.474266916513443, 'learning_rate': 8.82218008606716e-06, 'epoch': 69.48}
>>> 2025-08-17 21:27:47,781 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.4392281472682953, 'learning_rate': 8.673997576373205e-06, 'epoch': 69.64}
>>> 2025-08-17 21:27:52,217 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.17678482830524445, 'learning_rate': 8.526951825942609e-06, 'epoch': 69.8}
>>> 2025-08-17 21:27:56,064 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.3558635711669922, 'learning_rate': 8.381046879580306e-06, 'epoch': 69.96}
>>> 2025-08-17 21:27:57,257 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.5567993521690369, 'learning_rate': 8.23628675071102e-06, 'epoch': 70.0}
>>> 2025-08-17 21:28:02,574 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3666876554489136, 'learning_rate': 8.092675421268826e-06, 'epoch': 70.16}
>>> 2025-08-17 21:28:06,866 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.32670941948890686, 'learning_rate': 7.950216841587638e-06, 'epoch': 70.32}
>>> 2025-08-17 21:28:11,931 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.1998499184846878, 'learning_rate': 7.808914930292543e-06, 'epoch': 70.48}
>>> 2025-08-17 21:28:15,281 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.3062269985675812, 'learning_rate': 7.66877357419204e-06, 'epoch': 70.64}
>>> 2025-08-17 21:28:20,380 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.21196414530277252, 'learning_rate': 7.5297966281710705e-06, 'epoch': 70.8}
>>> 2025-08-17 21:28:23,099 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.5279131531715393, 'learning_rate': 7.391987915085013e-06, 'epoch': 70.96}
>>> 2025-08-17 21:28:24,539 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.4651696979999542, 'learning_rate': 7.255351225654527e-06, 'epoch': 71.0}
>>> 2025-08-17 21:28:29,652 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.1445193737745285, 'learning_rate': 7.119890318361277e-06, 'epoch': 71.16}
>>> 2025-08-17 21:28:32,649 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.22271758317947388, 'learning_rate': 6.98560891934455e-06, 'epoch': 71.32}
>>> 2025-08-17 21:28:35,091 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.23222216963768005, 'learning_rate': 6.852510722298761e-06, 'epoch': 71.48}
>>> 2025-08-17 21:28:40,152 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.27428126335144043, 'learning_rate': 6.72059938837184e-06, 'epoch': 71.64}
>>> 2025-08-17 21:28:45,530 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.18503928184509277, 'learning_rate': 6.589878546064543e-06, 'epoch': 71.8}
>>> 2025-08-17 21:28:49,538 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.27568280696868896, 'learning_rate': 6.46035179113062e-06, 'epoch': 71.96}
>>> 2025-08-17 21:28:50,840 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.31770554184913635, 'learning_rate': 6.332022686477928e-06, 'epoch': 72.0}
>>> 2025-08-17 21:28:52,502 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.3629257082939148, 'learning_rate': 6.204894762070407e-06, 'epoch': 72.16}
>>> 2025-08-17 21:28:57,095 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3607461452484131, 'learning_rate': 6.078971514830989e-06, 'epoch': 72.32}
>>> 2025-08-17 21:29:01,580 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.29655921459198, 'learning_rate': 5.9542564085454165e-06, 'epoch': 72.48}
>>> 2025-08-17 21:29:05,379 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.17799383401870728, 'learning_rate': 5.830752873766948e-06, 'epoch': 72.64}
>>> 2025-08-17 21:29:09,943 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.32534259557724, 'learning_rate': 5.708464307722006e-06, 'epoch': 72.8}
>>> 2025-08-17 21:29:15,310 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.5719350576400757, 'learning_rate': 5.587394074216712e-06, 'epoch': 72.96}
>>> 2025-08-17 21:29:16,089 - INFO - >>> {'loss': 0.0117, 'grad_norm': 1.1908494234085083, 'learning_rate': 5.46754550354438e-06, 'epoch': 73.0}
>>> 2025-08-17 21:29:20,470 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.8118867874145508, 'learning_rate': 5.348921892393904e-06, 'epoch': 73.16}
>>> 2025-08-17 21:29:23,980 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.2829921245574951, 'learning_rate': 5.231526503759054e-06, 'epoch': 73.32}
>>> 2025-08-17 21:29:27,990 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3650405704975128, 'learning_rate': 5.115362566848747e-06, 'epoch': 73.48}
>>> 2025-08-17 21:29:32,368 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.2032015025615692, 'learning_rate': 5.000433276998218e-06, 'epoch': 73.64}
>>> 2025-08-17 21:29:37,137 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.38620221614837646, 'learning_rate': 4.886741795581101e-06, 'epoch': 73.8}
>>> 2025-08-17 21:29:41,992 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.16699668765068054, 'learning_rate': 4.774291249922508e-06, 'epoch': 73.96}
>>> 2025-08-17 21:29:42,753 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.25527316331863403, 'learning_rate': 4.6630847332129575e-06, 'epoch': 74.0}
>>> 2025-08-17 21:29:47,036 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.1838293820619583, 'learning_rate': 4.553125304423339e-06, 'epoch': 74.16}
>>> 2025-08-17 21:29:50,650 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.42751434445381165, 'learning_rate': 4.44441598822074e-06, 'epoch': 74.32}
>>> 2025-08-17 21:29:55,900 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.19698336720466614, 'learning_rate': 4.336959774885241e-06, 'epoch': 74.48}
>>> 2025-08-17 21:29:59,834 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.2897232472896576, 'learning_rate': 4.2307596202276815e-06, 'epoch': 74.64}
>>> 2025-08-17 21:30:03,652 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.17920325696468353, 'learning_rate': 4.1258184455083505e-06, 'epoch': 74.8}
>>> 2025-08-17 21:30:07,566 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.33049699664115906, 'learning_rate': 4.022139137356623e-06, 'epoch': 74.96}
>>> 2025-08-17 21:30:09,004 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.4891146421432495, 'learning_rate': 3.919724547691556e-06, 'epoch': 75.0}
>>> 2025-08-17 21:30:14,348 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.31777507066726685, 'learning_rate': 3.818577493643444e-06, 'epoch': 75.16}
>>> 2025-08-17 21:30:17,874 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.3399280309677124, 'learning_rate': 3.7187007574763232e-06, 'epoch': 75.32}
>>> 2025-08-17 21:30:22,046 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.21371425688266754, 'learning_rate': 3.6200970865114704e-06, 'epoch': 75.48}
>>> 2025-08-17 21:30:25,231 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.27640190720558167, 'learning_rate': 3.522769193051789e-06, 'epoch': 75.64}
>>> 2025-08-17 21:30:30,661 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.14849308133125305, 'learning_rate': 3.426719754307206e-06, 'epoch': 75.8}
>>> 2025-08-17 21:30:33,302 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.2245253473520279, 'learning_rate': 3.331951412321066e-06, 'epoch': 75.96}
>>> 2025-08-17 21:30:35,478 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.5508373379707336, 'learning_rate': 3.2384667738974196e-06, 'epoch': 76.0}
>>> 2025-08-17 21:30:39,687 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.13869619369506836, 'learning_rate': 3.1462684105293293e-06, 'epoch': 76.16}
>>> 2025-08-17 21:30:43,983 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.22093364596366882, 'learning_rate': 3.0553588583281444e-06, 'epoch': 76.32}
>>> 2025-08-17 21:30:48,989 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.3223094344139099, 'learning_rate': 2.965740617953733e-06, 'epoch': 76.48}
>>> 2025-08-17 21:30:53,188 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.3380628824234009, 'learning_rate': 2.877416154545681e-06, 'epoch': 76.64}
>>> 2025-08-17 21:30:56,414 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.141328826546669, 'learning_rate': 2.7903878976555163e-06, 'epoch': 76.8}
>>> 2025-08-17 21:31:02,460 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.27932268381118774, 'learning_rate': 2.7046582411798473e-06, 'epoch': 76.96}
>>> 2025-08-17 21:31:03,481 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.43361201882362366, 'learning_rate': 2.620229543294528e-06, 'epoch': 77.0}
>>> 2025-08-17 21:31:08,114 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.16526344418525696, 'learning_rate': 2.537104126389794e-06, 'epoch': 77.16}
>>> 2025-08-17 21:31:12,183 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.2725640535354614, 'learning_rate': 2.4552842770063757e-06, 'epoch': 77.32}
>>> 2025-08-17 21:31:15,219 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.20869745314121246, 'learning_rate': 2.3747722457725996e-06, 'epoch': 77.48}
>>> 2025-08-17 21:31:19,898 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.23360683023929596, 'learning_rate': 2.2955702473424824e-06, 'epoch': 77.64}
>>> 2025-08-17 21:31:24,692 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.14107178151607513, 'learning_rate': 2.217680460334809e-06, 'epoch': 77.8}
>>> 2025-08-17 21:31:29,035 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.3144178092479706, 'learning_rate': 2.141105027273227e-06, 'epoch': 77.96}
>>> 2025-08-17 21:31:30,882 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.5408238768577576, 'learning_rate': 2.065846054527265e-06, 'epoch': 78.0}
>>> 2025-08-17 21:31:35,283 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.2129574567079544, 'learning_rate': 1.9919056122544465e-06, 'epoch': 78.16}
>>> 2025-08-17 21:31:41,145 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.17208635807037354, 'learning_rate': 1.919285734343307e-06, 'epoch': 78.32}
>>> 2025-08-17 21:31:45,182 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.1915288120508194, 'learning_rate': 1.8479884183574657e-06, 'epoch': 78.48}
>>> 2025-08-17 21:31:49,569 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.34394165873527527, 'learning_rate': 1.7780156254806779e-06, 'epoch': 78.64}
>>> 2025-08-17 21:31:53,059 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.5112475156784058, 'learning_rate': 1.7093692804628635e-06, 'epoch': 78.8}
>>> 2025-08-17 21:31:58,094 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.19873303174972534, 'learning_rate': 1.6420512715672131e-06, 'epoch': 78.96}
>>> 2025-08-17 21:31:59,812 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.4772596061229706, 'learning_rate': 1.5760634505182004e-06, 'epoch': 79.0}
>>> 2025-08-17 21:32:04,374 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.2087588757276535, 'learning_rate': 1.5114076324506565e-06, 'epoch': 79.16}
>>> 2025-08-17 21:32:08,980 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.24680855870246887, 'learning_rate': 1.4480855958598715e-06, 'epoch': 79.32}
>>> 2025-08-17 21:32:13,331 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.19236089289188385, 'learning_rate': 1.3860990825526333e-06, 'epoch': 79.48}
>>> 2025-08-17 21:32:18,097 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.22367054224014282, 'learning_rate': 1.3254497975993264e-06, 'epoch': 79.64}
>>> 2025-08-17 21:32:21,956 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.30871278047561646, 'learning_rate': 1.2661394092870537e-06, 'epoch': 79.8}
>>> 2025-08-17 21:32:24,823 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.19001677632331848, 'learning_rate': 1.2081695490737178e-06, 'epoch': 79.96}
>>> 2025-08-17 21:32:26,494 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3286115229129791, 'learning_rate': 1.1515418115431553e-06, 'epoch': 80.0}
>>> 2025-08-17 21:32:30,402 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.44364967942237854, 'learning_rate': 1.0962577543612795e-06, 'epoch': 80.16}
>>> 2025-08-17 21:32:35,913 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.23697702586650848, 'learning_rate': 1.04231889823323e-06, 'epoch': 80.32}
>>> 2025-08-17 21:32:41,173 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.15033327043056488, 'learning_rate': 9.897267268615284e-07, 'epoch': 80.48}
>>> 2025-08-17 21:32:45,392 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.3253293037414551, 'learning_rate': 9.384826869052898e-07, 'epoch': 80.64}
>>> 2025-08-17 21:32:49,077 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.23589035868644714, 'learning_rate': 8.885881879404201e-07, 'epoch': 80.8}
>>> 2025-08-17 21:32:52,384 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.2723000645637512, 'learning_rate': 8.400446024208309e-07, 'epoch': 80.96}
>>> 2025-08-17 21:32:53,910 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.4642127752304077, 'learning_rate': 7.928532656407029e-07, 'epoch': 81.0}
>>> 2025-08-17 21:32:58,272 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.2997261583805084, 'learning_rate': 7.470154756977543e-07, 'epoch': 81.16}
>>> 2025-08-17 21:33:03,288 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.2690511643886566, 'learning_rate': 7.025324934575139e-07, 'epoch': 81.32}
>>> 2025-08-17 21:33:08,107 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.2252957969903946, 'learning_rate': 6.594055425186763e-07, 'epoch': 81.48}
>>> 2025-08-17 21:33:11,801 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.21228760480880737, 'learning_rate': 6.176358091794011e-07, 'epoch': 81.64}
>>> 2025-08-17 21:33:17,221 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.20543695986270905, 'learning_rate': 5.772244424047169e-07, 'epoch': 81.8}
>>> 2025-08-17 21:33:21,297 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.25530025362968445, 'learning_rate': 5.381725537948856e-07, 'epoch': 81.96}
>>> 2025-08-17 21:33:22,078 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.20959070324897766, 'learning_rate': 5.004812175548656e-07, 'epoch': 82.0}
>>> 2025-08-17 21:33:25,629 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.3095237910747528, 'learning_rate': 4.641514704647132e-07, 'epoch': 82.16}
>>> 2025-08-17 21:33:30,090 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.18079935014247894, 'learning_rate': 4.2918431185110517e-07, 'epoch': 82.32}
>>> 2025-08-17 21:33:35,005 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.37668752670288086, 'learning_rate': 3.9558070355983357e-07, 'epoch': 82.48}
>>> 2025-08-17 21:33:39,815 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.2501738667488098, 'learning_rate': 3.6334156992935406e-07, 'epoch': 82.64}
>>> 2025-08-17 21:33:44,505 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.26342466473579407, 'learning_rate': 3.324677977653401e-07, 'epoch': 82.8}
>>> 2025-08-17 21:33:49,356 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.15438595414161682, 'learning_rate': 3.0296023631631865e-07, 'epoch': 82.96}
>>> 2025-08-17 21:33:50,387 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.20328445732593536, 'learning_rate': 2.748196972502892e-07, 'epoch': 83.0}
>>> 2025-08-17 21:33:55,667 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.2857784926891327, 'learning_rate': 2.4804695463240826e-07, 'epoch': 83.16}
>>> 2025-08-17 21:33:59,462 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.22093813121318817, 'learning_rate': 2.226427449036894e-07, 'epoch': 83.32}
>>> 2025-08-17 21:34:03,530 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.2080431878566742, 'learning_rate': 1.9860776686075332e-07, 'epoch': 83.48}
>>> 2025-08-17 21:34:07,074 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.2380305826663971, 'learning_rate': 1.7594268163659278e-07, 'epoch': 83.64}
>>> 2025-08-17 21:34:13,014 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.1533767282962799, 'learning_rate': 1.546481126824151e-07, 'epoch': 83.8}
>>> 2025-08-17 21:34:18,459 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.34528738260269165, 'learning_rate': 1.347246457504503e-07, 'epoch': 83.96}
>>> 2025-08-17 21:34:18,812 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.1617282887787518e-07, 'epoch': 84.0}
>>> 2025-08-17 21:34:24,195 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.28077933192253113, 'learning_rate': 9.899317237172523e-08, 'epoch': 84.16}
>>> 2025-08-17 21:34:27,948 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.25527438521385193, 'learning_rate': 8.318614879485043e-08, 'epoch': 84.32}
>>> 2025-08-17 21:34:31,359 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.23886224627494812, 'learning_rate': 6.875219295293111e-08, 'epoch': 84.48}
>>> 2025-08-17 21:34:35,053 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.30305609107017517, 'learning_rate': 5.569170188250983e-08, 'epoch': 84.64}
>>> 2025-08-17 21:34:40,641 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.2132035195827484, 'learning_rate': 4.400503484006113e-08, 'epoch': 84.8}
>>> 2025-08-17 21:34:44,787 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.2183547019958496, 'learning_rate': 3.369251329213285e-08, 'epoch': 84.96}
>>> 2025-08-17 21:34:45,673 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.26122137904167175, 'learning_rate': 2.4754420906475396e-08, 'epoch': 85.0}
>>> 2025-08-17 21:34:50,032 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.1381649225950241, 'learning_rate': 1.7191003544259064e-08, 'epoch': 85.16}
>>> 2025-08-17 21:34:54,815 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.12734577059745789, 'learning_rate': 1.100246925331283e-08, 'epoch': 85.32}
>>> 2025-08-17 21:34:58,922 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.3476632833480835, 'learning_rate': 6.188988262373352e-09, 'epoch': 85.48}
>>> 2025-08-17 21:35:02,501 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.29520854353904724, 'learning_rate': 2.750692976444258e-09, 'epoch': 85.64}
>>> 2025-08-17 21:35:08,387 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.22291423380374908, 'learning_rate': 6.876779731213035e-10, 'epoch': 85.8}
>>> 2025-08-17 21:35:09,252 - INFO - >>> {'train_runtime': 2380.7725, 'train_samples_per_second': 4.2, 'train_steps_per_second': 0.252, 'train_loss': 0.15632231111017367, 'epoch': 85.8}
>>> 2025-08-17 21:35:09,254 - INFO - 训练成功！
>>> 2025-08-17 21:35:11,300 - INFO - 模型存放位置：./output/qwen202508172055
>>> 2025-08-18 22:00:17,675 - INFO - 正在读取配置文件: config.yaml
>>> 2025-08-18 22:00:20,132 - INFO - 从配置文件中提取到模型路径: /home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-18 22:00:23,983 - INFO - 开始加载模型配置文件...
>>> 2025-08-18 22:00:26,450 - INFO - 正在加载模型: /home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-18 22:00:43,442 - INFO - 模型加载完成
>>> 2025-08-18 22:00:45,791 - INFO - ============================================================
>>> 2025-08-18 22:00:49,763 - INFO - 模型详细信息
>>> 2025-08-18 22:00:52,111 - INFO - ============================================================
>>> 2025-08-18 22:00:56,084 - INFO - 模型路径: /home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-18 22:01:00,026 - INFO - 模型类型: qwen2
>>> 2025-08-18 22:01:02,524 - INFO - 模型架构: ['Qwen2ForCausalLM']
>>> 2025-08-18 22:01:05,473 - INFO - ----------------------------------------
>>> 2025-08-18 22:01:08,843 - INFO - 参数信息:
>>> 2025-08-18 22:01:11,161 - INFO -   总参数量: 7.62B (7,615,616,512)
>>> 2025-08-18 22:01:14,200 - INFO -   可训练参数: 7.62B (7,615,616,512)
>>> 2025-08-18 22:01:17,270 - INFO -   冻结参数: 0 (0)
>>> 2025-08-18 22:01:19,828 - INFO -   可训练参数比例: 100.00%
>>> 2025-08-18 22:01:22,536 - INFO - ----------------------------------------
>>> 2025-08-18 22:01:25,907 - INFO - 模型结构信息:
>>> 2025-08-18 22:01:28,285 - INFO -   层数: 28
>>> 2025-08-18 22:01:30,693 - INFO -   隐藏层大小: 3584
>>> 2025-08-18 22:01:33,252 - INFO -   注意力头数: 28
>>> 2025-08-18 22:01:35,750 - INFO -   总层数量: 339
>>> 2025-08-18 22:01:38,248 - INFO -   可训练层数量: 339
>>> 2025-08-18 22:01:40,807 - INFO - ----------------------------------------
>>> 2025-08-18 22:01:44,177 - INFO - 特殊Token:
>>> 2025-08-18 22:01:46,585 - INFO -   bos_token: <｜begin▁of▁sentence｜>
>>> 2025-08-18 22:01:49,774 - INFO -   eos_token: <｜end▁of▁sentence｜>
>>> 2025-08-18 22:01:52,904 - INFO -   unk_token: None
>>> 2025-08-18 22:01:55,582 - INFO -   pad_token: <｜end▁of▁sentence｜>
>>> 2025-08-18 22:01:58,711 - INFO -   sep_token: None
>>> 2025-08-18 22:02:01,389 - INFO -   mask_token: None
>>> 2025-08-18 22:02:04,097 - INFO -   vocab_size: 151643
>>> 2025-08-18 22:02:06,866 - INFO - ----------------------------------------
>>> 2025-08-18 22:02:10,236 - INFO - 量化信息:
>>> 2025-08-18 22:02:12,553 - INFO -   数据类型: torch.float32
>>> 2025-08-18 22:02:15,351 - INFO -   量化方式: None
>>> 2025-08-18 22:02:17,879 - INFO - ----------------------------------------
>>> 2025-08-18 22:02:21,249 - INFO - 可训练层 (前10个):
>>> 2025-08-18 22:02:23,778 - INFO -   1. model.embed_tokens.weight
>>> 2025-08-18 22:02:26,847 - INFO -   2. model.layers.0.self_attn.q_proj.weight
>>> 2025-08-18 22:02:30,308 - INFO -   3. model.layers.0.self_attn.q_proj.bias
>>> 2025-08-18 22:02:33,708 - INFO -   4. model.layers.0.self_attn.k_proj.weight
>>> 2025-08-18 22:02:37,169 - INFO -   5. model.layers.0.self_attn.k_proj.bias
>>> 2025-08-18 22:02:40,569 - INFO -   6. model.layers.0.self_attn.v_proj.weight
>>> 2025-08-18 22:02:44,029 - INFO -   7. model.layers.0.self_attn.v_proj.bias
>>> 2025-08-18 22:02:47,438 - INFO -   8. model.layers.0.self_attn.o_proj.weight
>>> 2025-08-18 22:02:50,898 - INFO -   9. model.layers.0.mlp.gate_proj.weight
>>> 2025-08-18 22:02:54,268 - INFO -   10. model.layers.0.mlp.up_proj.weight
>>> 2025-08-18 22:02:57,610 - INFO -   ... 还有 329 个可训练层
>>> 2025-08-18 22:03:50,315 - INFO - 正在读取配置文件: config.yaml
>>> 2025-08-18 22:03:50,322 - INFO - 从配置文件中提取到模型路径: /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-18 22:03:50,322 - INFO - 开始加载模型配置文件...
>>> 2025-08-18 22:03:50,323 - INFO - 正在加载模型: /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-18 22:04:03,517 - INFO - 模型加载完成
>>> 2025-08-18 22:04:03,588 - INFO - ============================================================
>>> 2025-08-18 22:04:03,589 - INFO - 模型详细信息
>>> 2025-08-18 22:04:03,589 - INFO - ============================================================
>>> 2025-08-18 22:04:03,589 - INFO - 模型路径: /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-18 22:04:03,590 - INFO - 模型类型: qwen2
>>> 2025-08-18 22:04:03,590 - INFO - 模型架构: ['Qwen2ForCausalLM']
>>> 2025-08-18 22:04:03,591 - INFO - ----------------------------------------
>>> 2025-08-18 22:04:03,591 - INFO - 参数信息:
>>> 2025-08-18 22:04:03,591 - INFO -   总参数量: 7.62B (7,615,616,512)
>>> 2025-08-18 22:04:03,592 - INFO -   可训练参数: 7.62B (7,615,616,512)
>>> 2025-08-18 22:04:03,592 - INFO -   冻结参数: 0 (0)
>>> 2025-08-18 22:04:03,593 - INFO -   可训练参数比例: 100.00%
>>> 2025-08-18 22:04:03,593 - INFO - ----------------------------------------
>>> 2025-08-18 22:04:03,593 - INFO - 模型结构信息:
>>> 2025-08-18 22:04:03,594 - INFO -   层数: 28
>>> 2025-08-18 22:04:03,594 - INFO -   隐藏层大小: 3584
>>> 2025-08-18 22:04:03,594 - INFO -   注意力头数: 28
>>> 2025-08-18 22:04:03,595 - INFO -   总层数量: 339
>>> 2025-08-18 22:04:03,595 - INFO -   可训练层数量: 339
>>> 2025-08-18 22:04:03,595 - INFO - ----------------------------------------
>>> 2025-08-18 22:04:03,596 - INFO - 特殊Token:
>>> 2025-08-18 22:04:03,596 - INFO -   bos_token: None
>>> 2025-08-18 22:04:03,596 - INFO -   eos_token: <|endoftext|>
>>> 2025-08-18 22:04:03,597 - INFO -   unk_token: None
>>> 2025-08-18 22:04:03,597 - INFO -   pad_token: <|endoftext|>
>>> 2025-08-18 22:04:03,598 - INFO -   sep_token: None
>>> 2025-08-18 22:04:03,598 - INFO -   mask_token: None
>>> 2025-08-18 22:04:03,598 - INFO -   vocab_size: 151643
>>> 2025-08-18 22:04:03,599 - INFO - ----------------------------------------
>>> 2025-08-18 22:04:03,599 - INFO - 量化信息:
>>> 2025-08-18 22:04:03,599 - INFO -   数据类型: torch.float32
>>> 2025-08-18 22:04:03,600 - INFO -   量化方式: None
>>> 2025-08-18 22:04:03,600 - INFO - ----------------------------------------
>>> 2025-08-18 22:04:03,601 - INFO - 可训练层 (前10个):
>>> 2025-08-18 22:04:03,601 - INFO -   1. model.embed_tokens.weight
>>> 2025-08-18 22:04:03,601 - INFO -   2. model.layers.0.self_attn.q_proj.weight
>>> 2025-08-18 22:04:03,602 - INFO -   3. model.layers.0.self_attn.q_proj.bias
>>> 2025-08-18 22:04:03,602 - INFO -   4. model.layers.0.self_attn.k_proj.weight
>>> 2025-08-18 22:04:03,603 - INFO -   5. model.layers.0.self_attn.k_proj.bias
>>> 2025-08-18 22:04:03,603 - INFO -   6. model.layers.0.self_attn.v_proj.weight
>>> 2025-08-18 22:04:03,604 - INFO -   7. model.layers.0.self_attn.v_proj.bias
>>> 2025-08-18 22:04:03,604 - INFO -   8. model.layers.0.self_attn.o_proj.weight
>>> 2025-08-18 22:04:03,605 - INFO -   9. model.layers.0.mlp.gate_proj.weight
>>> 2025-08-18 22:04:03,605 - INFO -   10. model.layers.0.mlp.up_proj.weight
>>> 2025-08-18 22:04:03,605 - INFO -   ... 还有 329 个可训练层
>>> 2025-08-18 22:06:47,941 - INFO - 正在读取配置文件: config.yaml
>>> 2025-08-18 22:06:47,949 - INFO - 从配置文件中提取到模型路径: /home/liangshuqiao/models/qwen3
>>> 2025-08-18 22:06:47,949 - INFO - 开始加载模型配置文件...
>>> 2025-08-18 22:06:47,949 - INFO - 正在加载模型: /home/liangshuqiao/models/qwen3
>>> 2025-08-18 22:14:06,087 - INFO - 模型加载完成
>>> 2025-08-18 22:14:06,200 - INFO - ============================================================
>>> 2025-08-18 22:14:06,200 - INFO - 模型详细信息
>>> 2025-08-18 22:14:06,201 - INFO - ============================================================
>>> 2025-08-18 22:14:06,201 - INFO - 模型路径: /home/liangshuqiao/models/qwen3
>>> 2025-08-18 22:14:06,202 - INFO - 模型类型: qwen3
>>> 2025-08-18 22:14:06,202 - INFO - 模型架构: ['Qwen3ForCausalLM']
>>> 2025-08-18 22:14:06,202 - INFO - ----------------------------------------
>>> 2025-08-18 22:14:06,203 - INFO - 参数信息:
>>> 2025-08-18 22:14:06,203 - INFO -   总参数量: 32.76B (32,762,123,264)
>>> 2025-08-18 22:14:06,204 - INFO -   可训练参数: 32.76B (32,762,123,264)
>>> 2025-08-18 22:14:06,204 - INFO -   冻结参数: 0 (0)
>>> 2025-08-18 22:14:06,204 - INFO -   可训练参数比例: 100.00%
>>> 2025-08-18 22:14:06,205 - INFO - ----------------------------------------
>>> 2025-08-18 22:14:06,205 - INFO - 模型结构信息:
>>> 2025-08-18 22:14:06,205 - INFO -   层数: 64
>>> 2025-08-18 22:14:06,206 - INFO -   隐藏层大小: 5120
>>> 2025-08-18 22:14:06,206 - INFO -   注意力头数: 64
>>> 2025-08-18 22:14:06,207 - INFO -   总层数量: 707
>>> 2025-08-18 22:14:06,207 - INFO -   可训练层数量: 707
>>> 2025-08-18 22:14:06,207 - INFO - ----------------------------------------
>>> 2025-08-18 22:14:06,208 - INFO - 特殊Token:
>>> 2025-08-18 22:14:06,208 - INFO -   bos_token: None
>>> 2025-08-18 22:14:06,209 - INFO -   eos_token: <|im_end|>
>>> 2025-08-18 22:14:06,209 - INFO -   unk_token: None
>>> 2025-08-18 22:14:06,209 - INFO -   pad_token: <|endoftext|>
>>> 2025-08-18 22:14:06,210 - INFO -   sep_token: None
>>> 2025-08-18 22:14:06,210 - INFO -   mask_token: None
>>> 2025-08-18 22:14:06,211 - INFO -   vocab_size: 151643
>>> 2025-08-18 22:14:06,211 - INFO - ----------------------------------------
>>> 2025-08-18 22:14:06,211 - INFO - 量化信息:
>>> 2025-08-18 22:14:06,212 - INFO -   数据类型: torch.float32
>>> 2025-08-18 22:14:06,212 - INFO -   量化方式: None
>>> 2025-08-18 22:14:06,212 - INFO - ----------------------------------------
>>> 2025-08-18 22:14:06,213 - INFO - 可训练层 (前10个):
>>> 2025-08-18 22:14:06,213 - INFO -   1. model.embed_tokens.weight
>>> 2025-08-18 22:14:06,214 - INFO -   2. model.layers.0.self_attn.q_proj.weight
>>> 2025-08-18 22:14:06,214 - INFO -   3. model.layers.0.self_attn.k_proj.weight
>>> 2025-08-18 22:14:06,215 - INFO -   4. model.layers.0.self_attn.v_proj.weight
>>> 2025-08-18 22:14:06,215 - INFO -   5. model.layers.0.self_attn.o_proj.weight
>>> 2025-08-18 22:14:06,215 - INFO -   6. model.layers.0.self_attn.q_norm.weight
>>> 2025-08-18 22:14:06,216 - INFO -   7. model.layers.0.self_attn.k_norm.weight
>>> 2025-08-18 22:14:06,216 - INFO -   8. model.layers.0.mlp.gate_proj.weight
>>> 2025-08-18 22:14:06,217 - INFO -   9. model.layers.0.mlp.up_proj.weight
>>> 2025-08-18 22:14:06,217 - INFO -   10. model.layers.0.mlp.down_proj.weight
>>> 2025-08-18 22:14:06,218 - INFO -   ... 还有 697 个可训练层
>>> 2025-08-18 22:22:37,273 - INFO - 正在读取配置文件: config.yaml
>>> 2025-08-18 22:22:37,306 - INFO - 从配置文件中提取到模型路径: /home/liangshuqiao/models/qwen3-8b
>>> 2025-08-18 22:22:37,307 - INFO - 开始加载模型配置文件...
>>> 2025-08-18 22:22:37,307 - INFO - 正在加载模型: /home/liangshuqiao/models/qwen3-8b
>>> 2025-08-18 22:22:51,741 - INFO - 模型加载完成
>>> 2025-08-18 22:22:51,803 - INFO - ============================================================
>>> 2025-08-18 22:22:51,804 - INFO - 模型详细信息
>>> 2025-08-18 22:22:51,804 - INFO - ============================================================
>>> 2025-08-18 22:22:51,805 - INFO - 模型路径: /home/liangshuqiao/models/qwen3-8b
>>> 2025-08-18 22:22:51,805 - INFO - 模型类型: qwen3
>>> 2025-08-18 22:22:51,805 - INFO - 模型架构: ['Qwen3ForCausalLM']
>>> 2025-08-18 22:22:51,806 - INFO - ----------------------------------------
>>> 2025-08-18 22:22:51,806 - INFO - 参数信息:
>>> 2025-08-18 22:22:51,807 - INFO -   总参数量: 8.19B (8,190,735,360)
>>> 2025-08-18 22:22:51,807 - INFO -   可训练参数: 8.19B (8,190,735,360)
>>> 2025-08-18 22:22:51,807 - INFO -   冻结参数: 0 (0)
>>> 2025-08-18 22:22:51,808 - INFO -   可训练参数比例: 100.00%
>>> 2025-08-18 22:22:51,808 - INFO - ----------------------------------------
>>> 2025-08-18 22:22:51,808 - INFO - 模型结构信息:
>>> 2025-08-18 22:22:51,809 - INFO -   层数: 36
>>> 2025-08-18 22:22:51,809 - INFO -   隐藏层大小: 4096
>>> 2025-08-18 22:22:51,809 - INFO -   注意力头数: 32
>>> 2025-08-18 22:22:51,810 - INFO -   总层数量: 399
>>> 2025-08-18 22:22:51,810 - INFO -   可训练层数量: 399
>>> 2025-08-18 22:22:51,810 - INFO - ----------------------------------------
>>> 2025-08-18 22:22:51,811 - INFO - 特殊Token:
>>> 2025-08-18 22:22:51,811 - INFO -   bos_token: None
>>> 2025-08-18 22:22:51,811 - INFO -   eos_token: <|im_end|>
>>> 2025-08-18 22:22:51,812 - INFO -   unk_token: None
>>> 2025-08-18 22:22:51,812 - INFO -   pad_token: <|endoftext|>
>>> 2025-08-18 22:22:51,812 - INFO -   sep_token: None
>>> 2025-08-18 22:22:51,813 - INFO -   mask_token: None
>>> 2025-08-18 22:22:51,813 - INFO -   vocab_size: 151643
>>> 2025-08-18 22:22:51,813 - INFO - ----------------------------------------
>>> 2025-08-18 22:22:51,814 - INFO - 量化信息:
>>> 2025-08-18 22:22:51,814 - INFO -   数据类型: torch.float32
>>> 2025-08-18 22:22:51,814 - INFO -   量化方式: None
>>> 2025-08-18 22:22:51,815 - INFO - ----------------------------------------
>>> 2025-08-18 22:22:51,815 - INFO - 可训练层 (前10个):
>>> 2025-08-18 22:22:51,815 - INFO -   1. model.embed_tokens.weight
>>> 2025-08-18 22:22:51,816 - INFO -   2. model.layers.0.self_attn.q_proj.weight
>>> 2025-08-18 22:22:51,816 - INFO -   3. model.layers.0.self_attn.k_proj.weight
>>> 2025-08-18 22:22:51,817 - INFO -   4. model.layers.0.self_attn.v_proj.weight
>>> 2025-08-18 22:22:51,817 - INFO -   5. model.layers.0.self_attn.o_proj.weight
>>> 2025-08-18 22:22:51,817 - INFO -   6. model.layers.0.self_attn.q_norm.weight
>>> 2025-08-18 22:22:51,818 - INFO -   7. model.layers.0.self_attn.k_norm.weight
>>> 2025-08-18 22:22:51,818 - INFO -   8. model.layers.0.mlp.gate_proj.weight
>>> 2025-08-18 22:22:51,818 - INFO -   9. model.layers.0.mlp.up_proj.weight
>>> 2025-08-18 22:22:51,819 - INFO -   10. model.layers.0.mlp.down_proj.weight
>>> 2025-08-18 22:22:51,819 - INFO -   ... 还有 389 个可训练层
>>> 2025-08-18 22:25:56,846 - INFO - 正在读取配置文件: config.yaml
>>> 2025-08-18 22:25:56,853 - INFO - 从配置文件中提取到模型路径: /home/liangshuqiao/models/gemma-3
>>> 2025-08-18 22:25:56,854 - INFO - 开始加载模型配置文件...
>>> 2025-08-18 22:25:56,854 - INFO - 正在加载模型: /home/liangshuqiao/models/gemma-3
>>> 2025-08-18 22:30:09,090 - INFO - 模型加载完成
>>> 2025-08-18 22:30:09,226 - INFO - ============================================================
>>> 2025-08-18 22:30:09,227 - INFO - 模型详细信息
>>> 2025-08-18 22:30:09,227 - INFO - ============================================================
>>> 2025-08-18 22:30:09,228 - INFO - 模型路径: /home/liangshuqiao/models/gemma-3
>>> 2025-08-18 22:30:09,228 - INFO - 模型类型: gemma3
>>> 2025-08-18 22:30:09,229 - INFO - 模型架构: ['Gemma3ForConditionalGeneration']
>>> 2025-08-18 22:30:09,229 - INFO - ----------------------------------------
>>> 2025-08-18 22:30:09,229 - INFO - 参数信息:
>>> 2025-08-18 22:30:09,230 - INFO -   总参数量: 27.43B (27,432,406,640)
>>> 2025-08-18 22:30:09,230 - INFO -   可训练参数: 27.43B (27,432,406,640)
>>> 2025-08-18 22:30:09,231 - INFO -   冻结参数: 0 (0)
>>> 2025-08-18 22:30:09,231 - INFO -   可训练参数比例: 100.00%
>>> 2025-08-18 22:30:09,231 - INFO - ----------------------------------------
>>> 2025-08-18 22:30:09,232 - INFO - 模型结构信息:
>>> 2025-08-18 22:30:09,232 - INFO -   层数: unknown
>>> 2025-08-18 22:30:09,232 - INFO -   隐藏层大小: unknown
>>> 2025-08-18 22:30:09,233 - INFO -   注意力头数: unknown
>>> 2025-08-18 22:30:09,233 - INFO -   总层数量: 1247
>>> 2025-08-18 22:30:09,233 - INFO -   可训练层数量: 1247
>>> 2025-08-18 22:30:09,234 - INFO - ----------------------------------------
>>> 2025-08-18 22:30:09,234 - INFO - 特殊Token:
>>> 2025-08-18 22:30:09,235 - INFO -   bos_token: <bos>
>>> 2025-08-18 22:30:09,235 - INFO -   eos_token: <eos>
>>> 2025-08-18 22:30:09,235 - INFO -   unk_token: <unk>
>>> 2025-08-18 22:30:09,236 - INFO -   pad_token: <pad>
>>> 2025-08-18 22:30:09,236 - INFO -   sep_token: None
>>> 2025-08-18 22:30:09,236 - INFO -   mask_token: None
>>> 2025-08-18 22:30:09,237 - INFO -   vocab_size: 262144
>>> 2025-08-18 22:30:09,237 - INFO - ----------------------------------------
>>> 2025-08-18 22:30:09,237 - INFO - 量化信息:
>>> 2025-08-18 22:30:09,238 - INFO -   数据类型: torch.float32
>>> 2025-08-18 22:30:09,238 - INFO -   量化方式: None
>>> 2025-08-18 22:30:09,238 - INFO - ----------------------------------------
>>> 2025-08-18 22:30:09,239 - INFO - 可训练层 (前10个):
>>> 2025-08-18 22:30:09,239 - INFO -   1. vision_tower.vision_model.embeddings.patch_embedding.weight
>>> 2025-08-18 22:30:09,240 - INFO -   2. vision_tower.vision_model.embeddings.patch_embedding.bias
>>> 2025-08-18 22:30:09,240 - INFO -   3. vision_tower.vision_model.embeddings.position_embedding.weight
>>> 2025-08-18 22:30:09,241 - INFO -   4. vision_tower.vision_model.encoder.layers.0.layer_norm1.weight
>>> 2025-08-18 22:30:09,241 - INFO -   5. vision_tower.vision_model.encoder.layers.0.layer_norm1.bias
>>> 2025-08-18 22:30:09,242 - INFO -   6. vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight
>>> 2025-08-18 22:30:09,242 - INFO -   7. vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias
>>> 2025-08-18 22:30:09,243 - INFO -   8. vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight
>>> 2025-08-18 22:30:09,244 - INFO -   9. vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias
>>> 2025-08-18 22:30:09,244 - INFO -   10. vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight
>>> 2025-08-18 22:30:09,245 - INFO -   ... 还有 1237 个可训练层
>>> 2025-08-18 22:35:55,897 - INFO - ========__main__  202508182235========
>>> 2025-08-18 22:35:55,898 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-18 22:35:55,898 - INFO - 开始进行训练
>>> 2025-08-18 22:36:06,297 - INFO - 导入包完成
>>> 2025-08-18 22:36:06,317 - INFO - 配置文件读取完成
>>> 2025-08-18 22:36:06,317 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-18 22:36:06,317 - INFO - 模型路径:/home/liangshuqiao/models/Qwen2-7B
>>> 2025-08-18 22:36:06,649 - INFO - tokenizer读取完成
>>> 2025-08-18 22:36:33,590 - INFO - model dtype:torch.float16
>>> 2025-08-18 22:36:33,591 - INFO - 模型导入完成
>>> 2025-08-18 22:36:34,610 - INFO - 读取数据集成功
>>> 2025-08-18 22:36:39,381 - INFO - 数据处理成功
>>> 2025-08-18 22:36:55,264 - INFO - 开始训练！
>>> 2025-08-18 22:36:55,265 - INFO - 批次大小  : 4
>>> 2025-08-18 22:36:55,266 - INFO - 训练轮数  : 100
>>> 2025-08-18 22:36:55,267 - INFO - 学习率    : 0.0001
>>> 2025-08-18 22:36:55,267 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-18 22:36:55,268 - INFO - 模型路径  : /home/liangshuqiao/models/Qwen2-7B
>>> 2025-08-18 22:37:00,306 - INFO - >>> {'loss': 2.7863, 'grad_norm': 1.0161962509155273, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-18 22:37:30,697 - INFO - ========__main__  202508182237========
>>> 2025-08-18 22:37:30,697 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-18 22:37:30,698 - INFO - 开始进行训练
>>> 2025-08-18 22:37:33,102 - INFO - 导入包完成
>>> 2025-08-18 22:37:33,118 - INFO - 配置文件读取完成
>>> 2025-08-18 22:37:33,118 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-18 22:37:33,119 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-18 22:37:33,749 - INFO - tokenizer读取完成
>>> 2025-08-18 22:38:47,673 - INFO - model dtype:torch.float16
>>> 2025-08-18 22:38:47,674 - INFO - 模型导入完成
>>> 2025-08-18 22:38:49,380 - INFO - 读取数据集成功
>>> 2025-08-18 22:38:49,606 - INFO - 数据处理成功
>>> 2025-08-18 22:39:04,300 - INFO - 开始训练！
>>> 2025-08-18 22:39:04,300 - INFO - 批次大小  : 4
>>> 2025-08-18 22:39:04,301 - INFO - 训练轮数  : 100
>>> 2025-08-18 22:39:04,301 - INFO - 学习率    : 0.0001
>>> 2025-08-18 22:39:04,302 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-18 22:39:04,302 - INFO - 模型路径  : /home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-18 22:39:08,854 - INFO - >>> {'loss': 4.3327, 'grad_norm': 1.2303386926651, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-18 22:41:49,894 - INFO - ========__main__  202508182241========
>>> 2025-08-18 22:41:49,895 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-18 22:41:49,896 - INFO - 开始进行模型测试
>>> 2025-08-18 22:42:27,310 - INFO - ========train Qwen2ForCausalLM  202508182242========
>>> 2025-08-18 22:42:27,311 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-18 22:42:27,311 - INFO - 开始进行训练
>>> 2025-08-18 22:42:29,717 - INFO - 导入包完成
>>> 2025-08-18 22:42:29,733 - INFO - 配置文件读取完成
>>> 2025-08-18 22:42:29,734 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-18 22:42:29,734 - INFO - 模型路径:/home/liangshuqiao/models/qwen3
>>> 2025-08-18 22:42:30,166 - INFO - tokenizer读取完成
>>> 2025-08-18 22:49:26,420 - INFO - model dtype:torch.float16
>>> 2025-08-18 22:49:26,421 - INFO - 模型导入完成
>>> 2025-08-18 22:49:27,860 - INFO - 读取数据集成功
>>> 2025-08-18 22:49:32,684 - INFO - 数据处理成功
>>> 2025-08-18 22:50:32,345 - INFO - 正在读取配置文件: config.yaml
>>> 2025-08-18 22:50:32,352 - INFO - 从配置文件中提取到模型路径: /home/liangshuqiao/models/qwen3
>>> 2025-08-18 22:50:32,352 - INFO - 开始加载模型配置文件...
>>> 2025-08-18 22:50:32,353 - INFO - 正在加载模型: /home/liangshuqiao/models/qwen3
>>> 2025-08-18 22:50:59,412 - INFO - 模型加载完成
>>> 2025-08-18 22:50:59,474 - INFO - ============================================================
>>> 2025-08-18 22:50:59,474 - INFO - 模型详细信息
>>> 2025-08-18 22:50:59,475 - INFO - ============================================================
>>> 2025-08-18 22:50:59,475 - INFO - 模型路径: /home/liangshuqiao/models/qwen3
>>> 2025-08-18 22:50:59,476 - INFO - 模型类型: qwen3
>>> 2025-08-18 22:50:59,476 - INFO - 模型架构: ['Qwen3ForCausalLM']
>>> 2025-08-18 22:50:59,477 - INFO - ----------------------------------------
>>> 2025-08-18 22:50:59,477 - INFO - 参数信息:
>>> 2025-08-18 22:50:59,477 - INFO -   总参数量: 32.76B (32,762,123,264)
>>> 2025-08-18 22:50:59,478 - INFO -   可训练参数: 32.76B (32,762,123,264)
>>> 2025-08-18 22:50:59,478 - INFO -   冻结参数: 0 (0)
>>> 2025-08-18 22:50:59,478 - INFO -   可训练参数比例: 100.00%
>>> 2025-08-18 22:50:59,479 - INFO - ----------------------------------------
>>> 2025-08-18 22:50:59,479 - INFO - 模型结构信息:
>>> 2025-08-18 22:50:59,479 - INFO -   层数: 64
>>> 2025-08-18 22:50:59,480 - INFO -   隐藏层大小: 5120
>>> 2025-08-18 22:50:59,480 - INFO -   注意力头数: 64
>>> 2025-08-18 22:50:59,480 - INFO -   总层数量: 707
>>> 2025-08-18 22:50:59,481 - INFO -   可训练层数量: 707
>>> 2025-08-18 22:50:59,481 - INFO - ----------------------------------------
>>> 2025-08-18 22:50:59,481 - INFO - 特殊Token:
>>> 2025-08-18 22:50:59,482 - INFO -   bos_token: None
>>> 2025-08-18 22:50:59,482 - INFO -   eos_token: <|im_end|>
>>> 2025-08-18 22:50:59,483 - INFO -   unk_token: None
>>> 2025-08-18 22:50:59,483 - INFO -   pad_token: <|endoftext|>
>>> 2025-08-18 22:50:59,483 - INFO -   sep_token: None
>>> 2025-08-18 22:50:59,484 - INFO -   mask_token: None
>>> 2025-08-18 22:50:59,484 - INFO -   vocab_size: 151643
>>> 2025-08-18 22:50:59,484 - INFO - ----------------------------------------
>>> 2025-08-18 22:50:59,485 - INFO - 量化信息:
>>> 2025-08-18 22:50:59,485 - INFO -   数据类型: torch.float32
>>> 2025-08-18 22:50:59,485 - INFO -   量化方式: None
>>> 2025-08-18 22:50:59,486 - INFO - ----------------------------------------
>>> 2025-08-18 22:50:59,486 - INFO - 可训练层 (前10个):
>>> 2025-08-18 22:50:59,487 - INFO -   1. model.embed_tokens.weight
>>> 2025-08-18 22:50:59,487 - INFO -   2. model.layers.0.self_attn.q_proj.weight
>>> 2025-08-18 22:50:59,487 - INFO -   3. model.layers.0.self_attn.k_proj.weight
>>> 2025-08-18 22:50:59,488 - INFO -   4. model.layers.0.self_attn.v_proj.weight
>>> 2025-08-18 22:50:59,488 - INFO -   5. model.layers.0.self_attn.o_proj.weight
>>> 2025-08-18 22:50:59,489 - INFO -   6. model.layers.0.self_attn.q_norm.weight
>>> 2025-08-18 22:50:59,489 - INFO -   7. model.layers.0.self_attn.k_norm.weight
>>> 2025-08-18 22:50:59,489 - INFO -   8. model.layers.0.mlp.gate_proj.weight
>>> 2025-08-18 22:50:59,490 - INFO -   9. model.layers.0.mlp.up_proj.weight
>>> 2025-08-18 22:50:59,490 - INFO -   10. model.layers.0.mlp.down_proj.weight
>>> 2025-08-18 22:50:59,491 - INFO -   ... 还有 697 个可训练层
>>> 2025-08-18 22:51:36,649 - INFO - ========train Qwen2ForCausalLM  202508182251========
>>> 2025-08-18 22:51:36,649 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-18 22:51:36,650 - INFO - 开始进行训练
>>> 2025-08-18 22:51:45,053 - INFO - 导入包完成
>>> 2025-08-18 22:51:45,069 - INFO - 配置文件读取完成
>>> 2025-08-18 22:51:45,069 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-18 22:51:45,070 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-08-18 22:51:45,445 - INFO - tokenizer读取完成
>>> 2025-08-18 22:52:48,010 - INFO - model dtype:torch.float16
>>> 2025-08-18 22:52:48,012 - INFO - 模型导入完成
>>> 2025-08-18 22:52:48,769 - INFO - 读取数据集成功
>>> 2025-08-18 22:52:53,380 - INFO - 数据处理成功
>>> 2025-08-18 22:53:09,966 - INFO - 开始训练！
>>> 2025-08-18 22:53:09,967 - INFO - 批次大小  : 4
>>> 2025-08-18 22:53:09,967 - INFO - 训练轮数  : 100
>>> 2025-08-18 22:53:09,968 - INFO - 学习率    : 0.0001
>>> 2025-08-18 22:53:09,968 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-18 22:53:09,969 - INFO - 模型路径  : /home/liangshuqiao/models/qwen3-8b
>>> 2025-08-18 22:53:16,916 - INFO - >>> {'loss': 3.5214, 'grad_norm': 2.4722843170166016, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-18 22:53:21,337 - INFO - >>> {'loss': 3.6667, 'grad_norm': 2.729246139526367, 'learning_rate': 0.0001, 'epoch': 0.32}
>>> 2025-08-18 22:53:24,246 - INFO - >>> {'loss': 3.5722, 'grad_norm': 2.8817481994628906, 'learning_rate': 9.999931232202689e-05, 'epoch': 0.48}
>>> 2025-08-18 22:53:29,291 - INFO - >>> {'loss': 3.4778, 'grad_norm': 2.374586582183838, 'learning_rate': 9.999724930702356e-05, 'epoch': 0.64}
>>> 2025-08-18 22:53:35,399 - INFO - >>> {'loss': 3.0199, 'grad_norm': 1.8116371631622314, 'learning_rate': 9.999381101173764e-05, 'epoch': 0.8}
>>> 2025-08-18 22:53:40,604 - INFO - >>> {'loss': 2.8654, 'grad_norm': 2.404141902923584, 'learning_rate': 9.998899753074669e-05, 'epoch': 0.96}
>>> 2025-08-18 22:53:41,918 - INFO - >>> {'loss': 3.057, 'grad_norm': 2.398003339767456, 'learning_rate': 9.998280899645574e-05, 'epoch': 1.0}
>>> 2025-08-18 22:53:46,026 - INFO - >>> {'loss': 2.7339, 'grad_norm': 1.4306901693344116, 'learning_rate': 9.997524557909352e-05, 'epoch': 1.16}
>>> 2025-08-18 22:53:51,740 - INFO - >>> {'loss': 2.4594, 'grad_norm': 0.8629317879676819, 'learning_rate': 9.996630748670787e-05, 'epoch': 1.32}
>>> 2025-08-18 22:53:57,371 - INFO - >>> {'loss': 2.4327, 'grad_norm': 1.130467414855957, 'learning_rate': 9.995599496515995e-05, 'epoch': 1.48}
>>> 2025-08-18 22:54:02,493 - INFO - >>> {'loss': 2.2238, 'grad_norm': 0.9580157995223999, 'learning_rate': 9.99443082981175e-05, 'epoch': 1.6400000000000001}
>>> 2025-08-18 22:54:08,150 - INFO - >>> {'loss': 2.4198, 'grad_norm': 0.805099368095398, 'learning_rate': 9.993124780704707e-05, 'epoch': 1.8}
>>> 2025-08-18 22:54:12,619 - INFO - >>> {'loss': 2.2084, 'grad_norm': 0.7685917615890503, 'learning_rate': 9.991681385120515e-05, 'epoch': 1.96}
>>> 2025-08-18 22:54:13,926 - INFO - >>> {'loss': 2.4142, 'grad_norm': 1.095949411392212, 'learning_rate': 9.990100682762828e-05, 'epoch': 2.0}
>>> 2025-08-18 22:54:19,573 - INFO - >>> {'loss': 2.2859, 'grad_norm': 0.8037468791007996, 'learning_rate': 9.988382717112213e-05, 'epoch': 2.16}
>>> 2025-08-18 22:54:24,277 - INFO - >>> {'loss': 2.1981, 'grad_norm': 0.9759875535964966, 'learning_rate': 9.986527535424957e-05, 'epoch': 2.32}
>>> 2025-08-18 22:54:30,888 - INFO - >>> {'loss': 2.1146, 'grad_norm': 1.065224528312683, 'learning_rate': 9.984535188731759e-05, 'epoch': 2.48}
>>> 2025-08-18 22:54:36,528 - INFO - >>> {'loss': 2.1205, 'grad_norm': 0.9849202036857605, 'learning_rate': 9.982405731836342e-05, 'epoch': 2.64}
>>> 2025-08-18 22:54:41,446 - INFO - >>> {'loss': 2.0412, 'grad_norm': 0.8978034257888794, 'learning_rate': 9.980139223313925e-05, 'epoch': 2.8}
>>> 2025-08-18 22:54:47,485 - INFO - >>> {'loss': 1.8905, 'grad_norm': 0.8013890385627747, 'learning_rate': 9.977735725509632e-05, 'epoch': 2.96}
>>> 2025-08-18 22:54:48,614 - INFO - >>> {'loss': 2.1251, 'grad_norm': 1.7441943883895874, 'learning_rate': 9.97519530453676e-05, 'epoch': 3.0}
>>> 2025-08-18 22:54:56,578 - INFO - >>> {'loss': 2.0951, 'grad_norm': 0.693774938583374, 'learning_rate': 9.972518030274971e-05, 'epoch': 3.16}
>>> 2025-08-18 22:55:04,183 - INFO - >>> {'loss': 2.0908, 'grad_norm': 0.9120144844055176, 'learning_rate': 9.969703976368368e-05, 'epoch': 3.32}
>>> 2025-08-18 22:55:10,256 - INFO - >>> {'loss': 1.8634, 'grad_norm': 0.7634895443916321, 'learning_rate': 9.966753220223465e-05, 'epoch': 3.48}
>>> 2025-08-18 22:55:17,025 - INFO - >>> {'loss': 1.8528, 'grad_norm': 0.6661707162857056, 'learning_rate': 9.963665843007064e-05, 'epoch': 3.64}
>>> 2025-08-18 22:55:21,884 - INFO - >>> {'loss': 1.6401, 'grad_norm': 1.034502625465393, 'learning_rate': 9.960441929644017e-05, 'epoch': 3.8}
>>> 2025-08-18 22:55:25,518 - INFO - >>> {'loss': 1.7955, 'grad_norm': 1.056842565536499, 'learning_rate': 9.95708156881489e-05, 'epoch': 3.96}
>>> 2025-08-18 22:55:26,382 - INFO - >>> {'loss': 1.5696, 'grad_norm': 1.7164450883865356, 'learning_rate': 9.953584852953529e-05, 'epoch': 4.0}
>>> 2025-08-18 22:55:30,758 - INFO - >>> {'loss': 1.9188, 'grad_norm': 0.92156982421875, 'learning_rate': 9.949951878244515e-05, 'epoch': 4.16}
>>> 2025-08-18 22:55:37,912 - INFO - >>> {'loss': 1.6113, 'grad_norm': 0.7887139916419983, 'learning_rate': 9.946182744620512e-05, 'epoch': 4.32}
>>> 2025-08-18 22:55:43,549 - INFO - >>> {'loss': 1.6785, 'grad_norm': 0.8889738321304321, 'learning_rate': 9.942277555759529e-05, 'epoch': 4.48}
>>> 2025-08-18 22:55:49,454 - INFO - >>> {'loss': 1.62, 'grad_norm': 0.9654514193534851, 'learning_rate': 9.938236419082061e-05, 'epoch': 4.64}
>>> 2025-08-18 22:55:53,886 - INFO - >>> {'loss': 1.8119, 'grad_norm': 0.9884551167488098, 'learning_rate': 9.934059445748134e-05, 'epoch': 4.8}
>>> 2025-08-18 22:55:58,926 - INFO - >>> {'loss': 1.7417, 'grad_norm': 0.955915093421936, 'learning_rate': 9.929746750654249e-05, 'epoch': 4.96}
>>> 2025-08-18 22:55:59,400 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.925298452430226e-05, 'epoch': 5.0}
>>> 2025-08-18 22:56:04,685 - INFO - >>> {'loss': 1.7753, 'grad_norm': 1.114658236503601, 'learning_rate': 9.92071467343593e-05, 'epoch': 5.16}
>>> 2025-08-18 22:56:09,885 - INFO - >>> {'loss': 1.4479, 'grad_norm': 1.1293212175369263, 'learning_rate': 9.915995539757917e-05, 'epoch': 5.32}
>>> 2025-08-18 22:56:14,176 - INFO - >>> {'loss': 1.2969, 'grad_norm': 1.1030386686325073, 'learning_rate': 9.911141181205958e-05, 'epoch': 5.48}
>>> 2025-08-18 22:56:17,320 - INFO - >>> {'loss': 1.3279, 'grad_norm': 1.574678659439087, 'learning_rate': 9.906151731309472e-05, 'epoch': 5.64}
>>> 2025-08-18 22:56:22,333 - INFO - >>> {'loss': 1.4527, 'grad_norm': 1.1507753133773804, 'learning_rate': 9.901027327313848e-05, 'epoch': 5.8}
>>> 2025-08-18 22:56:28,166 - INFO - >>> {'loss': 1.5515, 'grad_norm': 1.1794321537017822, 'learning_rate': 9.895768110176678e-05, 'epoch': 5.96}
>>> 2025-08-18 22:56:29,298 - INFO - >>> {'loss': 1.7463, 'grad_norm': 3.6624975204467773, 'learning_rate': 9.890374224563872e-05, 'epoch': 6.0}
>>> 2025-08-18 22:56:34,157 - INFO - >>> {'loss': 1.2844, 'grad_norm': 1.5334272384643555, 'learning_rate': 9.884845818845685e-05, 'epoch': 6.16}
>>> 2025-08-18 22:56:39,444 - INFO - >>> {'loss': 1.3559, 'grad_norm': 1.3690836429595947, 'learning_rate': 9.879183045092628e-05, 'epoch': 6.32}
>>> 2025-08-18 22:56:41,971 - INFO - >>> {'loss': 0.9209, 'grad_norm': 2.684189796447754, 'learning_rate': 9.873386059071294e-05, 'epoch': 6.48}
>>> 2025-08-18 22:56:47,130 - INFO - >>> {'loss': 1.3934, 'grad_norm': 1.190879225730896, 'learning_rate': 9.867455020240069e-05, 'epoch': 6.64}
>>> 2025-08-18 22:56:50,977 - INFO - >>> {'loss': 1.2412, 'grad_norm': 1.8097891807556152, 'learning_rate': 9.861390091744737e-05, 'epoch': 6.8}
>>> 2025-08-18 22:56:56,899 - INFO - >>> {'loss': 1.2832, 'grad_norm': 1.6276631355285645, 'learning_rate': 9.855191440414013e-05, 'epoch': 6.96}
>>> 2025-08-18 22:56:58,178 - INFO - >>> {'loss': 1.0964, 'grad_norm': 2.0984256267547607, 'learning_rate': 9.848859236754935e-05, 'epoch': 7.0}
>>> 2025-08-18 22:57:02,329 - INFO - >>> {'loss': 1.1247, 'grad_norm': 1.772642731666565, 'learning_rate': 9.842393654948181e-05, 'epoch': 7.16}
>>> 2025-08-18 22:57:06,730 - INFO - >>> {'loss': 1.1784, 'grad_norm': 1.5941823720932007, 'learning_rate': 9.83579487284328e-05, 'epoch': 7.32}
>>> 2025-08-18 22:57:11,208 - INFO - >>> {'loss': 1.1526, 'grad_norm': 1.7045868635177612, 'learning_rate': 9.829063071953714e-05, 'epoch': 7.48}
>>> 2025-08-18 22:57:15,776 - INFO - >>> {'loss': 1.1626, 'grad_norm': 2.1328389644622803, 'learning_rate': 9.822198437451932e-05, 'epoch': 7.64}
>>> 2025-08-18 22:57:22,198 - INFO - >>> {'loss': 1.0055, 'grad_norm': 1.6781446933746338, 'learning_rate': 9.815201158164254e-05, 'epoch': 7.8}
>>> 2025-08-18 22:57:26,032 - INFO - >>> {'loss': 0.8492, 'grad_norm': 1.9403092861175537, 'learning_rate': 9.808071426565671e-05, 'epoch': 7.96}
>>> 2025-08-18 22:57:27,370 - INFO - >>> {'loss': 0.759, 'grad_norm': 3.8297598361968994, 'learning_rate': 9.800809438774556e-05, 'epoch': 8.0}
>>> 2025-08-18 22:57:32,460 - INFO - >>> {'loss': 1.0339, 'grad_norm': 1.5909249782562256, 'learning_rate': 9.793415394547274e-05, 'epoch': 8.16}
>>> 2025-08-18 22:57:38,404 - INFO - >>> {'loss': 0.8963, 'grad_norm': 1.4716216325759888, 'learning_rate': 9.785889497272677e-05, 'epoch': 8.32}
>>> 2025-08-18 22:57:42,996 - INFO - >>> {'loss': 0.6753, 'grad_norm': 1.6603351831436157, 'learning_rate': 9.778231953966519e-05, 'epoch': 8.48}
>>> 2025-08-18 22:57:47,661 - INFO - >>> {'loss': 0.6282, 'grad_norm': 1.8428432941436768, 'learning_rate': 9.770442975265752e-05, 'epoch': 8.64}
>>> 2025-08-18 22:57:52,472 - INFO - >>> {'loss': 0.7151, 'grad_norm': 2.432737112045288, 'learning_rate': 9.762522775422741e-05, 'epoch': 8.8}
>>> 2025-08-18 22:57:57,982 - INFO - >>> {'loss': 0.7967, 'grad_norm': 2.433476686477661, 'learning_rate': 9.754471572299363e-05, 'epoch': 8.96}
>>> 2025-08-18 22:57:58,455 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.746289587361021e-05, 'epoch': 9.0}
>>> 2025-08-18 22:58:02,735 - INFO - >>> {'loss': 0.5377, 'grad_norm': 2.002361536026001, 'learning_rate': 9.737977045670548e-05, 'epoch': 9.16}
>>> 2025-08-18 22:58:06,269 - INFO - >>> {'loss': 0.4838, 'grad_norm': 2.444549560546875, 'learning_rate': 9.729534175882016e-05, 'epoch': 9.32}
>>> 2025-08-18 22:58:11,201 - INFO - >>> {'loss': 0.5397, 'grad_norm': 1.9118387699127197, 'learning_rate': 9.720961210234449e-05, 'epoch': 9.48}
>>> 2025-08-18 22:58:17,361 - INFO - >>> {'loss': 0.4843, 'grad_norm': 2.066490650177002, 'learning_rate': 9.712258384545432e-05, 'epoch': 9.64}
>>> 2025-08-18 22:58:22,674 - INFO - >>> {'loss': 0.4776, 'grad_norm': 2.491283655166626, 'learning_rate': 9.703425938204627e-05, 'epoch': 9.8}
>>> 2025-08-18 22:58:26,469 - INFO - >>> {'loss': 0.2741, 'grad_norm': 2.4404828548431396, 'learning_rate': 9.694464114167186e-05, 'epoch': 9.96}
>>> 2025-08-18 22:58:28,291 - INFO - >>> {'loss': 0.7027, 'grad_norm': 4.1286444664001465, 'learning_rate': 9.685373158947067e-05, 'epoch': 10.0}
>>> 2025-08-18 22:58:32,975 - INFO - >>> {'loss': 0.3624, 'grad_norm': 2.272171974182129, 'learning_rate': 9.676153322610259e-05, 'epoch': 10.16}
>>> 2025-08-18 22:58:39,197 - INFO - >>> {'loss': 0.3012, 'grad_norm': 2.286703586578369, 'learning_rate': 9.666804858767894e-05, 'epoch': 10.32}
>>> 2025-08-18 22:58:44,635 - INFO - >>> {'loss': 0.2429, 'grad_norm': 1.3879894018173218, 'learning_rate': 9.65732802456928e-05, 'epoch': 10.48}
>>> 2025-08-18 22:58:48,332 - INFO - >>> {'loss': 0.2505, 'grad_norm': 2.7232158184051514, 'learning_rate': 9.647723080694821e-05, 'epoch': 10.64}
>>> 2025-08-18 22:58:51,477 - INFO - >>> {'loss': 0.175, 'grad_norm': 4.404017448425293, 'learning_rate': 9.637990291348853e-05, 'epoch': 10.8}
>>> 2025-08-18 22:58:56,717 - INFO - >>> {'loss': 0.2694, 'grad_norm': 2.075380563735962, 'learning_rate': 9.628129924252369e-05, 'epoch': 10.96}
>>> 2025-08-18 22:58:58,801 - INFO - >>> {'loss': 0.2547, 'grad_norm': 5.211668014526367, 'learning_rate': 9.618142250635658e-05, 'epoch': 11.0}
>>> 2025-08-18 22:59:02,833 - INFO - >>> {'loss': 0.1506, 'grad_norm': 2.0120863914489746, 'learning_rate': 9.608027545230847e-05, 'epoch': 11.16}
>>> 2025-08-18 22:59:09,073 - INFO - >>> {'loss': 0.1083, 'grad_norm': 1.328035831451416, 'learning_rate': 9.597786086264338e-05, 'epoch': 11.32}
>>> 2025-08-18 22:59:14,184 - INFO - >>> {'loss': 0.1659, 'grad_norm': 2.1218953132629395, 'learning_rate': 9.587418155449167e-05, 'epoch': 11.48}
>>> 2025-08-18 22:59:19,044 - INFO - >>> {'loss': 0.1191, 'grad_norm': 2.720994472503662, 'learning_rate': 9.576924037977233e-05, 'epoch': 11.64}
>>> 2025-08-18 22:59:24,178 - INFO - >>> {'loss': 0.1351, 'grad_norm': 1.5727256536483765, 'learning_rate': 9.566304022511477e-05, 'epoch': 11.8}
>>> 2025-08-18 22:59:28,911 - INFO - >>> {'loss': 0.102, 'grad_norm': 1.535103440284729, 'learning_rate': 9.555558401177926e-05, 'epoch': 11.96}
>>> 2025-08-18 22:59:29,807 - INFO - >>> {'loss': 0.1206, 'grad_norm': 3.5696611404418945, 'learning_rate': 9.544687469557666e-05, 'epoch': 12.0}
>>> 2025-08-18 22:59:35,231 - INFO - >>> {'loss': 0.0689, 'grad_norm': 1.0176078081130981, 'learning_rate': 9.533691526678705e-05, 'epoch': 12.16}
>>> 2025-08-18 22:59:40,435 - INFO - >>> {'loss': 0.0543, 'grad_norm': 1.120534062385559, 'learning_rate': 9.52257087500775e-05, 'epoch': 12.32}
>>> 2025-08-18 22:59:45,435 - INFO - >>> {'loss': 0.0539, 'grad_norm': 1.306990623474121, 'learning_rate': 9.51132582044189e-05, 'epoch': 12.48}
>>> 2025-08-18 22:59:50,376 - INFO - >>> {'loss': 0.0509, 'grad_norm': 1.5009374618530273, 'learning_rate': 9.499956672300178e-05, 'epoch': 12.64}
>>> 2025-08-18 22:59:54,588 - INFO - >>> {'loss': 0.0712, 'grad_norm': 1.4249498844146729, 'learning_rate': 9.488463743315126e-05, 'epoch': 12.8}
>>> 2025-08-18 22:59:59,618 - INFO - >>> {'loss': 0.0741, 'grad_norm': 1.795935034751892, 'learning_rate': 9.476847349624097e-05, 'epoch': 12.96}
>>> 2025-08-18 23:00:00,890 - INFO - >>> {'loss': 0.0595, 'grad_norm': 2.124384880065918, 'learning_rate': 9.46510781076061e-05, 'epoch': 13.0}
>>> 2025-08-18 23:00:05,575 - INFO - >>> {'loss': 0.0464, 'grad_norm': 0.9159435629844666, 'learning_rate': 9.453245449645563e-05, 'epoch': 13.16}
>>> 2025-08-18 23:00:09,050 - INFO - >>> {'loss': 0.0228, 'grad_norm': 1.756107211112976, 'learning_rate': 9.441260592578329e-05, 'epoch': 13.32}
>>> 2025-08-18 23:00:14,433 - INFO - >>> {'loss': 0.0293, 'grad_norm': 0.778384268283844, 'learning_rate': 9.4291535692278e-05, 'epoch': 13.48}
>>> 2025-08-18 23:00:17,743 - INFO - >>> {'loss': 0.0451, 'grad_norm': 0.8698577284812927, 'learning_rate': 9.416924712623305e-05, 'epoch': 13.64}
>>> 2025-08-18 23:00:23,793 - INFO - >>> {'loss': 0.0289, 'grad_norm': 1.1980944871902466, 'learning_rate': 9.404574359145459e-05, 'epoch': 13.8}
>>> 2025-08-18 23:00:28,885 - INFO - >>> {'loss': 0.0263, 'grad_norm': 1.1731983423233032, 'learning_rate': 9.392102848516901e-05, 'epoch': 13.96}
>>> 2025-08-18 23:00:30,022 - INFO - >>> {'loss': 0.0286, 'grad_norm': 1.6292579174041748, 'learning_rate': 9.379510523792961e-05, 'epoch': 14.0}
>>> 2025-08-18 23:00:34,697 - INFO - >>> {'loss': 0.0131, 'grad_norm': 0.3969302475452423, 'learning_rate': 9.366797731352209e-05, 'epoch': 14.16}
>>> 2025-08-18 23:00:39,281 - INFO - >>> {'loss': 0.0242, 'grad_norm': 1.1300102472305298, 'learning_rate': 9.353964820886938e-05, 'epoch': 14.32}
>>> 2025-08-18 23:00:44,985 - INFO - >>> {'loss': 0.0171, 'grad_norm': 0.7280820608139038, 'learning_rate': 9.341012145393547e-05, 'epoch': 14.48}
>>> 2025-08-18 23:00:50,272 - INFO - >>> {'loss': 0.0176, 'grad_norm': 0.5881336331367493, 'learning_rate': 9.327940061162817e-05, 'epoch': 14.64}
>>> 2025-08-18 23:00:54,828 - INFO - >>> {'loss': 0.0083, 'grad_norm': 0.518279492855072, 'learning_rate': 9.314748927770125e-05, 'epoch': 14.8}
>>> 2025-08-18 23:00:58,506 - INFO - >>> {'loss': 0.0133, 'grad_norm': 0.6370105743408203, 'learning_rate': 9.301439108065546e-05, 'epoch': 14.96}
>>> 2025-08-18 23:01:00,911 - INFO - >>> {'loss': 0.0109, 'grad_norm': 0.7949324250221252, 'learning_rate': 9.288010968163872e-05, 'epoch': 15.0}
>>> 2025-08-18 23:01:04,989 - INFO - >>> {'loss': 0.0057, 'grad_norm': 0.2827036678791046, 'learning_rate': 9.274464877434548e-05, 'epoch': 15.16}
>>> 2025-08-18 23:01:10,438 - INFO - >>> {'loss': 0.0172, 'grad_norm': 0.6313324570655823, 'learning_rate': 9.260801208491498e-05, 'epoch': 15.32}
>>> 2025-08-18 23:01:13,944 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.24185669422149658, 'learning_rate': 9.247020337182893e-05, 'epoch': 15.48}
>>> 2025-08-18 23:01:18,988 - INFO - >>> {'loss': 0.0137, 'grad_norm': 0.7565328478813171, 'learning_rate': 9.233122642580796e-05, 'epoch': 15.64}
>>> 2025-08-18 23:01:24,177 - INFO - >>> {'loss': 0.0138, 'grad_norm': 0.7050886750221252, 'learning_rate': 9.219108506970746e-05, 'epoch': 15.8}
>>> 2025-08-18 23:01:28,911 - INFO - >>> {'loss': 0.0071, 'grad_norm': 0.5611216425895691, 'learning_rate': 9.204978315841237e-05, 'epoch': 15.96}
>>> 2025-08-18 23:01:31,319 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.2050769180059433, 'learning_rate': 9.190732457873119e-05, 'epoch': 16.0}
>>> 2025-08-18 23:01:37,265 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.5633108019828796, 'learning_rate': 9.176371324928899e-05, 'epoch': 16.16}
>>> 2025-08-18 23:01:41,398 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.1392214447259903, 'learning_rate': 9.161895312041971e-05, 'epoch': 16.32}
>>> 2025-08-18 23:01:45,147 - INFO - >>> {'loss': 0.0155, 'grad_norm': 0.6103439927101135, 'learning_rate': 9.14730481740574e-05, 'epoch': 16.48}
>>> 2025-08-18 23:01:50,325 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.5625730156898499, 'learning_rate': 9.132600242362681e-05, 'epoch': 16.64}
>>> 2025-08-18 23:01:55,595 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.16580578684806824, 'learning_rate': 9.117781991393283e-05, 'epoch': 16.8}
>>> 2025-08-18 23:01:59,607 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.21536080539226532, 'learning_rate': 9.102850472104944e-05, 'epoch': 16.96}
>>> 2025-08-18 23:02:00,531 - INFO - >>> {'loss': 0.0466, 'grad_norm': 1.2441617250442505, 'learning_rate': 9.087806095220739e-05, 'epoch': 17.0}
>>> 2025-08-18 23:02:03,702 - INFO - >>> {'loss': 0.0069, 'grad_norm': 2.0999176502227783, 'learning_rate': 9.072649274568129e-05, 'epoch': 17.16}
>>> 2025-08-18 23:02:09,641 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.08648134022951126, 'learning_rate': 9.057380427067584e-05, 'epoch': 17.32}
>>> 2025-08-18 23:02:15,123 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.2883474826812744, 'learning_rate': 9.041999972721109e-05, 'epoch': 17.48}
>>> 2025-08-18 23:02:19,832 - INFO - >>> {'loss': 0.0086, 'grad_norm': 0.20095194876194, 'learning_rate': 9.02650833460069e-05, 'epoch': 17.64}
>>> 2025-08-18 23:02:24,290 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.5384610295295715, 'learning_rate': 9.010905938836661e-05, 'epoch': 17.8}
>>> 2025-08-18 23:02:27,738 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.2769697904586792, 'learning_rate': 8.995193214605973e-05, 'epoch': 17.96}
>>> 2025-08-18 23:02:29,044 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.30867069959640503, 'learning_rate': 8.979370594120402e-05, 'epoch': 18.0}
>>> 2025-08-18 23:02:33,652 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.7534919381141663, 'learning_rate': 8.963438512614655e-05, 'epoch': 18.16}
>>> 2025-08-18 23:02:40,409 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.18059414625167847, 'learning_rate': 8.947397408334391e-05, 'epoch': 18.32}
>>> 2025-08-18 23:02:44,225 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.20807689428329468, 'learning_rate': 8.931247722524169e-05, 'epoch': 18.48}
>>> 2025-08-18 23:02:47,833 - INFO - >>> {'loss': 0.0068, 'grad_norm': 1.3376332521438599, 'learning_rate': 8.914989899415323e-05, 'epoch': 18.64}
>>> 2025-08-18 23:02:51,443 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.15156486630439758, 'learning_rate': 8.898624386213725e-05, 'epoch': 18.8}
>>> 2025-08-18 23:02:57,741 - INFO - >>> {'loss': 0.0095, 'grad_norm': 0.21964477002620697, 'learning_rate': 8.88215163308749e-05, 'epoch': 18.96}
>>> 2025-08-18 23:02:59,281 - INFO - >>> {'loss': 0.0144, 'grad_norm': 0.4994644522666931, 'learning_rate': 8.8655720931546e-05, 'epoch': 19.0}
>>> 2025-08-18 23:03:06,102 - INFO - >>> {'loss': 0.0086, 'grad_norm': 0.23954300582408905, 'learning_rate': 8.84888622247043e-05, 'epoch': 19.16}
>>> 2025-08-18 23:03:10,589 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.5450769662857056, 'learning_rate': 8.83209448001521e-05, 'epoch': 19.32}
>>> 2025-08-18 23:03:15,980 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.4193078279495239, 'learning_rate': 8.815197327681399e-05, 'epoch': 19.48}
>>> 2025-08-18 23:03:20,219 - INFO - >>> {'loss': 0.0068, 'grad_norm': 0.6384969353675842, 'learning_rate': 8.798195230260973e-05, 'epoch': 19.64}
>>> 2025-08-18 23:03:24,048 - INFO - >>> {'loss': 0.0186, 'grad_norm': 1.855926275253296, 'learning_rate': 8.781088655432648e-05, 'epoch': 19.8}
>>> 2025-08-18 23:03:28,928 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.6295043230056763, 'learning_rate': 8.763878073749012e-05, 'epoch': 19.96}
>>> 2025-08-18 23:03:29,854 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.388176292181015, 'learning_rate': 8.746563958623584e-05, 'epoch': 20.0}
>>> 2025-08-18 23:03:33,362 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.24108684062957764, 'learning_rate': 8.729146786317786e-05, 'epoch': 20.16}
>>> 2025-08-18 23:03:39,608 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.26688632369041443, 'learning_rate': 8.711627035927847e-05, 'epoch': 20.32}
>>> 2025-08-18 23:03:45,085 - INFO - >>> {'loss': 0.0065, 'grad_norm': 0.44204118847846985, 'learning_rate': 8.694005189371627e-05, 'epoch': 20.48}
>>> 2025-08-18 23:03:49,569 - INFO - >>> {'loss': 0.0103, 'grad_norm': 1.6523377895355225, 'learning_rate': 8.676281731375353e-05, 'epoch': 20.64}
>>> 2025-08-18 23:03:54,470 - INFO - >>> {'loss': 0.0075, 'grad_norm': 0.5310528874397278, 'learning_rate': 8.658457149460295e-05, 'epoch': 20.8}
>>> 2025-08-18 23:03:58,879 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.3437862992286682, 'learning_rate': 8.640531933929344e-05, 'epoch': 20.96}
>>> 2025-08-18 23:04:00,184 - INFO - >>> {'loss': 0.0263, 'grad_norm': 2.877471923828125, 'learning_rate': 8.622506577853538e-05, 'epoch': 21.0}
>>> 2025-08-18 23:04:04,955 - INFO - >>> {'loss': 0.0172, 'grad_norm': 1.939958095550537, 'learning_rate': 8.604381577058486e-05, 'epoch': 21.16}
>>> 2025-08-18 23:04:09,541 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.3517882823944092, 'learning_rate': 8.586157430110747e-05, 'epoch': 21.32}
>>> 2025-08-18 23:04:13,516 - INFO - >>> {'loss': 0.0113, 'grad_norm': 0.7493717074394226, 'learning_rate': 8.56783463830409e-05, 'epoch': 21.48}
>>> 2025-08-18 23:04:18,766 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.32853710651397705, 'learning_rate': 8.549413705645737e-05, 'epoch': 21.64}
>>> 2025-08-18 23:04:23,573 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.16678838431835175, 'learning_rate': 8.530895138842467e-05, 'epoch': 21.8}
>>> 2025-08-18 23:04:29,525 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.17991212010383606, 'learning_rate': 8.512279447286703e-05, 'epoch': 21.96}
>>> 2025-08-18 23:04:30,892 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.22661614418029785, 'learning_rate': 8.493567143042485e-05, 'epoch': 22.0}
>>> 2025-08-18 23:04:35,760 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.11305858939886093, 'learning_rate': 8.47475874083139e-05, 'epoch': 22.16}
>>> 2025-08-18 23:04:40,751 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.15679025650024414, 'learning_rate': 8.455854758018376e-05, 'epoch': 22.32}
>>> 2025-08-18 23:04:44,936 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.730616569519043, 'learning_rate': 8.436855714597546e-05, 'epoch': 22.48}
>>> 2025-08-18 23:04:48,337 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.11280831694602966, 'learning_rate': 8.417762133177848e-05, 'epoch': 22.64}
>>> 2025-08-18 23:04:52,982 - INFO - >>> {'loss': 0.013, 'grad_norm': 1.2798130512237549, 'learning_rate': 8.398574538968697e-05, 'epoch': 22.8}
>>> 2025-08-18 23:04:57,546 - INFO - >>> {'loss': 0.0094, 'grad_norm': 0.8199072480201721, 'learning_rate': 8.379293459765526e-05, 'epoch': 22.96}
>>> 2025-08-18 23:04:58,870 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.19160763919353485, 'learning_rate': 8.359919425935275e-05, 'epoch': 23.0}
>>> 2025-08-18 23:05:03,495 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.1489151120185852, 'learning_rate': 8.340452970401797e-05, 'epoch': 23.16}
>>> 2025-08-18 23:05:07,934 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.08717067539691925, 'learning_rate': 8.3208946286312e-05, 'epoch': 23.32}
>>> 2025-08-18 23:05:14,577 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.06755514442920685, 'learning_rate': 8.301244938617116e-05, 'epoch': 23.48}
>>> 2025-08-18 23:05:19,481 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.20558111369609833, 'learning_rate': 8.281504440865905e-05, 'epoch': 23.64}
>>> 2025-08-18 23:05:23,422 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.19218184053897858, 'learning_rate': 8.261673678381786e-05, 'epoch': 23.8}
>>> 2025-08-18 23:05:27,772 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.8058413863182068, 'learning_rate': 8.241753196651902e-05, 'epoch': 23.96}
>>> 2025-08-18 23:05:30,186 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.23953591287136078, 'learning_rate': 8.221743543631313e-05, 'epoch': 24.0}
>>> 2025-08-18 23:05:34,814 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.06891027092933655, 'learning_rate': 8.201645269727925e-05, 'epoch': 24.16}
>>> 2025-08-18 23:05:39,839 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.06586794555187225, 'learning_rate': 8.181458927787347e-05, 'epoch': 24.32}
>>> 2025-08-18 23:05:45,987 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.09760080277919769, 'learning_rate': 8.161185073077686e-05, 'epoch': 24.48}
>>> 2025-08-18 23:05:51,260 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.1895781010389328, 'learning_rate': 8.140824263274279e-05, 'epoch': 24.64}
>>> 2025-08-18 23:05:56,693 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.05413712188601494, 'learning_rate': 8.120377058444336e-05, 'epoch': 24.8}
>>> 2025-08-18 23:06:00,575 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.085154227912426, 'learning_rate': 8.09984402103156e-05, 'epoch': 24.96}
>>> 2025-08-18 23:06:01,044 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.079225715840646e-05, 'epoch': 25.0}
>>> 2025-08-18 23:06:05,509 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.03732124716043472, 'learning_rate': 8.058522710021772e-05, 'epoch': 25.16}
>>> 2025-08-18 23:06:10,670 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.11052514612674713, 'learning_rate': 8.037735573054979e-05, 'epoch': 25.32}
>>> 2025-08-18 23:06:14,657 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.04407893866300583, 'learning_rate': 8.016864876734514e-05, 'epoch': 25.48}
>>> 2025-08-18 23:06:19,056 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.11863203346729279, 'learning_rate': 7.995911195153105e-05, 'epoch': 25.64}
>>> 2025-08-18 23:06:24,457 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.11379394680261612, 'learning_rate': 7.974875104686163e-05, 'epoch': 25.8}
>>> 2025-08-18 23:06:30,638 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.5046608448028564, 'learning_rate': 7.95375718397593e-05, 'epoch': 25.96}
>>> 2025-08-18 23:06:31,108 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.932558013915562e-05, 'epoch': 26.0}
>>> 2025-08-18 23:06:34,133 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.1545914113521576, 'learning_rate': 7.911278177633151e-05, 'epoch': 26.16}
>>> 2025-08-18 23:06:36,993 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.03971666842699051, 'learning_rate': 7.889918260475685e-05, 'epoch': 26.32}
>>> 2025-08-18 23:06:42,703 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.20112298429012299, 'learning_rate': 7.868478849992945e-05, 'epoch': 26.48}
>>> 2025-08-18 23:06:48,161 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.13738222420215607, 'learning_rate': 7.846960535921344e-05, 'epoch': 26.64}
>>> 2025-08-18 23:06:53,744 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.03277839720249176, 'learning_rate': 7.825363910167708e-05, 'epoch': 26.8}
>>> 2025-08-18 23:06:59,476 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.06427926570177078, 'learning_rate': 7.803689566792989e-05, 'epoch': 26.96}
>>> 2025-08-18 23:07:00,233 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.05796707049012184, 'learning_rate': 7.781938101995927e-05, 'epoch': 27.0}
>>> 2025-08-18 23:07:04,715 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.03515656292438507, 'learning_rate': 7.76011011409665e-05, 'epoch': 27.16}
>>> 2025-08-18 23:07:09,986 - INFO - >>> {'loss': 0.007, 'grad_norm': 0.23092308640480042, 'learning_rate': 7.738206203520222e-05, 'epoch': 27.32}
>>> 2025-08-18 23:07:14,527 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.02417074516415596, 'learning_rate': 7.716226972780112e-05, 'epoch': 27.48}
>>> 2025-08-18 23:07:21,418 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.06944245845079422, 'learning_rate': 7.694173026461634e-05, 'epoch': 27.64}
>>> 2025-08-18 23:07:26,186 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.1582522839307785, 'learning_rate': 7.672044971205314e-05, 'epoch': 27.8}
>>> 2025-08-18 23:07:29,527 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.12012387067079544, 'learning_rate': 7.649843415690198e-05, 'epoch': 27.96}
>>> 2025-08-18 23:07:30,839 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.08132266998291016, 'learning_rate': 7.627568970617113e-05, 'epoch': 28.0}
>>> 2025-08-18 23:07:36,339 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.09255608916282654, 'learning_rate': 7.605222248691872e-05, 'epoch': 28.16}
>>> 2025-08-18 23:07:40,287 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.02673182263970375, 'learning_rate': 7.582803864608411e-05, 'epoch': 28.32}
>>> 2025-08-18 23:07:45,595 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.09066497534513474, 'learning_rate': 7.560314435031885e-05, 'epoch': 28.48}
>>> 2025-08-18 23:07:50,716 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.026935698464512825, 'learning_rate': 7.53775457858171e-05, 'epoch': 28.64}
>>> 2025-08-18 23:07:57,672 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.11782215535640717, 'learning_rate': 7.51512491581454e-05, 'epoch': 28.8}
>>> 2025-08-18 23:08:01,271 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.03283674269914627, 'learning_rate': 7.4924260692072e-05, 'epoch': 28.96}
>>> 2025-08-18 23:08:02,809 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.05563128739595413, 'learning_rate': 7.469658663139563e-05, 'epoch': 29.0}
>>> 2025-08-18 23:08:07,451 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.01392329391092062, 'learning_rate': 7.446823323877375e-05, 'epoch': 29.16}
>>> 2025-08-18 23:08:13,448 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.09992330521345139, 'learning_rate': 7.423920679555028e-05, 'epoch': 29.32}
>>> 2025-08-18 23:08:18,579 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.10740990936756134, 'learning_rate': 7.400951360158284e-05, 'epoch': 29.48}
>>> 2025-08-18 23:08:22,744 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.029460255056619644, 'learning_rate': 7.377915997506945e-05, 'epoch': 29.64}
>>> 2025-08-18 23:08:26,740 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.025172781199216843, 'learning_rate': 7.354815225237468e-05, 'epoch': 29.8}
>>> 2025-08-18 23:08:31,829 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.025933703407645226, 'learning_rate': 7.331649678785546e-05, 'epoch': 29.96}
>>> 2025-08-18 23:08:32,728 - INFO - >>> {'loss': 0.0146, 'grad_norm': 0.7354328036308289, 'learning_rate': 7.308419995368616e-05, 'epoch': 30.0}
>>> 2025-08-18 23:08:36,493 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.0232185460627079, 'learning_rate': 7.285126813968346e-05, 'epoch': 30.16}
>>> 2025-08-18 23:08:42,036 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.05397513136267662, 'learning_rate': 7.261770775313046e-05, 'epoch': 30.32}
>>> 2025-08-18 23:08:47,936 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.010718907229602337, 'learning_rate': 7.238352521860049e-05, 'epoch': 30.48}
>>> 2025-08-18 23:08:53,206 - INFO - >>> {'loss': 0.0091, 'grad_norm': 0.20613129436969757, 'learning_rate': 7.214872697778037e-05, 'epoch': 30.64}
>>> 2025-08-18 23:08:57,714 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.05453690141439438, 'learning_rate': 7.191331948929323e-05, 'epoch': 30.8}
>>> 2025-08-18 23:09:02,896 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.015524347312748432, 'learning_rate': 7.167730922852087e-05, 'epoch': 30.96}
>>> 2025-08-18 23:09:04,031 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.2672692537307739, 'learning_rate': 7.14407026874256e-05, 'epoch': 31.0}
>>> 2025-08-18 23:09:08,084 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.04520173370838165, 'learning_rate': 7.120350637437165e-05, 'epoch': 31.16}
>>> 2025-08-18 23:09:14,040 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.09607017785310745, 'learning_rate': 7.096572681394625e-05, 'epoch': 31.32}
>>> 2025-08-18 23:09:18,222 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.017822563648223877, 'learning_rate': 7.072737054678003e-05, 'epoch': 31.48}
>>> 2025-08-18 23:09:20,869 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.03463542088866234, 'learning_rate': 7.048844412936719e-05, 'epoch': 31.64}
>>> 2025-08-18 23:09:25,747 - INFO - >>> {'loss': 0.0124, 'grad_norm': 0.22105847299098969, 'learning_rate': 7.024895413388508e-05, 'epoch': 31.8}
>>> 2025-08-18 23:09:29,894 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.02972705103456974, 'learning_rate': 7.000890714801351e-05, 'epoch': 31.96}
>>> 2025-08-18 23:09:32,298 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.021341100335121155, 'learning_rate': 6.976830977475346e-05, 'epoch': 32.0}
>>> 2025-08-18 23:09:36,272 - INFO - >>> {'loss': 0.0105, 'grad_norm': 0.2170085906982422, 'learning_rate': 6.952716863224551e-05, 'epoch': 32.16}
>>> 2025-08-18 23:09:40,701 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.026288172230124474, 'learning_rate': 6.928549035358772e-05, 'epoch': 32.32}
>>> 2025-08-18 23:09:47,443 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.07394473999738693, 'learning_rate': 6.904328158665323e-05, 'epoch': 32.48}
>>> 2025-08-18 23:09:51,231 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.06796962022781372, 'learning_rate': 6.880054899390744e-05, 'epoch': 32.64}
>>> 2025-08-18 23:09:57,197 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.026827672496438026, 'learning_rate': 6.855729925222462e-05, 'epoch': 32.8}
>>> 2025-08-18 23:10:00,814 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.020442329347133636, 'learning_rate': 6.831353905270434e-05, 'epoch': 32.96}
>>> 2025-08-18 23:10:01,285 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.806927510048738e-05, 'epoch': 33.0}
>>> 2025-08-18 23:10:05,951 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.039962686598300934, 'learning_rate': 6.782451411457137e-05, 'epoch': 33.16}
>>> 2025-08-18 23:10:10,793 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.017809560522437096, 'learning_rate': 6.757926282762583e-05, 'epoch': 33.32}
>>> 2025-08-18 23:10:15,184 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.10867179930210114, 'learning_rate': 6.733352798580708e-05, 'epoch': 33.48}
>>> 2025-08-18 23:10:19,571 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.07418897747993469, 'learning_rate': 6.708731634857263e-05, 'epoch': 33.64}
>>> 2025-08-18 23:10:23,532 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.03248339146375656, 'learning_rate': 6.684063468849527e-05, 'epoch': 33.8}
>>> 2025-08-18 23:10:29,173 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.017204634845256805, 'learning_rate': 6.659348979107679e-05, 'epoch': 33.96}
>>> 2025-08-18 23:10:31,578 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.03452882170677185, 'learning_rate': 6.634588845456123e-05, 'epoch': 34.0}
>>> 2025-08-18 23:10:37,624 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.07385751605033875, 'learning_rate': 6.609783748974802e-05, 'epoch': 34.16}
>>> 2025-08-18 23:10:41,998 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.08872190862894058, 'learning_rate': 6.584934371980453e-05, 'epoch': 34.32}
>>> 2025-08-18 23:10:47,299 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.007497296668589115, 'learning_rate': 6.560041398007847e-05, 'epoch': 34.48}
>>> 2025-08-18 23:10:52,017 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.11004994809627533, 'learning_rate': 6.53510551179098e-05, 'epoch': 34.64}
>>> 2025-08-18 23:10:55,726 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.03283283859491348, 'learning_rate': 6.510127399244234e-05, 'epoch': 34.8}
>>> 2025-08-18 23:11:00,377 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.015244207344949245, 'learning_rate': 6.485107747443528e-05, 'epoch': 34.96}
>>> 2025-08-18 23:11:01,255 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.04960480332374573, 'learning_rate': 6.460047244607397e-05, 'epoch': 35.0}
>>> 2025-08-18 23:11:06,343 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.09486176073551178, 'learning_rate': 6.434946580078072e-05, 'epoch': 35.16}
>>> 2025-08-18 23:11:12,083 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.018561184406280518, 'learning_rate': 6.409806444302518e-05, 'epoch': 35.32}
>>> 2025-08-18 23:11:16,866 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.1108737364411354, 'learning_rate': 6.38462752881344e-05, 'epoch': 35.48}
>>> 2025-08-18 23:11:22,116 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.09548342972993851, 'learning_rate': 6.359410526210258e-05, 'epoch': 35.64}
>>> 2025-08-18 23:11:26,598 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.019287649542093277, 'learning_rate': 6.334156130140068e-05, 'epoch': 35.8}
>>> 2025-08-18 23:11:30,706 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.017511753365397453, 'learning_rate': 6.30886503527854e-05, 'epoch': 35.96}
>>> 2025-08-18 23:11:31,817 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.03342924267053604, 'learning_rate': 6.283537937310828e-05, 'epoch': 36.0}
>>> 2025-08-18 23:11:35,638 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.07192923128604889, 'learning_rate': 6.258175532912431e-05, 'epoch': 36.16}
>>> 2025-08-18 23:11:40,363 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.1354425996541977, 'learning_rate': 6.232778519730023e-05, 'epoch': 36.32}
>>> 2025-08-18 23:11:46,241 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.016192179173231125, 'learning_rate': 6.207347596362265e-05, 'epoch': 36.48}
>>> 2025-08-18 23:11:50,552 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.013161888346076012, 'learning_rate': 6.181883462340588e-05, 'epoch': 36.64}
>>> 2025-08-18 23:11:56,336 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.09415096044540405, 'learning_rate': 6.15638681810996e-05, 'epoch': 36.8}
>>> 2025-08-18 23:12:01,265 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.01341296173632145, 'learning_rate': 6.1308583650096e-05, 'epoch': 36.96}
>>> 2025-08-18 23:12:02,396 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.032469846308231354, 'learning_rate': 6.105298805253708e-05, 'epoch': 37.0}
>>> 2025-08-18 23:12:09,386 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.07422269135713577, 'learning_rate': 6.079708841912133e-05, 'epoch': 37.16}
>>> 2025-08-18 23:12:14,396 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.012089827097952366, 'learning_rate': 6.054089178891039e-05, 'epoch': 37.32}
>>> 2025-08-18 23:12:19,668 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.0685519427061081, 'learning_rate': 6.028440520913544e-05, 'epoch': 37.48}
>>> 2025-08-18 23:12:24,522 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.005144498776644468, 'learning_rate': 6.0027635735003316e-05, 'epoch': 37.64}
>>> 2025-08-18 23:12:29,679 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.13065829873085022, 'learning_rate': 5.9770590429502516e-05, 'epoch': 37.8}
>>> 2025-08-18 23:12:33,285 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.018640918657183647, 'learning_rate': 5.9513276363208784e-05, 'epoch': 37.96}
>>> 2025-08-18 23:12:33,756 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.925570061409077e-05, 'epoch': 38.0}
>>> 2025-08-18 23:12:38,146 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.07703384757041931, 'learning_rate': 5.8997870267315234e-05, 'epoch': 38.16}
>>> 2025-08-18 23:12:43,101 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.009370939806103706, 'learning_rate': 5.873979241505218e-05, 'epoch': 38.32}
>>> 2025-08-18 23:12:46,423 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.01394642237573862, 'learning_rate': 5.84814741562798e-05, 'epoch': 38.48}
>>> 2025-08-18 23:12:51,520 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.013226274400949478, 'learning_rate': 5.822292259658914e-05, 'epoch': 38.64}
>>> 2025-08-18 23:12:58,106 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.06863369047641754, 'learning_rate': 5.79641448479887e-05, 'epoch': 38.8}
>>> 2025-08-18 23:13:02,190 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0187369417399168, 'learning_rate': 5.770514802870879e-05, 'epoch': 38.96}
>>> 2025-08-18 23:13:03,326 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.027848688885569572, 'learning_rate': 5.7445939263005734e-05, 'epoch': 39.0}
>>> 2025-08-18 23:13:08,125 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.013313406147062778, 'learning_rate': 5.718652568096585e-05, 'epoch': 39.16}
>>> 2025-08-18 23:13:12,883 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.05108705163002014, 'learning_rate': 5.692691441830941e-05, 'epoch': 39.32}
>>> 2025-08-18 23:13:16,195 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.02439712919294834, 'learning_rate': 5.666711261619428e-05, 'epoch': 39.48}
>>> 2025-08-18 23:13:20,620 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.13807177543640137, 'learning_rate': 5.6407127421019534e-05, 'epoch': 39.64}
>>> 2025-08-18 23:13:24,134 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.016046566888689995, 'learning_rate': 5.614696598422885e-05, 'epoch': 39.8}
>>> 2025-08-18 23:13:30,771 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.07156931608915329, 'learning_rate': 5.5886635462113804e-05, 'epoch': 39.96}
>>> 2025-08-18 23:13:31,896 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.022516516968607903, 'learning_rate': 5.562614301561704e-05, 'epoch': 40.0}
>>> 2025-08-18 23:13:37,219 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.07772187888622284, 'learning_rate': 5.536549581013525e-05, 'epoch': 40.16}
>>> 2025-08-18 23:13:41,708 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.07602699846029282, 'learning_rate': 5.5104701015322125e-05, 'epoch': 40.32}
>>> 2025-08-18 23:13:47,927 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.003849644912406802, 'learning_rate': 5.48437658048911e-05, 'epoch': 40.48}
>>> 2025-08-18 23:13:51,979 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.02110930159687996, 'learning_rate': 5.4582697356418034e-05, 'epoch': 40.64}
>>> 2025-08-18 23:13:55,491 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.010424422100186348, 'learning_rate': 5.432150285114378e-05, 'epoch': 40.8}
>>> 2025-08-18 23:13:59,265 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.01273195818066597, 'learning_rate': 5.4060189473776676e-05, 'epoch': 40.96}
>>> 2025-08-18 23:13:59,885 - INFO - >>> {'loss': 0.0003, 'grad_norm': 0.033890191465616226, 'learning_rate': 5.379876441229486e-05, 'epoch': 41.0}
>>> 2025-08-18 23:14:03,621 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.015289274975657463, 'learning_rate': 5.3537234857748584e-05, 'epoch': 41.16}
>>> 2025-08-18 23:14:08,914 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.010172797366976738, 'learning_rate': 5.327560800406241e-05, 'epoch': 41.32}
>>> 2025-08-18 23:14:13,318 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.11168941855430603, 'learning_rate': 5.30138910478373e-05, 'epoch': 41.48}
>>> 2025-08-18 23:14:17,332 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.09011177718639374, 'learning_rate': 5.275209118815273e-05, 'epoch': 41.64}
>>> 2025-08-18 23:14:23,702 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.062407974153757095, 'learning_rate': 5.249021562636857e-05, 'epoch': 41.8}
>>> 2025-08-18 23:14:28,847 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.008598986081779003, 'learning_rate': 5.222827156592701e-05, 'epoch': 41.96}
>>> 2025-08-18 23:14:30,150 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.0219357218593359, 'learning_rate': 5.196626621215449e-05, 'epoch': 42.0}
>>> 2025-08-18 23:14:34,641 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.011959007941186428, 'learning_rate': 5.170420677206343e-05, 'epoch': 42.16}
>>> 2025-08-18 23:14:38,833 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.02086619846522808, 'learning_rate': 5.144210045415402e-05, 'epoch': 42.32}
>>> 2025-08-18 23:14:43,829 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.007073949556797743, 'learning_rate': 5.1179954468215915e-05, 'epoch': 42.48}
>>> 2025-08-18 23:14:48,769 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0032688353676348925, 'learning_rate': 5.0917776025129926e-05, 'epoch': 42.64}
>>> 2025-08-18 23:14:54,989 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.07227982580661774, 'learning_rate': 5.065557233666968e-05, 'epoch': 42.8}
>>> 2025-08-18 23:14:59,812 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.007676925044506788, 'learning_rate': 5.039335061530319e-05, 'epoch': 42.96}
>>> 2025-08-18 23:15:02,103 - INFO - >>> {'loss': 0.0071, 'grad_norm': 0.3363591134548187, 'learning_rate': 5.0131118073994556e-05, 'epoch': 43.0}
>>> 2025-08-18 23:15:05,960 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.015140675008296967, 'learning_rate': 4.986888192600546e-05, 'epoch': 43.16}
>>> 2025-08-18 23:15:12,095 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.005949033424258232, 'learning_rate': 4.9606649384696826e-05, 'epoch': 43.32}
>>> 2025-08-18 23:15:16,763 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.008990355767309666, 'learning_rate': 4.934442766333034e-05, 'epoch': 43.48}
>>> 2025-08-18 23:15:19,924 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.01705150119960308, 'learning_rate': 4.9082223974870086e-05, 'epoch': 43.64}
>>> 2025-08-18 23:15:25,282 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.03470096364617348, 'learning_rate': 4.8820045531784096e-05, 'epoch': 43.8}
>>> 2025-08-18 23:15:30,740 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.19851483404636383, 'learning_rate': 4.8557899545846e-05, 'epoch': 43.96}
>>> 2025-08-18 23:15:31,641 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.045459065586328506, 'learning_rate': 4.829579322793659e-05, 'epoch': 44.0}
>>> 2025-08-18 23:15:37,762 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.002166881924495101, 'learning_rate': 4.8033733787845535e-05, 'epoch': 44.16}
>>> 2025-08-18 23:15:41,125 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.02203340269625187, 'learning_rate': 4.7771728434073e-05, 'epoch': 44.32}
>>> 2025-08-18 23:15:45,969 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.05246927961707115, 'learning_rate': 4.7509784373631444e-05, 'epoch': 44.48}
>>> 2025-08-18 23:15:50,540 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.1416529268026352, 'learning_rate': 4.724790881184727e-05, 'epoch': 44.64}
>>> 2025-08-18 23:15:54,698 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.13306289911270142, 'learning_rate': 4.6986108952162695e-05, 'epoch': 44.8}
>>> 2025-08-18 23:16:00,094 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.005381083115935326, 'learning_rate': 4.6724391995937604e-05, 'epoch': 44.96}
>>> 2025-08-18 23:16:00,994 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.030954331159591675, 'learning_rate': 4.646276514225143e-05, 'epoch': 45.0}
>>> 2025-08-18 23:16:04,730 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.020105578005313873, 'learning_rate': 4.6201235587705154e-05, 'epoch': 45.16}
>>> 2025-08-18 23:16:08,911 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.11185196042060852, 'learning_rate': 4.5939810526223336e-05, 'epoch': 45.32}
>>> 2025-08-18 23:16:13,799 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.020133081823587418, 'learning_rate': 4.567849714885623e-05, 'epoch': 45.48}
>>> 2025-08-18 23:16:18,389 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.004711218643933535, 'learning_rate': 4.5417302643581985e-05, 'epoch': 45.64}
>>> 2025-08-18 23:16:24,237 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0010867643868550658, 'learning_rate': 4.5156234195108916e-05, 'epoch': 45.8}
>>> 2025-08-18 23:16:28,731 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.1750466525554657, 'learning_rate': 4.4895298984677886e-05, 'epoch': 45.96}
>>> 2025-08-18 23:16:30,737 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.12578780949115753, 'learning_rate': 4.4634504189864765e-05, 'epoch': 46.0}
>>> 2025-08-18 23:16:34,519 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.10036025941371918, 'learning_rate': 4.4373856984382984e-05, 'epoch': 46.16}
>>> 2025-08-18 23:16:39,813 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004683477804064751, 'learning_rate': 4.4113364537886215e-05, 'epoch': 46.32}
>>> 2025-08-18 23:16:44,890 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.007643351797014475, 'learning_rate': 4.385303401577118e-05, 'epoch': 46.48}
>>> 2025-08-18 23:16:48,546 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.006433811504393816, 'learning_rate': 4.359287257898049e-05, 'epoch': 46.64}
>>> 2025-08-18 23:16:53,522 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.10191885381937027, 'learning_rate': 4.333288738380573e-05, 'epoch': 46.8}
>>> 2025-08-18 23:16:58,174 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.009987132623791695, 'learning_rate': 4.3073085581690605e-05, 'epoch': 46.96}
>>> 2025-08-18 23:16:58,646 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.281347431903416e-05, 'epoch': 47.0}
>>> 2025-08-18 23:17:02,754 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.020400432869791985, 'learning_rate': 4.2554060736994284e-05, 'epoch': 47.16}
>>> 2025-08-18 23:17:06,663 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.014667509123682976, 'learning_rate': 4.229485197129122e-05, 'epoch': 47.32}
>>> 2025-08-18 23:17:12,863 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0022905562072992325, 'learning_rate': 4.203585515201131e-05, 'epoch': 47.48}
>>> 2025-08-18 23:17:17,248 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.08624711632728577, 'learning_rate': 4.177707740341087e-05, 'epoch': 47.64}
>>> 2025-08-18 23:17:22,531 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.09745950251817703, 'learning_rate': 4.1518525843720216e-05, 'epoch': 47.8}
>>> 2025-08-18 23:17:28,036 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.05047424137592316, 'learning_rate': 4.126020758494782e-05, 'epoch': 47.96}
>>> 2025-08-18 23:17:28,508 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.100212973268478e-05, 'epoch': 48.0}
>>> 2025-08-18 23:17:34,368 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.008252942003309727, 'learning_rate': 4.074429938590924e-05, 'epoch': 48.16}
>>> 2025-08-18 23:17:39,673 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0032488442957401276, 'learning_rate': 4.0486723636791234e-05, 'epoch': 48.32}
>>> 2025-08-18 23:17:42,807 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.01173826027661562, 'learning_rate': 4.022940957049751e-05, 'epoch': 48.48}
>>> 2025-08-18 23:17:46,797 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.1095028892159462, 'learning_rate': 3.9972364264996696e-05, 'epoch': 48.64}
>>> 2025-08-18 23:17:50,461 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.12894010543823242, 'learning_rate': 3.9715594790864586e-05, 'epoch': 48.8}
>>> 2025-08-18 23:17:55,767 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.07532110065221786, 'learning_rate': 3.945910821108963e-05, 'epoch': 48.96}
>>> 2025-08-18 23:17:57,031 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.036854393780231476, 'learning_rate': 3.920291158087869e-05, 'epoch': 49.0}
>>> 2025-08-18 23:18:01,521 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.012703105807304382, 'learning_rate': 3.894701194746291e-05, 'epoch': 49.16}
>>> 2025-08-18 23:18:07,844 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.003950438927859068, 'learning_rate': 3.869141634990399e-05, 'epoch': 49.32}
>>> 2025-08-18 23:18:13,131 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.04923038184642792, 'learning_rate': 3.8436131818900416e-05, 'epoch': 49.48}
>>> 2025-08-18 23:18:17,550 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.16553887724876404, 'learning_rate': 3.818116537659412e-05, 'epoch': 49.64}
>>> 2025-08-18 23:18:22,861 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.09152540564537048, 'learning_rate': 3.7926524036377364e-05, 'epoch': 49.8}
>>> 2025-08-18 23:18:26,161 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.007606880739331245, 'learning_rate': 3.767221480269978e-05, 'epoch': 49.96}
>>> 2025-08-18 23:18:26,920 - INFO - >>> {'loss': 0.0002, 'grad_norm': 0.01896461471915245, 'learning_rate': 3.741824467087569e-05, 'epoch': 50.0}
>>> 2025-08-18 23:18:32,283 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.10401087254285812, 'learning_rate': 3.716462062689172e-05, 'epoch': 50.16}
>>> 2025-08-18 23:18:37,898 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0049211555160582066, 'learning_rate': 3.691134964721462e-05, 'epoch': 50.32}
>>> 2025-08-18 23:18:42,768 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.08549652248620987, 'learning_rate': 3.665843869859934e-05, 'epoch': 50.48}
>>> 2025-08-18 23:18:47,955 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.08420350402593613, 'learning_rate': 3.6405894737897414e-05, 'epoch': 50.64}
>>> 2025-08-18 23:18:52,694 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0047166734002530575, 'learning_rate': 3.615372471186562e-05, 'epoch': 50.8}
>>> 2025-08-18 23:18:55,621 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.026511207222938538, 'learning_rate': 3.5901935556974834e-05, 'epoch': 50.96}
>>> 2025-08-18 23:18:56,893 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0226121935993433, 'learning_rate': 3.5650534199219296e-05, 'epoch': 51.0}
>>> 2025-08-18 23:19:00,603 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.09136425703763962, 'learning_rate': 3.539952755392605e-05, 'epoch': 51.16}
>>> 2025-08-18 23:19:05,188 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.09198153018951416, 'learning_rate': 3.514892252556474e-05, 'epoch': 51.32}
>>> 2025-08-18 23:19:10,372 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.005594566464424133, 'learning_rate': 3.489872600755765e-05, 'epoch': 51.48}
>>> 2025-08-18 23:19:14,835 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.007446436211466789, 'learning_rate': 3.464894488209022e-05, 'epoch': 51.64}
>>> 2025-08-18 23:19:20,016 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.005569907370954752, 'learning_rate': 3.4399586019921534e-05, 'epoch': 51.8}
>>> 2025-08-18 23:19:24,420 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.004945731721818447, 'learning_rate': 3.415065628019547e-05, 'epoch': 51.96}
>>> 2025-08-18 23:19:25,344 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.03125704079866409, 'learning_rate': 3.3902162510252e-05, 'epoch': 52.0}
>>> 2025-08-18 23:19:30,726 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0031676338985562325, 'learning_rate': 3.365411154543878e-05, 'epoch': 52.16}
>>> 2025-08-18 23:19:34,912 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.009780405089259148, 'learning_rate': 3.3406510208923224e-05, 'epoch': 52.32}
>>> 2025-08-18 23:19:39,474 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.10925029218196869, 'learning_rate': 3.315936531150473e-05, 'epoch': 52.48}
>>> 2025-08-18 23:19:44,435 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.009141850285232067, 'learning_rate': 3.291268365142738e-05, 'epoch': 52.64}
>>> 2025-08-18 23:19:49,448 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.008223462849855423, 'learning_rate': 3.266647201419294e-05, 'epoch': 52.8}
>>> 2025-08-18 23:19:55,521 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.10287006944417953, 'learning_rate': 3.242073717237418e-05, 'epoch': 52.96}
>>> 2025-08-18 23:19:56,654 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.028684232383966446, 'learning_rate': 3.217548588542864e-05, 'epoch': 53.0}
>>> 2025-08-18 23:20:00,114 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.008655928075313568, 'learning_rate': 3.193072489951263e-05, 'epoch': 53.16}
>>> 2025-08-18 23:20:05,912 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.04767279326915741, 'learning_rate': 3.1686460947295695e-05, 'epoch': 53.32}
>>> 2025-08-18 23:20:09,816 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.10958976298570633, 'learning_rate': 3.1442700747775414e-05, 'epoch': 53.48}
>>> 2025-08-18 23:20:13,683 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004106566775590181, 'learning_rate': 3.1199451006092584e-05, 'epoch': 53.64}
>>> 2025-08-18 23:20:19,484 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.00401054322719574, 'learning_rate': 3.095671841334678e-05, 'epoch': 53.8}
>>> 2025-08-18 23:20:22,796 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.015488306991755962, 'learning_rate': 3.0714509646412296e-05, 'epoch': 53.96}
>>> 2025-08-18 23:20:24,802 - INFO - >>> {'loss': 0.0078, 'grad_norm': 0.3863213062286377, 'learning_rate': 3.0472831367754494e-05, 'epoch': 54.0}
>>> 2025-08-18 23:20:27,435 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.030297592282295227, 'learning_rate': 3.0231690225246535e-05, 'epoch': 54.16}
>>> 2025-08-18 23:20:32,208 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.09776744991540909, 'learning_rate': 2.999109285198649e-05, 'epoch': 54.32}
>>> 2025-08-18 23:20:36,321 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.003818454220890999, 'learning_rate': 2.9751045866114922e-05, 'epoch': 54.48}
>>> 2025-08-18 23:20:41,335 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.05785239115357399, 'learning_rate': 2.9511555870632824e-05, 'epoch': 54.64}
>>> 2025-08-18 23:20:46,042 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0019847347866743803, 'learning_rate': 2.927262945321998e-05, 'epoch': 54.8}
>>> 2025-08-18 23:20:52,229 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.004348308779299259, 'learning_rate': 2.9034273186053755e-05, 'epoch': 54.96}
>>> 2025-08-18 23:20:53,803 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.023933786898851395, 'learning_rate': 2.8796493625628356e-05, 'epoch': 55.0}
>>> 2025-08-18 23:20:59,219 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.0315011702477932, 'learning_rate': 2.8559297312574417e-05, 'epoch': 55.16}
>>> 2025-08-18 23:21:04,480 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.009128526784479618, 'learning_rate': 2.832269077147913e-05, 'epoch': 55.32}
>>> 2025-08-18 23:21:08,950 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.006877806503325701, 'learning_rate': 2.8086680510706774e-05, 'epoch': 55.48}
>>> 2025-08-18 23:21:13,749 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.12256662547588348, 'learning_rate': 2.7851273022219644e-05, 'epoch': 55.64}
>>> 2025-08-18 23:21:17,882 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.01470860093832016, 'learning_rate': 2.7616474781399526e-05, 'epoch': 55.8}
>>> 2025-08-18 23:21:23,075 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.13231560587882996, 'learning_rate': 2.7382292246869547e-05, 'epoch': 55.96}
>>> 2025-08-18 23:21:25,474 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.027249814942479134, 'learning_rate': 2.7148731860316546e-05, 'epoch': 56.0}
>>> 2025-08-18 23:21:29,193 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.008665354922413826, 'learning_rate': 2.6915800046313848e-05, 'epoch': 56.16}
>>> 2025-08-18 23:21:32,786 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.016938848420977592, 'learning_rate': 2.6683503212144563e-05, 'epoch': 56.32}
>>> 2025-08-18 23:21:37,622 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.01104164868593216, 'learning_rate': 2.645184774762533e-05, 'epoch': 56.48}
>>> 2025-08-18 23:21:43,498 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.07561159878969193, 'learning_rate': 2.622084002493056e-05, 'epoch': 56.64}
>>> 2025-08-18 23:21:47,698 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.007749611511826515, 'learning_rate': 2.599048639841717e-05, 'epoch': 56.8}
>>> 2025-08-18 23:21:53,854 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.07736805826425552, 'learning_rate': 2.5760793204449735e-05, 'epoch': 56.96}
>>> 2025-08-18 23:21:54,775 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.022214755415916443, 'learning_rate': 2.5531766761226272e-05, 'epoch': 57.0}
>>> 2025-08-18 23:22:00,119 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.058853257447481155, 'learning_rate': 2.530341336860439e-05, 'epoch': 57.16}
>>> 2025-08-18 23:22:06,885 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0041630081832408905, 'learning_rate': 2.5075739307928014e-05, 'epoch': 57.32}
>>> 2025-08-18 23:22:10,249 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.017444437369704247, 'learning_rate': 2.4848750841854616e-05, 'epoch': 57.48}
>>> 2025-08-18 23:22:15,757 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.0835849791765213, 'learning_rate': 2.4622454214182917e-05, 'epoch': 57.64}
>>> 2025-08-18 23:22:18,507 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.021244481205940247, 'learning_rate': 2.4396855649681166e-05, 'epoch': 57.8}
>>> 2025-08-18 23:22:22,764 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.003018573857843876, 'learning_rate': 2.417196135391591e-05, 'epoch': 57.96}
>>> 2025-08-18 23:22:23,892 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.028449414297938347, 'learning_rate': 2.3947777513081292e-05, 'epoch': 58.0}
>>> 2025-08-18 23:22:28,314 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.06080387532711029, 'learning_rate': 2.372431029382888e-05, 'epoch': 58.16}
>>> 2025-08-18 23:22:33,830 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.002518431516364217, 'learning_rate': 2.350156584309804e-05, 'epoch': 58.32}
>>> 2025-08-18 23:22:36,996 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.24880103766918182, 'learning_rate': 2.327955028794688e-05, 'epoch': 58.48}
>>> 2025-08-18 23:22:41,278 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.003961901646107435, 'learning_rate': 2.305826973538366e-05, 'epoch': 58.64}
>>> 2025-08-18 23:22:45,515 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.010103500448167324, 'learning_rate': 2.2837730272198888e-05, 'epoch': 58.8}
>>> 2025-08-18 23:22:51,772 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.06926265358924866, 'learning_rate': 2.2617937964797785e-05, 'epoch': 58.96}
>>> 2025-08-18 23:22:52,894 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.021034495905041695, 'learning_rate': 2.2398898859033494e-05, 'epoch': 59.0}
>>> 2025-08-18 23:22:58,062 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.05239913612604141, 'learning_rate': 2.2180618980040747e-05, 'epoch': 59.16}
>>> 2025-08-18 23:23:03,290 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.006746627856045961, 'learning_rate': 2.1963104332070127e-05, 'epoch': 59.32}
>>> 2025-08-18 23:23:07,430 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.006736589130014181, 'learning_rate': 2.1746360898322933e-05, 'epoch': 59.48}
>>> 2025-08-18 23:23:13,326 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.003490633796900511, 'learning_rate': 2.1530394640786567e-05, 'epoch': 59.64}
>>> 2025-08-18 23:23:18,768 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.007120236288756132, 'learning_rate': 2.1315211500070558e-05, 'epoch': 59.8}
>>> 2025-08-18 23:23:22,484 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.015092043206095695, 'learning_rate': 2.1100817395243157e-05, 'epoch': 59.96}
>>> 2025-08-18 23:23:23,758 - INFO - >>> {'loss': 0.008, 'grad_norm': 0.38937950134277344, 'learning_rate': 2.088721822366849e-05, 'epoch': 60.0}
>>> 2025-08-18 23:23:27,335 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.01424422301352024, 'learning_rate': 2.0674419860844384e-05, 'epoch': 60.16}
>>> 2025-08-18 23:23:30,500 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.015150327235460281, 'learning_rate': 2.046242816024071e-05, 'epoch': 60.32}
>>> 2025-08-18 23:23:35,366 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.010817834176123142, 'learning_rate': 2.0251248953138374e-05, 'epoch': 60.48}
>>> 2025-08-18 23:23:40,609 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.06471984833478928, 'learning_rate': 2.0040888048468954e-05, 'epoch': 60.64}
>>> 2025-08-18 23:23:46,345 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.12280818074941635, 'learning_rate': 1.9831351232654872e-05, 'epoch': 60.8}
>>> 2025-08-18 23:23:52,842 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0013751363148912787, 'learning_rate': 1.962264426945023e-05, 'epoch': 60.96}
>>> 2025-08-18 23:23:53,742 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.030011771246790886, 'learning_rate': 1.9414772899782276e-05, 'epoch': 61.0}
>>> 2025-08-18 23:23:58,470 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.08800584077835083, 'learning_rate': 1.920774284159353e-05, 'epoch': 61.16}
>>> 2025-08-18 23:24:02,879 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.010932614095509052, 'learning_rate': 1.9001559789684404e-05, 'epoch': 61.32}
>>> 2025-08-18 23:24:09,210 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.08460935205221176, 'learning_rate': 1.8796229415556628e-05, 'epoch': 61.48}
>>> 2025-08-18 23:24:13,837 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.009618093259632587, 'learning_rate': 1.859175736725724e-05, 'epoch': 61.64}
>>> 2025-08-18 23:24:19,808 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.009221166372299194, 'learning_rate': 1.8388149269223153e-05, 'epoch': 61.8}
>>> 2025-08-18 23:24:24,178 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.06094168871641159, 'learning_rate': 1.8185410722126556e-05, 'epoch': 61.96}
>>> 2025-08-18 23:24:26,011 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.02435390278697014, 'learning_rate': 1.798354730272077e-05, 'epoch': 62.0}
>>> 2025-08-18 23:24:28,936 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.01845930889248848, 'learning_rate': 1.7782564563686884e-05, 'epoch': 62.16}
>>> 2025-08-18 23:24:33,489 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.006386721972376108, 'learning_rate': 1.7582468033480992e-05, 'epoch': 62.32}
>>> 2025-08-18 23:24:37,377 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.007958500646054745, 'learning_rate': 1.7383263216182157e-05, 'epoch': 62.48}
>>> 2025-08-18 23:24:43,061 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.0035728972870856524, 'learning_rate': 1.7184955591340974e-05, 'epoch': 62.64}
>>> 2025-08-18 23:24:47,850 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.07027547806501389, 'learning_rate': 1.6987550613828862e-05, 'epoch': 62.8}
>>> 2025-08-18 23:24:53,367 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.09729073941707611, 'learning_rate': 1.679105371368802e-05, 'epoch': 62.96}
>>> 2025-08-18 23:24:55,165 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.023822395130991936, 'learning_rate': 1.6595470295982045e-05, 'epoch': 63.0}
>>> 2025-08-18 23:24:58,326 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.019894642755389214, 'learning_rate': 1.6400805740647267e-05, 'epoch': 63.16}
>>> 2025-08-18 23:25:02,856 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.12326792627573013, 'learning_rate': 1.6207065402344747e-05, 'epoch': 63.32}
>>> 2025-08-18 23:25:09,201 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004140727687627077, 'learning_rate': 1.6014254610313033e-05, 'epoch': 63.48}
>>> 2025-08-18 23:25:14,257 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0031778442207723856, 'learning_rate': 1.582237866822151e-05, 'epoch': 63.64}
>>> 2025-08-18 23:25:19,707 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.07212559878826141, 'learning_rate': 1.563144285402453e-05, 'epoch': 63.8}
>>> 2025-08-18 23:25:24,448 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.003832305781543255, 'learning_rate': 1.5441452419816237e-05, 'epoch': 63.96}
>>> 2025-08-18 23:25:25,162 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.0279505867511034, 'learning_rate': 1.5252412591686105e-05, 'epoch': 64.0}
>>> 2025-08-18 23:25:30,550 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.0675617903470993, 'learning_rate': 1.5064328569575165e-05, 'epoch': 64.16}
>>> 2025-08-18 23:25:35,206 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.010759114287793636, 'learning_rate': 1.4877205527132982e-05, 'epoch': 64.32}
>>> 2025-08-18 23:25:40,523 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.002295452170073986, 'learning_rate': 1.4691048611575337e-05, 'epoch': 64.48}
>>> 2025-08-18 23:25:44,542 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.008399845100939274, 'learning_rate': 1.4505862943542642e-05, 'epoch': 64.64}
>>> 2025-08-18 23:25:48,598 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.1031566709280014, 'learning_rate': 1.4321653616959097e-05, 'epoch': 64.8}
>>> 2025-08-18 23:25:54,301 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.004158213268965483, 'learning_rate': 1.4138425698892555e-05, 'epoch': 64.96}
>>> 2025-08-18 23:25:55,363 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.03313802555203438, 'learning_rate': 1.3956184229415148e-05, 'epoch': 65.0}
>>> 2025-08-18 23:25:59,174 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.013536494225263596, 'learning_rate': 1.3774934221464642e-05, 'epoch': 65.16}
>>> 2025-08-18 23:26:03,999 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.008767371065914631, 'learning_rate': 1.359468066070657e-05, 'epoch': 65.32}
>>> 2025-08-18 23:26:07,954 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.009114250540733337, 'learning_rate': 1.341542850539706e-05, 'epoch': 65.48}
>>> 2025-08-18 23:26:13,010 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.09854069352149963, 'learning_rate': 1.3237182686246468e-05, 'epoch': 65.64}
>>> 2025-08-18 23:26:16,853 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.008624358102679253, 'learning_rate': 1.3059948106283725e-05, 'epoch': 65.8}
>>> 2025-08-18 23:26:23,896 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0011726479278877378, 'learning_rate': 1.2883729640721531e-05, 'epoch': 65.96}
>>> 2025-08-18 23:26:24,829 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.28316494822502136, 'learning_rate': 1.2708532136822155e-05, 'epoch': 66.0}
>>> 2025-08-18 23:26:28,968 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004200359340757132, 'learning_rate': 1.2534360413764169e-05, 'epoch': 66.16}
>>> 2025-08-18 23:26:33,382 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.007350051309913397, 'learning_rate': 1.2361219262509883e-05, 'epoch': 66.32}
>>> 2025-08-18 23:26:38,405 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.006483597680926323, 'learning_rate': 1.2189113445673528e-05, 'epoch': 66.48}
>>> 2025-08-18 23:26:43,464 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.09412561357021332, 'learning_rate': 1.2018047697390279e-05, 'epoch': 66.64}
>>> 2025-08-18 23:26:49,197 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.10470934957265854, 'learning_rate': 1.1848026723186012e-05, 'epoch': 66.8}
>>> 2025-08-18 23:26:54,287 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004741950426250696, 'learning_rate': 1.1679055199847893e-05, 'epoch': 66.96}
>>> 2025-08-18 23:26:55,655 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.19118908047676086, 'learning_rate': 1.1511137775295704e-05, 'epoch': 67.0}
>>> 2025-08-18 23:27:01,627 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.001981391105800867, 'learning_rate': 1.1344279068454011e-05, 'epoch': 67.16}
>>> 2025-08-18 23:27:05,904 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.011281667277216911, 'learning_rate': 1.1178483669125112e-05, 'epoch': 67.32}
>>> 2025-08-18 23:27:10,075 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.009347238577902317, 'learning_rate': 1.101375613786278e-05, 'epoch': 67.48}
>>> 2025-08-18 23:27:14,898 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0026519198436290026, 'learning_rate': 1.0850101005846786e-05, 'epoch': 67.64}
>>> 2025-08-18 23:27:18,662 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.18734996020793915, 'learning_rate': 1.0687522774758319e-05, 'epoch': 67.8}
>>> 2025-08-18 23:27:24,129 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.1168823093175888, 'learning_rate': 1.0526025916656119e-05, 'epoch': 67.96}
>>> 2025-08-18 23:27:25,402 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.11865725368261337, 'learning_rate': 1.0365614873853462e-05, 'epoch': 68.0}
>>> 2025-08-18 23:27:30,359 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.003922325558960438, 'learning_rate': 1.0206294058795973e-05, 'epoch': 68.16}
>>> 2025-08-18 23:27:35,032 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.007783702574670315, 'learning_rate': 1.0048067853940285e-05, 'epoch': 68.32}
>>> 2025-08-18 23:27:38,922 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004926962312310934, 'learning_rate': 9.890940611633414e-06, 'epoch': 68.48}
>>> 2025-08-18 23:27:45,097 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.08029253035783768, 'learning_rate': 9.734916653993103e-06, 'epoch': 68.64}
>>> 2025-08-18 23:27:50,075 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.09326094388961792, 'learning_rate': 9.580000272788914e-06, 'epoch': 68.8}
>>> 2025-08-18 23:27:54,987 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0039893873035907745, 'learning_rate': 9.426195729324161e-06, 'epoch': 68.96}
>>> 2025-08-18 23:27:55,457 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.27350725431872e-06, 'epoch': 69.0}
>>> 2025-08-18 23:27:59,669 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.04366350546479225, 'learning_rate': 9.121939047792621e-06, 'epoch': 69.16}
>>> 2025-08-18 23:28:04,060 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.005757212173193693, 'learning_rate': 8.971495278950559e-06, 'epoch': 69.32}
>>> 2025-08-18 23:28:09,454 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.109444759786129, 'learning_rate': 8.82218008606716e-06, 'epoch': 69.48}
>>> 2025-08-18 23:28:15,238 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0069655380211770535, 'learning_rate': 8.673997576373205e-06, 'epoch': 69.64}
>>> 2025-08-18 23:28:20,287 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0049010710790753365, 'learning_rate': 8.526951825942609e-06, 'epoch': 69.8}
>>> 2025-08-18 23:28:24,651 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.12912146747112274, 'learning_rate': 8.381046879580306e-06, 'epoch': 69.96}
>>> 2025-08-18 23:28:25,971 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.019533630460500717, 'learning_rate': 8.23628675071102e-06, 'epoch': 70.0}
>>> 2025-08-18 23:28:31,891 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.0050545730628073215, 'learning_rate': 8.092675421268826e-06, 'epoch': 70.16}
>>> 2025-08-18 23:28:36,745 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.08569394052028656, 'learning_rate': 7.950216841587638e-06, 'epoch': 70.32}
>>> 2025-08-18 23:28:42,486 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.08876031637191772, 'learning_rate': 7.808914930292543e-06, 'epoch': 70.48}
>>> 2025-08-18 23:28:46,112 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.009339418262243271, 'learning_rate': 7.66877357419204e-06, 'epoch': 70.64}
>>> 2025-08-18 23:28:51,846 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0017646568594500422, 'learning_rate': 7.5297966281710705e-06, 'epoch': 70.8}
>>> 2025-08-18 23:28:54,964 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.014719669707119465, 'learning_rate': 7.391987915085013e-06, 'epoch': 70.96}
>>> 2025-08-18 23:28:56,558 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.026792148128151894, 'learning_rate': 7.255351225654527e-06, 'epoch': 71.0}
>>> 2025-08-18 23:29:02,225 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0025009524542838335, 'learning_rate': 7.119890318361277e-06, 'epoch': 71.16}
>>> 2025-08-18 23:29:05,612 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.014401976019144058, 'learning_rate': 6.98560891934455e-06, 'epoch': 71.32}
>>> 2025-08-18 23:29:08,500 - INFO - >>> {'loss': 0.0003, 'grad_norm': 0.013641014695167542, 'learning_rate': 6.852510722298761e-06, 'epoch': 71.48}
>>> 2025-08-18 23:29:14,323 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.05227920413017273, 'learning_rate': 6.72059938837184e-06, 'epoch': 71.64}
>>> 2025-08-18 23:29:20,399 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.052947018295526505, 'learning_rate': 6.589878546064543e-06, 'epoch': 71.8}
>>> 2025-08-18 23:29:24,880 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.12690497934818268, 'learning_rate': 6.46035179113062e-06, 'epoch': 71.96}
>>> 2025-08-18 23:29:26,249 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.028972197324037552, 'learning_rate': 6.332022686477928e-06, 'epoch': 72.0}
>>> 2025-08-18 23:29:28,329 - INFO - >>> {'loss': 0.0003, 'grad_norm': 0.02766072191298008, 'learning_rate': 6.204894762070407e-06, 'epoch': 72.16}
>>> 2025-08-18 23:29:33,522 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004009603522717953, 'learning_rate': 6.078971514830989e-06, 'epoch': 72.32}
>>> 2025-08-18 23:29:38,638 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.10923086106777191, 'learning_rate': 5.9542564085454165e-06, 'epoch': 72.48}
>>> 2025-08-18 23:29:42,842 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.007445318624377251, 'learning_rate': 5.830752873766948e-06, 'epoch': 72.64}
>>> 2025-08-18 23:29:47,966 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.0032042977400124073, 'learning_rate': 5.708464307722006e-06, 'epoch': 72.8}
>>> 2025-08-18 23:29:53,940 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0009411493665538728, 'learning_rate': 5.587394074216712e-06, 'epoch': 72.96}
>>> 2025-08-18 23:29:54,841 - INFO - >>> {'loss': 0.0098, 'grad_norm': 0.559840738773346, 'learning_rate': 5.46754550354438e-06, 'epoch': 73.0}
>>> 2025-08-18 23:29:59,781 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.007131586782634258, 'learning_rate': 5.348921892393904e-06, 'epoch': 73.16}
>>> 2025-08-18 23:30:03,805 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.009562677703797817, 'learning_rate': 5.231526503759054e-06, 'epoch': 73.32}
>>> 2025-08-18 23:30:08,252 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.006855725310742855, 'learning_rate': 5.115362566848747e-06, 'epoch': 73.48}
>>> 2025-08-18 23:30:13,244 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.07663769274950027, 'learning_rate': 5.000433276998218e-06, 'epoch': 73.64}
>>> 2025-08-18 23:30:18,811 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.10022594034671783, 'learning_rate': 4.886741795581101e-06, 'epoch': 73.8}
>>> 2025-08-18 23:30:24,278 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.03847019746899605, 'learning_rate': 4.774291249922508e-06, 'epoch': 73.96}
>>> 2025-08-18 23:30:25,033 - INFO - >>> {'loss': 0.0002, 'grad_norm': 0.014926127158105373, 'learning_rate': 4.6630847332129575e-06, 'epoch': 74.0}
>>> 2025-08-18 23:30:29,889 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0058985063806176186, 'learning_rate': 4.553125304423339e-06, 'epoch': 74.16}
>>> 2025-08-18 23:30:34,326 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.11738493293523788, 'learning_rate': 4.44441598822074e-06, 'epoch': 74.32}
>>> 2025-08-18 23:30:40,393 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.09695232659578323, 'learning_rate': 4.336959774885241e-06, 'epoch': 74.48}
>>> 2025-08-18 23:30:44,859 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.008986622095108032, 'learning_rate': 4.2307596202276815e-06, 'epoch': 74.64}
>>> 2025-08-18 23:30:49,279 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0029709753580391407, 'learning_rate': 4.1258184455083505e-06, 'epoch': 74.8}
>>> 2025-08-18 23:30:53,860 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.0443987175822258, 'learning_rate': 4.022139137356623e-06, 'epoch': 74.96}
>>> 2025-08-18 23:30:55,573 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.034455131739377975, 'learning_rate': 3.919724547691556e-06, 'epoch': 75.0}
>>> 2025-08-18 23:31:01,855 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.07581954449415207, 'learning_rate': 3.818577493643444e-06, 'epoch': 75.16}
>>> 2025-08-18 23:31:06,000 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.0787229835987091, 'learning_rate': 3.7187007574763232e-06, 'epoch': 75.32}
>>> 2025-08-18 23:31:10,624 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.11214999109506607, 'learning_rate': 3.6200970865114704e-06, 'epoch': 75.48}
>>> 2025-08-18 23:31:14,269 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.01021606381982565, 'learning_rate': 3.522769193051789e-06, 'epoch': 75.64}
>>> 2025-08-18 23:31:20,360 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0014609472127631307, 'learning_rate': 3.426719754307206e-06, 'epoch': 75.8}
>>> 2025-08-18 23:31:23,496 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.007761560380458832, 'learning_rate': 3.331951412321066e-06, 'epoch': 75.96}
>>> 2025-08-18 23:31:25,908 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.023265060037374496, 'learning_rate': 3.2384667738974196e-06, 'epoch': 76.0}
>>> 2025-08-18 23:31:30,387 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.0023405267857015133, 'learning_rate': 3.1462684105293293e-06, 'epoch': 76.16}
>>> 2025-08-18 23:31:35,190 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0015553507255390286, 'learning_rate': 3.0553588583281444e-06, 'epoch': 76.32}
>>> 2025-08-18 23:31:40,793 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.005074668675661087, 'learning_rate': 2.965740617953733e-06, 'epoch': 76.48}
>>> 2025-08-18 23:31:45,206 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.1277151256799698, 'learning_rate': 2.877416154545681e-06, 'epoch': 76.64}
>>> 2025-08-18 23:31:48,868 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.011517118662595749, 'learning_rate': 2.7903878976555163e-06, 'epoch': 76.8}
>>> 2025-08-18 23:31:55,477 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.0636163204908371, 'learning_rate': 2.7046582411798473e-06, 'epoch': 76.96}
>>> 2025-08-18 23:31:56,543 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.02406676858663559, 'learning_rate': 2.620229543294528e-06, 'epoch': 77.0}
>>> 2025-08-18 23:32:01,787 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0031256824731826782, 'learning_rate': 2.537104126389794e-06, 'epoch': 77.16}
>>> 2025-08-18 23:32:06,372 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.008950839750468731, 'learning_rate': 2.4552842770063757e-06, 'epoch': 77.32}
>>> 2025-08-18 23:32:09,570 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.011056013405323029, 'learning_rate': 2.3747722457725996e-06, 'epoch': 77.48}
>>> 2025-08-18 23:32:14,920 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.01605074852705002, 'learning_rate': 2.2955702473424824e-06, 'epoch': 77.64}
>>> 2025-08-18 23:32:20,067 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.03137494996190071, 'learning_rate': 2.217680460334809e-06, 'epoch': 77.8}
>>> 2025-08-18 23:32:24,711 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.12763969600200653, 'learning_rate': 2.141105027273227e-06, 'epoch': 77.96}
>>> 2025-08-18 23:32:26,785 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.226125106215477, 'learning_rate': 2.065846054527265e-06, 'epoch': 78.0}
>>> 2025-08-18 23:32:31,573 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.014428003691136837, 'learning_rate': 1.9919056122544465e-06, 'epoch': 78.16}
>>> 2025-08-18 23:32:37,901 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.07358027249574661, 'learning_rate': 1.919285734343307e-06, 'epoch': 78.32}
>>> 2025-08-18 23:32:42,219 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.0015347019070759416, 'learning_rate': 1.8479884183574657e-06, 'epoch': 78.48}
>>> 2025-08-18 23:32:47,117 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.1414608359336853, 'learning_rate': 1.7780156254806779e-06, 'epoch': 78.64}
>>> 2025-08-18 23:32:50,946 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.08002154529094696, 'learning_rate': 1.7093692804628635e-06, 'epoch': 78.8}
>>> 2025-08-18 23:32:56,280 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0012557763839140534, 'learning_rate': 1.6420512715672131e-06, 'epoch': 78.96}
>>> 2025-08-18 23:32:58,198 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.029906217008829117, 'learning_rate': 1.5760634505182004e-06, 'epoch': 79.0}
>>> 2025-08-18 23:33:03,259 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.003511270275339484, 'learning_rate': 1.5114076324506565e-06, 'epoch': 79.16}
>>> 2025-08-18 23:33:08,295 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004169537220150232, 'learning_rate': 1.4480855958598715e-06, 'epoch': 79.32}
>>> 2025-08-18 23:33:13,017 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.08068802952766418, 'learning_rate': 1.3860990825526333e-06, 'epoch': 79.48}
>>> 2025-08-18 23:33:18,143 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.0790487602353096, 'learning_rate': 1.3254497975993264e-06, 'epoch': 79.64}
>>> 2025-08-18 23:33:22,403 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.01520640030503273, 'learning_rate': 1.2661394092870537e-06, 'epoch': 79.8}
>>> 2025-08-18 23:33:25,758 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.014693567529320717, 'learning_rate': 1.2081695490737178e-06, 'epoch': 79.96}
>>> 2025-08-18 23:33:27,592 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.02483856864273548, 'learning_rate': 1.1515418115431553e-06, 'epoch': 80.0}
>>> 2025-08-18 23:33:32,005 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.1246614158153534, 'learning_rate': 1.0962577543612795e-06, 'epoch': 80.16}
>>> 2025-08-18 23:33:37,898 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.002754403045400977, 'learning_rate': 1.04231889823323e-06, 'epoch': 80.32}
>>> 2025-08-18 23:33:43,423 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0007895840099081397, 'learning_rate': 9.897267268615284e-07, 'epoch': 80.48}
>>> 2025-08-18 23:33:48,213 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.009380772709846497, 'learning_rate': 9.384826869052898e-07, 'epoch': 80.64}
>>> 2025-08-18 23:33:52,301 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.014402750879526138, 'learning_rate': 8.885881879404201e-07, 'epoch': 80.8}
>>> 2025-08-18 23:33:56,008 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.12320474535226822, 'learning_rate': 8.400446024208309e-07, 'epoch': 80.96}
>>> 2025-08-18 23:33:57,581 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.18811608850955963, 'learning_rate': 7.928532656407029e-07, 'epoch': 81.0}
>>> 2025-08-18 23:34:02,242 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.05784808471798897, 'learning_rate': 7.470154756977543e-07, 'epoch': 81.16}
>>> 2025-08-18 23:34:07,841 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.10028359293937683, 'learning_rate': 7.025324934575139e-07, 'epoch': 81.32}
>>> 2025-08-18 23:34:13,057 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.006725553888827562, 'learning_rate': 6.594055425186763e-07, 'epoch': 81.48}
>>> 2025-08-18 23:34:17,171 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.007739493157714605, 'learning_rate': 6.176358091794011e-07, 'epoch': 81.64}
>>> 2025-08-18 23:34:22,875 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0017092523630708456, 'learning_rate': 5.772244424047169e-07, 'epoch': 81.8}
>>> 2025-08-18 23:34:27,540 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.007194116711616516, 'learning_rate': 5.381725537948856e-07, 'epoch': 81.96}
>>> 2025-08-18 23:34:28,440 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.029552021995186806, 'learning_rate': 5.004812175548656e-07, 'epoch': 82.0}
>>> 2025-08-18 23:34:32,397 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.012835017405450344, 'learning_rate': 4.641514704647132e-07, 'epoch': 82.16}
>>> 2025-08-18 23:34:37,232 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.005210192874073982, 'learning_rate': 4.2918431185110517e-07, 'epoch': 82.32}
>>> 2025-08-18 23:34:42,703 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.004064058419317007, 'learning_rate': 3.9558070355983357e-07, 'epoch': 82.48}
>>> 2025-08-18 23:34:47,860 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0030486932955682278, 'learning_rate': 3.6334156992935406e-07, 'epoch': 82.64}
>>> 2025-08-18 23:34:52,983 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0049436381086707115, 'learning_rate': 3.324677977653401e-07, 'epoch': 82.8}
>>> 2025-08-18 23:34:58,111 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.05957726761698723, 'learning_rate': 3.0296023631631865e-07, 'epoch': 82.96}
>>> 2025-08-18 23:34:59,241 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.021899938583374023, 'learning_rate': 2.748196972502892e-07, 'epoch': 83.0}
>>> 2025-08-18 23:35:04,929 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004962593782693148, 'learning_rate': 2.4804695463240826e-07, 'epoch': 83.16}
>>> 2025-08-18 23:35:09,074 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0089088324457407, 'learning_rate': 2.226427449036894e-07, 'epoch': 83.32}
>>> 2025-08-18 23:35:13,232 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.06971079111099243, 'learning_rate': 1.9860776686075332e-07, 'epoch': 83.48}
>>> 2025-08-18 23:35:17,155 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.1236102432012558, 'learning_rate': 1.7594268163659278e-07, 'epoch': 83.64}
>>> 2025-08-18 23:35:23,397 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0015684941317886114, 'learning_rate': 1.546481126824151e-07, 'epoch': 83.8}
>>> 2025-08-18 23:35:29,306 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.0748811736702919, 'learning_rate': 1.347246457504503e-07, 'epoch': 83.96}
>>> 2025-08-18 23:35:29,782 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.1617282887787518e-07, 'epoch': 84.0}
>>> 2025-08-18 23:35:35,477 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.09199398010969162, 'learning_rate': 9.899317237172523e-08, 'epoch': 84.16}
>>> 2025-08-18 23:35:39,761 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.0719766691327095, 'learning_rate': 8.318614879485043e-08, 'epoch': 84.32}
>>> 2025-08-18 23:35:43,573 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.014738624915480614, 'learning_rate': 6.875219295293111e-08, 'epoch': 84.48}
>>> 2025-08-18 23:35:47,704 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.008031872101128101, 'learning_rate': 5.569170188250983e-08, 'epoch': 84.64}
>>> 2025-08-18 23:35:53,509 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.055576857179403305, 'learning_rate': 4.400503484006113e-08, 'epoch': 84.8}
>>> 2025-08-18 23:35:58,103 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.0017734664725139737, 'learning_rate': 3.369251329213285e-08, 'epoch': 84.96}
>>> 2025-08-18 23:35:59,028 - INFO - >>> {'loss': 0.0003, 'grad_norm': 0.020600179210305214, 'learning_rate': 2.4754420906475396e-08, 'epoch': 85.0}
>>> 2025-08-18 23:36:03,736 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0007452918798662722, 'learning_rate': 1.7191003544259064e-08, 'epoch': 85.16}
>>> 2025-08-18 23:36:08,713 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.003833540016785264, 'learning_rate': 1.100246925331283e-08, 'epoch': 85.32}
>>> 2025-08-18 23:36:13,264 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.00748040247708559, 'learning_rate': 6.188988262373352e-09, 'epoch': 85.48}
>>> 2025-08-18 23:36:17,220 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.012610828503966331, 'learning_rate': 2.750692976444258e-09, 'epoch': 85.64}
>>> 2025-08-18 23:36:23,485 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.0515986792743206, 'learning_rate': 6.876779731213035e-10, 'epoch': 85.8}
>>> 2025-08-18 23:36:24,503 - INFO - >>> {'train_runtime': 2594.2329, 'train_samples_per_second': 3.855, 'train_steps_per_second': 0.231, 'train_loss': 0.19567491081543267, 'epoch': 85.8}
>>> 2025-08-18 23:36:24,505 - INFO - 训练成功！
>>> 2025-08-18 23:36:24,505 - INFO - 模型存放位置：./output/qwen202508182252
>>> 2025-08-18 23:58:03,394 - INFO - ========__main__  202508182358========
>>> 2025-08-18 23:58:03,395 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-18 23:58:03,396 - INFO - 开始进行模型测试
>>> 2025-08-18 23:58:45,203 - INFO - 已选择模型文件夹: qwen202508182252
>>> 2025-08-18 23:58:45,206 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508182252/checkpoint-600
>>> 2025-08-19 12:51:05,588 - INFO - ========__main__  202508191251========
>>> 2025-08-19 12:51:05,589 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 12:51:05,589 - INFO - 开始进行模型测试
>>> 2025-08-19 12:51:12,090 - INFO - 已选择模型文件夹: qwen202508182252
>>> 2025-08-19 12:51:12,093 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508182252/checkpoint-600
>>> 2025-08-19 13:15:19,279 - INFO - ========__main__  202508191315========
>>> 2025-08-19 13:15:19,279 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 13:15:19,280 - INFO - 开始进行模型测试
>>> 2025-08-19 13:15:23,402 - INFO - 已选择模型文件夹: qwen202508182252
>>> 2025-08-19 13:15:23,405 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508182252/checkpoint-600
>>> 2025-08-19 13:15:54,808 - INFO - ========__main__  202508191315========
>>> 2025-08-19 13:15:54,809 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 13:15:54,809 - INFO - 开始进行模型测试
>>> 2025-08-19 13:16:33,826 - INFO - ========__main__  202508191316========
>>> 2025-08-19 13:16:33,826 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 13:16:33,827 - INFO - 开始进行模型测试
>>> 2025-08-19 13:16:41,884 - INFO - 已选择模型文件夹: qwen202508182252
>>> 2025-08-19 13:16:41,888 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508182252/checkpoint-600
>>> 2025-08-19 13:17:53,507 - INFO - ========__main__  202508191317========
>>> 2025-08-19 13:17:53,508 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 13:17:53,508 - INFO - 开始进行模型测试
>>> 2025-08-19 13:19:36,886 - INFO - ========__main__  202508191319========
>>> 2025-08-19 13:19:36,887 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 13:19:36,888 - INFO - 开始进行模型测试
>>> 2025-08-19 13:19:41,769 - INFO - 已选择模型文件夹: qwen202508182252
>>> 2025-08-19 13:19:41,772 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508182252/checkpoint-600
>>> 2025-08-19 13:32:45,993 - INFO - ========train Qwen2ForCausalLM  202508191332========
>>> 2025-08-19 13:32:45,993 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 13:32:45,994 - INFO - 开始进行训练
>>> 2025-08-19 13:32:48,464 - INFO - 导入包完成
>>> 2025-08-19 13:32:48,480 - INFO - 配置文件读取完成
>>> 2025-08-19 13:32:48,480 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-19 13:32:48,481 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-14b
>>> 2025-08-19 13:32:48,938 - INFO - tokenizer读取完成
>>> 2025-08-19 13:32:49,162 - ERROR - 模型导入失败：FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100), actual = `7.0`
>>> 2025-08-19 13:33:21,584 - INFO - ========train Qwen2ForCausalLM  202508191333========
>>> 2025-08-19 13:33:21,584 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 13:33:21,585 - INFO - 开始进行训练
>>> 2025-08-19 13:33:23,939 - INFO - 导入包完成
>>> 2025-08-19 13:33:23,954 - INFO - 配置文件读取完成
>>> 2025-08-19 13:33:23,955 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-19 13:33:23,955 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-08-19 13:33:24,252 - INFO - tokenizer读取完成
>>> 2025-08-19 13:33:26,471 - INFO - model dtype:torch.float16
>>> 2025-08-19 13:33:26,471 - INFO - 模型导入完成
>>> 2025-08-19 13:33:27,251 - INFO - 读取数据集成功
>>> 2025-08-19 13:33:31,944 - INFO - 数据处理成功
>>> 2025-08-19 13:33:47,168 - INFO - 开始训练！
>>> 2025-08-19 13:33:47,169 - INFO - 批次大小  : 8
>>> 2025-08-19 13:33:47,169 - INFO - 训练轮数  : 100
>>> 2025-08-19 13:33:47,169 - INFO - 学习率    : 0.0001
>>> 2025-08-19 13:33:47,170 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-19 13:33:47,170 - INFO - 模型路径  : /home/liangshuqiao/models/qwen3-8b
>>> 2025-08-19 13:34:08,563 - INFO - >>> {'loss': 3.73, 'grad_norm': 2.679239511489868, 'learning_rate': 0.0, 'epoch': 0.6153846153846154}
>>> 2025-08-19 13:34:46,883 - INFO - >>> {'loss': 7.3034, 'grad_norm': 5.217886447906494, 'learning_rate': 0.0001, 'epoch': 1.6153846153846154}
>>> 2025-08-19 13:35:26,375 - INFO - >>> {'loss': 7.0448, 'grad_norm': 4.494332790374756, 'learning_rate': 9.997482711915927e-05, 'epoch': 2.6153846153846154}
>>> 2025-08-19 13:36:10,125 - INFO - >>> {'loss': 6.3615, 'grad_norm': 4.408072471618652, 'learning_rate': 9.989933382359422e-05, 'epoch': 3.6153846153846154}
>>> 2025-08-19 13:36:46,781 - INFO - >>> {'loss': 6.0066, 'grad_norm': 4.30141544342041, 'learning_rate': 9.977359612865423e-05, 'epoch': 4.615384615384615}
>>> 2025-08-19 13:37:22,991 - INFO - >>> {'loss': 5.7125, 'grad_norm': 3.8214030265808105, 'learning_rate': 9.959774064153977e-05, 'epoch': 5.615384615384615}
>>> 2025-08-19 13:37:57,472 - INFO - >>> {'loss': 5.2807, 'grad_norm': 2.8880324363708496, 'learning_rate': 9.937194443381972e-05, 'epoch': 6.615384615384615}
>>> 2025-08-19 13:38:35,005 - INFO - >>> {'loss': 5.0778, 'grad_norm': 2.283903121948242, 'learning_rate': 9.909643486313533e-05, 'epoch': 7.615384615384615}
>>> 2025-08-19 13:39:12,334 - INFO - >>> {'loss': 4.8367, 'grad_norm': 1.8459539413452148, 'learning_rate': 9.877148934427037e-05, 'epoch': 8.615384615384615}
>>> 2025-08-19 13:39:49,138 - INFO - >>> {'loss': 4.7035, 'grad_norm': 1.5910844802856445, 'learning_rate': 9.839743506981782e-05, 'epoch': 9.615384615384615}
>>> 2025-08-19 13:40:24,519 - INFO - >>> {'loss': 4.4329, 'grad_norm': 1.3121286630630493, 'learning_rate': 9.797464868072488e-05, 'epoch': 10.615384615384615}
>>> 2025-08-19 13:41:02,292 - INFO - >>> {'loss': 4.4943, 'grad_norm': 1.1351940631866455, 'learning_rate': 9.750355588704727e-05, 'epoch': 11.615384615384615}
>>> 2025-08-19 13:41:42,193 - INFO - >>> {'loss': 4.365, 'grad_norm': 1.2947834730148315, 'learning_rate': 9.698463103929542e-05, 'epoch': 12.615384615384615}
>>> 2025-08-19 13:42:15,841 - INFO - >>> {'loss': 4.3231, 'grad_norm': 1.38405442237854, 'learning_rate': 9.641839665080363e-05, 'epoch': 13.615384615384615}
>>> 2025-08-19 13:42:53,779 - INFO - >>> {'loss': 4.2759, 'grad_norm': 1.5579955577850342, 'learning_rate': 9.580542287160348e-05, 'epoch': 14.615384615384615}
>>> 2025-08-19 13:43:27,429 - INFO - >>> {'loss': 4.1747, 'grad_norm': 1.6610922813415527, 'learning_rate': 9.514632691433107e-05, 'epoch': 15.615384615384615}
>>> 2025-08-19 13:44:04,139 - INFO - >>> {'loss': 3.9279, 'grad_norm': 1.5170201063156128, 'learning_rate': 9.444177243274618e-05, 'epoch': 16.615384615384617}
>>> 2025-08-19 13:44:43,025 - INFO - >>> {'loss': 4.0483, 'grad_norm': 1.5556542873382568, 'learning_rate': 9.369246885348926e-05, 'epoch': 17.615384615384617}
>>> 2025-08-19 13:45:16,613 - INFO - >>> {'loss': 3.8814, 'grad_norm': 1.5455849170684814, 'learning_rate': 9.289917066174886e-05, 'epoch': 18.615384615384617}
>>> 2025-08-19 13:45:54,145 - INFO - >>> {'loss': 3.8563, 'grad_norm': 1.3530528545379639, 'learning_rate': 9.206267664155907e-05, 'epoch': 19.615384615384617}
>>> 2025-08-19 13:46:31,271 - INFO - >>> {'loss': 3.7593, 'grad_norm': 1.4175540208816528, 'learning_rate': 9.118382907149165e-05, 'epoch': 20.615384615384617}
>>> 2025-08-19 13:47:07,423 - INFO - >>> {'loss': 3.6351, 'grad_norm': 1.3358339071273804, 'learning_rate': 9.026351287655294e-05, 'epoch': 21.615384615384617}
>>> 2025-08-19 13:47:45,805 - INFO - >>> {'loss': 3.6876, 'grad_norm': 1.2302806377410889, 'learning_rate': 8.930265473713938e-05, 'epoch': 22.615384615384617}
>>> 2025-08-19 13:48:20,113 - INFO - >>> {'loss': 3.5451, 'grad_norm': 1.3283789157867432, 'learning_rate': 8.83022221559489e-05, 'epoch': 23.615384615384617}
