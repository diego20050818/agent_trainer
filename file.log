>>> 2025-08-16 21:24:49,821 - INFO - 开始进行训练
>>> 2025-08-16 21:26:15,103 - INFO - 开始进行训练
>>> 2025-08-16 21:29:13,259 - INFO - 开始进行训练
>>> 2025-08-16 23:11:39,316 - INFO - 当前环境：/home/liangshuqiao/anaconda3/bin/python
>>> 2025-08-16 23:11:42,597 - INFO - 开始进行训练
>>> 2025-08-16 23:15:38,382 - INFO - 当前环境：/bin/python
>>> 2025-08-16 23:15:40,819 - INFO - 开始进行训练
>>> 2025-08-16 23:16:11,544 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-16 23:16:15,907 - INFO - 开始进行训练
>>> 2025-08-16 23:17:10,779 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-16 23:17:15,142 - INFO - 开始进行训练
>>> 2025-08-16 23:17:21,573 - INFO - 导入包完成
>>> 2025-08-16 23:17:23,743 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-16 23:17:26,902 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-16 23:17:31,143 - INFO - tokenizer读取完成
>>> 2025-08-16 23:17:35,370 - INFO - model dtype:torch.float16
>>> 2025-08-16 23:17:38,108 - INFO - 模型导入完成
>>> 2025-08-16 23:17:41,062 - INFO - 读取数据集成功
>>> 2025-08-16 23:17:43,389 - INFO - 数据处理成功
>>> 2025-08-16 23:17:47,799 - INFO - None
>>> 2025-08-16 23:18:00,028 - INFO - 开始训练！
>>> 2025-08-16 23:19:17,983 - INFO - 训练成功！
>>> 2025-08-16 23:19:20,149 - INFO - 模型存放位置：./output/DeepSeek202508162317
>>> 2025-08-16 23:32:46,181 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-16 23:32:50,544 - INFO - 开始进行训练
>>> 2025-08-16 23:32:56,994 - INFO - 导入包完成
>>> 2025-08-16 23:32:59,163 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-16 23:33:02,323 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-16 23:33:06,594 - INFO - tokenizer读取完成
>>> 2025-08-16 23:33:10,925 - INFO - model dtype:torch.float16
>>> 2025-08-16 23:33:13,663 - INFO - 模型导入完成
>>> 2025-08-16 23:33:16,590 - INFO - 读取数据集成功
>>> 2025-08-16 23:33:18,938 - INFO - 数据处理成功
>>> 2025-08-16 23:33:23,321 - INFO - None
>>> 2025-08-16 23:33:35,484 - INFO - 开始训练！
>>> 2025-08-16 23:34:53,557 - INFO - 训练成功！
>>> 2025-08-16 23:34:55,723 - INFO - 模型存放位置：./output/DeepSeek202508162333
>>> 2025-08-16 23:35:33,925 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-16 23:35:38,286 - INFO - 开始进行训练
>>> 2025-08-16 23:35:44,608 - INFO - 导入包完成
>>> 2025-08-16 23:35:46,775 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-16 23:35:49,933 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-16 23:35:54,179 - INFO - tokenizer读取完成
>>> 2025-08-16 23:35:58,384 - INFO - model dtype:torch.float16
>>> 2025-08-16 23:36:01,123 - INFO - 模型导入完成
>>> 2025-08-16 23:36:04,047 - INFO - 读取数据集成功
>>> 2025-08-16 23:36:06,387 - INFO - 数据处理成功
>>> 2025-08-16 23:36:10,679 - INFO - None
>>> 2025-08-16 23:36:18,227 - INFO - 开始训练！
>>> 2025-08-16 23:37:51,533 - INFO - 训练成功！
>>> 2025-08-16 23:37:53,699 - INFO - 模型存放位置：./output/DeepSeek202508162336
>>> 2025-08-16 23:43:51,958 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-16 23:43:56,319 - INFO - 开始进行训练
>>> 2025-08-16 23:44:02,638 - INFO - 导入包完成
>>> 2025-08-16 23:44:04,806 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-16 23:44:07,965 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-16 23:44:12,209 - INFO - tokenizer读取完成
>>> 2025-08-16 23:44:16,520 - INFO - model dtype:torch.float16
>>> 2025-08-16 23:44:19,259 - INFO - 模型导入完成
>>> 2025-08-16 23:44:23,572 - INFO - 读取数据集成功
>>> 2025-08-16 23:44:25,910 - INFO - 数据处理成功
>>> 2025-08-16 23:44:30,253 - INFO - None
>>> 2025-08-16 23:44:41,806 - INFO - 开始训练！
>>> 2025-08-16 23:46:02,028 - INFO - 训练成功！
>>> 2025-08-16 23:46:04,195 - INFO - 模型存放位置：./output/DeepSeek202508162344
>>> 2025-08-16 23:47:26,207 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-16 23:47:30,569 - INFO - 开始进行训练
>>> 2025-08-16 23:47:36,926 - INFO - 导入包完成
>>> 2025-08-16 23:47:39,095 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-16 23:47:42,254 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-16 23:47:46,495 - INFO - tokenizer读取完成
>>> 2025-08-16 23:47:50,861 - INFO - model dtype:torch.float16
>>> 2025-08-16 23:47:53,599 - INFO - 模型导入完成
>>> 2025-08-16 23:47:56,511 - INFO - 读取数据集成功
>>> 2025-08-16 23:47:58,854 - INFO - 数据处理成功
>>> 2025-08-16 23:48:03,152 - INFO - None
>>> 2025-08-16 23:48:14,790 - INFO - 开始训练！
>>> 2025-08-16 23:49:46,396 - INFO - 训练成功！
>>> 2025-08-16 23:49:48,562 - INFO - 模型存放位置：./output/DeepSeek202508162348
>>> 2025-08-16 23:52:13,360 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-16 23:52:17,723 - INFO - 开始进行训练
>>> 2025-08-16 23:52:24,021 - INFO - 导入包完成
>>> 2025-08-16 23:52:26,189 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-16 23:52:29,348 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-16 23:52:33,580 - INFO - tokenizer读取完成
>>> 2025-08-16 23:52:37,852 - INFO - model dtype:torch.float16
>>> 2025-08-16 23:52:40,590 - INFO - 模型导入完成
>>> 2025-08-16 23:52:43,571 - INFO - 读取数据集成功
>>> 2025-08-16 23:52:46,011 - INFO - 数据处理成功
>>> 2025-08-16 23:52:50,351 - INFO - None
>>> 2025-08-16 23:53:01,685 - INFO - 开始训练！
>>> 2025-08-16 23:54:26,014 - INFO - 训练成功！
>>> 2025-08-16 23:54:28,180 - INFO - 模型存放位置：./output/DeepSeek202508162352
>>> 2025-08-16 23:56:42,025 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-16 23:56:46,387 - INFO - 开始进行训练
>>> 2025-08-16 23:56:52,699 - INFO - 导入包完成
>>> 2025-08-16 23:56:54,868 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-16 23:56:58,026 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-16 23:57:02,276 - INFO - tokenizer读取完成
>>> 2025-08-16 23:57:06,682 - INFO - model dtype:torch.float16
>>> 2025-08-16 23:57:09,420 - INFO - 模型导入完成
>>> 2025-08-16 23:57:12,635 - INFO - 读取数据集成功
>>> 2025-08-16 23:57:14,974 - INFO - 数据处理成功
>>> 2025-08-16 23:57:19,249 - INFO - None
>>> 2025-08-16 23:57:32,304 - INFO - 开始训练！
>>> 2025-08-16 23:59:03,038 - INFO - 训练成功！
>>> 2025-08-16 23:59:05,204 - INFO - 模型存放位置：./output/DeepSeek202508162357
>>> 2025-08-16 23:59:55,952 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 00:00:00,975 - INFO - 开始进行训练
>>> 2025-08-17 00:00:25,670 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 00:00:30,033 - INFO - 开始进行训练
>>> 2025-08-17 00:00:36,331 - INFO - 导入包完成
>>> 2025-08-17 00:00:38,497 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 00:00:41,656 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 00:00:45,888 - INFO - tokenizer读取完成
>>> 2025-08-17 00:00:50,156 - INFO - model dtype:torch.float16
>>> 2025-08-17 00:00:52,894 - INFO - 模型导入完成
>>> 2025-08-17 00:00:55,828 - INFO - 读取数据集成功
>>> 2025-08-17 00:00:58,168 - INFO - 数据处理成功
>>> 2025-08-17 00:01:02,583 - INFO - None
>>> 2025-08-17 00:01:14,047 - INFO - 开始训练！
>>> 2025-08-17 00:05:54,119 - INFO - 训练成功！
>>> 2025-08-17 00:05:56,285 - INFO - 模型存放位置：./output/DeepSeek202508170001
>>> 2025-08-17 08:49:10,321 - INFO - 当前环境：/bin/python
>>> 2025-08-17 08:49:12,758 - INFO - 开始进行训练
>>> 2025-08-17 08:49:42,594 - INFO - 当前环境：/home/liangshuqiao/anaconda3/bin/python
>>> 2025-08-17 08:49:45,874 - INFO - 开始进行训练
>>> 2025-08-17 08:50:06,023 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 08:50:10,386 - INFO - 开始进行训练
>>> 2025-08-17 08:50:16,731 - INFO - 导入包完成
>>> 2025-08-17 08:50:18,900 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 08:50:22,059 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 08:50:26,301 - INFO - tokenizer读取完成
>>> 2025-08-17 09:03:18,983 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:03:23,345 - INFO - 开始进行训练
>>> 2025-08-17 09:03:26,060 - ERROR - 没有找到 checkpoint 文件夹!
>>> 2025-08-17 09:09:00,616 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:09:04,978 - INFO - 开始进行模型测试
>>> 2025-08-17 09:09:12,622 - INFO - 已选择模型文件夹: DeepSeek202508170001
>>> 2025-08-17 09:12:36,319 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:12:40,681 - INFO - 开始进行模型测试
>>> 2025-08-17 09:12:46,729 - INFO - 已选择模型文件夹: DeepSeek202508170001
>>> 2025-08-17 09:12:49,982 - INFO - 最新的 LoRA checkpoint 路径:output/DeepSeek202508170001/checkpoint-120
>>> 2025-08-17 09:22:21,964 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:22:26,325 - INFO - 开始进行模型测试
>>> 2025-08-17 09:22:33,503 - INFO - 已选择模型文件夹: Qwen2.5_instruct_lora
>>> 2025-08-17 09:22:36,783 - INFO - 最新的 LoRA checkpoint 路径:output/Qwen2.5_instruct_lora/checkpoint-180
>>> 2025-08-17 09:23:28,094 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:23:32,456 - INFO - 开始进行模型测试
>>> 2025-08-17 09:23:43,805 - INFO - 已选择模型文件夹: Qwen2.5_instruct_lora
>>> 2025-08-17 09:23:47,084 - ERROR - 你选了一个不是deepseek的模型，接下来可能会报错
>>> 2025-08-17 09:24:13,180 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:24:17,542 - INFO - 开始进行模型测试
>>> 2025-08-17 09:24:23,395 - INFO - 已选择模型文件夹: DeepSeek202508170001
>>> 2025-08-17 09:24:26,646 - ERROR - 你选了一个不是deepseek的模型，接下来可能会报错
>>> 2025-08-17 09:25:47,713 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:25:52,074 - INFO - 开始进行模型测试
>>> 2025-08-17 09:25:59,992 - INFO - 已选择模型文件夹: DeepSeek202508170001
>>> 2025-08-17 09:26:03,242 - INFO - 最新的 LoRA checkpoint 路径:output/DeepSeek202508170001/checkpoint-120
>>> 2025-08-17 09:26:47,763 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:26:52,125 - INFO - 开始进行模型测试
>>> 2025-08-17 09:26:57,913 - INFO - 已选择模型文件夹: DeepSeek202508170001
>>> 2025-08-17 09:27:01,165 - INFO - 最新的 LoRA checkpoint 路径:output/DeepSeek202508170001/checkpoint-120
>>> 2025-08-17 09:29:34,633 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:29:38,994 - INFO - 开始进行模型测试
>>> 2025-08-17 09:29:44,700 - INFO - 已选择模型文件夹: DeepSeek202508170001
>>> 2025-08-17 09:29:47,951 - INFO - 最新的 LoRA checkpoint 路径:output/DeepSeek202508170001/checkpoint-120
>>> 2025-08-17 09:41:59,400 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:42:03,763 - INFO - 开始进行训练
>>> 2025-08-17 09:42:10,245 - INFO - 导入包完成
>>> 2025-08-17 09:42:12,414 - INFO - 配置文件读取完成
>>> 2025-08-17 09:42:14,641 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 09:42:17,801 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 09:42:22,038 - INFO - tokenizer读取完成
>>> 2025-08-17 09:42:26,368 - INFO - model dtype:torch.float16
>>> 2025-08-17 09:42:29,107 - INFO - 模型导入完成
>>> 2025-08-17 09:42:32,048 - INFO - 读取数据集成功
>>> 2025-08-17 09:42:34,494 - INFO - 数据处理成功
>>> 2025-08-17 09:42:38,767 - INFO - None
>>> 2025-08-17 09:42:51,184 - INFO - 开始训练！
>>> 2025-08-17 09:43:16,535 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 4.79 GiB. GPU 0 has a total capacity of 31.73 GiB of which 2.50 GiB is free. Including non-PyTorch memory, this process has 29.23 GiB memory in use. Of the allocated memory 25.76 GiB is allocated by PyTorch, and 3.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-08-17 09:44:11,057 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:44:15,421 - INFO - 开始进行训练
>>> 2025-08-17 09:44:21,877 - INFO - 导入包完成
>>> 2025-08-17 09:44:24,046 - INFO - 配置文件读取完成
>>> 2025-08-17 09:44:26,272 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 09:44:29,431 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 09:44:33,663 - INFO - tokenizer读取完成
>>> 2025-08-17 09:44:38,043 - INFO - model dtype:torch.float16
>>> 2025-08-17 09:44:40,781 - INFO - 模型导入完成
>>> 2025-08-17 09:44:43,720 - INFO - 读取数据集成功
>>> 2025-08-17 09:44:46,050 - INFO - 数据处理成功
>>> 2025-08-17 09:44:50,375 - INFO - None
>>> 2025-08-17 09:45:02,548 - INFO - 开始训练！
>>> 2025-08-17 09:45:27,823 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 4.79 GiB. GPU 0 has a total capacity of 31.73 GiB of which 2.50 GiB is free. Including non-PyTorch memory, this process has 29.23 GiB memory in use. Of the allocated memory 25.76 GiB is allocated by PyTorch, and 3.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-08-17 09:48:12,379 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 09:48:16,743 - INFO - 开始进行训练
>>> 2025-08-17 09:48:23,109 - INFO - 导入包完成
>>> 2025-08-17 09:48:25,275 - INFO - 配置文件读取完成
>>> 2025-08-17 09:48:27,501 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 09:48:30,660 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 09:48:34,900 - INFO - tokenizer读取完成
>>> 2025-08-17 09:48:39,211 - INFO - model dtype:torch.float16
>>> 2025-08-17 09:48:41,950 - INFO - 模型导入完成
>>> 2025-08-17 09:48:44,886 - INFO - 读取数据集成功
>>> 2025-08-17 09:48:47,326 - INFO - 数据处理成功
>>> 2025-08-17 09:48:51,566 - INFO - None
>>> 2025-08-17 09:49:03,312 - INFO - 开始训练！
>>> 2025-08-17 09:57:42,407 - INFO - 训练成功！
>>> 2025-08-17 09:57:44,573 - INFO - 模型存放位置：./output/DeepSeek202508170948
>>> 2025-08-17 10:11:12,934 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 10:11:17,296 - INFO - 开始进行模型测试
>>> 2025-08-17 10:11:23,862 - INFO - 已选择模型文件夹: DeepSeek202508170948
>>> 2025-08-17 10:11:27,113 - INFO - 最新的 LoRA checkpoint 路径:output/DeepSeek202508170948/checkpoint-240

>>> 2025-08-17 10:30:14,046 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 10:30:18,409 - INFO - 开始进行训练
>>> 2025-08-17 10:30:22,789 - INFO - 导入包完成
>>> 2025-08-17 10:30:24,957 - INFO - 配置文件读取完成
>>> 2025-08-17 10:30:27,183 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 10:31:47,927 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 10:31:52,289 - INFO - 开始进行训练
>>> 2025-08-17 10:31:56,675 - INFO - 导入包完成
>>> 2025-08-17 10:31:58,843 - INFO - 配置文件读取完成
>>> 2025-08-17 10:32:01,069 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 10:32:04,229 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 10:32:08,468 - INFO - tokenizer读取完成
>>> 2025-08-17 10:32:12,798 - INFO - model dtype:torch.float16
>>> 2025-08-17 10:32:15,536 - INFO - 模型导入完成
>>> 2025-08-17 10:32:18,477 - INFO - 读取数据集成功
>>> 2025-08-17 10:32:20,913 - INFO - 数据处理成功
>>> 2025-08-17 10:32:36,036 - INFO - 开始训练！
>>> 2025-08-17 10:32:38,203 - INFO - 批次大小  : 4
>>> 2025-08-17 10:32:40,489 - INFO - 训练轮数  : 20
>>> 2025-08-17 10:32:42,806 - INFO - 学习率    : 0.0001
>>> 2025-08-17 10:32:45,273 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 10:32:48,493 - INFO - 模型路径  : /home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 10:32:54,748 - INFO - 训练日志: >>> 训练进度: Epoch 0.08 | Loss 4.0652 | LR 1.00e-04 | Grad Norm 1.6210
>>> 2025-08-17 10:33:01,713 - INFO - 训练日志: >>> 训练进度: Epoch 0.40 | Loss 4.0982 | LR 9.99e-05 | Grad Norm 2.2548
>>> 2025-08-17 10:33:12,593 - INFO - 训练日志: >>> 训练进度: Epoch 0.80 | Loss 3.7997 | LR 9.97e-05 | Grad Norm 1.3718
>>> 2025-08-17 10:33:21,621 - INFO - 训练日志: >>> 训练进度: Epoch 1.16 | Loss 3.0882 | LR 9.92e-05 | Grad Norm 2.6920
>>> 2025-08-17 10:33:33,631 - INFO - 训练日志: >>> 训练进度: Epoch 1.56 | Loss 2.7578 | LR 9.85e-05 | Grad Norm 1.4040
>>> 2025-08-17 10:34:32,384 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 10:34:36,746 - INFO - 开始进行训练
>>> 2025-08-17 10:34:41,126 - INFO - 导入包完成
>>> 2025-08-17 10:34:43,293 - INFO - 配置文件读取完成
>>> 2025-08-17 10:34:45,520 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 10:34:48,679 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 10:34:52,922 - INFO - tokenizer读取完成
>>> 2025-08-17 10:34:57,172 - INFO - model dtype:torch.float16
>>> 2025-08-17 10:34:59,910 - INFO - 模型导入完成
>>> 2025-08-17 10:35:02,837 - INFO - 读取数据集成功
>>> 2025-08-17 10:35:05,176 - INFO - 数据处理成功
>>> 2025-08-17 10:35:19,356 - INFO - 开始训练！
>>> 2025-08-17 10:35:21,522 - INFO - 批次大小  : 4
>>> 2025-08-17 10:35:23,808 - INFO - 训练轮数  : 20
>>> 2025-08-17 10:35:26,125 - INFO - 学习率    : 0.0001
>>> 2025-08-17 10:35:28,592 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 10:35:31,811 - INFO - 模型路径  : /home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-17 10:35:38,061 - INFO - >>> {'loss': 4.0652, 'grad_norm': 1.623105764389038, 'learning_rate': 0.0001, 'epoch': 0.08}
>>> 2025-08-17 10:35:45,074 - INFO - >>> {'loss': 4.095, 'grad_norm': 2.280607223510742, 'learning_rate': 9.99314767377287e-05, 'epoch': 0.4}
>>> 2025-08-17 10:35:56,024 - INFO - >>> {'loss': 3.791, 'grad_norm': 1.3859643936157227, 'learning_rate': 9.965342284774632e-05, 'epoch': 0.8}
>>> 2025-08-17 10:36:05,150 - INFO - >>> {'loss': 3.076, 'grad_norm': 2.666529655456543, 'learning_rate': 9.916274537819775e-05, 'epoch': 1.16}
>>> 2025-08-17 10:36:17,242 - INFO - >>> {'loss': 2.7519, 'grad_norm': 1.4053139686584473, 'learning_rate': 9.846154548533773e-05, 'epoch': 1.56}
>>> 2025-08-17 10:36:28,735 - INFO - >>> {'loss': 2.5726, 'grad_norm': 1.5842623710632324, 'learning_rate': 9.755282581475769e-05, 'epoch': 1.96}
>>> 2025-08-17 10:36:40,598 - INFO - >>> {'loss': 2.5449, 'grad_norm': 1.4177361726760864, 'learning_rate': 9.644047764359622e-05, 'epoch': 2.32}
>>> 2025-08-17 10:36:54,027 - INFO - >>> {'loss': 2.2919, 'grad_norm': 1.8708523511886597, 'learning_rate': 9.512926421749304e-05, 'epoch': 2.7199999999999998}
>>> 2025-08-17 10:37:07,275 - INFO - >>> {'loss': 2.027, 'grad_norm': 1.7385317087173462, 'learning_rate': 9.362480035363986e-05, 'epoch': 3.08}
>>> 2025-08-17 10:37:20,961 - INFO - >>> {'loss': 2.1722, 'grad_norm': 2.201190233230591, 'learning_rate': 9.193352839727121e-05, 'epoch': 3.48}
>>> 2025-08-17 10:37:32,603 - INFO - >>> {'loss': 1.589, 'grad_norm': 3.2426958084106445, 'learning_rate': 9.006269063455304e-05, 'epoch': 3.88}
>>> 2025-08-17 10:37:41,779 - INFO - >>> {'loss': 1.7189, 'grad_norm': 2.098285436630249, 'learning_rate': 8.802029828000156e-05, 'epoch': 4.24}
>>> 2025-08-17 10:37:55,646 - INFO - >>> {'loss': 1.5384, 'grad_norm': 3.510235071182251, 'learning_rate': 8.581509717123273e-05, 'epoch': 4.64}
>>> 2025-08-17 10:38:05,240 - INFO - >>> {'loss': 1.5435, 'grad_norm': 0.0, 'learning_rate': 8.345653031794292e-05, 'epoch': 5.0}
>>> 2025-08-17 10:38:17,276 - INFO - >>> {'loss': 1.192, 'grad_norm': 3.0777924060821533, 'learning_rate': 8.095469746549172e-05, 'epoch': 5.4}
>>> 2025-08-17 10:38:27,191 - INFO - >>> {'loss': 1.0794, 'grad_norm': 2.4594388008117676, 'learning_rate': 7.832031184624164e-05, 'epoch': 5.8}
>>> 2025-08-17 10:38:38,493 - INFO - >>> {'loss': 1.0162, 'grad_norm': 2.8182291984558105, 'learning_rate': 7.55646543038526e-05, 'epoch': 6.16}
>>> 2025-08-17 10:38:48,447 - INFO - >>> {'loss': 0.676, 'grad_norm': 2.8934922218322754, 'learning_rate': 7.269952498697734e-05, 'epoch': 6.5600000000000005}
>>> 2025-08-17 10:39:00,369 - INFO - >>> {'loss': 0.7846, 'grad_norm': 3.3559560775756836, 'learning_rate': 6.973719281921335e-05, 'epoch': 6.96}
>>> 2025-08-17 10:39:09,603 - INFO - >>> {'loss': 0.5525, 'grad_norm': 3.6404635906219482, 'learning_rate': 6.669034296168855e-05, 'epoch': 7.32}
>>> 2025-08-17 10:39:20,181 - INFO - >>> {'loss': 0.5257, 'grad_norm': 2.0878188610076904, 'learning_rate': 6.357202249325371e-05, 'epoch': 7.72}
>>> 2025-08-17 10:39:32,492 - INFO - >>> {'loss': 0.3458, 'grad_norm': 2.4022514820098877, 'learning_rate': 6.0395584540887963e-05, 'epoch': 8.08}
>>> 2025-08-17 10:39:44,515 - INFO - >>> {'loss': 0.2044, 'grad_norm': 2.1720547676086426, 'learning_rate': 5.717463109955896e-05, 'epoch': 8.48}
>>> 2025-08-17 10:39:56,749 - INFO - >>> {'loss': 0.1671, 'grad_norm': 2.999216318130493, 'learning_rate': 5.392295478639225e-05, 'epoch': 8.88}
>>> 2025-08-17 10:40:04,620 - INFO - >>> {'loss': 0.0774, 'grad_norm': 0.7733047604560852, 'learning_rate': 5.0654479778567223e-05, 'epoch': 9.24}
>>> 2025-08-17 10:40:17,045 - INFO - >>> {'loss': 0.0715, 'grad_norm': 1.7389665842056274, 'learning_rate': 4.738320218785281e-05, 'epoch': 9.64}
>>> 2025-08-17 10:40:27,072 - INFO - >>> {'loss': 0.0986, 'grad_norm': 2.620321750640869, 'learning_rate': 4.412313012710813e-05, 'epoch': 10.0}
>>> 2025-08-17 10:40:39,587 - INFO - >>> {'loss': 0.0262, 'grad_norm': 0.8122404217720032, 'learning_rate': 4.088822372539263e-05, 'epoch': 10.4}
>>> 2025-08-17 10:40:48,475 - INFO - >>> {'loss': 0.0196, 'grad_norm': 1.4553475379943848, 'learning_rate': 3.769233534855035e-05, 'epoch': 10.8}
>>> 2025-08-17 10:40:59,455 - INFO - >>> {'loss': 0.037, 'grad_norm': 0.4439069330692291, 'learning_rate': 3.4549150281252636e-05, 'epoch': 11.16}
>>> 2025-08-17 10:41:12,589 - INFO - >>> {'loss': 0.0121, 'grad_norm': 0.48753395676612854, 'learning_rate': 3.147212812450819e-05, 'epoch': 11.56}
>>> 2025-08-17 10:41:24,864 - INFO - >>> {'loss': 0.0232, 'grad_norm': 0.6335101127624512, 'learning_rate': 2.8474445159585235e-05, 'epoch': 11.96}
>>> 2025-08-17 10:41:35,979 - INFO - >>> {'loss': 0.0089, 'grad_norm': 0.3817635476589203, 'learning_rate': 2.556893792515227e-05, 'epoch': 12.32}
>>> 2025-08-17 10:41:47,951 - INFO - >>> {'loss': 0.014, 'grad_norm': 0.6407415866851807, 'learning_rate': 2.2768048249248648e-05, 'epoch': 12.72}
>>> 2025-08-17 10:41:58,048 - INFO - >>> {'loss': 0.0118, 'grad_norm': 0.6664864420890808, 'learning_rate': 2.008376997146705e-05, 'epoch': 13.08}
>>> 2025-08-17 10:42:08,586 - INFO - >>> {'loss': 0.0095, 'grad_norm': 0.6174389719963074, 'learning_rate': 1.7527597583490822e-05, 'epoch': 13.48}
>>> 2025-08-17 10:42:19,124 - INFO - >>> {'loss': 0.0127, 'grad_norm': 2.5650634765625, 'learning_rate': 1.5110477007916001e-05, 'epoch': 13.88}
>>> 2025-08-17 10:42:29,739 - INFO - >>> {'loss': 0.0079, 'grad_norm': 0.717056393623352, 'learning_rate': 1.2842758726130283e-05, 'epoch': 14.24}
>>> 2025-08-17 10:42:40,906 - INFO - >>> {'loss': 0.0068, 'grad_norm': 0.5853997468948364, 'learning_rate': 1.0734153455962765e-05, 'epoch': 14.64}
>>> 2025-08-17 10:42:50,856 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.578515350818634, 'learning_rate': 8.793690568899216e-06, 'epoch': 15.0}
>>> 2025-08-17 10:43:00,883 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.37233641743659973, 'learning_rate': 7.029679424927365e-06, 'epoch': 15.4}
>>> 2025-08-17 10:43:12,820 - INFO - >>> {'loss': 0.0075, 'grad_norm': 0.3451235592365265, 'learning_rate': 5.449673790581611e-06, 'epoch': 15.8}
>>> 2025-08-17 10:43:25,111 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.4977540075778961, 'learning_rate': 4.06043949255509e-06, 'epoch': 16.16}
>>> 2025-08-17 10:43:34,587 - INFO - >>> {'loss': 0.0077, 'grad_norm': 0.5045515894889832, 'learning_rate': 2.8679254453910785e-06, 'epoch': 16.56}
>>> 2025-08-17 10:43:45,497 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.3286968171596527, 'learning_rate': 1.8772381773176417e-06, 'epoch': 16.96}
>>> 2025-08-17 10:43:54,554 - INFO - >>> {'loss': 0.0065, 'grad_norm': 0.4072408974170685, 'learning_rate': 1.0926199633097157e-06, 'epoch': 17.32}
>>> 2025-08-17 10:44:05,669 - INFO - >>> {'loss': 0.0078, 'grad_norm': 0.6165383458137512, 'learning_rate': 5.174306590164879e-07, 'epoch': 17.72}
>>> 2025-08-17 10:44:14,102 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.6236782073974609, 'learning_rate': 1.5413331334360182e-07, 'epoch': 18.08}
>>> 2025-08-17 10:44:26,304 - INFO - >>> {'loss': 0.0061, 'grad_norm': 0.3075663447380066, 'learning_rate': 4.2836212996499865e-09, 'epoch': 18.48}
>>> 2025-08-17 10:44:26,825 - INFO - >>> {'train_runtime': 530.8602, 'train_samples_per_second': 3.767, 'train_steps_per_second': 0.452, 'train_loss': 0.8072815732176726, 'epoch': 18.48}
>>> 2025-08-17 10:44:26,826 - INFO - 训练成功！
>>> 2025-08-17 10:44:28,993 - INFO - 模型存放位置：./output/DeepSeek202508171035
>>> 2025-08-17 10:45:35,235 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 10:45:39,597 - INFO - 开始进行模型测试
>>> 2025-08-17 10:45:51,838 - INFO - 已选择模型文件夹: DeepSeek202508171035
>>> 2025-08-17 10:45:55,090 - INFO - 最新的 LoRA checkpoint 路径:output/DeepSeek202508171035/checkpoint-240
>>> 2025-08-17 10:58:35,684 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 10:58:39,926 - INFO - 开始进行训练
>>> 2025-08-17 10:58:44,215 - INFO - 导入包完成
>>> 2025-08-17 10:58:46,262 - INFO - 配置文件读取完成
>>> 2025-08-17 10:58:48,369 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 10:58:51,410 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-14b
>>> 2025-08-17 10:58:55,074 - INFO - tokenizer读取完成
>>> 2025-08-17 10:58:57,585 - ERROR - 模型导入失败：FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100), actual = `7.0`
>>> 2025-08-17 11:02:14,556 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 11:02:18,798 - INFO - 开始进行模型测试
>>> 2025-08-17 11:02:50,678 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 11:02:54,920 - INFO - 开始进行训练
>>> 2025-08-17 11:02:59,207 - INFO - 导入包完成
>>> 2025-08-17 11:03:01,255 - INFO - 配置文件读取完成
>>> 2025-08-17 11:03:03,361 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 11:03:06,400 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2.5-14B-Instruct
>>> 2025-08-17 11:03:10,308 - INFO - tokenizer读取完成
>>> 2025-08-17 11:05:40,344 - INFO - model dtype:torch.float16
>>> 2025-08-17 11:05:42,963 - INFO - 模型导入完成
>>> 2025-08-17 11:05:45,763 - INFO - 读取数据集成功
>>> 2025-08-17 11:05:51,969 - INFO - 数据处理成功
>>> 2025-08-17 11:06:14,391 - INFO - 开始训练！
>>> 2025-08-17 11:06:16,438 - INFO - 批次大小  : 4
>>> 2025-08-17 11:06:18,604 - INFO - 训练轮数  : 20
>>> 2025-08-17 11:06:20,801 - INFO - 学习率    : 0.0001
>>> 2025-08-17 11:06:23,148 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 11:06:26,247 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2.5-14B-Instruct
>>> 2025-08-17 11:06:33,836 - INFO - >>> {'loss': 2.9513, 'grad_norm': 3.123476028442383, 'learning_rate': 0.0001, 'epoch': 0.08}
>>> 2025-08-17 11:06:36,321 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 788.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 780.19 MiB is free. Including non-PyTorch memory, this process has 30.97 GiB memory in use. Of the allocated memory 30.37 GiB is allocated by PyTorch, and 230.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-08-17 11:09:27,173 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 11:09:31,415 - INFO - 开始进行训练
>>> 2025-08-17 11:09:37,367 - INFO - 导入包完成
>>> 2025-08-17 11:09:39,417 - INFO - 配置文件读取完成
>>> 2025-08-17 11:09:41,522 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 11:09:44,562 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2.5-14B-Instruct
>>> 2025-08-17 11:09:48,443 - INFO - tokenizer读取完成
>>> 2025-08-17 11:10:05,651 - INFO - model dtype:torch.float16
>>> 2025-08-17 11:10:08,269 - INFO - 模型导入完成
>>> 2025-08-17 11:10:11,106 - INFO - 读取数据集成功
>>> 2025-08-17 11:10:17,298 - INFO - 数据处理成功
>>> 2025-08-17 11:10:43,953 - INFO - 开始训练！
>>> 2025-08-17 11:10:45,999 - INFO - 批次大小  : 4
>>> 2025-08-17 11:10:48,165 - INFO - 训练轮数  : 20
>>> 2025-08-17 11:10:50,362 - INFO - 学习率    : 0.0001
>>> 2025-08-17 11:10:52,708 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 11:10:55,807 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2.5-14B-Instruct
>>> 2025-08-17 11:11:07,322 - INFO - >>> {'loss': 3.2679, 'grad_norm': 2.452756404876709, 'learning_rate': 0.0001, 'epoch': 0.16}
>>> 2025-08-17 11:11:11,763 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 830.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 722.19 MiB is free. Including non-PyTorch memory, this process has 31.02 GiB memory in use. Of the allocated memory 30.09 GiB is allocated by PyTorch, and 569.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-08-17 11:13:06,830 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 11:13:11,072 - INFO - 开始进行训练
>>> 2025-08-17 11:13:15,336 - INFO - 导入包完成
>>> 2025-08-17 11:13:17,383 - INFO - 配置文件读取完成
>>> 2025-08-17 11:13:19,490 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 11:13:22,529 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 11:13:26,046 - INFO - tokenizer读取完成
>>> 2025-08-17 11:15:02,201 - INFO - model dtype:torch.float16
>>> 2025-08-17 11:15:04,819 - INFO - 模型导入完成
>>> 2025-08-17 11:15:07,654 - INFO - 读取数据集成功
>>> 2025-08-17 11:15:13,782 - INFO - 数据处理成功
>>> 2025-08-17 11:15:29,124 - INFO - 开始训练！
>>> 2025-08-17 11:15:31,170 - INFO - 批次大小  : 4
>>> 2025-08-17 11:15:33,337 - INFO - 训练轮数  : 20
>>> 2025-08-17 11:15:35,534 - INFO - 学习率    : 0.0001
>>> 2025-08-17 11:15:37,881 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 11:15:40,981 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 11:15:48,603 - INFO - >>> {'loss': 2.7863, 'grad_norm': 1.015097975730896, 'learning_rate': 0.0001, 'epoch': 0.16}
>>> 2025-08-17 11:16:05,036 - INFO - >>> {'loss': 2.4808, 'grad_norm': 0.9193146228790283, 'learning_rate': 9.972609476841367e-05, 'epoch': 0.8}
>>> 2025-08-17 11:16:24,456 - INFO - >>> {'loss': 2.1814, 'grad_norm': 0.6623067855834961, 'learning_rate': 9.861849601988383e-05, 'epoch': 1.48}
>>> 2025-08-17 11:16:44,300 - INFO - >>> {'loss': 1.9734, 'grad_norm': 0.7996106743812561, 'learning_rate': 9.667902132486009e-05, 'epoch': 2.16}
>>> 2025-08-17 11:17:08,241 - INFO - >>> {'loss': 1.7759, 'grad_norm': 1.227807879447937, 'learning_rate': 9.394085563309827e-05, 'epoch': 2.96}
>>> 2025-08-17 11:17:32,189 - INFO - >>> {'loss': 1.6904, 'grad_norm': 1.161568284034729, 'learning_rate': 9.045084971874738e-05, 'epoch': 3.64}
>>> 2025-08-17 11:17:48,152 - INFO - >>> {'loss': 1.4271, 'grad_norm': 1.9831335544586182, 'learning_rate': 8.626871855061438e-05, 'epoch': 4.32}
>>> 2025-08-17 11:18:06,142 - INFO - >>> {'loss': 1.3286, 'grad_norm': 0.0, 'learning_rate': 8.146601955249188e-05, 'epoch': 5.0}
>>> 2025-08-17 11:18:26,440 - INFO - >>> {'loss': 1.1546, 'grad_norm': 2.1271982192993164, 'learning_rate': 7.612492823579745e-05, 'epoch': 5.8}
>>> 2025-08-17 11:18:43,488 - INFO - >>> {'loss': 0.9996, 'grad_norm': 4.529637813568115, 'learning_rate': 7.033683215379002e-05, 'epoch': 6.48}
>>> 2025-08-17 11:19:01,730 - INFO - >>> {'loss': 0.908, 'grad_norm': 2.806048631668091, 'learning_rate': 6.420076723519614e-05, 'epoch': 7.16}
>>> 2025-08-17 11:19:22,643 - INFO - >>> {'loss': 0.7948, 'grad_norm': 2.7288081645965576, 'learning_rate': 5.782172325201155e-05, 'epoch': 7.96}
>>> 2025-08-17 11:19:42,042 - INFO - >>> {'loss': 0.5652, 'grad_norm': 3.690415143966675, 'learning_rate': 5.1308847415393666e-05, 'epoch': 8.64}
>>> 2025-08-17 11:19:58,409 - INFO - >>> {'loss': 0.4398, 'grad_norm': 2.947415351867676, 'learning_rate': 4.477357683661734e-05, 'epoch': 9.32}
>>> 2025-08-17 11:20:17,318 - INFO - >>> {'loss': 0.4094, 'grad_norm': 4.265871524810791, 'learning_rate': 3.832773180720475e-05, 'epoch': 10.0}
>>> 2025-08-17 11:20:37,226 - INFO - >>> {'loss': 0.2616, 'grad_norm': 3.2889950275421143, 'learning_rate': 3.2081602522734986e-05, 'epoch': 10.8}
>>> 2025-08-17 11:20:57,404 - INFO - >>> {'loss': 0.2502, 'grad_norm': 3.36405873298645, 'learning_rate': 2.6142061987019577e-05, 'epoch': 11.48}
>>> 2025-08-17 11:21:16,184 - INFO - >>> {'loss': 0.174, 'grad_norm': 3.382486581802368, 'learning_rate': 2.061073738537635e-05, 'epoch': 12.16}
>>> 2025-08-17 11:21:37,839 - INFO - >>> {'loss': 0.1346, 'grad_norm': 3.4515035152435303, 'learning_rate': 1.5582271215312294e-05, 'epoch': 12.96}
>>> 2025-08-17 11:21:53,854 - INFO - >>> {'loss': 0.1304, 'grad_norm': 3.5875556468963623, 'learning_rate': 1.1142701927151456e-05, 'epoch': 13.64}
>>> 2025-08-17 11:22:13,057 - INFO - >>> {'loss': 0.1618, 'grad_norm': 5.078430652618408, 'learning_rate': 7.367991782295391e-06, 'epoch': 14.32}
>>> 2025-08-17 11:22:32,043 - INFO - >>> {'loss': 0.1208, 'grad_norm': 6.18014669418335, 'learning_rate': 4.322727117869951e-06, 'epoch': 15.0}
>>> 2025-08-17 11:22:52,665 - INFO - >>> {'loss': 0.1081, 'grad_norm': 4.762761116027832, 'learning_rate': 2.0590132565903476e-06, 'epoch': 15.8}
>>> 2025-08-17 11:23:11,515 - INFO - >>> {'loss': 0.1142, 'grad_norm': 4.100128650665283, 'learning_rate': 6.15582970243117e-07, 'epoch': 16.48}
>>> 2025-08-17 11:23:27,943 - INFO - >>> {'loss': 0.1126, 'grad_norm': 3.707332134246826, 'learning_rate': 1.7133751222137007e-08, 'epoch': 17.16}
>>> 2025-08-17 11:23:28,867 - INFO - >>> {'train_runtime': 464.4559, 'train_samples_per_second': 4.306, 'train_steps_per_second': 0.258, 'train_loss': 0.823268016676108, 'epoch': 17.16}
>>> 2025-08-17 11:23:28,869 - INFO - 训练成功！
>>> 2025-08-17 11:23:30,914 - INFO - 模型存放位置：./output/qwen202508171115
>>> 2025-08-17 11:24:07,657 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 11:24:11,899 - INFO - 开始进行模型测试
>>> 2025-08-17 11:24:18,524 - INFO - 已选择模型文件夹: qwen202508171115
>>> 2025-08-17 11:24:21,535 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508171115/checkpoint-120
>>> 2025-08-17 17:32:17,809 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 17:32:22,051 - INFO - 开始进行训练
>>> 2025-08-17 17:32:26,368 - INFO - 导入包完成
>>> 2025-08-17 17:32:28,416 - INFO - 配置文件读取完成
>>> 2025-08-17 17:32:30,521 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 17:32:33,561 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 17:32:36,878 - INFO - tokenizer读取完成
>>> 2025-08-17 17:32:41,028 - INFO - model dtype:torch.float16
>>> 2025-08-17 17:32:43,645 - INFO - 模型导入完成
>>> 2025-08-17 17:32:46,490 - INFO - 读取数据集成功
>>> 2025-08-17 17:32:52,712 - INFO - 数据处理成功
>>> 2025-08-17 17:33:07,801 - INFO - 开始训练！
>>> 2025-08-17 17:33:09,848 - INFO - 批次大小  : 4
>>> 2025-08-17 17:33:12,014 - INFO - 训练轮数  : 40
>>> 2025-08-17 17:33:14,211 - INFO - 学习率    : 0.0001
>>> 2025-08-17 17:33:16,558 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 17:33:19,657 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 17:33:27,369 - INFO - >>> {'loss': 2.7863, 'grad_norm': 1.0297833681106567, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-17 17:33:43,862 - INFO - >>> {'loss': 2.6032, 'grad_norm': 1.007665991783142, 'learning_rate': 9.996112860009688e-05, 'epoch': 0.8}
>>> 2025-08-17 17:34:03,297 - INFO - >>> {'loss': 2.2361, 'grad_norm': 0.7062309384346008, 'learning_rate': 9.972379999624936e-05, 'epoch': 1.48}
>>> 2025-08-17 17:34:23,132 - INFO - >>> {'loss': 2.0122, 'grad_norm': 0.855418860912323, 'learning_rate': 9.927176156353899e-05, 'epoch': 2.16}
>>> 2025-08-17 17:34:45,265 - INFO - >>> {'loss': 1.8093, 'grad_norm': 1.1970126628875732, 'learning_rate': 9.860696522628639e-05, 'epoch': 2.96}
>>> 2025-08-17 17:35:11,130 - INFO - >>> {'loss': 1.7271, 'grad_norm': 1.1044740676879883, 'learning_rate': 9.773228160797188e-05, 'epoch': 3.64}
>>> 2025-08-17 17:35:30,988 - INFO - >>> {'loss': 1.4416, 'grad_norm': 2.0451571941375732, 'learning_rate': 9.665148763574123e-05, 'epoch': 4.32}
>>> 2025-08-17 17:35:53,208 - INFO - >>> {'loss': 1.3632, 'grad_norm': 0.0, 'learning_rate': 9.536925023144742e-05, 'epoch': 5.0}
>>> 2025-08-17 17:36:16,678 - INFO - >>> {'loss': 1.1481, 'grad_norm': 1.9663816690444946, 'learning_rate': 9.389110615965102e-05, 'epoch': 5.8}
>>> 2025-08-17 17:36:36,478 - INFO - >>> {'loss': 0.9855, 'grad_norm': 4.434081077575684, 'learning_rate': 9.222343811959693e-05, 'epoch': 6.48}
>>> 2025-08-17 17:36:57,678 - INFO - >>> {'loss': 0.8584, 'grad_norm': 2.818603515625, 'learning_rate': 9.037344718440322e-05, 'epoch': 7.16}
>>> 2025-08-17 17:37:21,501 - INFO - >>> {'loss': 0.7118, 'grad_norm': 3.204150438308716, 'learning_rate': 8.834912170647101e-05, 'epoch': 7.96}
>>> 2025-08-17 17:37:42,988 - INFO - >>> {'loss': 0.4359, 'grad_norm': 3.5836267471313477, 'learning_rate': 8.615920282338355e-05, 'epoch': 8.64}
>>> 2025-08-17 17:38:01,351 - INFO - >>> {'loss': 0.3304, 'grad_norm': 3.0895211696624756, 'learning_rate': 8.381314671324159e-05, 'epoch': 9.32}
>>> 2025-08-17 17:38:22,576 - INFO - >>> {'loss': 0.2429, 'grad_norm': 8.262640953063965, 'learning_rate': 8.132108376241849e-05, 'epoch': 10.0}
>>> 2025-08-17 17:38:45,026 - INFO - >>> {'loss': 0.1474, 'grad_norm': 4.9767279624938965, 'learning_rate': 7.869377482205042e-05, 'epoch': 10.8}
>>> 2025-08-17 17:39:07,686 - INFO - >>> {'loss': 0.1373, 'grad_norm': 4.9953436851501465, 'learning_rate': 7.594256474214882e-05, 'epoch': 11.48}
>>> 2025-08-17 17:39:28,659 - INFO - >>> {'loss': 0.1547, 'grad_norm': 6.998010158538818, 'learning_rate': 7.307933338397667e-05, 'epoch': 12.16}
>>> 2025-08-17 17:39:52,970 - INFO - >>> {'loss': 0.1148, 'grad_norm': 4.62699031829834, 'learning_rate': 7.011644432221958e-05, 'epoch': 12.96}
>>> 2025-08-17 17:40:10,973 - INFO - >>> {'loss': 0.0812, 'grad_norm': 3.5343194007873535, 'learning_rate': 6.706669145845863e-05, 'epoch': 13.64}
>>> 2025-08-17 17:40:32,127 - INFO - >>> {'loss': 0.0683, 'grad_norm': 3.118391752243042, 'learning_rate': 6.394324377647028e-05, 'epoch': 14.32}
>>> 2025-08-17 17:40:52,714 - INFO - >>> {'loss': 0.0548, 'grad_norm': 6.904941558837891, 'learning_rate': 6.075958847790262e-05, 'epoch': 15.0}
>>> 2025-08-17 17:41:15,038 - INFO - >>> {'loss': 0.0261, 'grad_norm': 2.253222703933716, 'learning_rate': 5.752947274387147e-05, 'epoch': 15.8}
>>> 2025-08-17 17:41:35,176 - INFO - >>> {'loss': 0.0197, 'grad_norm': 1.5420961380004883, 'learning_rate': 5.426684437395196e-05, 'epoch': 16.48}
>>> 2025-08-17 17:41:52,705 - INFO - >>> {'loss': 0.0317, 'grad_norm': 3.0907204151153564, 'learning_rate': 5.0985791558889785e-05, 'epoch': 17.16}
>>> 2025-08-17 17:42:15,659 - INFO - >>> {'loss': 0.018, 'grad_norm': 1.6252354383468628, 'learning_rate': 4.770048204709648e-05, 'epoch': 17.96}
>>> 2025-08-17 17:42:34,742 - INFO - >>> {'loss': 0.01, 'grad_norm': 1.2538872957229614, 'learning_rate': 4.4425101967610674e-05, 'epoch': 18.64}
>>> 2025-08-17 17:42:56,398 - INFO - >>> {'loss': 0.016, 'grad_norm': 1.392032504081726, 'learning_rate': 4.1173794573690996e-05, 'epoch': 19.32}
>>> 2025-08-17 17:43:14,572 - INFO - >>> {'loss': 0.0125, 'grad_norm': 1.11179518699646, 'learning_rate': 3.7960599171548574e-05, 'epoch': 20.0}
>>> 2025-08-17 17:43:37,662 - INFO - >>> {'loss': 0.0122, 'grad_norm': 2.1364760398864746, 'learning_rate': 3.479939049792817e-05, 'epoch': 20.8}
>>> 2025-08-17 17:43:56,085 - INFO - >>> {'loss': 0.0083, 'grad_norm': 0.9926274418830872, 'learning_rate': 3.1703818808308324e-05, 'epoch': 21.48}
>>> 2025-08-17 17:44:17,251 - INFO - >>> {'loss': 0.0074, 'grad_norm': 0.9881607890129089, 'learning_rate': 2.8687250934422772e-05, 'epoch': 22.16}
>>> 2025-08-17 17:44:37,564 - INFO - >>> {'loss': 0.0079, 'grad_norm': 1.23251211643219, 'learning_rate': 2.5762712565619528e-05, 'epoch': 22.96}
>>> 2025-08-17 17:44:58,518 - INFO - >>> {'loss': 0.0061, 'grad_norm': 0.9311179518699646, 'learning_rate': 2.2942832003289823e-05, 'epoch': 23.64}
>>> 2025-08-17 17:45:17,342 - INFO - >>> {'loss': 0.0076, 'grad_norm': 0.9657589197158813, 'learning_rate': 2.0239785631237705e-05, 'epoch': 24.32}
>>> 2025-08-17 17:45:37,457 - INFO - >>> {'loss': 0.007, 'grad_norm': 0.0, 'learning_rate': 1.7665245337452368e-05, 'epoch': 25.0}
>>> 2025-08-17 17:45:59,530 - INFO - >>> {'loss': 0.0059, 'grad_norm': 1.1263786554336548, 'learning_rate': 1.5230328114318127e-05, 'epoch': 25.8}
>>> 2025-08-17 17:46:16,172 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.9226062297821045, 'learning_rate': 1.2945548054891321e-05, 'epoch': 26.48}
>>> 2025-08-17 17:46:37,122 - INFO - >>> {'loss': 0.0071, 'grad_norm': 1.0169086456298828, 'learning_rate': 1.0820770952526155e-05, 'epoch': 27.16}
>>> 2025-08-17 17:47:00,710 - INFO - >>> {'loss': 0.0071, 'grad_norm': 0.6976758241653442, 'learning_rate': 8.865171699890834e-06, 'epoch': 27.96}
>>> 2025-08-17 17:47:20,773 - INFO - >>> {'loss': 0.0059, 'grad_norm': 1.1627002954483032, 'learning_rate': 7.0871946713269856e-06, 'epoch': 28.64}
>>> 2025-08-17 17:47:42,419 - INFO - >>> {'loss': 0.0066, 'grad_norm': 1.3052295446395874, 'learning_rate': 5.494517259623477e-06, 'epoch': 29.32}
>>> 2025-08-17 17:48:00,382 - INFO - >>> {'loss': 0.006, 'grad_norm': 1.1731914281845093, 'learning_rate': 4.094016724654359e-06, 'epoch': 30.0}
>>> 2025-08-17 17:48:24,360 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.8093262910842896, 'learning_rate': 2.8917404970305097e-06, 'epoch': 30.8}
>>> 2025-08-17 17:48:43,746 - INFO - >>> {'loss': 0.0047, 'grad_norm': 1.0727428197860718, 'learning_rate': 1.892880064994934e-06, 'epoch': 31.48}
>>> 2025-08-17 17:49:00,572 - INFO - >>> {'loss': 0.0065, 'grad_norm': 1.2016245126724243, 'learning_rate': 1.101748557319715e-06, 'epoch': 32.16}
>>> 2025-08-17 17:49:23,950 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.9226978421211243, 'learning_rate': 5.217621190024779e-07, 'epoch': 32.96}
>>> 2025-08-17 17:49:41,459 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.9359661936759949, 'learning_rate': 1.554251601833201e-07, 'epoch': 33.64}
>>> 2025-08-17 17:50:02,842 - INFO - >>> {'loss': 0.0055, 'grad_norm': 1.1070713996887207, 'learning_rate': 4.319541977831909e-09, 'epoch': 34.32}
>>> 2025-08-17 17:50:03,681 - INFO - >>> {'train_runtime': 1000.5849, 'train_samples_per_second': 3.998, 'train_steps_per_second': 0.24, 'train_loss': 0.3951395073517536, 'epoch': 34.32}
>>> 2025-08-17 17:50:03,682 - INFO - 训练成功！
>>> 2025-08-17 17:50:05,728 - INFO - 模型存放位置：./output/qwen202508171732
>>> 2025-08-17 19:31:12,667 - INFO - 当前环境：/home/liangshuqiao/anaconda3/bin/python
>>> 2025-08-17 19:31:15,826 - INFO - 开始进行训练
>>> 2025-08-17 19:35:01,581 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 19:35:05,823 - INFO - 开始进行训练
>>> 2025-08-17 19:35:10,081 - INFO - 导入包完成
>>> 2025-08-17 19:35:12,129 - INFO - 配置文件读取完成
>>> 2025-08-17 19:35:14,236 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 19:35:17,275 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 19:35:20,581 - INFO - tokenizer读取完成
>>> 2025-08-17 19:35:24,675 - INFO - model dtype:torch.float16
>>> 2025-08-17 19:35:27,294 - INFO - 模型导入完成
>>> 2025-08-17 19:35:30,139 - INFO - 读取数据集成功
>>> 2025-08-17 19:35:36,413 - INFO - 数据处理成功
>>> 2025-08-17 19:35:51,976 - INFO - 开始训练！
>>> 2025-08-17 19:35:54,022 - INFO - 批次大小  : 4
>>> 2025-08-17 19:35:56,189 - INFO - 训练轮数  : 15
>>> 2025-08-17 19:35:58,387 - INFO - 学习率    : 0.0001
>>> 2025-08-17 19:36:00,734 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 19:36:03,833 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 19:36:11,575 - INFO - >>> {'loss': 2.7863, 'grad_norm': 1.0117459297180176, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-17 19:36:28,078 - INFO - >>> {'loss': 2.6094, 'grad_norm': 1.0005830526351929, 'learning_rate': 9.97199108003991e-05, 'epoch': 0.8}
>>> 2025-08-17 19:36:47,509 - INFO - >>> {'loss': 2.2424, 'grad_norm': 0.6899375915527344, 'learning_rate': 9.801960531718896e-05, 'epoch': 1.48}
>>> 2025-08-17 19:37:07,343 - INFO - >>> {'loss': 2.0214, 'grad_norm': 0.865118682384491, 'learning_rate': 9.482736218434143e-05, 'epoch': 2.16}
>>> 2025-08-17 19:37:30,029 - INFO - >>> {'loss': 1.8197, 'grad_norm': 1.1208182573318481, 'learning_rate': 9.024236230276629e-05, 'epoch': 2.96}
>>> 2025-08-17 19:37:57,012 - INFO - >>> {'loss': 1.7477, 'grad_norm': 1.0795193910598755, 'learning_rate': 8.440705861229344e-05, 'epoch': 3.64}
>>> 2025-08-17 19:38:17,921 - INFO - >>> {'loss': 1.4791, 'grad_norm': 1.6308071613311768, 'learning_rate': 7.750275017224207e-05, 'epoch': 4.32}
>>> 2025-08-17 19:38:40,822 - INFO - >>> {'loss': 1.4116, 'grad_norm': 0.0, 'learning_rate': 6.974394931852956e-05, 'epoch': 5.0}
>>> 2025-08-17 19:39:04,831 - INFO - >>> {'loss': 1.2849, 'grad_norm': 2.068526268005371, 'learning_rate': 6.137171690605533e-05, 'epoch': 5.8}
>>> 2025-08-17 19:39:23,869 - INFO - >>> {'loss': 1.1688, 'grad_norm': 3.6393239498138428, 'learning_rate': 5.2646172706008156e-05, 'epoch': 6.48}
>>> 2025-08-17 19:39:43,865 - INFO - >>> {'loss': 1.0738, 'grad_norm': 3.213078260421753, 'learning_rate': 4.383841365514208e-05, 'epoch': 7.16}
>>> 2025-08-17 19:40:06,562 - INFO - >>> {'loss': 1.0397, 'grad_norm': 2.7370388507843018, 'learning_rate': 3.52220910517158e-05, 'epoch': 7.96}
>>> 2025-08-17 19:40:27,332 - INFO - >>> {'loss': 0.8703, 'grad_norm': 2.4724602699279785, 'learning_rate': 2.7064908389095468e-05, 'epoch': 8.64}
>>> 2025-08-17 19:40:44,854 - INFO - >>> {'loss': 0.7392, 'grad_norm': 2.813042163848877, 'learning_rate': 1.962030398375506e-05, 'epoch': 9.32}
>>> 2025-08-17 19:41:05,039 - INFO - >>> {'loss': 0.7888, 'grad_norm': 3.85652232170105, 'learning_rate': 1.3119576812968892e-05, 'epoch': 10.0}
>>> 2025-08-17 19:41:26,465 - INFO - >>> {'loss': 0.6945, 'grad_norm': 3.477707862854004, 'learning_rate': 7.764700207255903e-06, 'epoch': 10.8}
>>> 2025-08-17 19:41:47,746 - INFO - >>> {'loss': 0.7131, 'grad_norm': 3.4374613761901855, 'learning_rate': 3.72204667143895e-06, 'epoch': 11.48}
>>> 2025-08-17 19:42:08,054 - INFO - >>> {'loss': 0.6819, 'grad_norm': 3.0345020294189453, 'learning_rate': 1.1172188000142802e-06, 'epoch': 12.16}
>>> 2025-08-17 19:42:31,119 - INFO - >>> {'loss': 0.637, 'grad_norm': 2.82218074798584, 'learning_rate': 3.1146886901090025e-08, 'epoch': 12.96}
>>> 2025-08-17 19:42:32,020 - INFO - >>> {'train_runtime': 384.7441, 'train_samples_per_second': 3.899, 'train_steps_per_second': 0.234, 'train_loss': 1.2810447560416327, 'epoch': 12.96}
>>> 2025-08-17 19:42:32,021 - INFO - 训练成功！
>>> 2025-08-17 19:42:34,067 - INFO - 模型存放位置：./output/qwen202508171935
>>> 2025-08-17 20:00:26,594 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 20:00:30,836 - INFO - 开始进行训练
>>> 2025-08-17 20:00:35,078 - INFO - 导入包完成
>>> 2025-08-17 20:00:37,127 - INFO - 配置文件读取完成
>>> 2025-08-17 20:00:39,233 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 20:00:42,271 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 20:00:45,588 - INFO - tokenizer读取完成
>>> 2025-08-17 20:00:49,643 - INFO - model dtype:torch.float16
>>> 2025-08-17 20:00:52,261 - INFO - 模型导入完成
>>> 2025-08-17 20:00:55,298 - INFO - 读取数据集成功
>>> 2025-08-17 20:01:01,042 - INFO - 数据处理成功
>>> 2025-08-17 20:01:11,473 - INFO - 开始训练！
>>> 2025-08-17 20:01:13,519 - INFO - 批次大小  : 4
>>> 2025-08-17 20:01:15,685 - INFO - 训练轮数  : 15
>>> 2025-08-17 20:01:17,882 - INFO - 学习率    : 0.0001
>>> 2025-08-17 20:01:20,229 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 20:01:23,328 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 20:01:30,922 - INFO - >>> {'loss': 2.7863, 'grad_norm': 1.0164543390274048, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-17 20:01:47,356 - INFO - >>> {'loss': 2.6038, 'grad_norm': 0.9784812331199646, 'learning_rate': 9.97199108003991e-05, 'epoch': 0.8}
>>> 2025-08-17 20:02:06,732 - INFO - >>> {'loss': 2.241, 'grad_norm': 0.6563040018081665, 'learning_rate': 9.801960531718896e-05, 'epoch': 1.48}
>>> 2025-08-17 20:02:26,555 - INFO - >>> {'loss': 2.0233, 'grad_norm': 0.820602297782898, 'learning_rate': 9.482736218434143e-05, 'epoch': 2.16}
>>> 2025-08-17 20:02:48,681 - INFO - >>> {'loss': 1.822, 'grad_norm': 1.1099566221237183, 'learning_rate': 9.024236230276629e-05, 'epoch': 2.96}
>>> 2025-08-17 20:03:15,264 - INFO - >>> {'loss': 1.7477, 'grad_norm': 1.0675932168960571, 'learning_rate': 8.440705861229344e-05, 'epoch': 3.64}
>>> 2025-08-17 20:03:35,332 - INFO - >>> {'loss': 1.4799, 'grad_norm': 1.7475078105926514, 'learning_rate': 7.750275017224207e-05, 'epoch': 4.32}
>>> 2025-08-17 20:07:23,989 - INFO - ========__main__  202508172007========
>>> 2025-08-17 20:07:26,967 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/.venv/bin/python
>>> 2025-08-17 20:07:31,209 - INFO - 开始进行训练
>>> 2025-08-17 20:07:35,509 - INFO - 导入包完成
>>> 2025-08-17 20:07:37,556 - INFO - 配置文件读取完成
>>> 2025-08-17 20:07:39,662 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 20:07:42,701 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 20:07:46,018 - INFO - tokenizer读取完成
>>> 2025-08-17 20:07:50,015 - INFO - model dtype:torch.float16
>>> 2025-08-17 20:07:52,634 - INFO - 模型导入完成
>>> 2025-08-17 20:07:55,740 - INFO - 读取数据集成功
>>> 2025-08-17 20:08:01,542 - INFO - 数据处理成功
>>> 2025-08-17 20:08:17,172 - INFO - 开始训练！
>>> 2025-08-17 20:08:19,218 - INFO - 批次大小  : 4
>>> 2025-08-17 20:08:21,386 - INFO - 训练轮数  : 15
>>> 2025-08-17 20:08:23,583 - INFO - 学习率    : 0.0001
>>> 2025-08-17 20:08:25,930 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 20:08:29,029 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 20:08:36,748 - INFO - >>> {'loss': 2.7863, 'grad_norm': 1.0209243297576904, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-17 20:08:53,242 - INFO - >>> {'loss': 2.6081, 'grad_norm': 1.0063488483428955, 'learning_rate': 9.97199108003991e-05, 'epoch': 0.8}
>>> 2025-08-17 20:09:12,693 - INFO - >>> {'loss': 2.2409, 'grad_norm': 0.695899486541748, 'learning_rate': 9.801960531718896e-05, 'epoch': 1.48}
>>> 2025-08-17 20:09:32,693 - INFO - >>> {'loss': 2.019, 'grad_norm': 0.8584234118461609, 'learning_rate': 9.482736218434143e-05, 'epoch': 2.16}
>>> 2025-08-17 20:09:58,186 - INFO - >>> {'loss': 1.8172, 'grad_norm': 1.120789885520935, 'learning_rate': 9.024236230276629e-05, 'epoch': 2.96}
>>> 2025-08-17 20:10:27,084 - INFO - >>> {'loss': 1.7432, 'grad_norm': 1.076008915901184, 'learning_rate': 8.440705861229344e-05, 'epoch': 3.64}
>>> 2025-08-17 20:10:47,774 - INFO - >>> {'loss': 1.4734, 'grad_norm': 1.842250943183899, 'learning_rate': 7.750275017224207e-05, 'epoch': 4.32}
>>> 2025-08-17 20:11:09,443 - INFO - >>> {'loss': 1.4016, 'grad_norm': 0.0, 'learning_rate': 6.974394931852956e-05, 'epoch': 5.0}
>>> 2025-08-17 20:11:31,718 - INFO - >>> {'loss': 1.2719, 'grad_norm': 2.347533941268921, 'learning_rate': 6.137171690605533e-05, 'epoch': 5.8}
>>> 2025-08-17 20:11:49,936 - INFO - >>> {'loss': 1.1532, 'grad_norm': 3.7989020347595215, 'learning_rate': 5.2646172706008156e-05, 'epoch': 6.48}
>>> 2025-08-17 20:12:09,398 - INFO - >>> {'loss': 1.0536, 'grad_norm': 3.1761789321899414, 'learning_rate': 4.383841365514208e-05, 'epoch': 7.16}
>>> 2025-08-17 20:12:31,671 - INFO - >>> {'loss': 1.0118, 'grad_norm': 2.4481639862060547, 'learning_rate': 3.52220910517158e-05, 'epoch': 7.96}
>>> 2025-08-17 20:12:51,999 - INFO - >>> {'loss': 0.851, 'grad_norm': 2.530978202819824, 'learning_rate': 2.7064908389095468e-05, 'epoch': 8.64}
>>> 2025-08-17 20:13:09,002 - INFO - >>> {'loss': 0.7189, 'grad_norm': 3.0225746631622314, 'learning_rate': 1.962030398375506e-05, 'epoch': 9.32}
>>> 2025-08-17 20:13:28,333 - INFO - >>> {'loss': 0.7655, 'grad_norm': 3.9357149600982666, 'learning_rate': 1.3119576812968892e-05, 'epoch': 10.0}
>>> 2025-08-17 20:13:49,006 - INFO - >>> {'loss': 0.6705, 'grad_norm': 3.4395463466644287, 'learning_rate': 7.764700207255903e-06, 'epoch': 10.8}
>>> 2025-08-17 20:14:09,569 - INFO - >>> {'loss': 0.6973, 'grad_norm': 3.356724977493286, 'learning_rate': 3.72204667143895e-06, 'epoch': 11.48}
>>> 2025-08-17 20:14:28,894 - INFO - >>> {'loss': 0.6574, 'grad_norm': 2.6476917266845703, 'learning_rate': 1.1172188000142802e-06, 'epoch': 12.16}
>>> 2025-08-17 20:14:51,169 - INFO - >>> {'loss': 0.6185, 'grad_norm': 2.7238802909851074, 'learning_rate': 3.1146886901090025e-08, 'epoch': 12.96}
>>> 2025-08-17 20:14:52,054 - INFO - >>> {'train_runtime': 379.5735, 'train_samples_per_second': 3.952, 'train_steps_per_second': 0.237, 'train_loss': 1.2671371777852376, 'epoch': 12.96}
>>> 2025-08-17 20:14:52,055 - INFO - 训练成功！
>>> 2025-08-17 20:14:54,101 - INFO - 模型存放位置：./output/qwen202508172008
>>> 2025-08-17 20:23:36,307 - INFO - ========__main__  202508172023========
>>> 2025-08-17 20:23:39,285 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-17 20:23:43,467 - INFO - 开始进行训练
>>> 2025-08-17 20:24:25,596 - INFO - 导入包完成
>>> 2025-08-17 20:24:27,645 - INFO - 配置文件读取完成
>>> 2025-08-17 20:24:29,751 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 20:24:32,789 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 20:24:36,540 - INFO - tokenizer读取完成
>>> 2025-08-17 20:24:44,016 - INFO - model dtype:torch.float16
>>> 2025-08-17 20:24:46,634 - INFO - 模型导入完成
>>> 2025-08-17 20:24:49,514 - INFO - 读取数据集成功
>>> 2025-08-17 20:24:56,336 - INFO - 数据处理成功
>>> 2025-08-17 20:25:13,189 - INFO - 开始训练！
>>> 2025-08-17 20:25:15,235 - INFO - 批次大小  : 4
>>> 2025-08-17 20:25:17,401 - INFO - 训练轮数  : 15
>>> 2025-08-17 20:25:19,598 - INFO - 学习率    : 0.0001
>>> 2025-08-17 20:25:21,944 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 20:25:25,044 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 20:25:32,715 - INFO - >>> {'loss': 2.7863, 'grad_norm': 1.01213538646698, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-17 20:54:16,095 - INFO - ========__main__  202508172054========
>>> 2025-08-17 20:54:19,073 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-17 20:54:23,255 - INFO - 开始进行训练
>>> 2025-08-17 20:54:27,748 - INFO - 导入包完成
>>> 2025-08-17 20:54:29,796 - INFO - 配置文件读取完成
>>> 2025-08-17 20:54:31,902 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-17 20:54:34,941 - INFO - 模型路径:/home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 20:54:38,273 - INFO - tokenizer读取完成
>>> 2025-08-17 20:54:42,496 - INFO - model dtype:torch.float16
>>> 2025-08-17 20:54:45,114 - INFO - 模型导入完成
>>> 2025-08-17 20:54:49,852 - INFO - 读取数据集成功
>>> 2025-08-17 20:54:56,665 - INFO - 数据处理成功
>>> 2025-08-17 20:55:13,134 - INFO - 开始训练！
>>> 2025-08-17 20:55:15,180 - INFO - 批次大小  : 4
>>> 2025-08-17 20:55:17,348 - INFO - 训练轮数  : 100
>>> 2025-08-17 20:55:19,575 - INFO - 学习率    : 0.0001
>>> 2025-08-17 20:55:21,922 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-17 20:55:25,021 - INFO - 模型路径  : /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-17 20:55:32,743 - INFO - >>> {'loss': 2.7863, 'grad_norm': 1.0173797607421875, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-17 20:55:36,698 - INFO - >>> {'loss': 2.6459, 'grad_norm': 1.290208101272583, 'learning_rate': 0.0001, 'epoch': 0.32}
>>> 2025-08-17 20:55:39,124 - INFO - >>> {'loss': 2.6612, 'grad_norm': 1.2508149147033691, 'learning_rate': 9.999931232202689e-05, 'epoch': 0.48}
>>> 2025-08-17 20:55:43,681 - INFO - >>> {'loss': 2.6739, 'grad_norm': 1.191503643989563, 'learning_rate': 9.999724930702356e-05, 'epoch': 0.64}
>>> 2025-08-17 20:55:49,234 - INFO - >>> {'loss': 2.4469, 'grad_norm': 1.0047636032104492, 'learning_rate': 9.999381101173764e-05, 'epoch': 0.8}
>>> 2025-08-17 20:55:53,832 - INFO - >>> {'loss': 2.3147, 'grad_norm': 1.0620521306991577, 'learning_rate': 9.998899753074669e-05, 'epoch': 0.96}
>>> 2025-08-17 20:55:55,013 - INFO - >>> {'loss': 2.487, 'grad_norm': 1.0654444694519043, 'learning_rate': 9.998280899645574e-05, 'epoch': 1.0}
>>> 2025-08-17 20:55:58,580 - INFO - >>> {'loss': 2.2683, 'grad_norm': 0.7157343029975891, 'learning_rate': 9.997524557909352e-05, 'epoch': 1.16}
>>> 2025-08-17 20:56:03,643 - INFO - >>> {'loss': 2.0667, 'grad_norm': 0.7022306323051453, 'learning_rate': 9.996630748670787e-05, 'epoch': 1.32}
>>> 2025-08-17 20:56:08,677 - INFO - >>> {'loss': 2.0492, 'grad_norm': 0.7264428734779358, 'learning_rate': 9.995599496515995e-05, 'epoch': 1.48}
>>> 2025-08-17 20:56:13,393 - INFO - >>> {'loss': 1.9002, 'grad_norm': 0.8291198015213013, 'learning_rate': 9.99443082981175e-05, 'epoch': 1.6400000000000001}
>>> 2025-08-17 20:56:18,397 - INFO - >>> {'loss': 2.1107, 'grad_norm': 0.7863041758537292, 'learning_rate': 9.993124780704707e-05, 'epoch': 1.8}
>>> 2025-08-17 20:56:22,322 - INFO - >>> {'loss': 1.9528, 'grad_norm': 1.0440808534622192, 'learning_rate': 9.991681385120515e-05, 'epoch': 1.96}
>>> 2025-08-17 20:56:23,505 - INFO - >>> {'loss': 2.1167, 'grad_norm': 1.5966235399246216, 'learning_rate': 9.990100682762828e-05, 'epoch': 2.0}
>>> 2025-08-17 20:56:28,524 - INFO - >>> {'loss': 1.9726, 'grad_norm': 0.8806989789009094, 'learning_rate': 9.988382717112213e-05, 'epoch': 2.16}
>>> 2025-08-17 20:56:32,521 - INFO - >>> {'loss': 1.8739, 'grad_norm': 1.026495337486267, 'learning_rate': 9.986527535424957e-05, 'epoch': 2.32}
>>> 2025-08-17 20:56:37,731 - INFO - >>> {'loss': 1.8457, 'grad_norm': 1.020254135131836, 'learning_rate': 9.984535188731759e-05, 'epoch': 2.48}
>>> 2025-08-17 20:56:42,256 - INFO - >>> {'loss': 1.7595, 'grad_norm': 1.1757465600967407, 'learning_rate': 9.982405731836342e-05, 'epoch': 2.64}
>>> 2025-08-17 20:56:46,308 - INFO - >>> {'loss': 1.8646, 'grad_norm': 1.3792724609375, 'learning_rate': 9.980139223313925e-05, 'epoch': 2.8}
>>> 2025-08-17 20:56:50,964 - INFO - >>> {'loss': 1.6911, 'grad_norm': 1.2141661643981934, 'learning_rate': 9.977735725509632e-05, 'epoch': 2.96}
>>> 2025-08-17 20:56:51,870 - INFO - >>> {'loss': 1.8056, 'grad_norm': 3.012831926345825, 'learning_rate': 9.97519530453676e-05, 'epoch': 3.0}
>>> 2025-08-17 20:56:58,813 - INFO - >>> {'loss': 1.8187, 'grad_norm': 1.2221587896347046, 'learning_rate': 9.972518030274971e-05, 'epoch': 3.16}
>>> 2025-08-17 20:57:05,587 - INFO - >>> {'loss': 1.829, 'grad_norm': 1.3938701152801514, 'learning_rate': 9.969703976368368e-05, 'epoch': 3.32}
>>> 2025-08-17 20:57:11,290 - INFO - >>> {'loss': 1.5846, 'grad_norm': 1.2890825271606445, 'learning_rate': 9.966753220223465e-05, 'epoch': 3.48}
>>> 2025-08-17 20:57:17,710 - INFO - >>> {'loss': 1.5832, 'grad_norm': 1.136777639389038, 'learning_rate': 9.963665843007064e-05, 'epoch': 3.64}
>>> 2025-08-17 20:57:23,094 - INFO - >>> {'loss': 1.3122, 'grad_norm': 1.6488423347473145, 'learning_rate': 9.960441929644017e-05, 'epoch': 3.8}
>>> 2025-08-17 20:57:27,094 - INFO - >>> {'loss': 1.5984, 'grad_norm': 2.1675922870635986, 'learning_rate': 9.95708156881489e-05, 'epoch': 3.96}
>>> 2025-08-17 20:57:28,032 - INFO - >>> {'loss': 1.3204, 'grad_norm': 3.1852738857269287, 'learning_rate': 9.953584852953529e-05, 'epoch': 4.0}
>>> 2025-08-17 20:57:33,071 - INFO - >>> {'loss': 1.6746, 'grad_norm': 2.1651525497436523, 'learning_rate': 9.949951878244515e-05, 'epoch': 4.16}
>>> 2025-08-17 20:57:40,680 - INFO - >>> {'loss': 1.2596, 'grad_norm': 1.912469744682312, 'learning_rate': 9.946182744620512e-05, 'epoch': 4.32}
>>> 2025-08-17 20:57:47,216 - INFO - >>> {'loss': 1.3666, 'grad_norm': 1.8060182332992554, 'learning_rate': 9.942277555759529e-05, 'epoch': 4.48}
>>> 2025-08-17 20:57:54,042 - INFO - >>> {'loss': 1.3421, 'grad_norm': 2.3263466358184814, 'learning_rate': 9.938236419082061e-05, 'epoch': 4.64}
>>> 2025-08-17 20:57:59,250 - INFO - >>> {'loss': 1.4773, 'grad_norm': 2.544668674468994, 'learning_rate': 9.934059445748134e-05, 'epoch': 4.8}
>>> 2025-08-17 20:58:04,970 - INFO - >>> {'loss': 1.4329, 'grad_norm': 2.158216953277588, 'learning_rate': 9.929746750654249e-05, 'epoch': 4.96}
>>> 2025-08-17 20:58:05,325 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.925298452430226e-05, 'epoch': 5.0}
>>> 2025-08-17 20:58:11,545 - INFO - >>> {'loss': 1.4524, 'grad_norm': 3.991312026977539, 'learning_rate': 9.92071467343593e-05, 'epoch': 5.16}
>>> 2025-08-17 20:58:17,205 - INFO - >>> {'loss': 1.0954, 'grad_norm': 3.485146999359131, 'learning_rate': 9.915995539757917e-05, 'epoch': 5.32}
>>> 2025-08-17 20:58:21,576 - INFO - >>> {'loss': 1.0042, 'grad_norm': 2.37394118309021, 'learning_rate': 9.911141181205958e-05, 'epoch': 5.48}
>>> 2025-08-17 20:58:24,560 - INFO - >>> {'loss': 1.0101, 'grad_norm': 2.986771583557129, 'learning_rate': 9.906151731309472e-05, 'epoch': 5.64}
>>> 2025-08-17 20:58:30,137 - INFO - >>> {'loss': 1.1786, 'grad_norm': 2.0024349689483643, 'learning_rate': 9.901027327313848e-05, 'epoch': 5.8}
>>> 2025-08-17 20:58:36,183 - INFO - >>> {'loss': 1.1703, 'grad_norm': 2.76664137840271, 'learning_rate': 9.895768110176678e-05, 'epoch': 5.96}
>>> 2025-08-17 20:58:37,329 - INFO - >>> {'loss': 1.2249, 'grad_norm': 5.462205410003662, 'learning_rate': 9.890374224563872e-05, 'epoch': 6.0}
>>> 2025-08-17 20:58:42,183 - INFO - >>> {'loss': 0.9084, 'grad_norm': 2.4526126384735107, 'learning_rate': 9.884845818845685e-05, 'epoch': 6.16}
>>> 2025-08-17 20:58:47,254 - INFO - >>> {'loss': 1.0056, 'grad_norm': 2.3017690181732178, 'learning_rate': 9.879183045092628e-05, 'epoch': 6.32}
>>> 2025-08-17 20:58:49,405 - INFO - >>> {'loss': 0.5658, 'grad_norm': 4.7032647132873535, 'learning_rate': 9.873386059071294e-05, 'epoch': 6.48}
>>> 2025-08-17 20:58:54,566 - INFO - >>> {'loss': 1.0249, 'grad_norm': 4.029311180114746, 'learning_rate': 9.867455020240069e-05, 'epoch': 6.64}
>>> 2025-08-17 20:58:58,521 - INFO - >>> {'loss': 0.8876, 'grad_norm': 4.303473949432373, 'learning_rate': 9.861390091744737e-05, 'epoch': 6.8}
>>> 2025-08-17 20:59:04,593 - INFO - >>> {'loss': 0.959, 'grad_norm': 4.377981662750244, 'learning_rate': 9.855191440414013e-05, 'epoch': 6.96}
>>> 2025-08-17 20:59:05,891 - INFO - >>> {'loss': 0.658, 'grad_norm': 4.439909934997559, 'learning_rate': 9.848859236754935e-05, 'epoch': 7.0}
>>> 2025-08-17 20:59:09,837 - INFO - >>> {'loss': 0.7305, 'grad_norm': 3.2496185302734375, 'learning_rate': 9.842393654948181e-05, 'epoch': 7.16}
>>> 2025-08-17 20:59:14,430 - INFO - >>> {'loss': 0.7731, 'grad_norm': 2.9710726737976074, 'learning_rate': 9.83579487284328e-05, 'epoch': 7.32}
>>> 2025-08-17 20:59:18,595 - INFO - >>> {'loss': 0.8792, 'grad_norm': 3.2282509803771973, 'learning_rate': 9.829063071953714e-05, 'epoch': 7.48}
>>> 2025-08-17 20:59:22,919 - INFO - >>> {'loss': 0.7678, 'grad_norm': 3.210278272628784, 'learning_rate': 9.822198437451932e-05, 'epoch': 7.64}
>>> 2025-08-17 20:59:29,466 - INFO - >>> {'loss': 0.577, 'grad_norm': 2.2897067070007324, 'learning_rate': 9.815201158164254e-05, 'epoch': 7.8}
>>> 2025-08-17 20:59:33,045 - INFO - >>> {'loss': 0.5184, 'grad_norm': 2.874008893966675, 'learning_rate': 9.808071426565671e-05, 'epoch': 7.96}
>>> 2025-08-17 20:59:34,291 - INFO - >>> {'loss': 0.3794, 'grad_norm': 6.452005386352539, 'learning_rate': 9.800809438774556e-05, 'epoch': 8.0}
>>> 2025-08-17 20:59:39,228 - INFO - >>> {'loss': 0.5158, 'grad_norm': 3.017939567565918, 'learning_rate': 9.793415394547274e-05, 'epoch': 8.16}
>>> 2025-08-17 20:59:45,219 - INFO - >>> {'loss': 0.463, 'grad_norm': 2.849076986312866, 'learning_rate': 9.785889497272677e-05, 'epoch': 8.32}
>>> 2025-08-17 20:59:49,703 - INFO - >>> {'loss': 0.3681, 'grad_norm': 3.2446975708007812, 'learning_rate': 9.778231953966519e-05, 'epoch': 8.48}
>>> 2025-08-17 20:59:54,164 - INFO - >>> {'loss': 0.3144, 'grad_norm': 3.3243820667266846, 'learning_rate': 9.770442975265752e-05, 'epoch': 8.64}
>>> 2025-08-17 20:59:58,985 - INFO - >>> {'loss': 0.3885, 'grad_norm': 3.4383037090301514, 'learning_rate': 9.762522775422741e-05, 'epoch': 8.8}
>>> 2025-08-17 21:00:04,404 - INFO - >>> {'loss': 0.3756, 'grad_norm': 4.245587348937988, 'learning_rate': 9.754471572299363e-05, 'epoch': 8.96}
>>> 2025-08-17 21:00:04,756 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.746289587361021e-05, 'epoch': 9.0}
>>> 2025-08-17 21:00:08,727 - INFO - >>> {'loss': 0.1697, 'grad_norm': 2.860780715942383, 'learning_rate': 9.737977045670548e-05, 'epoch': 9.16}
>>> 2025-08-17 21:00:11,813 - INFO - >>> {'loss': 0.2193, 'grad_norm': 2.5530498027801514, 'learning_rate': 9.729534175882016e-05, 'epoch': 9.32}
>>> 2025-08-17 21:00:16,301 - INFO - >>> {'loss': 0.2092, 'grad_norm': 2.986985683441162, 'learning_rate': 9.720961210234449e-05, 'epoch': 9.48}
>>> 2025-08-17 21:00:21,960 - INFO - >>> {'loss': 0.2067, 'grad_norm': 3.346466064453125, 'learning_rate': 9.712258384545432e-05, 'epoch': 9.64}
>>> 2025-08-17 21:00:26,859 - INFO - >>> {'loss': 0.2453, 'grad_norm': 3.290935754776001, 'learning_rate': 9.703425938204627e-05, 'epoch': 9.8}
>>> 2025-08-17 21:00:30,148 - INFO - >>> {'loss': 0.0965, 'grad_norm': 2.5676097869873047, 'learning_rate': 9.694464114167186e-05, 'epoch': 9.96}
>>> 2025-08-17 21:00:31,817 - INFO - >>> {'loss': 0.2732, 'grad_norm': 4.494732856750488, 'learning_rate': 9.685373158947067e-05, 'epoch': 10.0}
>>> 2025-08-17 21:00:36,130 - INFO - >>> {'loss': 0.1085, 'grad_norm': 2.64713978767395, 'learning_rate': 9.676153322610259e-05, 'epoch': 10.16}
>>> 2025-08-17 21:00:41,764 - INFO - >>> {'loss': 0.1008, 'grad_norm': 2.662109375, 'learning_rate': 9.666804858767894e-05, 'epoch': 10.32}
>>> 2025-08-17 21:00:46,751 - INFO - >>> {'loss': 0.0779, 'grad_norm': 3.140854835510254, 'learning_rate': 9.65732802456928e-05, 'epoch': 10.48}
>>> 2025-08-17 21:00:50,070 - INFO - >>> {'loss': 0.1278, 'grad_norm': 3.852860689163208, 'learning_rate': 9.647723080694821e-05, 'epoch': 10.64}
>>> 2025-08-17 21:00:52,856 - INFO - >>> {'loss': 0.0894, 'grad_norm': 4.641670227050781, 'learning_rate': 9.637990291348853e-05, 'epoch': 10.8}
>>> 2025-08-17 21:00:57,792 - INFO - >>> {'loss': 0.1736, 'grad_norm': 5.802214622497559, 'learning_rate': 9.628129924252369e-05, 'epoch': 10.96}
>>> 2025-08-17 21:00:59,671 - INFO - >>> {'loss': 0.1976, 'grad_norm': 10.585649490356445, 'learning_rate': 9.618142250635658e-05, 'epoch': 11.0}
>>> 2025-08-17 21:01:03,351 - INFO - >>> {'loss': 0.1392, 'grad_norm': 5.242000102996826, 'learning_rate': 9.608027545230847e-05, 'epoch': 11.16}
>>> 2025-08-17 21:01:09,125 - INFO - >>> {'loss': 0.0643, 'grad_norm': 3.950962543487549, 'learning_rate': 9.597786086264338e-05, 'epoch': 11.32}
>>> 2025-08-17 21:01:13,860 - INFO - >>> {'loss': 0.0983, 'grad_norm': 4.87555456161499, 'learning_rate': 9.587418155449167e-05, 'epoch': 11.48}
>>> 2025-08-17 21:01:18,386 - INFO - >>> {'loss': 0.1299, 'grad_norm': 4.934397220611572, 'learning_rate': 9.576924037977233e-05, 'epoch': 11.64}
>>> 2025-08-17 21:01:23,189 - INFO - >>> {'loss': 0.1483, 'grad_norm': 5.4744038581848145, 'learning_rate': 9.566304022511477e-05, 'epoch': 11.8}
>>> 2025-08-17 21:01:27,737 - INFO - >>> {'loss': 0.0961, 'grad_norm': 3.600605010986328, 'learning_rate': 9.555558401177926e-05, 'epoch': 11.96}
>>> 2025-08-17 21:01:28,544 - INFO - >>> {'loss': 0.0796, 'grad_norm': 5.543358325958252, 'learning_rate': 9.544687469557666e-05, 'epoch': 12.0}
>>> 2025-08-17 21:01:33,700 - INFO - >>> {'loss': 0.0522, 'grad_norm': 2.934706449508667, 'learning_rate': 9.533691526678705e-05, 'epoch': 12.16}
>>> 2025-08-17 21:01:38,507 - INFO - >>> {'loss': 0.0675, 'grad_norm': 3.883074998855591, 'learning_rate': 9.52257087500775e-05, 'epoch': 12.32}
>>> 2025-08-17 21:01:43,319 - INFO - >>> {'loss': 0.0646, 'grad_norm': 3.373567581176758, 'learning_rate': 9.51132582044189e-05, 'epoch': 12.48}
>>> 2025-08-17 21:01:47,903 - INFO - >>> {'loss': 0.0693, 'grad_norm': 3.4116883277893066, 'learning_rate': 9.499956672300178e-05, 'epoch': 12.64}
>>> 2025-08-17 21:01:51,815 - INFO - >>> {'loss': 0.0562, 'grad_norm': 2.266843557357788, 'learning_rate': 9.488463743315126e-05, 'epoch': 12.8}
>>> 2025-08-17 21:01:56,498 - INFO - >>> {'loss': 0.0896, 'grad_norm': 3.125359535217285, 'learning_rate': 9.476847349624097e-05, 'epoch': 12.96}
>>> 2025-08-17 21:01:57,657 - INFO - >>> {'loss': 0.0552, 'grad_norm': 2.631122350692749, 'learning_rate': 9.46510781076061e-05, 'epoch': 13.0}
>>> 2025-08-17 21:02:02,244 - INFO - >>> {'loss': 0.0415, 'grad_norm': 2.4663124084472656, 'learning_rate': 9.453245449645563e-05, 'epoch': 13.16}
>>> 2025-08-17 21:02:05,339 - INFO - >>> {'loss': 0.0347, 'grad_norm': 2.583225727081299, 'learning_rate': 9.441260592578329e-05, 'epoch': 13.32}
>>> 2025-08-17 21:02:10,618 - INFO - >>> {'loss': 0.0339, 'grad_norm': 2.2474465370178223, 'learning_rate': 9.4291535692278e-05, 'epoch': 13.48}
>>> 2025-08-17 21:02:13,377 - INFO - >>> {'loss': 0.07, 'grad_norm': 3.596053123474121, 'learning_rate': 9.416924712623305e-05, 'epoch': 13.64}
>>> 2025-08-17 21:02:19,146 - INFO - >>> {'loss': 0.0401, 'grad_norm': 2.656689405441284, 'learning_rate': 9.404574359145459e-05, 'epoch': 13.8}
>>> 2025-08-17 21:02:23,716 - INFO - >>> {'loss': 0.0617, 'grad_norm': 3.672759771347046, 'learning_rate': 9.392102848516901e-05, 'epoch': 13.96}
>>> 2025-08-17 21:02:24,763 - INFO - >>> {'loss': 0.1051, 'grad_norm': 6.739542007446289, 'learning_rate': 9.379510523792961e-05, 'epoch': 14.0}
>>> 2025-08-17 21:02:29,298 - INFO - >>> {'loss': 0.0465, 'grad_norm': 3.166274070739746, 'learning_rate': 9.366797731352209e-05, 'epoch': 14.16}
>>> 2025-08-17 21:02:33,386 - INFO - >>> {'loss': 0.0579, 'grad_norm': 3.4504430294036865, 'learning_rate': 9.353964820886938e-05, 'epoch': 14.32}
>>> 2025-08-17 21:02:38,443 - INFO - >>> {'loss': 0.0386, 'grad_norm': 2.862034559249878, 'learning_rate': 9.341012145393547e-05, 'epoch': 14.48}
>>> 2025-08-17 21:02:43,110 - INFO - >>> {'loss': 0.0579, 'grad_norm': 3.517169713973999, 'learning_rate': 9.327940061162817e-05, 'epoch': 14.64}
>>> 2025-08-17 21:02:47,498 - INFO - >>> {'loss': 0.0666, 'grad_norm': 3.5698351860046387, 'learning_rate': 9.314748927770125e-05, 'epoch': 14.8}
>>> 2025-08-17 21:02:50,841 - INFO - >>> {'loss': 0.066, 'grad_norm': 3.3121917247772217, 'learning_rate': 9.301439108065546e-05, 'epoch': 14.96}
>>> 2025-08-17 21:02:53,344 - INFO - >>> {'loss': 0.1123, 'grad_norm': 7.365694046020508, 'learning_rate': 9.288010968163872e-05, 'epoch': 15.0}
>>> 2025-08-17 21:02:57,130 - INFO - >>> {'loss': 0.0448, 'grad_norm': 3.034092426300049, 'learning_rate': 9.274464877434548e-05, 'epoch': 15.16}
>>> 2025-08-17 21:03:02,344 - INFO - >>> {'loss': 0.0526, 'grad_norm': 2.8314144611358643, 'learning_rate': 9.260801208491498e-05, 'epoch': 15.32}
>>> 2025-08-17 21:03:05,444 - INFO - >>> {'loss': 0.0273, 'grad_norm': 3.1524288654327393, 'learning_rate': 9.247020337182893e-05, 'epoch': 15.48}
>>> 2025-08-17 21:03:10,035 - INFO - >>> {'loss': 0.0372, 'grad_norm': 2.7801220417022705, 'learning_rate': 9.233122642580796e-05, 'epoch': 15.64}
>>> 2025-08-17 21:03:14,960 - INFO - >>> {'loss': 0.0329, 'grad_norm': 2.031961679458618, 'learning_rate': 9.219108506970746e-05, 'epoch': 15.8}
>>> 2025-08-17 21:03:19,639 - INFO - >>> {'loss': 0.024, 'grad_norm': 1.7320843935012817, 'learning_rate': 9.204978315841237e-05, 'epoch': 15.96}
>>> 2025-08-17 21:03:21,946 - INFO - >>> {'loss': 0.0167, 'grad_norm': 1.7324095964431763, 'learning_rate': 9.190732457873119e-05, 'epoch': 16.0}
>>> 2025-08-17 21:03:27,608 - INFO - >>> {'loss': 0.0162, 'grad_norm': 1.373884916305542, 'learning_rate': 9.176371324928899e-05, 'epoch': 16.16}
>>> 2025-08-17 21:03:31,408 - INFO - >>> {'loss': 0.0141, 'grad_norm': 1.2642136812210083, 'learning_rate': 9.161895312041971e-05, 'epoch': 16.32}
>>> 2025-08-17 21:03:34,760 - INFO - >>> {'loss': 0.0211, 'grad_norm': 1.4947547912597656, 'learning_rate': 9.14730481740574e-05, 'epoch': 16.48}
>>> 2025-08-17 21:03:39,643 - INFO - >>> {'loss': 0.0323, 'grad_norm': 2.3414814472198486, 'learning_rate': 9.132600242362681e-05, 'epoch': 16.64}
>>> 2025-08-17 21:03:44,593 - INFO - >>> {'loss': 0.0298, 'grad_norm': 2.465996265411377, 'learning_rate': 9.117781991393283e-05, 'epoch': 16.8}
>>> 2025-08-17 21:03:48,227 - INFO - >>> {'loss': 0.0383, 'grad_norm': 3.2072834968566895, 'learning_rate': 9.102850472104944e-05, 'epoch': 16.96}
>>> 2025-08-17 21:03:49,177 - INFO - >>> {'loss': 0.1518, 'grad_norm': 7.438885688781738, 'learning_rate': 9.087806095220739e-05, 'epoch': 17.0}
>>> 2025-08-17 21:03:52,070 - INFO - >>> {'loss': 0.0097, 'grad_norm': 1.311116099357605, 'learning_rate': 9.072649274568129e-05, 'epoch': 17.16}
>>> 2025-08-17 21:03:57,827 - INFO - >>> {'loss': 0.0192, 'grad_norm': 1.7304191589355469, 'learning_rate': 9.057380427067584e-05, 'epoch': 17.32}
>>> 2025-08-17 21:04:02,729 - INFO - >>> {'loss': 0.0361, 'grad_norm': 2.586108922958374, 'learning_rate': 9.041999972721109e-05, 'epoch': 17.48}
>>> 2025-08-17 21:04:07,304 - INFO - >>> {'loss': 0.0271, 'grad_norm': 1.8590465784072876, 'learning_rate': 9.02650833460069e-05, 'epoch': 17.64}
>>> 2025-08-17 21:04:11,409 - INFO - >>> {'loss': 0.0271, 'grad_norm': 2.1433210372924805, 'learning_rate': 9.010905938836661e-05, 'epoch': 17.8}
>>> 2025-08-17 21:04:14,454 - INFO - >>> {'loss': 0.0656, 'grad_norm': 3.521125555038452, 'learning_rate': 8.995193214605973e-05, 'epoch': 17.96}
>>> 2025-08-17 21:04:15,680 - INFO - >>> {'loss': 0.0274, 'grad_norm': 4.479522228240967, 'learning_rate': 8.979370594120402e-05, 'epoch': 18.0}
>>> 2025-08-17 21:04:20,201 - INFO - >>> {'loss': 0.0197, 'grad_norm': 1.4567850828170776, 'learning_rate': 8.963438512614655e-05, 'epoch': 18.16}
>>> 2025-08-17 21:04:26,682 - INFO - >>> {'loss': 0.0217, 'grad_norm': 1.8890377283096313, 'learning_rate': 8.947397408334391e-05, 'epoch': 18.32}
>>> 2025-08-17 21:04:30,158 - INFO - >>> {'loss': 0.017, 'grad_norm': 2.076076030731201, 'learning_rate': 8.931247722524169e-05, 'epoch': 18.48}
>>> 2025-08-17 21:04:33,514 - INFO - >>> {'loss': 0.0173, 'grad_norm': 2.1239280700683594, 'learning_rate': 8.914989899415323e-05, 'epoch': 18.64}
>>> 2025-08-17 21:04:36,691 - INFO - >>> {'loss': 0.0175, 'grad_norm': 2.582585573196411, 'learning_rate': 8.898624386213725e-05, 'epoch': 18.8}
>>> 2025-08-17 21:04:42,787 - INFO - >>> {'loss': 0.024, 'grad_norm': 1.7372187376022339, 'learning_rate': 8.88215163308749e-05, 'epoch': 18.96}
>>> 2025-08-17 21:04:44,153 - INFO - >>> {'loss': 0.0557, 'grad_norm': 4.235774517059326, 'learning_rate': 8.8655720931546e-05, 'epoch': 19.0}
>>> 2025-08-17 21:04:50,564 - INFO - >>> {'loss': 0.0254, 'grad_norm': 2.0951590538024902, 'learning_rate': 8.84888622247043e-05, 'epoch': 19.16}
>>> 2025-08-17 21:04:54,717 - INFO - >>> {'loss': 0.0082, 'grad_norm': 1.170570969581604, 'learning_rate': 8.83209448001521e-05, 'epoch': 19.32}
>>> 2025-08-17 21:04:59,585 - INFO - >>> {'loss': 0.0292, 'grad_norm': 2.645371198654175, 'learning_rate': 8.815197327681399e-05, 'epoch': 19.48}
>>> 2025-08-17 21:05:03,587 - INFO - >>> {'loss': 0.079, 'grad_norm': 4.706268310546875, 'learning_rate': 8.798195230260973e-05, 'epoch': 19.64}
>>> 2025-08-17 21:05:07,007 - INFO - >>> {'loss': 0.0215, 'grad_norm': 2.23045015335083, 'learning_rate': 8.781088655432648e-05, 'epoch': 19.8}
>>> 2025-08-17 21:05:11,655 - INFO - >>> {'loss': 0.0369, 'grad_norm': 2.5727646350860596, 'learning_rate': 8.763878073749012e-05, 'epoch': 19.96}
>>> 2025-08-17 21:05:12,583 - INFO - >>> {'loss': 0.0117, 'grad_norm': 1.5812387466430664, 'learning_rate': 8.746563958623584e-05, 'epoch': 20.0}
>>> 2025-08-17 21:05:15,633 - INFO - >>> {'loss': 0.0127, 'grad_norm': 2.362565755844116, 'learning_rate': 8.729146786317786e-05, 'epoch': 20.16}
>>> 2025-08-17 21:05:21,583 - INFO - >>> {'loss': 0.0109, 'grad_norm': 1.707464575767517, 'learning_rate': 8.711627035927847e-05, 'epoch': 20.32}
>>> 2025-08-17 21:05:26,667 - INFO - >>> {'loss': 0.0138, 'grad_norm': 1.8794419765472412, 'learning_rate': 8.694005189371627e-05, 'epoch': 20.48}
>>> 2025-08-17 21:05:30,718 - INFO - >>> {'loss': 0.0129, 'grad_norm': 1.56442391872406, 'learning_rate': 8.676281731375353e-05, 'epoch': 20.64}
>>> 2025-08-17 21:05:35,497 - INFO - >>> {'loss': 0.0154, 'grad_norm': 1.524240255355835, 'learning_rate': 8.658457149460295e-05, 'epoch': 20.8}
>>> 2025-08-17 21:05:39,513 - INFO - >>> {'loss': 0.0168, 'grad_norm': 1.028961181640625, 'learning_rate': 8.640531933929344e-05, 'epoch': 20.96}
>>> 2025-08-17 21:05:40,805 - INFO - >>> {'loss': 0.0054, 'grad_norm': 1.1576275825500488, 'learning_rate': 8.622506577853538e-05, 'epoch': 21.0}
>>> 2025-08-17 21:05:45,228 - INFO - >>> {'loss': 0.0077, 'grad_norm': 0.9163605570793152, 'learning_rate': 8.604381577058486e-05, 'epoch': 21.16}
>>> 2025-08-17 21:05:49,750 - INFO - >>> {'loss': 0.012, 'grad_norm': 2.4403693675994873, 'learning_rate': 8.586157430110747e-05, 'epoch': 21.32}
>>> 2025-08-17 21:05:53,498 - INFO - >>> {'loss': 0.017, 'grad_norm': 2.16485595703125, 'learning_rate': 8.56783463830409e-05, 'epoch': 21.48}
>>> 2025-08-17 21:05:58,443 - INFO - >>> {'loss': 0.0272, 'grad_norm': 2.9175631999969482, 'learning_rate': 8.549413705645737e-05, 'epoch': 21.64}
>>> 2025-08-17 21:06:02,899 - INFO - >>> {'loss': 0.0136, 'grad_norm': 2.039775848388672, 'learning_rate': 8.530895138842467e-05, 'epoch': 21.8}
>>> 2025-08-17 21:06:08,536 - INFO - >>> {'loss': 0.0237, 'grad_norm': 1.853742241859436, 'learning_rate': 8.512279447286703e-05, 'epoch': 21.96}
>>> 2025-08-17 21:06:09,850 - INFO - >>> {'loss': 0.0097, 'grad_norm': 2.4441630840301514, 'learning_rate': 8.493567143042485e-05, 'epoch': 22.0}
>>> 2025-08-17 21:06:14,488 - INFO - >>> {'loss': 0.0069, 'grad_norm': 0.7790737152099609, 'learning_rate': 8.47475874083139e-05, 'epoch': 22.16}
>>> 2025-08-17 21:06:19,347 - INFO - >>> {'loss': 0.0102, 'grad_norm': 1.4448556900024414, 'learning_rate': 8.455854758018376e-05, 'epoch': 22.32}
>>> 2025-08-17 21:06:23,155 - INFO - >>> {'loss': 0.0271, 'grad_norm': 3.594365119934082, 'learning_rate': 8.436855714597546e-05, 'epoch': 22.48}
>>> 2025-08-17 21:06:26,341 - INFO - >>> {'loss': 0.0085, 'grad_norm': 1.2823960781097412, 'learning_rate': 8.417762133177848e-05, 'epoch': 22.64}
>>> 2025-08-17 21:06:30,796 - INFO - >>> {'loss': 0.0142, 'grad_norm': 1.6589875221252441, 'learning_rate': 8.398574538968697e-05, 'epoch': 22.8}
>>> 2025-08-17 21:06:35,087 - INFO - >>> {'loss': 0.0265, 'grad_norm': 2.3302271366119385, 'learning_rate': 8.379293459765526e-05, 'epoch': 22.96}
>>> 2025-08-17 21:06:36,314 - INFO - >>> {'loss': 0.01, 'grad_norm': 1.2795532941818237, 'learning_rate': 8.359919425935275e-05, 'epoch': 23.0}
>>> 2025-08-17 21:06:40,772 - INFO - >>> {'loss': 0.0081, 'grad_norm': 0.9481780529022217, 'learning_rate': 8.340452970401797e-05, 'epoch': 23.16}
>>> 2025-08-17 21:06:44,997 - INFO - >>> {'loss': 0.0051, 'grad_norm': 1.141900897026062, 'learning_rate': 8.3208946286312e-05, 'epoch': 23.32}
>>> 2025-08-17 21:06:51,491 - INFO - >>> {'loss': 0.0148, 'grad_norm': 2.4153380393981934, 'learning_rate': 8.301244938617116e-05, 'epoch': 23.48}
>>> 2025-08-17 21:06:56,150 - INFO - >>> {'loss': 0.035, 'grad_norm': 1.5227601528167725, 'learning_rate': 8.281504440865905e-05, 'epoch': 23.64}
>>> 2025-08-17 21:06:59,763 - INFO - >>> {'loss': 0.0472, 'grad_norm': 2.4658052921295166, 'learning_rate': 8.261673678381786e-05, 'epoch': 23.8}
>>> 2025-08-17 21:07:03,728 - INFO - >>> {'loss': 0.0149, 'grad_norm': 1.370026707649231, 'learning_rate': 8.241753196651902e-05, 'epoch': 23.96}
>>> 2025-08-17 21:07:05,995 - INFO - >>> {'loss': 0.0215, 'grad_norm': 2.5703272819519043, 'learning_rate': 8.221743543631313e-05, 'epoch': 24.0}
>>> 2025-08-17 21:07:10,406 - INFO - >>> {'loss': 0.0181, 'grad_norm': 1.5673118829727173, 'learning_rate': 8.201645269727925e-05, 'epoch': 24.16}
>>> 2025-08-17 21:07:15,105 - INFO - >>> {'loss': 0.0091, 'grad_norm': 0.8069869875907898, 'learning_rate': 8.181458927787347e-05, 'epoch': 24.32}
>>> 2025-08-17 21:07:20,983 - INFO - >>> {'loss': 0.0167, 'grad_norm': 1.192679762840271, 'learning_rate': 8.161185073077686e-05, 'epoch': 24.48}
>>> 2025-08-17 21:07:26,111 - INFO - >>> {'loss': 0.0048, 'grad_norm': 1.0230449438095093, 'learning_rate': 8.140824263274279e-05, 'epoch': 24.64}
>>> 2025-08-17 21:07:31,523 - INFO - >>> {'loss': 0.0195, 'grad_norm': 1.3390306234359741, 'learning_rate': 8.120377058444336e-05, 'epoch': 24.8}
>>> 2025-08-17 21:07:35,044 - INFO - >>> {'loss': 0.0076, 'grad_norm': 1.1758581399917603, 'learning_rate': 8.09984402103156e-05, 'epoch': 24.96}
>>> 2025-08-17 21:07:35,398 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.079225715840646e-05, 'epoch': 25.0}
>>> 2025-08-17 21:07:39,538 - INFO - >>> {'loss': 0.0171, 'grad_norm': 1.8780646324157715, 'learning_rate': 8.058522710021772e-05, 'epoch': 25.16}
>>> 2025-08-17 21:07:44,537 - INFO - >>> {'loss': 0.0081, 'grad_norm': 0.9783213138580322, 'learning_rate': 8.037735573054979e-05, 'epoch': 25.32}
>>> 2025-08-17 21:07:48,027 - INFO - >>> {'loss': 0.007, 'grad_norm': 0.7963345646858215, 'learning_rate': 8.016864876734514e-05, 'epoch': 25.48}
>>> 2025-08-17 21:07:52,142 - INFO - >>> {'loss': 0.0128, 'grad_norm': 1.3200629949569702, 'learning_rate': 7.995911195153105e-05, 'epoch': 25.64}
>>> 2025-08-17 21:07:57,273 - INFO - >>> {'loss': 0.0232, 'grad_norm': 2.2831311225891113, 'learning_rate': 7.974875104686163e-05, 'epoch': 25.8}
>>> 2025-08-17 21:08:03,099 - INFO - >>> {'loss': 0.0149, 'grad_norm': 1.652957558631897, 'learning_rate': 7.95375718397593e-05, 'epoch': 25.96}
>>> 2025-08-17 21:08:03,450 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.932558013915562e-05, 'epoch': 26.0}
>>> 2025-08-17 21:08:06,055 - INFO - >>> {'loss': 0.012, 'grad_norm': 1.4806269407272339, 'learning_rate': 7.911278177633151e-05, 'epoch': 26.16}
>>> 2025-08-17 21:08:08,569 - INFO - >>> {'loss': 0.0076, 'grad_norm': 2.407878875732422, 'learning_rate': 7.889918260475685e-05, 'epoch': 26.32}
>>> 2025-08-17 21:08:13,917 - INFO - >>> {'loss': 0.0085, 'grad_norm': 1.3548510074615479, 'learning_rate': 7.868478849992945e-05, 'epoch': 26.48}
>>> 2025-08-17 21:08:19,181 - INFO - >>> {'loss': 0.0173, 'grad_norm': 1.309993863105774, 'learning_rate': 7.846960535921344e-05, 'epoch': 26.64}
>>> 2025-08-17 21:08:24,525 - INFO - >>> {'loss': 0.0098, 'grad_norm': 1.750665307044983, 'learning_rate': 7.825363910167708e-05, 'epoch': 26.8}
>>> 2025-08-17 21:08:30,058 - INFO - >>> {'loss': 0.0145, 'grad_norm': 1.1915336847305298, 'learning_rate': 7.803689566792989e-05, 'epoch': 26.96}
>>> 2025-08-17 21:08:30,855 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.8992814421653748, 'learning_rate': 7.781938101995927e-05, 'epoch': 27.0}
>>> 2025-08-17 21:08:34,934 - INFO - >>> {'loss': 0.0076, 'grad_norm': 1.0606049299240112, 'learning_rate': 7.76011011409665e-05, 'epoch': 27.16}
>>> 2025-08-17 21:08:39,886 - INFO - >>> {'loss': 0.0164, 'grad_norm': 1.5137531757354736, 'learning_rate': 7.738206203520222e-05, 'epoch': 27.32}
>>> 2025-08-17 21:08:44,240 - INFO - >>> {'loss': 0.0096, 'grad_norm': 1.3489478826522827, 'learning_rate': 7.716226972780112e-05, 'epoch': 27.48}
>>> 2025-08-17 21:08:50,904 - INFO - >>> {'loss': 0.0099, 'grad_norm': 1.3621671199798584, 'learning_rate': 7.694173026461634e-05, 'epoch': 27.64}
>>> 2025-08-17 21:08:55,262 - INFO - >>> {'loss': 0.0103, 'grad_norm': 2.354633331298828, 'learning_rate': 7.672044971205314e-05, 'epoch': 27.8}
>>> 2025-08-17 21:08:58,266 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.7150498628616333, 'learning_rate': 7.649843415690198e-05, 'epoch': 27.96}
>>> 2025-08-17 21:08:59,540 - INFO - >>> {'loss': 0.0062, 'grad_norm': 1.254191279411316, 'learning_rate': 7.627568970617113e-05, 'epoch': 28.0}
>>> 2025-08-17 21:09:04,873 - INFO - >>> {'loss': 0.0061, 'grad_norm': 1.0978652238845825, 'learning_rate': 7.605222248691872e-05, 'epoch': 28.16}
>>> 2025-08-17 21:09:08,473 - INFO - >>> {'loss': 0.0045, 'grad_norm': 0.9851598143577576, 'learning_rate': 7.582803864608411e-05, 'epoch': 28.32}
>>> 2025-08-17 21:09:13,345 - INFO - >>> {'loss': 0.0062, 'grad_norm': 1.1417886018753052, 'learning_rate': 7.560314435031885e-05, 'epoch': 28.48}
>>> 2025-08-17 21:09:18,342 - INFO - >>> {'loss': 0.0295, 'grad_norm': 2.268519639968872, 'learning_rate': 7.53775457858171e-05, 'epoch': 28.64}
>>> 2025-08-17 21:09:24,867 - INFO - >>> {'loss': 0.014, 'grad_norm': 1.645309567451477, 'learning_rate': 7.51512491581454e-05, 'epoch': 28.8}
>>> 2025-08-17 21:09:28,065 - INFO - >>> {'loss': 0.0166, 'grad_norm': 3.1022121906280518, 'learning_rate': 7.4924260692072e-05, 'epoch': 28.96}
>>> 2025-08-17 21:09:29,389 - INFO - >>> {'loss': 0.0062, 'grad_norm': 0.7484157681465149, 'learning_rate': 7.469658663139563e-05, 'epoch': 29.0}
>>> 2025-08-17 21:09:33,748 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.6447039246559143, 'learning_rate': 7.446823323877375e-05, 'epoch': 29.16}
>>> 2025-08-17 21:09:39,641 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.8156490325927734, 'learning_rate': 7.423920679555028e-05, 'epoch': 29.32}
>>> 2025-08-17 21:09:44,532 - INFO - >>> {'loss': 0.0054, 'grad_norm': 1.1648179292678833, 'learning_rate': 7.400951360158284e-05, 'epoch': 29.48}
>>> 2025-08-17 21:09:48,271 - INFO - >>> {'loss': 0.0144, 'grad_norm': 2.7978334426879883, 'learning_rate': 7.377915997506945e-05, 'epoch': 29.64}
>>> 2025-08-17 21:09:51,681 - INFO - >>> {'loss': 0.0191, 'grad_norm': 2.367030143737793, 'learning_rate': 7.354815225237468e-05, 'epoch': 29.8}
>>> 2025-08-17 21:09:56,477 - INFO - >>> {'loss': 0.0119, 'grad_norm': 1.4594712257385254, 'learning_rate': 7.331649678785546e-05, 'epoch': 29.96}
>>> 2025-08-17 21:09:57,305 - INFO - >>> {'loss': 0.027, 'grad_norm': 2.0073256492614746, 'learning_rate': 7.308419995368616e-05, 'epoch': 30.0}
>>> 2025-08-17 21:10:00,636 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.7153834700584412, 'learning_rate': 7.285126813968346e-05, 'epoch': 30.16}
>>> 2025-08-17 21:10:05,921 - INFO - >>> {'loss': 0.0091, 'grad_norm': 1.8414621353149414, 'learning_rate': 7.261770775313046e-05, 'epoch': 30.32}
>>> 2025-08-17 21:10:11,512 - INFO - >>> {'loss': 0.0041, 'grad_norm': 1.0855419635772705, 'learning_rate': 7.238352521860049e-05, 'epoch': 30.48}
>>> 2025-08-17 21:10:16,392 - INFO - >>> {'loss': 0.0183, 'grad_norm': 1.9059703350067139, 'learning_rate': 7.214872697778037e-05, 'epoch': 30.64}
>>> 2025-08-17 21:10:20,687 - INFO - >>> {'loss': 0.0063, 'grad_norm': 1.0402671098709106, 'learning_rate': 7.191331948929323e-05, 'epoch': 30.8}
>>> 2025-08-17 21:10:25,558 - INFO - >>> {'loss': 0.0145, 'grad_norm': 3.0978283882141113, 'learning_rate': 7.167730922852087e-05, 'epoch': 30.96}
>>> 2025-08-17 21:10:26,600 - INFO - >>> {'loss': 0.0031, 'grad_norm': 1.0754096508026123, 'learning_rate': 7.14407026874256e-05, 'epoch': 31.0}
>>> 2025-08-17 21:10:30,447 - INFO - >>> {'loss': 0.0075, 'grad_norm': 1.1924004554748535, 'learning_rate': 7.120350637437165e-05, 'epoch': 31.16}
>>> 2025-08-17 21:10:36,093 - INFO - >>> {'loss': 0.0101, 'grad_norm': 1.1130542755126953, 'learning_rate': 7.096572681394625e-05, 'epoch': 31.32}
>>> 2025-08-17 21:10:39,845 - INFO - >>> {'loss': 0.0095, 'grad_norm': 1.4943312406539917, 'learning_rate': 7.072737054678003e-05, 'epoch': 31.48}
>>> 2025-08-17 21:10:42,063 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.7229160070419312, 'learning_rate': 7.048844412936719e-05, 'epoch': 31.64}
>>> 2025-08-17 21:10:46,651 - INFO - >>> {'loss': 0.0192, 'grad_norm': 1.8435544967651367, 'learning_rate': 7.024895413388508e-05, 'epoch': 31.8}
>>> 2025-08-17 21:10:50,358 - INFO - >>> {'loss': 0.0172, 'grad_norm': 3.0233449935913086, 'learning_rate': 7.000890714801351e-05, 'epoch': 31.96}
>>> 2025-08-17 21:10:52,661 - INFO - >>> {'loss': 0.0115, 'grad_norm': 1.5615718364715576, 'learning_rate': 6.976830977475346e-05, 'epoch': 32.0}
>>> 2025-08-17 21:10:56,384 - INFO - >>> {'loss': 0.0128, 'grad_norm': 0.8489910364151001, 'learning_rate': 6.952716863224551e-05, 'epoch': 32.16}
>>> 2025-08-17 21:11:00,497 - INFO - >>> {'loss': 0.0119, 'grad_norm': 1.4952822923660278, 'learning_rate': 6.928549035358772e-05, 'epoch': 32.32}
>>> 2025-08-17 21:11:06,857 - INFO - >>> {'loss': 0.0089, 'grad_norm': 1.0704649686813354, 'learning_rate': 6.904328158665323e-05, 'epoch': 32.48}
>>> 2025-08-17 21:11:10,323 - INFO - >>> {'loss': 0.0501, 'grad_norm': 3.048722505569458, 'learning_rate': 6.880054899390744e-05, 'epoch': 32.64}
>>> 2025-08-17 21:11:16,106 - INFO - >>> {'loss': 0.0069, 'grad_norm': 1.3664151430130005, 'learning_rate': 6.855729925222462e-05, 'epoch': 32.8}
>>> 2025-08-17 21:11:19,408 - INFO - >>> {'loss': 0.0248, 'grad_norm': 2.2372350692749023, 'learning_rate': 6.831353905270434e-05, 'epoch': 32.96}
>>> 2025-08-17 21:11:19,759 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.806927510048738e-05, 'epoch': 33.0}
>>> 2025-08-17 21:11:24,064 - INFO - >>> {'loss': 0.0094, 'grad_norm': 1.2972642183303833, 'learning_rate': 6.782451411457137e-05, 'epoch': 33.16}
>>> 2025-08-17 21:11:28,559 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.973983645439148, 'learning_rate': 6.757926282762583e-05, 'epoch': 33.32}
>>> 2025-08-17 21:11:32,460 - INFO - >>> {'loss': 0.0066, 'grad_norm': 0.9776307940483093, 'learning_rate': 6.733352798580708e-05, 'epoch': 33.48}
>>> 2025-08-17 21:11:36,678 - INFO - >>> {'loss': 0.0103, 'grad_norm': 0.9161984920501709, 'learning_rate': 6.708731634857263e-05, 'epoch': 33.64}
>>> 2025-08-17 21:11:40,435 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.7971829771995544, 'learning_rate': 6.684063468849527e-05, 'epoch': 33.8}
>>> 2025-08-17 21:11:45,701 - INFO - >>> {'loss': 0.0067, 'grad_norm': 1.35382080078125, 'learning_rate': 6.659348979107679e-05, 'epoch': 33.96}
>>> 2025-08-17 21:11:48,352 - INFO - >>> {'loss': 0.0173, 'grad_norm': 1.5322333574295044, 'learning_rate': 6.634588845456123e-05, 'epoch': 34.0}
>>> 2025-08-17 21:11:54,019 - INFO - >>> {'loss': 0.012, 'grad_norm': 1.1205878257751465, 'learning_rate': 6.609783748974802e-05, 'epoch': 34.16}
>>> 2025-08-17 21:11:57,924 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.6310208439826965, 'learning_rate': 6.584934371980453e-05, 'epoch': 34.32}
>>> 2025-08-17 21:12:02,904 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.9723160266876221, 'learning_rate': 6.560041398007847e-05, 'epoch': 34.48}
>>> 2025-08-17 21:12:07,163 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.745212733745575, 'learning_rate': 6.53510551179098e-05, 'epoch': 34.64}
>>> 2025-08-17 21:12:10,496 - INFO - >>> {'loss': 0.0034, 'grad_norm': 1.2540435791015625, 'learning_rate': 6.510127399244234e-05, 'epoch': 34.8}
>>> 2025-08-17 21:12:14,762 - INFO - >>> {'loss': 0.0036, 'grad_norm': 1.6081446409225464, 'learning_rate': 6.485107747443528e-05, 'epoch': 34.96}
>>> 2025-08-17 21:12:15,528 - INFO - >>> {'loss': 0.0051, 'grad_norm': 1.3212448358535767, 'learning_rate': 6.460047244607397e-05, 'epoch': 35.0}
>>> 2025-08-17 21:12:20,245 - INFO - >>> {'loss': 0.0066, 'grad_norm': 0.8732432126998901, 'learning_rate': 6.434946580078072e-05, 'epoch': 35.16}
>>> 2025-08-17 21:12:25,662 - INFO - >>> {'loss': 0.0058, 'grad_norm': 1.0761480331420898, 'learning_rate': 6.409806444302518e-05, 'epoch': 35.32}
>>> 2025-08-17 21:12:30,136 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.6055358052253723, 'learning_rate': 6.38462752881344e-05, 'epoch': 35.48}
>>> 2025-08-17 21:12:35,173 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.8692769408226013, 'learning_rate': 6.359410526210258e-05, 'epoch': 35.64}
>>> 2025-08-17 21:12:39,443 - INFO - >>> {'loss': 0.0082, 'grad_norm': 1.4108455181121826, 'learning_rate': 6.334156130140068e-05, 'epoch': 35.8}
>>> 2025-08-17 21:12:43,309 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.5231736302375793, 'learning_rate': 6.30886503527854e-05, 'epoch': 35.96}
>>> 2025-08-17 21:12:44,296 - INFO - >>> {'loss': 0.0037, 'grad_norm': 1.0871553421020508, 'learning_rate': 6.283537937310828e-05, 'epoch': 36.0}
>>> 2025-08-17 21:12:47,889 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.866411566734314, 'learning_rate': 6.258175532912431e-05, 'epoch': 36.16}
>>> 2025-08-17 21:12:52,360 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.6033278703689575, 'learning_rate': 6.232778519730023e-05, 'epoch': 36.32}
>>> 2025-08-17 21:12:57,923 - INFO - >>> {'loss': 0.0062, 'grad_norm': 1.4613431692123413, 'learning_rate': 6.207347596362265e-05, 'epoch': 36.48}
>>> 2025-08-17 21:13:01,782 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.6857220530509949, 'learning_rate': 6.181883462340588e-05, 'epoch': 36.64}
>>> 2025-08-17 21:13:07,120 - INFO - >>> {'loss': 0.0242, 'grad_norm': 2.1261179447174072, 'learning_rate': 6.15638681810996e-05, 'epoch': 36.8}
>>> 2025-08-17 21:13:11,673 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.9280788898468018, 'learning_rate': 6.1308583650096e-05, 'epoch': 36.96}
>>> 2025-08-17 21:13:12,707 - INFO - >>> {'loss': 0.0088, 'grad_norm': 1.3152827024459839, 'learning_rate': 6.105298805253708e-05, 'epoch': 37.0}
>>> 2025-08-17 21:13:19,562 - INFO - >>> {'loss': 0.0079, 'grad_norm': 1.4838578701019287, 'learning_rate': 6.079708841912133e-05, 'epoch': 37.16}
>>> 2025-08-17 21:13:24,215 - INFO - >>> {'loss': 0.0124, 'grad_norm': 1.9778151512145996, 'learning_rate': 6.054089178891039e-05, 'epoch': 37.32}
>>> 2025-08-17 21:13:29,217 - INFO - >>> {'loss': 0.0049, 'grad_norm': 1.210724949836731, 'learning_rate': 6.028440520913544e-05, 'epoch': 37.48}
>>> 2025-08-17 21:13:33,795 - INFO - >>> {'loss': 0.0115, 'grad_norm': 1.7143999338150024, 'learning_rate': 6.0027635735003316e-05, 'epoch': 37.64}
>>> 2025-08-17 21:13:38,753 - INFO - >>> {'loss': 0.0096, 'grad_norm': 1.2449696063995361, 'learning_rate': 5.9770590429502516e-05, 'epoch': 37.8}
>>> 2025-08-17 21:13:42,105 - INFO - >>> {'loss': 0.0173, 'grad_norm': 2.677313804626465, 'learning_rate': 5.9513276363208784e-05, 'epoch': 37.96}
>>> 2025-08-17 21:13:42,455 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.925570061409077e-05, 'epoch': 38.0}
>>> 2025-08-17 21:13:46,461 - INFO - >>> {'loss': 0.0088, 'grad_norm': 1.680868148803711, 'learning_rate': 5.8997870267315234e-05, 'epoch': 38.16}
>>> 2025-08-17 21:13:51,219 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.8692765831947327, 'learning_rate': 5.873979241505218e-05, 'epoch': 38.32}
>>> 2025-08-17 21:13:54,188 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.6005377769470215, 'learning_rate': 5.84814741562798e-05, 'epoch': 38.48}
>>> 2025-08-17 21:13:59,117 - INFO - >>> {'loss': 0.0072, 'grad_norm': 1.8180360794067383, 'learning_rate': 5.822292259658914e-05, 'epoch': 38.64}
>>> 2025-08-17 21:14:05,327 - INFO - >>> {'loss': 0.0073, 'grad_norm': 1.1392555236816406, 'learning_rate': 5.79641448479887e-05, 'epoch': 38.8}
>>> 2025-08-17 21:14:09,178 - INFO - >>> {'loss': 0.0056, 'grad_norm': 2.48427414894104, 'learning_rate': 5.770514802870879e-05, 'epoch': 38.96}
>>> 2025-08-17 21:14:10,247 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.9060854315757751, 'learning_rate': 5.7445939263005734e-05, 'epoch': 39.0}
>>> 2025-08-17 21:14:14,594 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.5533854961395264, 'learning_rate': 5.718652568096585e-05, 'epoch': 39.16}
>>> 2025-08-17 21:14:19,074 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.6636012196540833, 'learning_rate': 5.692691441830941e-05, 'epoch': 39.32}
>>> 2025-08-17 21:14:22,012 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.6155008673667908, 'learning_rate': 5.666711261619428e-05, 'epoch': 39.48}
>>> 2025-08-17 21:14:26,051 - INFO - >>> {'loss': 0.0074, 'grad_norm': 0.9534177184104919, 'learning_rate': 5.6407127421019534e-05, 'epoch': 39.64}
>>> 2025-08-17 21:14:29,205 - INFO - >>> {'loss': 0.0052, 'grad_norm': 1.8087679147720337, 'learning_rate': 5.614696598422885e-05, 'epoch': 39.8}
>>> 2025-08-17 21:14:35,562 - INFO - >>> {'loss': 0.0071, 'grad_norm': 1.4274920225143433, 'learning_rate': 5.5886635462113804e-05, 'epoch': 39.96}
>>> 2025-08-17 21:14:36,628 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.46047982573509216, 'learning_rate': 5.562614301561704e-05, 'epoch': 40.0}
>>> 2025-08-17 21:14:41,622 - INFO - >>> {'loss': 0.0083, 'grad_norm': 1.2162624597549438, 'learning_rate': 5.536549581013525e-05, 'epoch': 40.16}
>>> 2025-08-17 21:14:46,072 - INFO - >>> {'loss': 0.0073, 'grad_norm': 1.0727481842041016, 'learning_rate': 5.5104701015322125e-05, 'epoch': 40.32}
>>> 2025-08-17 21:14:51,936 - INFO - >>> {'loss': 0.0125, 'grad_norm': 1.3253662586212158, 'learning_rate': 5.48437658048911e-05, 'epoch': 40.48}
>>> 2025-08-17 21:14:55,614 - INFO - >>> {'loss': 0.004, 'grad_norm': 1.319348692893982, 'learning_rate': 5.4582697356418034e-05, 'epoch': 40.64}
>>> 2025-08-17 21:14:58,677 - INFO - >>> {'loss': 0.0068, 'grad_norm': 2.5195086002349854, 'learning_rate': 5.432150285114378e-05, 'epoch': 40.8}
>>> 2025-08-17 21:15:02,158 - INFO - >>> {'loss': 0.0023, 'grad_norm': 1.1865192651748657, 'learning_rate': 5.4060189473776676e-05, 'epoch': 40.96}
>>> 2025-08-17 21:15:02,689 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.2702125906944275, 'learning_rate': 5.379876441229486e-05, 'epoch': 41.0}
>>> 2025-08-17 21:15:05,931 - INFO - >>> {'loss': 0.0026, 'grad_norm': 1.0353777408599854, 'learning_rate': 5.3537234857748584e-05, 'epoch': 41.16}
>>> 2025-08-17 21:15:10,856 - INFO - >>> {'loss': 0.004, 'grad_norm': 1.2623865604400635, 'learning_rate': 5.327560800406241e-05, 'epoch': 41.32}
>>> 2025-08-17 21:15:14,988 - INFO - >>> {'loss': 0.0137, 'grad_norm': 2.3149356842041016, 'learning_rate': 5.30138910478373e-05, 'epoch': 41.48}
>>> 2025-08-17 21:15:18,535 - INFO - >>> {'loss': 0.0057, 'grad_norm': 1.2343584299087524, 'learning_rate': 5.275209118815273e-05, 'epoch': 41.64}
>>> 2025-08-17 21:15:24,497 - INFO - >>> {'loss': 0.0169, 'grad_norm': 1.4609544277191162, 'learning_rate': 5.249021562636857e-05, 'epoch': 41.8}
>>> 2025-08-17 21:15:29,181 - INFO - >>> {'loss': 0.0046, 'grad_norm': 1.2148094177246094, 'learning_rate': 5.222827156592701e-05, 'epoch': 41.96}
>>> 2025-08-17 21:15:30,394 - INFO - >>> {'loss': 0.0192, 'grad_norm': 3.8533072471618652, 'learning_rate': 5.196626621215449e-05, 'epoch': 42.0}
>>> 2025-08-17 21:15:34,503 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.9030823707580566, 'learning_rate': 5.170420677206343e-05, 'epoch': 42.16}
>>> 2025-08-17 21:15:38,307 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.7241603136062622, 'learning_rate': 5.144210045415402e-05, 'epoch': 42.32}
>>> 2025-08-17 21:15:42,854 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.47551342844963074, 'learning_rate': 5.1179954468215915e-05, 'epoch': 42.48}
>>> 2025-08-17 21:15:47,527 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.8536728024482727, 'learning_rate': 5.0917776025129926e-05, 'epoch': 42.64}
>>> 2025-08-17 21:15:53,320 - INFO - >>> {'loss': 0.0068, 'grad_norm': 1.0095829963684082, 'learning_rate': 5.065557233666968e-05, 'epoch': 42.8}
>>> 2025-08-17 21:15:57,485 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.9984426498413086, 'learning_rate': 5.039335061530319e-05, 'epoch': 42.96}
>>> 2025-08-17 21:15:59,541 - INFO - >>> {'loss': 0.0104, 'grad_norm': 1.9479553699493408, 'learning_rate': 5.0131118073994556e-05, 'epoch': 43.0}
>>> 2025-08-17 21:16:02,907 - INFO - >>> {'loss': 0.0041, 'grad_norm': 1.0598315000534058, 'learning_rate': 4.986888192600546e-05, 'epoch': 43.16}
>>> 2025-08-17 21:16:08,680 - INFO - >>> {'loss': 0.0026, 'grad_norm': 1.0923937559127808, 'learning_rate': 4.9606649384696826e-05, 'epoch': 43.32}
>>> 2025-08-17 21:16:12,965 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.826282799243927, 'learning_rate': 4.934442766333034e-05, 'epoch': 43.48}
>>> 2025-08-17 21:16:15,700 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.31947028636932373, 'learning_rate': 4.9082223974870086e-05, 'epoch': 43.64}
>>> 2025-08-17 21:16:20,552 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.4616408348083496, 'learning_rate': 4.8820045531784096e-05, 'epoch': 43.8}
>>> 2025-08-17 21:16:25,488 - INFO - >>> {'loss': 0.0207, 'grad_norm': 1.6428930759429932, 'learning_rate': 4.8557899545846e-05, 'epoch': 43.96}
>>> 2025-08-17 21:16:26,272 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.44890278577804565, 'learning_rate': 4.829579322793659e-05, 'epoch': 44.0}
>>> 2025-08-17 21:16:31,988 - INFO - >>> {'loss': 0.0032, 'grad_norm': 1.3368526697158813, 'learning_rate': 4.8033733787845535e-05, 'epoch': 44.16}
>>> 2025-08-17 21:16:34,975 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.4994266629219055, 'learning_rate': 4.7771728434073e-05, 'epoch': 44.32}
>>> 2025-08-17 21:16:39,267 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.7380144000053406, 'learning_rate': 4.7509784373631444e-05, 'epoch': 44.48}
>>> 2025-08-17 21:16:43,360 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.5401855707168579, 'learning_rate': 4.724790881184727e-05, 'epoch': 44.64}
>>> 2025-08-17 21:16:47,207 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.6817562580108643, 'learning_rate': 4.6986108952162695e-05, 'epoch': 44.8}
>>> 2025-08-17 21:16:52,224 - INFO - >>> {'loss': 0.0099, 'grad_norm': 1.377633810043335, 'learning_rate': 4.6724391995937604e-05, 'epoch': 44.96}
>>> 2025-08-17 21:16:53,034 - INFO - >>> {'loss': 0.008, 'grad_norm': 3.38112211227417, 'learning_rate': 4.646276514225143e-05, 'epoch': 45.0}
>>> 2025-08-17 21:16:56,246 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.5445300340652466, 'learning_rate': 4.6201235587705154e-05, 'epoch': 45.16}
>>> 2025-08-17 21:16:59,936 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.7378478050231934, 'learning_rate': 4.5939810526223336e-05, 'epoch': 45.32}
>>> 2025-08-17 21:17:04,291 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.6580665707588196, 'learning_rate': 4.567849714885623e-05, 'epoch': 45.48}
>>> 2025-08-17 21:17:08,383 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.4603584110736847, 'learning_rate': 4.5417302643581985e-05, 'epoch': 45.64}
>>> 2025-08-17 21:17:13,698 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.4302370846271515, 'learning_rate': 4.5156234195108916e-05, 'epoch': 45.8}
>>> 2025-08-17 21:17:17,851 - INFO - >>> {'loss': 0.0144, 'grad_norm': 1.5240052938461304, 'learning_rate': 4.4895298984677886e-05, 'epoch': 45.96}
>>> 2025-08-17 21:17:19,636 - INFO - >>> {'loss': 0.0061, 'grad_norm': 1.0932419300079346, 'learning_rate': 4.4634504189864765e-05, 'epoch': 46.0}
>>> 2025-08-17 21:17:22,852 - INFO - >>> {'loss': 0.0065, 'grad_norm': 0.675493061542511, 'learning_rate': 4.4373856984382984e-05, 'epoch': 46.16}
>>> 2025-08-17 21:17:27,632 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.4269779920578003, 'learning_rate': 4.4113364537886215e-05, 'epoch': 46.32}
>>> 2025-08-17 21:17:32,059 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.5227886438369751, 'learning_rate': 4.385303401577118e-05, 'epoch': 46.48}
>>> 2025-08-17 21:17:35,324 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.7048608064651489, 'learning_rate': 4.359287257898049e-05, 'epoch': 46.64}
>>> 2025-08-17 21:17:39,788 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.35778915882110596, 'learning_rate': 4.333288738380573e-05, 'epoch': 46.8}
>>> 2025-08-17 21:17:44,082 - INFO - >>> {'loss': 0.0012, 'grad_norm': 1.6703506708145142, 'learning_rate': 4.3073085581690605e-05, 'epoch': 46.96}
>>> 2025-08-17 21:17:44,431 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.281347431903416e-05, 'epoch': 47.0}
>>> 2025-08-17 21:17:48,143 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.306733101606369, 'learning_rate': 4.2554060736994284e-05, 'epoch': 47.16}
>>> 2025-08-17 21:17:51,572 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.678516149520874, 'learning_rate': 4.229485197129122e-05, 'epoch': 47.32}
>>> 2025-08-17 21:17:57,379 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.36595964431762695, 'learning_rate': 4.203585515201131e-05, 'epoch': 47.48}
>>> 2025-08-17 21:18:01,228 - INFO - >>> {'loss': 0.0081, 'grad_norm': 0.866912305355072, 'learning_rate': 4.177707740341087e-05, 'epoch': 47.64}
>>> 2025-08-17 21:18:06,019 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.5142207145690918, 'learning_rate': 4.1518525843720216e-05, 'epoch': 47.8}
>>> 2025-08-17 21:18:11,006 - INFO - >>> {'loss': 0.0027, 'grad_norm': 1.2276042699813843, 'learning_rate': 4.126020758494782e-05, 'epoch': 47.96}
>>> 2025-08-17 21:18:11,358 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.100212973268478e-05, 'epoch': 48.0}
>>> 2025-08-17 21:18:16,705 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.5595277547836304, 'learning_rate': 4.074429938590924e-05, 'epoch': 48.16}
>>> 2025-08-17 21:18:21,424 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.3044368624687195, 'learning_rate': 4.0486723636791234e-05, 'epoch': 48.32}
>>> 2025-08-17 21:18:24,165 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.4707944691181183, 'learning_rate': 4.022940957049751e-05, 'epoch': 48.48}
>>> 2025-08-17 21:18:27,602 - INFO - >>> {'loss': 0.0139, 'grad_norm': 0.9070896506309509, 'learning_rate': 3.9972364264996696e-05, 'epoch': 48.64}
>>> 2025-08-17 21:18:30,819 - INFO - >>> {'loss': 0.0087, 'grad_norm': 0.6389329433441162, 'learning_rate': 3.9715594790864586e-05, 'epoch': 48.8}
>>> 2025-08-17 21:18:35,539 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.6341981887817383, 'learning_rate': 3.945910821108963e-05, 'epoch': 48.96}
>>> 2025-08-17 21:18:36,659 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.4818950891494751, 'learning_rate': 3.920291158087869e-05, 'epoch': 49.0}
>>> 2025-08-17 21:18:40,716 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.563865065574646, 'learning_rate': 3.894701194746291e-05, 'epoch': 49.16}
>>> 2025-08-17 21:18:46,371 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.3082135021686554, 'learning_rate': 3.869141634990399e-05, 'epoch': 49.32}
>>> 2025-08-17 21:18:51,049 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.33492982387542725, 'learning_rate': 3.8436131818900416e-05, 'epoch': 49.48}
>>> 2025-08-17 21:18:54,914 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.7305338382720947, 'learning_rate': 3.818116537659412e-05, 'epoch': 49.64}
>>> 2025-08-17 21:18:59,457 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.6297098994255066, 'learning_rate': 3.7926524036377364e-05, 'epoch': 49.8}
>>> 2025-08-17 21:19:02,375 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.3847680389881134, 'learning_rate': 3.767221480269978e-05, 'epoch': 49.96}
>>> 2025-08-17 21:19:03,133 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.8200938105583191, 'learning_rate': 3.741824467087569e-05, 'epoch': 50.0}
>>> 2025-08-17 21:19:07,719 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.7228386998176575, 'learning_rate': 3.716462062689172e-05, 'epoch': 50.16}
>>> 2025-08-17 21:19:12,485 - INFO - >>> {'loss': 0.0016, 'grad_norm': 1.0188071727752686, 'learning_rate': 3.691134964721462e-05, 'epoch': 50.32}
>>> 2025-08-17 21:19:16,747 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.3089901804924011, 'learning_rate': 3.665843869859934e-05, 'epoch': 50.48}
>>> 2025-08-17 21:19:21,198 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.740172266960144, 'learning_rate': 3.6405894737897414e-05, 'epoch': 50.64}
>>> 2025-08-17 21:19:25,434 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.23809053003787994, 'learning_rate': 3.615372471186562e-05, 'epoch': 50.8}
>>> 2025-08-17 21:19:27,853 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.6630768775939941, 'learning_rate': 3.5901935556974834e-05, 'epoch': 50.96}
>>> 2025-08-17 21:19:28,966 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.5688936114311218, 'learning_rate': 3.5650534199219296e-05, 'epoch': 51.0}
>>> 2025-08-17 21:19:32,176 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.4012785851955414, 'learning_rate': 3.539952755392605e-05, 'epoch': 51.16}
>>> 2025-08-17 21:19:36,171 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.9964006543159485, 'learning_rate': 3.514892252556474e-05, 'epoch': 51.32}
>>> 2025-08-17 21:19:40,669 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.45961993932724, 'learning_rate': 3.489872600755765e-05, 'epoch': 51.48}
>>> 2025-08-17 21:19:44,713 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.2151927500963211, 'learning_rate': 3.464894488209022e-05, 'epoch': 51.64}
>>> 2025-08-17 21:19:49,351 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.2938346862792969, 'learning_rate': 3.4399586019921534e-05, 'epoch': 51.8}
>>> 2025-08-17 21:19:53,339 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.47255682945251465, 'learning_rate': 3.415065628019547e-05, 'epoch': 51.96}
>>> 2025-08-17 21:19:54,196 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.15074051916599274, 'learning_rate': 3.3902162510252e-05, 'epoch': 52.0}
>>> 2025-08-17 21:19:58,982 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.32415953278541565, 'learning_rate': 3.365411154543878e-05, 'epoch': 52.16}
>>> 2025-08-17 21:20:02,586 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.4026709496974945, 'learning_rate': 3.3406510208923224e-05, 'epoch': 52.32}
>>> 2025-08-17 21:20:06,564 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.3492070436477661, 'learning_rate': 3.315936531150473e-05, 'epoch': 52.48}
>>> 2025-08-17 21:20:11,014 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.39531686902046204, 'learning_rate': 3.291268365142738e-05, 'epoch': 52.64}
>>> 2025-08-17 21:20:15,558 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.2763652205467224, 'learning_rate': 3.266647201419294e-05, 'epoch': 52.8}
>>> 2025-08-17 21:20:21,033 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.5182449221611023, 'learning_rate': 3.242073717237418e-05, 'epoch': 52.96}
>>> 2025-08-17 21:20:22,053 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.33522647619247437, 'learning_rate': 3.217548588542864e-05, 'epoch': 53.0}
>>> 2025-08-17 21:20:25,066 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.3119027316570282, 'learning_rate': 3.193072489951263e-05, 'epoch': 53.16}
>>> 2025-08-17 21:20:30,362 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.158472940325737, 'learning_rate': 3.1686460947295695e-05, 'epoch': 53.32}
>>> 2025-08-17 21:20:33,989 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.304156094789505, 'learning_rate': 3.1442700747775414e-05, 'epoch': 53.48}
>>> 2025-08-17 21:20:37,367 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.18697573244571686, 'learning_rate': 3.1199451006092584e-05, 'epoch': 53.64}
>>> 2025-08-17 21:20:42,604 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.4438560903072357, 'learning_rate': 3.095671841334678e-05, 'epoch': 53.8}
>>> 2025-08-17 21:20:45,424 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.41129711270332336, 'learning_rate': 3.0714509646412296e-05, 'epoch': 53.96}
>>> 2025-08-17 21:20:47,209 - INFO - >>> {'loss': 0.0103, 'grad_norm': 1.0863783359527588, 'learning_rate': 3.0472831367754494e-05, 'epoch': 54.0}
>>> 2025-08-17 21:20:49,329 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.3299413025379181, 'learning_rate': 3.0231690225246535e-05, 'epoch': 54.16}
>>> 2025-08-17 21:20:53,644 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.29573535919189453, 'learning_rate': 2.999109285198649e-05, 'epoch': 54.32}
>>> 2025-08-17 21:20:57,310 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.2928832769393921, 'learning_rate': 2.9751045866114922e-05, 'epoch': 54.48}
>>> 2025-08-17 21:21:01,662 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.44270220398902893, 'learning_rate': 2.9511555870632824e-05, 'epoch': 54.64}
>>> 2025-08-17 21:21:05,878 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.19284750521183014, 'learning_rate': 2.927262945321998e-05, 'epoch': 54.8}
>>> 2025-08-17 21:21:11,490 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.34578293561935425, 'learning_rate': 2.9034273186053755e-05, 'epoch': 54.96}
>>> 2025-08-17 21:21:12,929 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.3250220715999603, 'learning_rate': 2.8796493625628356e-05, 'epoch': 55.0}
>>> 2025-08-17 21:21:17,951 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.32891449332237244, 'learning_rate': 2.8559297312574417e-05, 'epoch': 55.16}
>>> 2025-08-17 21:21:22,676 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.5047342777252197, 'learning_rate': 2.832269077147913e-05, 'epoch': 55.32}
>>> 2025-08-17 21:21:26,761 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.3035094738006592, 'learning_rate': 2.8086680510706774e-05, 'epoch': 55.48}
>>> 2025-08-17 21:21:30,961 - INFO - >>> {'loss': 0.0065, 'grad_norm': 0.5967007875442505, 'learning_rate': 2.7851273022219644e-05, 'epoch': 55.64}
>>> 2025-08-17 21:21:34,586 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3656657636165619, 'learning_rate': 2.7616474781399526e-05, 'epoch': 55.8}
>>> 2025-08-17 21:21:39,301 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.36211806535720825, 'learning_rate': 2.7382292246869547e-05, 'epoch': 55.96}
>>> 2025-08-17 21:21:41,537 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.6400649547576904, 'learning_rate': 2.7148731860316546e-05, 'epoch': 56.0}
>>> 2025-08-17 21:21:44,934 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.2759789824485779, 'learning_rate': 2.6915800046313848e-05, 'epoch': 56.16}
>>> 2025-08-17 21:21:48,102 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.6662061810493469, 'learning_rate': 2.6683503212144563e-05, 'epoch': 56.32}
>>> 2025-08-17 21:21:52,393 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.37758195400238037, 'learning_rate': 2.645184774762533e-05, 'epoch': 56.48}
>>> 2025-08-17 21:21:57,677 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.32174891233444214, 'learning_rate': 2.622084002493056e-05, 'epoch': 56.64}
>>> 2025-08-17 21:22:01,398 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.303634911775589, 'learning_rate': 2.599048639841717e-05, 'epoch': 56.8}
>>> 2025-08-17 21:22:06,978 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.5297691226005554, 'learning_rate': 2.5760793204449735e-05, 'epoch': 56.96}
>>> 2025-08-17 21:22:07,843 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.31400367617607117, 'learning_rate': 2.5531766761226272e-05, 'epoch': 57.0}
>>> 2025-08-17 21:22:12,870 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.24325159192085266, 'learning_rate': 2.530341336860439e-05, 'epoch': 57.16}
>>> 2025-08-17 21:22:18,844 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3232419192790985, 'learning_rate': 2.5075739307928014e-05, 'epoch': 57.32}
>>> 2025-08-17 21:22:21,913 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.43053877353668213, 'learning_rate': 2.4848750841854616e-05, 'epoch': 57.48}
>>> 2025-08-17 21:22:26,637 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.3971806764602661, 'learning_rate': 2.4622454214182917e-05, 'epoch': 57.64}
>>> 2025-08-17 21:22:28,964 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.6526781320571899, 'learning_rate': 2.4396855649681166e-05, 'epoch': 57.8}
>>> 2025-08-17 21:22:32,864 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.1311187595129013, 'learning_rate': 2.417196135391591e-05, 'epoch': 57.96}
>>> 2025-08-17 21:22:33,858 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.3037683665752411, 'learning_rate': 2.3947777513081292e-05, 'epoch': 58.0}
>>> 2025-08-17 21:22:37,609 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.5531523823738098, 'learning_rate': 2.372431029382888e-05, 'epoch': 58.16}
>>> 2025-08-17 21:22:42,609 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.18134938180446625, 'learning_rate': 2.350156584309804e-05, 'epoch': 58.32}
>>> 2025-08-17 21:22:45,430 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.44128361344337463, 'learning_rate': 2.327955028794688e-05, 'epoch': 58.48}
>>> 2025-08-17 21:22:49,266 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.40385574102401733, 'learning_rate': 2.305826973538366e-05, 'epoch': 58.64}
>>> 2025-08-17 21:22:53,056 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.19510194659233093, 'learning_rate': 2.2837730272198888e-05, 'epoch': 58.8}
>>> 2025-08-17 21:22:58,882 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.19031380116939545, 'learning_rate': 2.2617937964797785e-05, 'epoch': 58.96}
>>> 2025-08-17 21:22:59,884 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.3273622393608093, 'learning_rate': 2.2398898859033494e-05, 'epoch': 59.0}
>>> 2025-08-17 21:23:04,703 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.2453145533800125, 'learning_rate': 2.2180618980040747e-05, 'epoch': 59.16}
>>> 2025-08-17 21:23:09,387 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.25175201892852783, 'learning_rate': 2.1963104332070127e-05, 'epoch': 59.32}
>>> 2025-08-17 21:23:13,100 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.20956936478614807, 'learning_rate': 2.1746360898322933e-05, 'epoch': 59.48}
>>> 2025-08-17 21:23:18,502 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.3251297175884247, 'learning_rate': 2.1530394640786567e-05, 'epoch': 59.64}
>>> 2025-08-17 21:23:23,436 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.31112316250801086, 'learning_rate': 2.1315211500070558e-05, 'epoch': 59.8}
>>> 2025-08-17 21:23:26,691 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.1318349540233612, 'learning_rate': 2.1100817395243157e-05, 'epoch': 59.96}
>>> 2025-08-17 21:23:27,822 - INFO - >>> {'loss': 0.0083, 'grad_norm': 0.8441581726074219, 'learning_rate': 2.088721822366849e-05, 'epoch': 60.0}
>>> 2025-08-17 21:23:30,952 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.293051153421402, 'learning_rate': 2.0674419860844384e-05, 'epoch': 60.16}
>>> 2025-08-17 21:23:33,592 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.45195290446281433, 'learning_rate': 2.046242816024071e-05, 'epoch': 60.32}
>>> 2025-08-17 21:23:37,797 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.5083240270614624, 'learning_rate': 2.0251248953138374e-05, 'epoch': 60.48}
>>> 2025-08-17 21:23:42,642 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.34483417868614197, 'learning_rate': 2.0040888048468954e-05, 'epoch': 60.64}
>>> 2025-08-17 21:23:47,957 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.3384074866771698, 'learning_rate': 1.9831351232654872e-05, 'epoch': 60.8}
>>> 2025-08-17 21:23:53,908 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.15531040728092194, 'learning_rate': 1.962264426945023e-05, 'epoch': 60.96}
>>> 2025-08-17 21:23:54,730 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.42457348108291626, 'learning_rate': 1.9414772899782276e-05, 'epoch': 61.0}
>>> 2025-08-17 21:23:59,031 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.2229946106672287, 'learning_rate': 1.920774284159353e-05, 'epoch': 61.16}
>>> 2025-08-17 21:24:02,928 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.4587106704711914, 'learning_rate': 1.9001559789684404e-05, 'epoch': 61.32}
>>> 2025-08-17 21:24:08,621 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.4049334228038788, 'learning_rate': 1.8796229415556628e-05, 'epoch': 61.48}
>>> 2025-08-17 21:24:12,849 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.2533396780490875, 'learning_rate': 1.859175736725724e-05, 'epoch': 61.64}
>>> 2025-08-17 21:24:18,270 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.39353951811790466, 'learning_rate': 1.8388149269223153e-05, 'epoch': 61.8}
>>> 2025-08-17 21:24:22,182 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.28583869338035583, 'learning_rate': 1.8185410722126556e-05, 'epoch': 61.96}
>>> 2025-08-17 21:24:23,872 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.24075384438037872, 'learning_rate': 1.798354730272077e-05, 'epoch': 62.0}
>>> 2025-08-17 21:24:26,412 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.3037663996219635, 'learning_rate': 1.7782564563686884e-05, 'epoch': 62.16}
>>> 2025-08-17 21:24:30,331 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3496171236038208, 'learning_rate': 1.7582468033480992e-05, 'epoch': 62.32}
>>> 2025-08-17 21:24:33,724 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.26031333208084106, 'learning_rate': 1.7383263216182157e-05, 'epoch': 62.48}
>>> 2025-08-17 21:24:38,817 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.22582334280014038, 'learning_rate': 1.7184955591340974e-05, 'epoch': 62.64}
>>> 2025-08-17 21:24:43,025 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.4832027852535248, 'learning_rate': 1.6987550613828862e-05, 'epoch': 62.8}
>>> 2025-08-17 21:24:48,001 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.3759043514728546, 'learning_rate': 1.679105371368802e-05, 'epoch': 62.96}
>>> 2025-08-17 21:24:49,595 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3774772584438324, 'learning_rate': 1.6595470295982045e-05, 'epoch': 63.0}
>>> 2025-08-17 21:24:52,241 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.44474563002586365, 'learning_rate': 1.6400805740647267e-05, 'epoch': 63.16}
>>> 2025-08-17 21:24:56,366 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.2855818569660187, 'learning_rate': 1.6207065402344747e-05, 'epoch': 63.32}
>>> 2025-08-17 21:25:02,010 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.23499058187007904, 'learning_rate': 1.6014254610313033e-05, 'epoch': 63.48}
>>> 2025-08-17 21:25:06,461 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.20770736038684845, 'learning_rate': 1.582237866822151e-05, 'epoch': 63.64}
>>> 2025-08-17 21:25:11,354 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.2855657935142517, 'learning_rate': 1.563144285402453e-05, 'epoch': 63.8}
>>> 2025-08-17 21:25:15,632 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.17562948167324066, 'learning_rate': 1.5441452419816237e-05, 'epoch': 63.96}
>>> 2025-08-17 21:25:16,281 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.1962798535823822, 'learning_rate': 1.5252412591686105e-05, 'epoch': 64.0}
>>> 2025-08-17 21:25:21,077 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.22240686416625977, 'learning_rate': 1.5064328569575165e-05, 'epoch': 64.16}
>>> 2025-08-17 21:25:25,151 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.37063249945640564, 'learning_rate': 1.4877205527132982e-05, 'epoch': 64.32}
>>> 2025-08-17 21:25:30,036 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.20925256609916687, 'learning_rate': 1.4691048611575337e-05, 'epoch': 64.48}
>>> 2025-08-17 21:25:33,547 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.30694806575775146, 'learning_rate': 1.4505862943542642e-05, 'epoch': 64.64}
>>> 2025-08-17 21:25:37,138 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.25528284907341003, 'learning_rate': 1.4321653616959097e-05, 'epoch': 64.8}
>>> 2025-08-17 21:25:42,270 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.26719239354133606, 'learning_rate': 1.4138425698892555e-05, 'epoch': 64.96}
>>> 2025-08-17 21:25:43,230 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.4302758276462555, 'learning_rate': 1.3956184229415148e-05, 'epoch': 65.0}
>>> 2025-08-17 21:25:46,604 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.1628751903772354, 'learning_rate': 1.3774934221464642e-05, 'epoch': 65.16}
>>> 2025-08-17 21:25:50,927 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.17398759722709656, 'learning_rate': 1.359468066070657e-05, 'epoch': 65.32}
>>> 2025-08-17 21:25:54,359 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.4978746473789215, 'learning_rate': 1.341542850539706e-05, 'epoch': 65.48}
>>> 2025-08-17 21:25:58,935 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.3128000795841217, 'learning_rate': 1.3237182686246468e-05, 'epoch': 65.64}
>>> 2025-08-17 21:26:02,199 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3444420099258423, 'learning_rate': 1.3059948106283725e-05, 'epoch': 65.8}
>>> 2025-08-17 21:26:08,531 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.18565772473812103, 'learning_rate': 1.2883729640721531e-05, 'epoch': 65.96}
>>> 2025-08-17 21:26:09,397 - INFO - >>> {'loss': 0.0061, 'grad_norm': 0.6479136943817139, 'learning_rate': 1.2708532136822155e-05, 'epoch': 66.0}
>>> 2025-08-17 21:26:13,060 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.20705607533454895, 'learning_rate': 1.2534360413764169e-05, 'epoch': 66.16}
>>> 2025-08-17 21:26:16,997 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.32825711369514465, 'learning_rate': 1.2361219262509883e-05, 'epoch': 66.32}
>>> 2025-08-17 21:26:21,474 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.5383176207542419, 'learning_rate': 1.2189113445673528e-05, 'epoch': 66.48}
>>> 2025-08-17 21:26:25,915 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.19427260756492615, 'learning_rate': 1.2018047697390279e-05, 'epoch': 66.64}
>>> 2025-08-17 21:26:31,020 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.4876630902290344, 'learning_rate': 1.1848026723186012e-05, 'epoch': 66.8}
>>> 2025-08-17 21:26:35,606 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.1216127797961235, 'learning_rate': 1.1679055199847893e-05, 'epoch': 66.96}
>>> 2025-08-17 21:26:36,908 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.4154532849788666, 'learning_rate': 1.1511137775295704e-05, 'epoch': 67.0}
>>> 2025-08-17 21:26:42,273 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.289681077003479, 'learning_rate': 1.1344279068454011e-05, 'epoch': 67.16}
>>> 2025-08-17 21:26:46,069 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.2614549696445465, 'learning_rate': 1.1178483669125112e-05, 'epoch': 67.32}
>>> 2025-08-17 21:26:49,808 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3874301016330719, 'learning_rate': 1.101375613786278e-05, 'epoch': 67.48}
>>> 2025-08-17 21:26:54,086 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.3392510414123535, 'learning_rate': 1.0850101005846786e-05, 'epoch': 67.64}
>>> 2025-08-17 21:26:57,358 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.37197086215019226, 'learning_rate': 1.0687522774758319e-05, 'epoch': 67.8}
>>> 2025-08-17 21:27:02,212 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.5446373224258423, 'learning_rate': 1.0526025916656119e-05, 'epoch': 67.96}
>>> 2025-08-17 21:27:03,325 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.4176971912384033, 'learning_rate': 1.0365614873853462e-05, 'epoch': 68.0}
>>> 2025-08-17 21:27:07,728 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.2263735830783844, 'learning_rate': 1.0206294058795973e-05, 'epoch': 68.16}
>>> 2025-08-17 21:27:11,927 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.45692864060401917, 'learning_rate': 1.0048067853940285e-05, 'epoch': 68.32}
>>> 2025-08-17 21:27:15,326 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.4368167519569397, 'learning_rate': 9.890940611633414e-06, 'epoch': 68.48}
>>> 2025-08-17 21:27:20,835 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.3362939953804016, 'learning_rate': 9.734916653993103e-06, 'epoch': 68.64}
>>> 2025-08-17 21:27:25,287 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.45249682664871216, 'learning_rate': 9.580000272788914e-06, 'epoch': 68.8}
>>> 2025-08-17 21:27:29,735 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.15380768477916718, 'learning_rate': 9.426195729324161e-06, 'epoch': 68.96}
>>> 2025-08-17 21:27:30,082 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.27350725431872e-06, 'epoch': 69.0}
>>> 2025-08-17 21:27:33,967 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.2568818926811218, 'learning_rate': 9.121939047792621e-06, 'epoch': 69.16}
>>> 2025-08-17 21:27:37,852 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.2821698784828186, 'learning_rate': 8.971495278950559e-06, 'epoch': 69.32}
>>> 2025-08-17 21:27:42,680 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.474266916513443, 'learning_rate': 8.82218008606716e-06, 'epoch': 69.48}
>>> 2025-08-17 21:27:47,781 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.4392281472682953, 'learning_rate': 8.673997576373205e-06, 'epoch': 69.64}
>>> 2025-08-17 21:27:52,217 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.17678482830524445, 'learning_rate': 8.526951825942609e-06, 'epoch': 69.8}
>>> 2025-08-17 21:27:56,064 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.3558635711669922, 'learning_rate': 8.381046879580306e-06, 'epoch': 69.96}
>>> 2025-08-17 21:27:57,257 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.5567993521690369, 'learning_rate': 8.23628675071102e-06, 'epoch': 70.0}
>>> 2025-08-17 21:28:02,574 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3666876554489136, 'learning_rate': 8.092675421268826e-06, 'epoch': 70.16}
>>> 2025-08-17 21:28:06,866 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.32670941948890686, 'learning_rate': 7.950216841587638e-06, 'epoch': 70.32}
>>> 2025-08-17 21:28:11,931 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.1998499184846878, 'learning_rate': 7.808914930292543e-06, 'epoch': 70.48}
>>> 2025-08-17 21:28:15,281 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.3062269985675812, 'learning_rate': 7.66877357419204e-06, 'epoch': 70.64}
>>> 2025-08-17 21:28:20,380 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.21196414530277252, 'learning_rate': 7.5297966281710705e-06, 'epoch': 70.8}
>>> 2025-08-17 21:28:23,099 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.5279131531715393, 'learning_rate': 7.391987915085013e-06, 'epoch': 70.96}
>>> 2025-08-17 21:28:24,539 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.4651696979999542, 'learning_rate': 7.255351225654527e-06, 'epoch': 71.0}
>>> 2025-08-17 21:28:29,652 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.1445193737745285, 'learning_rate': 7.119890318361277e-06, 'epoch': 71.16}
>>> 2025-08-17 21:28:32,649 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.22271758317947388, 'learning_rate': 6.98560891934455e-06, 'epoch': 71.32}
>>> 2025-08-17 21:28:35,091 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.23222216963768005, 'learning_rate': 6.852510722298761e-06, 'epoch': 71.48}
>>> 2025-08-17 21:28:40,152 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.27428126335144043, 'learning_rate': 6.72059938837184e-06, 'epoch': 71.64}
>>> 2025-08-17 21:28:45,530 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.18503928184509277, 'learning_rate': 6.589878546064543e-06, 'epoch': 71.8}
>>> 2025-08-17 21:28:49,538 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.27568280696868896, 'learning_rate': 6.46035179113062e-06, 'epoch': 71.96}
>>> 2025-08-17 21:28:50,840 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.31770554184913635, 'learning_rate': 6.332022686477928e-06, 'epoch': 72.0}
>>> 2025-08-17 21:28:52,502 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.3629257082939148, 'learning_rate': 6.204894762070407e-06, 'epoch': 72.16}
>>> 2025-08-17 21:28:57,095 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3607461452484131, 'learning_rate': 6.078971514830989e-06, 'epoch': 72.32}
>>> 2025-08-17 21:29:01,580 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.29655921459198, 'learning_rate': 5.9542564085454165e-06, 'epoch': 72.48}
>>> 2025-08-17 21:29:05,379 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.17799383401870728, 'learning_rate': 5.830752873766948e-06, 'epoch': 72.64}
>>> 2025-08-17 21:29:09,943 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.32534259557724, 'learning_rate': 5.708464307722006e-06, 'epoch': 72.8}
>>> 2025-08-17 21:29:15,310 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.5719350576400757, 'learning_rate': 5.587394074216712e-06, 'epoch': 72.96}
>>> 2025-08-17 21:29:16,089 - INFO - >>> {'loss': 0.0117, 'grad_norm': 1.1908494234085083, 'learning_rate': 5.46754550354438e-06, 'epoch': 73.0}
>>> 2025-08-17 21:29:20,470 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.8118867874145508, 'learning_rate': 5.348921892393904e-06, 'epoch': 73.16}
>>> 2025-08-17 21:29:23,980 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.2829921245574951, 'learning_rate': 5.231526503759054e-06, 'epoch': 73.32}
>>> 2025-08-17 21:29:27,990 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3650405704975128, 'learning_rate': 5.115362566848747e-06, 'epoch': 73.48}
>>> 2025-08-17 21:29:32,368 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.2032015025615692, 'learning_rate': 5.000433276998218e-06, 'epoch': 73.64}
>>> 2025-08-17 21:29:37,137 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.38620221614837646, 'learning_rate': 4.886741795581101e-06, 'epoch': 73.8}
>>> 2025-08-17 21:29:41,992 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.16699668765068054, 'learning_rate': 4.774291249922508e-06, 'epoch': 73.96}
>>> 2025-08-17 21:29:42,753 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.25527316331863403, 'learning_rate': 4.6630847332129575e-06, 'epoch': 74.0}
>>> 2025-08-17 21:29:47,036 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.1838293820619583, 'learning_rate': 4.553125304423339e-06, 'epoch': 74.16}
>>> 2025-08-17 21:29:50,650 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.42751434445381165, 'learning_rate': 4.44441598822074e-06, 'epoch': 74.32}
>>> 2025-08-17 21:29:55,900 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.19698336720466614, 'learning_rate': 4.336959774885241e-06, 'epoch': 74.48}
>>> 2025-08-17 21:29:59,834 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.2897232472896576, 'learning_rate': 4.2307596202276815e-06, 'epoch': 74.64}
>>> 2025-08-17 21:30:03,652 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.17920325696468353, 'learning_rate': 4.1258184455083505e-06, 'epoch': 74.8}
>>> 2025-08-17 21:30:07,566 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.33049699664115906, 'learning_rate': 4.022139137356623e-06, 'epoch': 74.96}
>>> 2025-08-17 21:30:09,004 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.4891146421432495, 'learning_rate': 3.919724547691556e-06, 'epoch': 75.0}
>>> 2025-08-17 21:30:14,348 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.31777507066726685, 'learning_rate': 3.818577493643444e-06, 'epoch': 75.16}
>>> 2025-08-17 21:30:17,874 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.3399280309677124, 'learning_rate': 3.7187007574763232e-06, 'epoch': 75.32}
>>> 2025-08-17 21:30:22,046 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.21371425688266754, 'learning_rate': 3.6200970865114704e-06, 'epoch': 75.48}
>>> 2025-08-17 21:30:25,231 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.27640190720558167, 'learning_rate': 3.522769193051789e-06, 'epoch': 75.64}
>>> 2025-08-17 21:30:30,661 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.14849308133125305, 'learning_rate': 3.426719754307206e-06, 'epoch': 75.8}
>>> 2025-08-17 21:30:33,302 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.2245253473520279, 'learning_rate': 3.331951412321066e-06, 'epoch': 75.96}
>>> 2025-08-17 21:30:35,478 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.5508373379707336, 'learning_rate': 3.2384667738974196e-06, 'epoch': 76.0}
>>> 2025-08-17 21:30:39,687 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.13869619369506836, 'learning_rate': 3.1462684105293293e-06, 'epoch': 76.16}
>>> 2025-08-17 21:30:43,983 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.22093364596366882, 'learning_rate': 3.0553588583281444e-06, 'epoch': 76.32}
>>> 2025-08-17 21:30:48,989 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.3223094344139099, 'learning_rate': 2.965740617953733e-06, 'epoch': 76.48}
>>> 2025-08-17 21:30:53,188 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.3380628824234009, 'learning_rate': 2.877416154545681e-06, 'epoch': 76.64}
>>> 2025-08-17 21:30:56,414 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.141328826546669, 'learning_rate': 2.7903878976555163e-06, 'epoch': 76.8}
>>> 2025-08-17 21:31:02,460 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.27932268381118774, 'learning_rate': 2.7046582411798473e-06, 'epoch': 76.96}
>>> 2025-08-17 21:31:03,481 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.43361201882362366, 'learning_rate': 2.620229543294528e-06, 'epoch': 77.0}
>>> 2025-08-17 21:31:08,114 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.16526344418525696, 'learning_rate': 2.537104126389794e-06, 'epoch': 77.16}
>>> 2025-08-17 21:31:12,183 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.2725640535354614, 'learning_rate': 2.4552842770063757e-06, 'epoch': 77.32}
>>> 2025-08-17 21:31:15,219 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.20869745314121246, 'learning_rate': 2.3747722457725996e-06, 'epoch': 77.48}
>>> 2025-08-17 21:31:19,898 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.23360683023929596, 'learning_rate': 2.2955702473424824e-06, 'epoch': 77.64}
>>> 2025-08-17 21:31:24,692 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.14107178151607513, 'learning_rate': 2.217680460334809e-06, 'epoch': 77.8}
>>> 2025-08-17 21:31:29,035 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.3144178092479706, 'learning_rate': 2.141105027273227e-06, 'epoch': 77.96}
>>> 2025-08-17 21:31:30,882 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.5408238768577576, 'learning_rate': 2.065846054527265e-06, 'epoch': 78.0}
>>> 2025-08-17 21:31:35,283 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.2129574567079544, 'learning_rate': 1.9919056122544465e-06, 'epoch': 78.16}
>>> 2025-08-17 21:31:41,145 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.17208635807037354, 'learning_rate': 1.919285734343307e-06, 'epoch': 78.32}
>>> 2025-08-17 21:31:45,182 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.1915288120508194, 'learning_rate': 1.8479884183574657e-06, 'epoch': 78.48}
>>> 2025-08-17 21:31:49,569 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.34394165873527527, 'learning_rate': 1.7780156254806779e-06, 'epoch': 78.64}
>>> 2025-08-17 21:31:53,059 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.5112475156784058, 'learning_rate': 1.7093692804628635e-06, 'epoch': 78.8}
>>> 2025-08-17 21:31:58,094 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.19873303174972534, 'learning_rate': 1.6420512715672131e-06, 'epoch': 78.96}
>>> 2025-08-17 21:31:59,812 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.4772596061229706, 'learning_rate': 1.5760634505182004e-06, 'epoch': 79.0}
>>> 2025-08-17 21:32:04,374 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.2087588757276535, 'learning_rate': 1.5114076324506565e-06, 'epoch': 79.16}
>>> 2025-08-17 21:32:08,980 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.24680855870246887, 'learning_rate': 1.4480855958598715e-06, 'epoch': 79.32}
>>> 2025-08-17 21:32:13,331 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.19236089289188385, 'learning_rate': 1.3860990825526333e-06, 'epoch': 79.48}
>>> 2025-08-17 21:32:18,097 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.22367054224014282, 'learning_rate': 1.3254497975993264e-06, 'epoch': 79.64}
>>> 2025-08-17 21:32:21,956 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.30871278047561646, 'learning_rate': 1.2661394092870537e-06, 'epoch': 79.8}
>>> 2025-08-17 21:32:24,823 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.19001677632331848, 'learning_rate': 1.2081695490737178e-06, 'epoch': 79.96}
>>> 2025-08-17 21:32:26,494 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.3286115229129791, 'learning_rate': 1.1515418115431553e-06, 'epoch': 80.0}
>>> 2025-08-17 21:32:30,402 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.44364967942237854, 'learning_rate': 1.0962577543612795e-06, 'epoch': 80.16}
>>> 2025-08-17 21:32:35,913 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.23697702586650848, 'learning_rate': 1.04231889823323e-06, 'epoch': 80.32}
>>> 2025-08-17 21:32:41,173 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.15033327043056488, 'learning_rate': 9.897267268615284e-07, 'epoch': 80.48}
>>> 2025-08-17 21:32:45,392 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.3253293037414551, 'learning_rate': 9.384826869052898e-07, 'epoch': 80.64}
>>> 2025-08-17 21:32:49,077 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.23589035868644714, 'learning_rate': 8.885881879404201e-07, 'epoch': 80.8}
>>> 2025-08-17 21:32:52,384 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.2723000645637512, 'learning_rate': 8.400446024208309e-07, 'epoch': 80.96}
>>> 2025-08-17 21:32:53,910 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.4642127752304077, 'learning_rate': 7.928532656407029e-07, 'epoch': 81.0}
>>> 2025-08-17 21:32:58,272 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.2997261583805084, 'learning_rate': 7.470154756977543e-07, 'epoch': 81.16}
>>> 2025-08-17 21:33:03,288 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.2690511643886566, 'learning_rate': 7.025324934575139e-07, 'epoch': 81.32}
>>> 2025-08-17 21:33:08,107 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.2252957969903946, 'learning_rate': 6.594055425186763e-07, 'epoch': 81.48}
>>> 2025-08-17 21:33:11,801 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.21228760480880737, 'learning_rate': 6.176358091794011e-07, 'epoch': 81.64}
>>> 2025-08-17 21:33:17,221 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.20543695986270905, 'learning_rate': 5.772244424047169e-07, 'epoch': 81.8}
>>> 2025-08-17 21:33:21,297 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.25530025362968445, 'learning_rate': 5.381725537948856e-07, 'epoch': 81.96}
>>> 2025-08-17 21:33:22,078 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.20959070324897766, 'learning_rate': 5.004812175548656e-07, 'epoch': 82.0}
>>> 2025-08-17 21:33:25,629 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.3095237910747528, 'learning_rate': 4.641514704647132e-07, 'epoch': 82.16}
>>> 2025-08-17 21:33:30,090 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.18079935014247894, 'learning_rate': 4.2918431185110517e-07, 'epoch': 82.32}
>>> 2025-08-17 21:33:35,005 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.37668752670288086, 'learning_rate': 3.9558070355983357e-07, 'epoch': 82.48}
>>> 2025-08-17 21:33:39,815 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.2501738667488098, 'learning_rate': 3.6334156992935406e-07, 'epoch': 82.64}
>>> 2025-08-17 21:33:44,505 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.26342466473579407, 'learning_rate': 3.324677977653401e-07, 'epoch': 82.8}
>>> 2025-08-17 21:33:49,356 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.15438595414161682, 'learning_rate': 3.0296023631631865e-07, 'epoch': 82.96}
>>> 2025-08-17 21:33:50,387 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.20328445732593536, 'learning_rate': 2.748196972502892e-07, 'epoch': 83.0}
>>> 2025-08-17 21:33:55,667 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.2857784926891327, 'learning_rate': 2.4804695463240826e-07, 'epoch': 83.16}
>>> 2025-08-17 21:33:59,462 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.22093813121318817, 'learning_rate': 2.226427449036894e-07, 'epoch': 83.32}
>>> 2025-08-17 21:34:03,530 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.2080431878566742, 'learning_rate': 1.9860776686075332e-07, 'epoch': 83.48}
>>> 2025-08-17 21:34:07,074 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.2380305826663971, 'learning_rate': 1.7594268163659278e-07, 'epoch': 83.64}
>>> 2025-08-17 21:34:13,014 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.1533767282962799, 'learning_rate': 1.546481126824151e-07, 'epoch': 83.8}
>>> 2025-08-17 21:34:18,459 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.34528738260269165, 'learning_rate': 1.347246457504503e-07, 'epoch': 83.96}
>>> 2025-08-17 21:34:18,812 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.1617282887787518e-07, 'epoch': 84.0}
>>> 2025-08-17 21:34:24,195 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.28077933192253113, 'learning_rate': 9.899317237172523e-08, 'epoch': 84.16}
>>> 2025-08-17 21:34:27,948 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.25527438521385193, 'learning_rate': 8.318614879485043e-08, 'epoch': 84.32}
>>> 2025-08-17 21:34:31,359 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.23886224627494812, 'learning_rate': 6.875219295293111e-08, 'epoch': 84.48}
>>> 2025-08-17 21:34:35,053 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.30305609107017517, 'learning_rate': 5.569170188250983e-08, 'epoch': 84.64}
>>> 2025-08-17 21:34:40,641 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.2132035195827484, 'learning_rate': 4.400503484006113e-08, 'epoch': 84.8}
>>> 2025-08-17 21:34:44,787 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.2183547019958496, 'learning_rate': 3.369251329213285e-08, 'epoch': 84.96}
>>> 2025-08-17 21:34:45,673 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.26122137904167175, 'learning_rate': 2.4754420906475396e-08, 'epoch': 85.0}
>>> 2025-08-17 21:34:50,032 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.1381649225950241, 'learning_rate': 1.7191003544259064e-08, 'epoch': 85.16}
>>> 2025-08-17 21:34:54,815 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.12734577059745789, 'learning_rate': 1.100246925331283e-08, 'epoch': 85.32}
>>> 2025-08-17 21:34:58,922 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.3476632833480835, 'learning_rate': 6.188988262373352e-09, 'epoch': 85.48}
>>> 2025-08-17 21:35:02,501 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.29520854353904724, 'learning_rate': 2.750692976444258e-09, 'epoch': 85.64}
>>> 2025-08-17 21:35:08,387 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.22291423380374908, 'learning_rate': 6.876779731213035e-10, 'epoch': 85.8}
>>> 2025-08-17 21:35:09,252 - INFO - >>> {'train_runtime': 2380.7725, 'train_samples_per_second': 4.2, 'train_steps_per_second': 0.252, 'train_loss': 0.15632231111017367, 'epoch': 85.8}
>>> 2025-08-17 21:35:09,254 - INFO - 训练成功！
>>> 2025-08-17 21:35:11,300 - INFO - 模型存放位置：./output/qwen202508172055
>>> 2025-08-18 22:00:17,675 - INFO - 正在读取配置文件: config.yaml
>>> 2025-08-18 22:00:20,132 - INFO - 从配置文件中提取到模型路径: /home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-18 22:00:23,983 - INFO - 开始加载模型配置文件...
>>> 2025-08-18 22:00:26,450 - INFO - 正在加载模型: /home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-18 22:00:43,442 - INFO - 模型加载完成
>>> 2025-08-18 22:00:45,791 - INFO - ============================================================
>>> 2025-08-18 22:00:49,763 - INFO - 模型详细信息
>>> 2025-08-18 22:00:52,111 - INFO - ============================================================
>>> 2025-08-18 22:00:56,084 - INFO - 模型路径: /home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-18 22:01:00,026 - INFO - 模型类型: qwen2
>>> 2025-08-18 22:01:02,524 - INFO - 模型架构: ['Qwen2ForCausalLM']
>>> 2025-08-18 22:01:05,473 - INFO - ----------------------------------------
>>> 2025-08-18 22:01:08,843 - INFO - 参数信息:
>>> 2025-08-18 22:01:11,161 - INFO -   总参数量: 7.62B (7,615,616,512)
>>> 2025-08-18 22:01:14,200 - INFO -   可训练参数: 7.62B (7,615,616,512)
>>> 2025-08-18 22:01:17,270 - INFO -   冻结参数: 0 (0)
>>> 2025-08-18 22:01:19,828 - INFO -   可训练参数比例: 100.00%
>>> 2025-08-18 22:01:22,536 - INFO - ----------------------------------------
>>> 2025-08-18 22:01:25,907 - INFO - 模型结构信息:
>>> 2025-08-18 22:01:28,285 - INFO -   层数: 28
>>> 2025-08-18 22:01:30,693 - INFO -   隐藏层大小: 3584
>>> 2025-08-18 22:01:33,252 - INFO -   注意力头数: 28
>>> 2025-08-18 22:01:35,750 - INFO -   总层数量: 339
>>> 2025-08-18 22:01:38,248 - INFO -   可训练层数量: 339
>>> 2025-08-18 22:01:40,807 - INFO - ----------------------------------------
>>> 2025-08-18 22:01:44,177 - INFO - 特殊Token:
>>> 2025-08-18 22:01:46,585 - INFO -   bos_token: <｜begin▁of▁sentence｜>
>>> 2025-08-18 22:01:49,774 - INFO -   eos_token: <｜end▁of▁sentence｜>
>>> 2025-08-18 22:01:52,904 - INFO -   unk_token: None
>>> 2025-08-18 22:01:55,582 - INFO -   pad_token: <｜end▁of▁sentence｜>
>>> 2025-08-18 22:01:58,711 - INFO -   sep_token: None
>>> 2025-08-18 22:02:01,389 - INFO -   mask_token: None
>>> 2025-08-18 22:02:04,097 - INFO -   vocab_size: 151643
>>> 2025-08-18 22:02:06,866 - INFO - ----------------------------------------
>>> 2025-08-18 22:02:10,236 - INFO - 量化信息:
>>> 2025-08-18 22:02:12,553 - INFO -   数据类型: torch.float32
>>> 2025-08-18 22:02:15,351 - INFO -   量化方式: None
>>> 2025-08-18 22:02:17,879 - INFO - ----------------------------------------
>>> 2025-08-18 22:02:21,249 - INFO - 可训练层 (前10个):
>>> 2025-08-18 22:02:23,778 - INFO -   1. model.embed_tokens.weight
>>> 2025-08-18 22:02:26,847 - INFO -   2. model.layers.0.self_attn.q_proj.weight
>>> 2025-08-18 22:02:30,308 - INFO -   3. model.layers.0.self_attn.q_proj.bias
>>> 2025-08-18 22:02:33,708 - INFO -   4. model.layers.0.self_attn.k_proj.weight
>>> 2025-08-18 22:02:37,169 - INFO -   5. model.layers.0.self_attn.k_proj.bias
>>> 2025-08-18 22:02:40,569 - INFO -   6. model.layers.0.self_attn.v_proj.weight
>>> 2025-08-18 22:02:44,029 - INFO -   7. model.layers.0.self_attn.v_proj.bias
>>> 2025-08-18 22:02:47,438 - INFO -   8. model.layers.0.self_attn.o_proj.weight
>>> 2025-08-18 22:02:50,898 - INFO -   9. model.layers.0.mlp.gate_proj.weight
>>> 2025-08-18 22:02:54,268 - INFO -   10. model.layers.0.mlp.up_proj.weight
>>> 2025-08-18 22:02:57,610 - INFO -   ... 还有 329 个可训练层
>>> 2025-08-18 22:03:50,315 - INFO - 正在读取配置文件: config.yaml
>>> 2025-08-18 22:03:50,322 - INFO - 从配置文件中提取到模型路径: /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-18 22:03:50,322 - INFO - 开始加载模型配置文件...
>>> 2025-08-18 22:03:50,323 - INFO - 正在加载模型: /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-18 22:04:03,517 - INFO - 模型加载完成
>>> 2025-08-18 22:04:03,588 - INFO - ============================================================
>>> 2025-08-18 22:04:03,589 - INFO - 模型详细信息
>>> 2025-08-18 22:04:03,589 - INFO - ============================================================
>>> 2025-08-18 22:04:03,589 - INFO - 模型路径: /home/liangshuqiao/agent/Qwen2-7B
>>> 2025-08-18 22:04:03,590 - INFO - 模型类型: qwen2
>>> 2025-08-18 22:04:03,590 - INFO - 模型架构: ['Qwen2ForCausalLM']
>>> 2025-08-18 22:04:03,591 - INFO - ----------------------------------------
>>> 2025-08-18 22:04:03,591 - INFO - 参数信息:
>>> 2025-08-18 22:04:03,591 - INFO -   总参数量: 7.62B (7,615,616,512)
>>> 2025-08-18 22:04:03,592 - INFO -   可训练参数: 7.62B (7,615,616,512)
>>> 2025-08-18 22:04:03,592 - INFO -   冻结参数: 0 (0)
>>> 2025-08-18 22:04:03,593 - INFO -   可训练参数比例: 100.00%
>>> 2025-08-18 22:04:03,593 - INFO - ----------------------------------------
>>> 2025-08-18 22:04:03,593 - INFO - 模型结构信息:
>>> 2025-08-18 22:04:03,594 - INFO -   层数: 28
>>> 2025-08-18 22:04:03,594 - INFO -   隐藏层大小: 3584
>>> 2025-08-18 22:04:03,594 - INFO -   注意力头数: 28
>>> 2025-08-18 22:04:03,595 - INFO -   总层数量: 339
>>> 2025-08-18 22:04:03,595 - INFO -   可训练层数量: 339
>>> 2025-08-18 22:04:03,595 - INFO - ----------------------------------------
>>> 2025-08-18 22:04:03,596 - INFO - 特殊Token:
>>> 2025-08-18 22:04:03,596 - INFO -   bos_token: None
>>> 2025-08-18 22:04:03,596 - INFO -   eos_token: <|endoftext|>
>>> 2025-08-18 22:04:03,597 - INFO -   unk_token: None
>>> 2025-08-18 22:04:03,597 - INFO -   pad_token: <|endoftext|>
>>> 2025-08-18 22:04:03,598 - INFO -   sep_token: None
>>> 2025-08-18 22:04:03,598 - INFO -   mask_token: None
>>> 2025-08-18 22:04:03,598 - INFO -   vocab_size: 151643
>>> 2025-08-18 22:04:03,599 - INFO - ----------------------------------------
>>> 2025-08-18 22:04:03,599 - INFO - 量化信息:
>>> 2025-08-18 22:04:03,599 - INFO -   数据类型: torch.float32
>>> 2025-08-18 22:04:03,600 - INFO -   量化方式: None
>>> 2025-08-18 22:04:03,600 - INFO - ----------------------------------------
>>> 2025-08-18 22:04:03,601 - INFO - 可训练层 (前10个):
>>> 2025-08-18 22:04:03,601 - INFO -   1. model.embed_tokens.weight
>>> 2025-08-18 22:04:03,601 - INFO -   2. model.layers.0.self_attn.q_proj.weight
>>> 2025-08-18 22:04:03,602 - INFO -   3. model.layers.0.self_attn.q_proj.bias
>>> 2025-08-18 22:04:03,602 - INFO -   4. model.layers.0.self_attn.k_proj.weight
>>> 2025-08-18 22:04:03,603 - INFO -   5. model.layers.0.self_attn.k_proj.bias
>>> 2025-08-18 22:04:03,603 - INFO -   6. model.layers.0.self_attn.v_proj.weight
>>> 2025-08-18 22:04:03,604 - INFO -   7. model.layers.0.self_attn.v_proj.bias
>>> 2025-08-18 22:04:03,604 - INFO -   8. model.layers.0.self_attn.o_proj.weight
>>> 2025-08-18 22:04:03,605 - INFO -   9. model.layers.0.mlp.gate_proj.weight
>>> 2025-08-18 22:04:03,605 - INFO -   10. model.layers.0.mlp.up_proj.weight
>>> 2025-08-18 22:04:03,605 - INFO -   ... 还有 329 个可训练层
>>> 2025-08-18 22:06:47,941 - INFO - 正在读取配置文件: config.yaml
>>> 2025-08-18 22:06:47,949 - INFO - 从配置文件中提取到模型路径: /home/liangshuqiao/models/qwen3
>>> 2025-08-18 22:06:47,949 - INFO - 开始加载模型配置文件...
>>> 2025-08-18 22:06:47,949 - INFO - 正在加载模型: /home/liangshuqiao/models/qwen3
>>> 2025-08-18 22:14:06,087 - INFO - 模型加载完成
>>> 2025-08-18 22:14:06,200 - INFO - ============================================================
>>> 2025-08-18 22:14:06,200 - INFO - 模型详细信息
>>> 2025-08-18 22:14:06,201 - INFO - ============================================================
>>> 2025-08-18 22:14:06,201 - INFO - 模型路径: /home/liangshuqiao/models/qwen3
>>> 2025-08-18 22:14:06,202 - INFO - 模型类型: qwen3
>>> 2025-08-18 22:14:06,202 - INFO - 模型架构: ['Qwen3ForCausalLM']
>>> 2025-08-18 22:14:06,202 - INFO - ----------------------------------------
>>> 2025-08-18 22:14:06,203 - INFO - 参数信息:
>>> 2025-08-18 22:14:06,203 - INFO -   总参数量: 32.76B (32,762,123,264)
>>> 2025-08-18 22:14:06,204 - INFO -   可训练参数: 32.76B (32,762,123,264)
>>> 2025-08-18 22:14:06,204 - INFO -   冻结参数: 0 (0)
>>> 2025-08-18 22:14:06,204 - INFO -   可训练参数比例: 100.00%
>>> 2025-08-18 22:14:06,205 - INFO - ----------------------------------------
>>> 2025-08-18 22:14:06,205 - INFO - 模型结构信息:
>>> 2025-08-18 22:14:06,205 - INFO -   层数: 64
>>> 2025-08-18 22:14:06,206 - INFO -   隐藏层大小: 5120
>>> 2025-08-18 22:14:06,206 - INFO -   注意力头数: 64
>>> 2025-08-18 22:14:06,207 - INFO -   总层数量: 707
>>> 2025-08-18 22:14:06,207 - INFO -   可训练层数量: 707
>>> 2025-08-18 22:14:06,207 - INFO - ----------------------------------------
>>> 2025-08-18 22:14:06,208 - INFO - 特殊Token:
>>> 2025-08-18 22:14:06,208 - INFO -   bos_token: None
>>> 2025-08-18 22:14:06,209 - INFO -   eos_token: <|im_end|>
>>> 2025-08-18 22:14:06,209 - INFO -   unk_token: None
>>> 2025-08-18 22:14:06,209 - INFO -   pad_token: <|endoftext|>
>>> 2025-08-18 22:14:06,210 - INFO -   sep_token: None
>>> 2025-08-18 22:14:06,210 - INFO -   mask_token: None
>>> 2025-08-18 22:14:06,211 - INFO -   vocab_size: 151643
>>> 2025-08-18 22:14:06,211 - INFO - ----------------------------------------
>>> 2025-08-18 22:14:06,211 - INFO - 量化信息:
>>> 2025-08-18 22:14:06,212 - INFO -   数据类型: torch.float32
>>> 2025-08-18 22:14:06,212 - INFO -   量化方式: None
>>> 2025-08-18 22:14:06,212 - INFO - ----------------------------------------
>>> 2025-08-18 22:14:06,213 - INFO - 可训练层 (前10个):
>>> 2025-08-18 22:14:06,213 - INFO -   1. model.embed_tokens.weight
>>> 2025-08-18 22:14:06,214 - INFO -   2. model.layers.0.self_attn.q_proj.weight
>>> 2025-08-18 22:14:06,214 - INFO -   3. model.layers.0.self_attn.k_proj.weight
>>> 2025-08-18 22:14:06,215 - INFO -   4. model.layers.0.self_attn.v_proj.weight
>>> 2025-08-18 22:14:06,215 - INFO -   5. model.layers.0.self_attn.o_proj.weight
>>> 2025-08-18 22:14:06,215 - INFO -   6. model.layers.0.self_attn.q_norm.weight
>>> 2025-08-18 22:14:06,216 - INFO -   7. model.layers.0.self_attn.k_norm.weight
>>> 2025-08-18 22:14:06,216 - INFO -   8. model.layers.0.mlp.gate_proj.weight
>>> 2025-08-18 22:14:06,217 - INFO -   9. model.layers.0.mlp.up_proj.weight
>>> 2025-08-18 22:14:06,217 - INFO -   10. model.layers.0.mlp.down_proj.weight
>>> 2025-08-18 22:14:06,218 - INFO -   ... 还有 697 个可训练层
>>> 2025-08-18 22:22:37,273 - INFO - 正在读取配置文件: config.yaml
>>> 2025-08-18 22:22:37,306 - INFO - 从配置文件中提取到模型路径: /home/liangshuqiao/models/qwen3-8b
>>> 2025-08-18 22:22:37,307 - INFO - 开始加载模型配置文件...
>>> 2025-08-18 22:22:37,307 - INFO - 正在加载模型: /home/liangshuqiao/models/qwen3-8b
>>> 2025-08-18 22:22:51,741 - INFO - 模型加载完成
>>> 2025-08-18 22:22:51,803 - INFO - ============================================================
>>> 2025-08-18 22:22:51,804 - INFO - 模型详细信息
>>> 2025-08-18 22:22:51,804 - INFO - ============================================================
>>> 2025-08-18 22:22:51,805 - INFO - 模型路径: /home/liangshuqiao/models/qwen3-8b
>>> 2025-08-18 22:22:51,805 - INFO - 模型类型: qwen3
>>> 2025-08-18 22:22:51,805 - INFO - 模型架构: ['Qwen3ForCausalLM']
>>> 2025-08-18 22:22:51,806 - INFO - ----------------------------------------
>>> 2025-08-18 22:22:51,806 - INFO - 参数信息:
>>> 2025-08-18 22:22:51,807 - INFO -   总参数量: 8.19B (8,190,735,360)
>>> 2025-08-18 22:22:51,807 - INFO -   可训练参数: 8.19B (8,190,735,360)
>>> 2025-08-18 22:22:51,807 - INFO -   冻结参数: 0 (0)
>>> 2025-08-18 22:22:51,808 - INFO -   可训练参数比例: 100.00%
>>> 2025-08-18 22:22:51,808 - INFO - ----------------------------------------
>>> 2025-08-18 22:22:51,808 - INFO - 模型结构信息:
>>> 2025-08-18 22:22:51,809 - INFO -   层数: 36
>>> 2025-08-18 22:22:51,809 - INFO -   隐藏层大小: 4096
>>> 2025-08-18 22:22:51,809 - INFO -   注意力头数: 32
>>> 2025-08-18 22:22:51,810 - INFO -   总层数量: 399
>>> 2025-08-18 22:22:51,810 - INFO -   可训练层数量: 399
>>> 2025-08-18 22:22:51,810 - INFO - ----------------------------------------
>>> 2025-08-18 22:22:51,811 - INFO - 特殊Token:
>>> 2025-08-18 22:22:51,811 - INFO -   bos_token: None
>>> 2025-08-18 22:22:51,811 - INFO -   eos_token: <|im_end|>
>>> 2025-08-18 22:22:51,812 - INFO -   unk_token: None
>>> 2025-08-18 22:22:51,812 - INFO -   pad_token: <|endoftext|>
>>> 2025-08-18 22:22:51,812 - INFO -   sep_token: None
>>> 2025-08-18 22:22:51,813 - INFO -   mask_token: None
>>> 2025-08-18 22:22:51,813 - INFO -   vocab_size: 151643
>>> 2025-08-18 22:22:51,813 - INFO - ----------------------------------------
>>> 2025-08-18 22:22:51,814 - INFO - 量化信息:
>>> 2025-08-18 22:22:51,814 - INFO -   数据类型: torch.float32
>>> 2025-08-18 22:22:51,814 - INFO -   量化方式: None
>>> 2025-08-18 22:22:51,815 - INFO - ----------------------------------------
>>> 2025-08-18 22:22:51,815 - INFO - 可训练层 (前10个):
>>> 2025-08-18 22:22:51,815 - INFO -   1. model.embed_tokens.weight
>>> 2025-08-18 22:22:51,816 - INFO -   2. model.layers.0.self_attn.q_proj.weight
>>> 2025-08-18 22:22:51,816 - INFO -   3. model.layers.0.self_attn.k_proj.weight
>>> 2025-08-18 22:22:51,817 - INFO -   4. model.layers.0.self_attn.v_proj.weight
>>> 2025-08-18 22:22:51,817 - INFO -   5. model.layers.0.self_attn.o_proj.weight
>>> 2025-08-18 22:22:51,817 - INFO -   6. model.layers.0.self_attn.q_norm.weight
>>> 2025-08-18 22:22:51,818 - INFO -   7. model.layers.0.self_attn.k_norm.weight
>>> 2025-08-18 22:22:51,818 - INFO -   8. model.layers.0.mlp.gate_proj.weight
>>> 2025-08-18 22:22:51,818 - INFO -   9. model.layers.0.mlp.up_proj.weight
>>> 2025-08-18 22:22:51,819 - INFO -   10. model.layers.0.mlp.down_proj.weight
>>> 2025-08-18 22:22:51,819 - INFO -   ... 还有 389 个可训练层
>>> 2025-08-18 22:25:56,846 - INFO - 正在读取配置文件: config.yaml
>>> 2025-08-18 22:25:56,853 - INFO - 从配置文件中提取到模型路径: /home/liangshuqiao/models/gemma-3
>>> 2025-08-18 22:25:56,854 - INFO - 开始加载模型配置文件...
>>> 2025-08-18 22:25:56,854 - INFO - 正在加载模型: /home/liangshuqiao/models/gemma-3
>>> 2025-08-18 22:30:09,090 - INFO - 模型加载完成
>>> 2025-08-18 22:30:09,226 - INFO - ============================================================
>>> 2025-08-18 22:30:09,227 - INFO - 模型详细信息
>>> 2025-08-18 22:30:09,227 - INFO - ============================================================
>>> 2025-08-18 22:30:09,228 - INFO - 模型路径: /home/liangshuqiao/models/gemma-3
>>> 2025-08-18 22:30:09,228 - INFO - 模型类型: gemma3
>>> 2025-08-18 22:30:09,229 - INFO - 模型架构: ['Gemma3ForConditionalGeneration']
>>> 2025-08-18 22:30:09,229 - INFO - ----------------------------------------
>>> 2025-08-18 22:30:09,229 - INFO - 参数信息:
>>> 2025-08-18 22:30:09,230 - INFO -   总参数量: 27.43B (27,432,406,640)
>>> 2025-08-18 22:30:09,230 - INFO -   可训练参数: 27.43B (27,432,406,640)
>>> 2025-08-18 22:30:09,231 - INFO -   冻结参数: 0 (0)
>>> 2025-08-18 22:30:09,231 - INFO -   可训练参数比例: 100.00%
>>> 2025-08-18 22:30:09,231 - INFO - ----------------------------------------
>>> 2025-08-18 22:30:09,232 - INFO - 模型结构信息:
>>> 2025-08-18 22:30:09,232 - INFO -   层数: unknown
>>> 2025-08-18 22:30:09,232 - INFO -   隐藏层大小: unknown
>>> 2025-08-18 22:30:09,233 - INFO -   注意力头数: unknown
>>> 2025-08-18 22:30:09,233 - INFO -   总层数量: 1247
>>> 2025-08-18 22:30:09,233 - INFO -   可训练层数量: 1247
>>> 2025-08-18 22:30:09,234 - INFO - ----------------------------------------
>>> 2025-08-18 22:30:09,234 - INFO - 特殊Token:
>>> 2025-08-18 22:30:09,235 - INFO -   bos_token: <bos>
>>> 2025-08-18 22:30:09,235 - INFO -   eos_token: <eos>
>>> 2025-08-18 22:30:09,235 - INFO -   unk_token: <unk>
>>> 2025-08-18 22:30:09,236 - INFO -   pad_token: <pad>
>>> 2025-08-18 22:30:09,236 - INFO -   sep_token: None
>>> 2025-08-18 22:30:09,236 - INFO -   mask_token: None
>>> 2025-08-18 22:30:09,237 - INFO -   vocab_size: 262144
>>> 2025-08-18 22:30:09,237 - INFO - ----------------------------------------
>>> 2025-08-18 22:30:09,237 - INFO - 量化信息:
>>> 2025-08-18 22:30:09,238 - INFO -   数据类型: torch.float32
>>> 2025-08-18 22:30:09,238 - INFO -   量化方式: None
>>> 2025-08-18 22:30:09,238 - INFO - ----------------------------------------
>>> 2025-08-18 22:30:09,239 - INFO - 可训练层 (前10个):
>>> 2025-08-18 22:30:09,239 - INFO -   1. vision_tower.vision_model.embeddings.patch_embedding.weight
>>> 2025-08-18 22:30:09,240 - INFO -   2. vision_tower.vision_model.embeddings.patch_embedding.bias
>>> 2025-08-18 22:30:09,240 - INFO -   3. vision_tower.vision_model.embeddings.position_embedding.weight
>>> 2025-08-18 22:30:09,241 - INFO -   4. vision_tower.vision_model.encoder.layers.0.layer_norm1.weight
>>> 2025-08-18 22:30:09,241 - INFO -   5. vision_tower.vision_model.encoder.layers.0.layer_norm1.bias
>>> 2025-08-18 22:30:09,242 - INFO -   6. vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight
>>> 2025-08-18 22:30:09,242 - INFO -   7. vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias
>>> 2025-08-18 22:30:09,243 - INFO -   8. vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight
>>> 2025-08-18 22:30:09,244 - INFO -   9. vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias
>>> 2025-08-18 22:30:09,244 - INFO -   10. vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight
>>> 2025-08-18 22:30:09,245 - INFO -   ... 还有 1237 个可训练层
>>> 2025-08-18 22:35:55,897 - INFO - ========__main__  202508182235========
>>> 2025-08-18 22:35:55,898 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-18 22:35:55,898 - INFO - 开始进行训练
>>> 2025-08-18 22:36:06,297 - INFO - 导入包完成
>>> 2025-08-18 22:36:06,317 - INFO - 配置文件读取完成
>>> 2025-08-18 22:36:06,317 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-18 22:36:06,317 - INFO - 模型路径:/home/liangshuqiao/models/Qwen2-7B
>>> 2025-08-18 22:36:06,649 - INFO - tokenizer读取完成
>>> 2025-08-18 22:36:33,590 - INFO - model dtype:torch.float16
>>> 2025-08-18 22:36:33,591 - INFO - 模型导入完成
>>> 2025-08-18 22:36:34,610 - INFO - 读取数据集成功
>>> 2025-08-18 22:36:39,381 - INFO - 数据处理成功
>>> 2025-08-18 22:36:55,264 - INFO - 开始训练！
>>> 2025-08-18 22:36:55,265 - INFO - 批次大小  : 4
>>> 2025-08-18 22:36:55,266 - INFO - 训练轮数  : 100
>>> 2025-08-18 22:36:55,267 - INFO - 学习率    : 0.0001
>>> 2025-08-18 22:36:55,267 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-18 22:36:55,268 - INFO - 模型路径  : /home/liangshuqiao/models/Qwen2-7B
>>> 2025-08-18 22:37:00,306 - INFO - >>> {'loss': 2.7863, 'grad_norm': 1.0161962509155273, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-18 22:37:30,697 - INFO - ========__main__  202508182237========
>>> 2025-08-18 22:37:30,697 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-18 22:37:30,698 - INFO - 开始进行训练
>>> 2025-08-18 22:37:33,102 - INFO - 导入包完成
>>> 2025-08-18 22:37:33,118 - INFO - 配置文件读取完成
>>> 2025-08-18 22:37:33,118 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-18 22:37:33,119 - INFO - 模型路径:/home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-18 22:37:33,749 - INFO - tokenizer读取完成
>>> 2025-08-18 22:38:47,673 - INFO - model dtype:torch.float16
>>> 2025-08-18 22:38:47,674 - INFO - 模型导入完成
>>> 2025-08-18 22:38:49,380 - INFO - 读取数据集成功
>>> 2025-08-18 22:38:49,606 - INFO - 数据处理成功
>>> 2025-08-18 22:39:04,300 - INFO - 开始训练！
>>> 2025-08-18 22:39:04,300 - INFO - 批次大小  : 4
>>> 2025-08-18 22:39:04,301 - INFO - 训练轮数  : 100
>>> 2025-08-18 22:39:04,301 - INFO - 学习率    : 0.0001
>>> 2025-08-18 22:39:04,302 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-18 22:39:04,302 - INFO - 模型路径  : /home/liangshuqiao/models/DeepSeek-R1-Distill-Qwen-7B
>>> 2025-08-18 22:39:08,854 - INFO - >>> {'loss': 4.3327, 'grad_norm': 1.2303386926651, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-18 22:41:49,894 - INFO - ========__main__  202508182241========
>>> 2025-08-18 22:41:49,895 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-18 22:41:49,896 - INFO - 开始进行模型测试
>>> 2025-08-18 22:42:27,310 - INFO - ========train Qwen2ForCausalLM  202508182242========
>>> 2025-08-18 22:42:27,311 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-18 22:42:27,311 - INFO - 开始进行训练
>>> 2025-08-18 22:42:29,717 - INFO - 导入包完成
>>> 2025-08-18 22:42:29,733 - INFO - 配置文件读取完成
>>> 2025-08-18 22:42:29,734 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-18 22:42:29,734 - INFO - 模型路径:/home/liangshuqiao/models/qwen3
>>> 2025-08-18 22:42:30,166 - INFO - tokenizer读取完成
>>> 2025-08-18 22:49:26,420 - INFO - model dtype:torch.float16
>>> 2025-08-18 22:49:26,421 - INFO - 模型导入完成
>>> 2025-08-18 22:49:27,860 - INFO - 读取数据集成功
>>> 2025-08-18 22:49:32,684 - INFO - 数据处理成功
>>> 2025-08-18 22:50:32,345 - INFO - 正在读取配置文件: config.yaml
>>> 2025-08-18 22:50:32,352 - INFO - 从配置文件中提取到模型路径: /home/liangshuqiao/models/qwen3
>>> 2025-08-18 22:50:32,352 - INFO - 开始加载模型配置文件...
>>> 2025-08-18 22:50:32,353 - INFO - 正在加载模型: /home/liangshuqiao/models/qwen3
>>> 2025-08-18 22:50:59,412 - INFO - 模型加载完成
>>> 2025-08-18 22:50:59,474 - INFO - ============================================================
>>> 2025-08-18 22:50:59,474 - INFO - 模型详细信息
>>> 2025-08-18 22:50:59,475 - INFO - ============================================================
>>> 2025-08-18 22:50:59,475 - INFO - 模型路径: /home/liangshuqiao/models/qwen3
>>> 2025-08-18 22:50:59,476 - INFO - 模型类型: qwen3
>>> 2025-08-18 22:50:59,476 - INFO - 模型架构: ['Qwen3ForCausalLM']
>>> 2025-08-18 22:50:59,477 - INFO - ----------------------------------------
>>> 2025-08-18 22:50:59,477 - INFO - 参数信息:
>>> 2025-08-18 22:50:59,477 - INFO -   总参数量: 32.76B (32,762,123,264)
>>> 2025-08-18 22:50:59,478 - INFO -   可训练参数: 32.76B (32,762,123,264)
>>> 2025-08-18 22:50:59,478 - INFO -   冻结参数: 0 (0)
>>> 2025-08-18 22:50:59,478 - INFO -   可训练参数比例: 100.00%
>>> 2025-08-18 22:50:59,479 - INFO - ----------------------------------------
>>> 2025-08-18 22:50:59,479 - INFO - 模型结构信息:
>>> 2025-08-18 22:50:59,479 - INFO -   层数: 64
>>> 2025-08-18 22:50:59,480 - INFO -   隐藏层大小: 5120
>>> 2025-08-18 22:50:59,480 - INFO -   注意力头数: 64
>>> 2025-08-18 22:50:59,480 - INFO -   总层数量: 707
>>> 2025-08-18 22:50:59,481 - INFO -   可训练层数量: 707
>>> 2025-08-18 22:50:59,481 - INFO - ----------------------------------------
>>> 2025-08-18 22:50:59,481 - INFO - 特殊Token:
>>> 2025-08-18 22:50:59,482 - INFO -   bos_token: None
>>> 2025-08-18 22:50:59,482 - INFO -   eos_token: <|im_end|>
>>> 2025-08-18 22:50:59,483 - INFO -   unk_token: None
>>> 2025-08-18 22:50:59,483 - INFO -   pad_token: <|endoftext|>
>>> 2025-08-18 22:50:59,483 - INFO -   sep_token: None
>>> 2025-08-18 22:50:59,484 - INFO -   mask_token: None
>>> 2025-08-18 22:50:59,484 - INFO -   vocab_size: 151643
>>> 2025-08-18 22:50:59,484 - INFO - ----------------------------------------
>>> 2025-08-18 22:50:59,485 - INFO - 量化信息:
>>> 2025-08-18 22:50:59,485 - INFO -   数据类型: torch.float32
>>> 2025-08-18 22:50:59,485 - INFO -   量化方式: None
>>> 2025-08-18 22:50:59,486 - INFO - ----------------------------------------
>>> 2025-08-18 22:50:59,486 - INFO - 可训练层 (前10个):
>>> 2025-08-18 22:50:59,487 - INFO -   1. model.embed_tokens.weight
>>> 2025-08-18 22:50:59,487 - INFO -   2. model.layers.0.self_attn.q_proj.weight
>>> 2025-08-18 22:50:59,487 - INFO -   3. model.layers.0.self_attn.k_proj.weight
>>> 2025-08-18 22:50:59,488 - INFO -   4. model.layers.0.self_attn.v_proj.weight
>>> 2025-08-18 22:50:59,488 - INFO -   5. model.layers.0.self_attn.o_proj.weight
>>> 2025-08-18 22:50:59,489 - INFO -   6. model.layers.0.self_attn.q_norm.weight
>>> 2025-08-18 22:50:59,489 - INFO -   7. model.layers.0.self_attn.k_norm.weight
>>> 2025-08-18 22:50:59,489 - INFO -   8. model.layers.0.mlp.gate_proj.weight
>>> 2025-08-18 22:50:59,490 - INFO -   9. model.layers.0.mlp.up_proj.weight
>>> 2025-08-18 22:50:59,490 - INFO -   10. model.layers.0.mlp.down_proj.weight
>>> 2025-08-18 22:50:59,491 - INFO -   ... 还有 697 个可训练层
>>> 2025-08-18 22:51:36,649 - INFO - ========train Qwen2ForCausalLM  202508182251========
>>> 2025-08-18 22:51:36,649 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-18 22:51:36,650 - INFO - 开始进行训练
>>> 2025-08-18 22:51:45,053 - INFO - 导入包完成
>>> 2025-08-18 22:51:45,069 - INFO - 配置文件读取完成
>>> 2025-08-18 22:51:45,069 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-18 22:51:45,070 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-08-18 22:51:45,445 - INFO - tokenizer读取完成
>>> 2025-08-18 22:52:48,010 - INFO - model dtype:torch.float16
>>> 2025-08-18 22:52:48,012 - INFO - 模型导入完成
>>> 2025-08-18 22:52:48,769 - INFO - 读取数据集成功
>>> 2025-08-18 22:52:53,380 - INFO - 数据处理成功
>>> 2025-08-18 22:53:09,966 - INFO - 开始训练！
>>> 2025-08-18 22:53:09,967 - INFO - 批次大小  : 4
>>> 2025-08-18 22:53:09,967 - INFO - 训练轮数  : 100
>>> 2025-08-18 22:53:09,968 - INFO - 学习率    : 0.0001
>>> 2025-08-18 22:53:09,968 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-18 22:53:09,969 - INFO - 模型路径  : /home/liangshuqiao/models/qwen3-8b
>>> 2025-08-18 22:53:16,916 - INFO - >>> {'loss': 3.5214, 'grad_norm': 2.4722843170166016, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-08-18 22:53:21,337 - INFO - >>> {'loss': 3.6667, 'grad_norm': 2.729246139526367, 'learning_rate': 0.0001, 'epoch': 0.32}
>>> 2025-08-18 22:53:24,246 - INFO - >>> {'loss': 3.5722, 'grad_norm': 2.8817481994628906, 'learning_rate': 9.999931232202689e-05, 'epoch': 0.48}
>>> 2025-08-18 22:53:29,291 - INFO - >>> {'loss': 3.4778, 'grad_norm': 2.374586582183838, 'learning_rate': 9.999724930702356e-05, 'epoch': 0.64}
>>> 2025-08-18 22:53:35,399 - INFO - >>> {'loss': 3.0199, 'grad_norm': 1.8116371631622314, 'learning_rate': 9.999381101173764e-05, 'epoch': 0.8}
>>> 2025-08-18 22:53:40,604 - INFO - >>> {'loss': 2.8654, 'grad_norm': 2.404141902923584, 'learning_rate': 9.998899753074669e-05, 'epoch': 0.96}
>>> 2025-08-18 22:53:41,918 - INFO - >>> {'loss': 3.057, 'grad_norm': 2.398003339767456, 'learning_rate': 9.998280899645574e-05, 'epoch': 1.0}
>>> 2025-08-18 22:53:46,026 - INFO - >>> {'loss': 2.7339, 'grad_norm': 1.4306901693344116, 'learning_rate': 9.997524557909352e-05, 'epoch': 1.16}
>>> 2025-08-18 22:53:51,740 - INFO - >>> {'loss': 2.4594, 'grad_norm': 0.8629317879676819, 'learning_rate': 9.996630748670787e-05, 'epoch': 1.32}
>>> 2025-08-18 22:53:57,371 - INFO - >>> {'loss': 2.4327, 'grad_norm': 1.130467414855957, 'learning_rate': 9.995599496515995e-05, 'epoch': 1.48}
>>> 2025-08-18 22:54:02,493 - INFO - >>> {'loss': 2.2238, 'grad_norm': 0.9580157995223999, 'learning_rate': 9.99443082981175e-05, 'epoch': 1.6400000000000001}
>>> 2025-08-18 22:54:08,150 - INFO - >>> {'loss': 2.4198, 'grad_norm': 0.805099368095398, 'learning_rate': 9.993124780704707e-05, 'epoch': 1.8}
>>> 2025-08-18 22:54:12,619 - INFO - >>> {'loss': 2.2084, 'grad_norm': 0.7685917615890503, 'learning_rate': 9.991681385120515e-05, 'epoch': 1.96}
>>> 2025-08-18 22:54:13,926 - INFO - >>> {'loss': 2.4142, 'grad_norm': 1.095949411392212, 'learning_rate': 9.990100682762828e-05, 'epoch': 2.0}
>>> 2025-08-18 22:54:19,573 - INFO - >>> {'loss': 2.2859, 'grad_norm': 0.8037468791007996, 'learning_rate': 9.988382717112213e-05, 'epoch': 2.16}
>>> 2025-08-18 22:54:24,277 - INFO - >>> {'loss': 2.1981, 'grad_norm': 0.9759875535964966, 'learning_rate': 9.986527535424957e-05, 'epoch': 2.32}
>>> 2025-08-18 22:54:30,888 - INFO - >>> {'loss': 2.1146, 'grad_norm': 1.065224528312683, 'learning_rate': 9.984535188731759e-05, 'epoch': 2.48}
>>> 2025-08-18 22:54:36,528 - INFO - >>> {'loss': 2.1205, 'grad_norm': 0.9849202036857605, 'learning_rate': 9.982405731836342e-05, 'epoch': 2.64}
>>> 2025-08-18 22:54:41,446 - INFO - >>> {'loss': 2.0412, 'grad_norm': 0.8978034257888794, 'learning_rate': 9.980139223313925e-05, 'epoch': 2.8}
>>> 2025-08-18 22:54:47,485 - INFO - >>> {'loss': 1.8905, 'grad_norm': 0.8013890385627747, 'learning_rate': 9.977735725509632e-05, 'epoch': 2.96}
>>> 2025-08-18 22:54:48,614 - INFO - >>> {'loss': 2.1251, 'grad_norm': 1.7441943883895874, 'learning_rate': 9.97519530453676e-05, 'epoch': 3.0}
>>> 2025-08-18 22:54:56,578 - INFO - >>> {'loss': 2.0951, 'grad_norm': 0.693774938583374, 'learning_rate': 9.972518030274971e-05, 'epoch': 3.16}
>>> 2025-08-18 22:55:04,183 - INFO - >>> {'loss': 2.0908, 'grad_norm': 0.9120144844055176, 'learning_rate': 9.969703976368368e-05, 'epoch': 3.32}
>>> 2025-08-18 22:55:10,256 - INFO - >>> {'loss': 1.8634, 'grad_norm': 0.7634895443916321, 'learning_rate': 9.966753220223465e-05, 'epoch': 3.48}
>>> 2025-08-18 22:55:17,025 - INFO - >>> {'loss': 1.8528, 'grad_norm': 0.6661707162857056, 'learning_rate': 9.963665843007064e-05, 'epoch': 3.64}
>>> 2025-08-18 22:55:21,884 - INFO - >>> {'loss': 1.6401, 'grad_norm': 1.034502625465393, 'learning_rate': 9.960441929644017e-05, 'epoch': 3.8}
>>> 2025-08-18 22:55:25,518 - INFO - >>> {'loss': 1.7955, 'grad_norm': 1.056842565536499, 'learning_rate': 9.95708156881489e-05, 'epoch': 3.96}
>>> 2025-08-18 22:55:26,382 - INFO - >>> {'loss': 1.5696, 'grad_norm': 1.7164450883865356, 'learning_rate': 9.953584852953529e-05, 'epoch': 4.0}
>>> 2025-08-18 22:55:30,758 - INFO - >>> {'loss': 1.9188, 'grad_norm': 0.92156982421875, 'learning_rate': 9.949951878244515e-05, 'epoch': 4.16}
>>> 2025-08-18 22:55:37,912 - INFO - >>> {'loss': 1.6113, 'grad_norm': 0.7887139916419983, 'learning_rate': 9.946182744620512e-05, 'epoch': 4.32}
>>> 2025-08-18 22:55:43,549 - INFO - >>> {'loss': 1.6785, 'grad_norm': 0.8889738321304321, 'learning_rate': 9.942277555759529e-05, 'epoch': 4.48}
>>> 2025-08-18 22:55:49,454 - INFO - >>> {'loss': 1.62, 'grad_norm': 0.9654514193534851, 'learning_rate': 9.938236419082061e-05, 'epoch': 4.64}
>>> 2025-08-18 22:55:53,886 - INFO - >>> {'loss': 1.8119, 'grad_norm': 0.9884551167488098, 'learning_rate': 9.934059445748134e-05, 'epoch': 4.8}
>>> 2025-08-18 22:55:58,926 - INFO - >>> {'loss': 1.7417, 'grad_norm': 0.955915093421936, 'learning_rate': 9.929746750654249e-05, 'epoch': 4.96}
>>> 2025-08-18 22:55:59,400 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.925298452430226e-05, 'epoch': 5.0}
>>> 2025-08-18 22:56:04,685 - INFO - >>> {'loss': 1.7753, 'grad_norm': 1.114658236503601, 'learning_rate': 9.92071467343593e-05, 'epoch': 5.16}
>>> 2025-08-18 22:56:09,885 - INFO - >>> {'loss': 1.4479, 'grad_norm': 1.1293212175369263, 'learning_rate': 9.915995539757917e-05, 'epoch': 5.32}
>>> 2025-08-18 22:56:14,176 - INFO - >>> {'loss': 1.2969, 'grad_norm': 1.1030386686325073, 'learning_rate': 9.911141181205958e-05, 'epoch': 5.48}
>>> 2025-08-18 22:56:17,320 - INFO - >>> {'loss': 1.3279, 'grad_norm': 1.574678659439087, 'learning_rate': 9.906151731309472e-05, 'epoch': 5.64}
>>> 2025-08-18 22:56:22,333 - INFO - >>> {'loss': 1.4527, 'grad_norm': 1.1507753133773804, 'learning_rate': 9.901027327313848e-05, 'epoch': 5.8}
>>> 2025-08-18 22:56:28,166 - INFO - >>> {'loss': 1.5515, 'grad_norm': 1.1794321537017822, 'learning_rate': 9.895768110176678e-05, 'epoch': 5.96}
>>> 2025-08-18 22:56:29,298 - INFO - >>> {'loss': 1.7463, 'grad_norm': 3.6624975204467773, 'learning_rate': 9.890374224563872e-05, 'epoch': 6.0}
>>> 2025-08-18 22:56:34,157 - INFO - >>> {'loss': 1.2844, 'grad_norm': 1.5334272384643555, 'learning_rate': 9.884845818845685e-05, 'epoch': 6.16}
>>> 2025-08-18 22:56:39,444 - INFO - >>> {'loss': 1.3559, 'grad_norm': 1.3690836429595947, 'learning_rate': 9.879183045092628e-05, 'epoch': 6.32}
>>> 2025-08-18 22:56:41,971 - INFO - >>> {'loss': 0.9209, 'grad_norm': 2.684189796447754, 'learning_rate': 9.873386059071294e-05, 'epoch': 6.48}
>>> 2025-08-18 22:56:47,130 - INFO - >>> {'loss': 1.3934, 'grad_norm': 1.190879225730896, 'learning_rate': 9.867455020240069e-05, 'epoch': 6.64}
>>> 2025-08-18 22:56:50,977 - INFO - >>> {'loss': 1.2412, 'grad_norm': 1.8097891807556152, 'learning_rate': 9.861390091744737e-05, 'epoch': 6.8}
>>> 2025-08-18 22:56:56,899 - INFO - >>> {'loss': 1.2832, 'grad_norm': 1.6276631355285645, 'learning_rate': 9.855191440414013e-05, 'epoch': 6.96}
>>> 2025-08-18 22:56:58,178 - INFO - >>> {'loss': 1.0964, 'grad_norm': 2.0984256267547607, 'learning_rate': 9.848859236754935e-05, 'epoch': 7.0}
>>> 2025-08-18 22:57:02,329 - INFO - >>> {'loss': 1.1247, 'grad_norm': 1.772642731666565, 'learning_rate': 9.842393654948181e-05, 'epoch': 7.16}
>>> 2025-08-18 22:57:06,730 - INFO - >>> {'loss': 1.1784, 'grad_norm': 1.5941823720932007, 'learning_rate': 9.83579487284328e-05, 'epoch': 7.32}
>>> 2025-08-18 22:57:11,208 - INFO - >>> {'loss': 1.1526, 'grad_norm': 1.7045868635177612, 'learning_rate': 9.829063071953714e-05, 'epoch': 7.48}
>>> 2025-08-18 22:57:15,776 - INFO - >>> {'loss': 1.1626, 'grad_norm': 2.1328389644622803, 'learning_rate': 9.822198437451932e-05, 'epoch': 7.64}
>>> 2025-08-18 22:57:22,198 - INFO - >>> {'loss': 1.0055, 'grad_norm': 1.6781446933746338, 'learning_rate': 9.815201158164254e-05, 'epoch': 7.8}
>>> 2025-08-18 22:57:26,032 - INFO - >>> {'loss': 0.8492, 'grad_norm': 1.9403092861175537, 'learning_rate': 9.808071426565671e-05, 'epoch': 7.96}
>>> 2025-08-18 22:57:27,370 - INFO - >>> {'loss': 0.759, 'grad_norm': 3.8297598361968994, 'learning_rate': 9.800809438774556e-05, 'epoch': 8.0}
>>> 2025-08-18 22:57:32,460 - INFO - >>> {'loss': 1.0339, 'grad_norm': 1.5909249782562256, 'learning_rate': 9.793415394547274e-05, 'epoch': 8.16}
>>> 2025-08-18 22:57:38,404 - INFO - >>> {'loss': 0.8963, 'grad_norm': 1.4716216325759888, 'learning_rate': 9.785889497272677e-05, 'epoch': 8.32}
>>> 2025-08-18 22:57:42,996 - INFO - >>> {'loss': 0.6753, 'grad_norm': 1.6603351831436157, 'learning_rate': 9.778231953966519e-05, 'epoch': 8.48}
>>> 2025-08-18 22:57:47,661 - INFO - >>> {'loss': 0.6282, 'grad_norm': 1.8428432941436768, 'learning_rate': 9.770442975265752e-05, 'epoch': 8.64}
>>> 2025-08-18 22:57:52,472 - INFO - >>> {'loss': 0.7151, 'grad_norm': 2.432737112045288, 'learning_rate': 9.762522775422741e-05, 'epoch': 8.8}
>>> 2025-08-18 22:57:57,982 - INFO - >>> {'loss': 0.7967, 'grad_norm': 2.433476686477661, 'learning_rate': 9.754471572299363e-05, 'epoch': 8.96}
>>> 2025-08-18 22:57:58,455 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.746289587361021e-05, 'epoch': 9.0}
>>> 2025-08-18 22:58:02,735 - INFO - >>> {'loss': 0.5377, 'grad_norm': 2.002361536026001, 'learning_rate': 9.737977045670548e-05, 'epoch': 9.16}
>>> 2025-08-18 22:58:06,269 - INFO - >>> {'loss': 0.4838, 'grad_norm': 2.444549560546875, 'learning_rate': 9.729534175882016e-05, 'epoch': 9.32}
>>> 2025-08-18 22:58:11,201 - INFO - >>> {'loss': 0.5397, 'grad_norm': 1.9118387699127197, 'learning_rate': 9.720961210234449e-05, 'epoch': 9.48}
>>> 2025-08-18 22:58:17,361 - INFO - >>> {'loss': 0.4843, 'grad_norm': 2.066490650177002, 'learning_rate': 9.712258384545432e-05, 'epoch': 9.64}
>>> 2025-08-18 22:58:22,674 - INFO - >>> {'loss': 0.4776, 'grad_norm': 2.491283655166626, 'learning_rate': 9.703425938204627e-05, 'epoch': 9.8}
>>> 2025-08-18 22:58:26,469 - INFO - >>> {'loss': 0.2741, 'grad_norm': 2.4404828548431396, 'learning_rate': 9.694464114167186e-05, 'epoch': 9.96}
>>> 2025-08-18 22:58:28,291 - INFO - >>> {'loss': 0.7027, 'grad_norm': 4.1286444664001465, 'learning_rate': 9.685373158947067e-05, 'epoch': 10.0}
>>> 2025-08-18 22:58:32,975 - INFO - >>> {'loss': 0.3624, 'grad_norm': 2.272171974182129, 'learning_rate': 9.676153322610259e-05, 'epoch': 10.16}
>>> 2025-08-18 22:58:39,197 - INFO - >>> {'loss': 0.3012, 'grad_norm': 2.286703586578369, 'learning_rate': 9.666804858767894e-05, 'epoch': 10.32}
>>> 2025-08-18 22:58:44,635 - INFO - >>> {'loss': 0.2429, 'grad_norm': 1.3879894018173218, 'learning_rate': 9.65732802456928e-05, 'epoch': 10.48}
>>> 2025-08-18 22:58:48,332 - INFO - >>> {'loss': 0.2505, 'grad_norm': 2.7232158184051514, 'learning_rate': 9.647723080694821e-05, 'epoch': 10.64}
>>> 2025-08-18 22:58:51,477 - INFO - >>> {'loss': 0.175, 'grad_norm': 4.404017448425293, 'learning_rate': 9.637990291348853e-05, 'epoch': 10.8}
>>> 2025-08-18 22:58:56,717 - INFO - >>> {'loss': 0.2694, 'grad_norm': 2.075380563735962, 'learning_rate': 9.628129924252369e-05, 'epoch': 10.96}
>>> 2025-08-18 22:58:58,801 - INFO - >>> {'loss': 0.2547, 'grad_norm': 5.211668014526367, 'learning_rate': 9.618142250635658e-05, 'epoch': 11.0}
>>> 2025-08-18 22:59:02,833 - INFO - >>> {'loss': 0.1506, 'grad_norm': 2.0120863914489746, 'learning_rate': 9.608027545230847e-05, 'epoch': 11.16}
>>> 2025-08-18 22:59:09,073 - INFO - >>> {'loss': 0.1083, 'grad_norm': 1.328035831451416, 'learning_rate': 9.597786086264338e-05, 'epoch': 11.32}
>>> 2025-08-18 22:59:14,184 - INFO - >>> {'loss': 0.1659, 'grad_norm': 2.1218953132629395, 'learning_rate': 9.587418155449167e-05, 'epoch': 11.48}
>>> 2025-08-18 22:59:19,044 - INFO - >>> {'loss': 0.1191, 'grad_norm': 2.720994472503662, 'learning_rate': 9.576924037977233e-05, 'epoch': 11.64}
>>> 2025-08-18 22:59:24,178 - INFO - >>> {'loss': 0.1351, 'grad_norm': 1.5727256536483765, 'learning_rate': 9.566304022511477e-05, 'epoch': 11.8}
>>> 2025-08-18 22:59:28,911 - INFO - >>> {'loss': 0.102, 'grad_norm': 1.535103440284729, 'learning_rate': 9.555558401177926e-05, 'epoch': 11.96}
>>> 2025-08-18 22:59:29,807 - INFO - >>> {'loss': 0.1206, 'grad_norm': 3.5696611404418945, 'learning_rate': 9.544687469557666e-05, 'epoch': 12.0}
>>> 2025-08-18 22:59:35,231 - INFO - >>> {'loss': 0.0689, 'grad_norm': 1.0176078081130981, 'learning_rate': 9.533691526678705e-05, 'epoch': 12.16}
>>> 2025-08-18 22:59:40,435 - INFO - >>> {'loss': 0.0543, 'grad_norm': 1.120534062385559, 'learning_rate': 9.52257087500775e-05, 'epoch': 12.32}
>>> 2025-08-18 22:59:45,435 - INFO - >>> {'loss': 0.0539, 'grad_norm': 1.306990623474121, 'learning_rate': 9.51132582044189e-05, 'epoch': 12.48}
>>> 2025-08-18 22:59:50,376 - INFO - >>> {'loss': 0.0509, 'grad_norm': 1.5009374618530273, 'learning_rate': 9.499956672300178e-05, 'epoch': 12.64}
>>> 2025-08-18 22:59:54,588 - INFO - >>> {'loss': 0.0712, 'grad_norm': 1.4249498844146729, 'learning_rate': 9.488463743315126e-05, 'epoch': 12.8}
>>> 2025-08-18 22:59:59,618 - INFO - >>> {'loss': 0.0741, 'grad_norm': 1.795935034751892, 'learning_rate': 9.476847349624097e-05, 'epoch': 12.96}
>>> 2025-08-18 23:00:00,890 - INFO - >>> {'loss': 0.0595, 'grad_norm': 2.124384880065918, 'learning_rate': 9.46510781076061e-05, 'epoch': 13.0}
>>> 2025-08-18 23:00:05,575 - INFO - >>> {'loss': 0.0464, 'grad_norm': 0.9159435629844666, 'learning_rate': 9.453245449645563e-05, 'epoch': 13.16}
>>> 2025-08-18 23:00:09,050 - INFO - >>> {'loss': 0.0228, 'grad_norm': 1.756107211112976, 'learning_rate': 9.441260592578329e-05, 'epoch': 13.32}
>>> 2025-08-18 23:00:14,433 - INFO - >>> {'loss': 0.0293, 'grad_norm': 0.778384268283844, 'learning_rate': 9.4291535692278e-05, 'epoch': 13.48}
>>> 2025-08-18 23:00:17,743 - INFO - >>> {'loss': 0.0451, 'grad_norm': 0.8698577284812927, 'learning_rate': 9.416924712623305e-05, 'epoch': 13.64}
>>> 2025-08-18 23:00:23,793 - INFO - >>> {'loss': 0.0289, 'grad_norm': 1.1980944871902466, 'learning_rate': 9.404574359145459e-05, 'epoch': 13.8}
>>> 2025-08-18 23:00:28,885 - INFO - >>> {'loss': 0.0263, 'grad_norm': 1.1731983423233032, 'learning_rate': 9.392102848516901e-05, 'epoch': 13.96}
>>> 2025-08-18 23:00:30,022 - INFO - >>> {'loss': 0.0286, 'grad_norm': 1.6292579174041748, 'learning_rate': 9.379510523792961e-05, 'epoch': 14.0}
>>> 2025-08-18 23:00:34,697 - INFO - >>> {'loss': 0.0131, 'grad_norm': 0.3969302475452423, 'learning_rate': 9.366797731352209e-05, 'epoch': 14.16}
>>> 2025-08-18 23:00:39,281 - INFO - >>> {'loss': 0.0242, 'grad_norm': 1.1300102472305298, 'learning_rate': 9.353964820886938e-05, 'epoch': 14.32}
>>> 2025-08-18 23:00:44,985 - INFO - >>> {'loss': 0.0171, 'grad_norm': 0.7280820608139038, 'learning_rate': 9.341012145393547e-05, 'epoch': 14.48}
>>> 2025-08-18 23:00:50,272 - INFO - >>> {'loss': 0.0176, 'grad_norm': 0.5881336331367493, 'learning_rate': 9.327940061162817e-05, 'epoch': 14.64}
>>> 2025-08-18 23:00:54,828 - INFO - >>> {'loss': 0.0083, 'grad_norm': 0.518279492855072, 'learning_rate': 9.314748927770125e-05, 'epoch': 14.8}
>>> 2025-08-18 23:00:58,506 - INFO - >>> {'loss': 0.0133, 'grad_norm': 0.6370105743408203, 'learning_rate': 9.301439108065546e-05, 'epoch': 14.96}
>>> 2025-08-18 23:01:00,911 - INFO - >>> {'loss': 0.0109, 'grad_norm': 0.7949324250221252, 'learning_rate': 9.288010968163872e-05, 'epoch': 15.0}
>>> 2025-08-18 23:01:04,989 - INFO - >>> {'loss': 0.0057, 'grad_norm': 0.2827036678791046, 'learning_rate': 9.274464877434548e-05, 'epoch': 15.16}
>>> 2025-08-18 23:01:10,438 - INFO - >>> {'loss': 0.0172, 'grad_norm': 0.6313324570655823, 'learning_rate': 9.260801208491498e-05, 'epoch': 15.32}
>>> 2025-08-18 23:01:13,944 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.24185669422149658, 'learning_rate': 9.247020337182893e-05, 'epoch': 15.48}
>>> 2025-08-18 23:01:18,988 - INFO - >>> {'loss': 0.0137, 'grad_norm': 0.7565328478813171, 'learning_rate': 9.233122642580796e-05, 'epoch': 15.64}
>>> 2025-08-18 23:01:24,177 - INFO - >>> {'loss': 0.0138, 'grad_norm': 0.7050886750221252, 'learning_rate': 9.219108506970746e-05, 'epoch': 15.8}
>>> 2025-08-18 23:01:28,911 - INFO - >>> {'loss': 0.0071, 'grad_norm': 0.5611216425895691, 'learning_rate': 9.204978315841237e-05, 'epoch': 15.96}
>>> 2025-08-18 23:01:31,319 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.2050769180059433, 'learning_rate': 9.190732457873119e-05, 'epoch': 16.0}
>>> 2025-08-18 23:01:37,265 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.5633108019828796, 'learning_rate': 9.176371324928899e-05, 'epoch': 16.16}
>>> 2025-08-18 23:01:41,398 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.1392214447259903, 'learning_rate': 9.161895312041971e-05, 'epoch': 16.32}
>>> 2025-08-18 23:01:45,147 - INFO - >>> {'loss': 0.0155, 'grad_norm': 0.6103439927101135, 'learning_rate': 9.14730481740574e-05, 'epoch': 16.48}
>>> 2025-08-18 23:01:50,325 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.5625730156898499, 'learning_rate': 9.132600242362681e-05, 'epoch': 16.64}
>>> 2025-08-18 23:01:55,595 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.16580578684806824, 'learning_rate': 9.117781991393283e-05, 'epoch': 16.8}
>>> 2025-08-18 23:01:59,607 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.21536080539226532, 'learning_rate': 9.102850472104944e-05, 'epoch': 16.96}
>>> 2025-08-18 23:02:00,531 - INFO - >>> {'loss': 0.0466, 'grad_norm': 1.2441617250442505, 'learning_rate': 9.087806095220739e-05, 'epoch': 17.0}
>>> 2025-08-18 23:02:03,702 - INFO - >>> {'loss': 0.0069, 'grad_norm': 2.0999176502227783, 'learning_rate': 9.072649274568129e-05, 'epoch': 17.16}
>>> 2025-08-18 23:02:09,641 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.08648134022951126, 'learning_rate': 9.057380427067584e-05, 'epoch': 17.32}
>>> 2025-08-18 23:02:15,123 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.2883474826812744, 'learning_rate': 9.041999972721109e-05, 'epoch': 17.48}
>>> 2025-08-18 23:02:19,832 - INFO - >>> {'loss': 0.0086, 'grad_norm': 0.20095194876194, 'learning_rate': 9.02650833460069e-05, 'epoch': 17.64}
>>> 2025-08-18 23:02:24,290 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.5384610295295715, 'learning_rate': 9.010905938836661e-05, 'epoch': 17.8}
>>> 2025-08-18 23:02:27,738 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.2769697904586792, 'learning_rate': 8.995193214605973e-05, 'epoch': 17.96}
>>> 2025-08-18 23:02:29,044 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.30867069959640503, 'learning_rate': 8.979370594120402e-05, 'epoch': 18.0}
>>> 2025-08-18 23:02:33,652 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.7534919381141663, 'learning_rate': 8.963438512614655e-05, 'epoch': 18.16}
>>> 2025-08-18 23:02:40,409 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.18059414625167847, 'learning_rate': 8.947397408334391e-05, 'epoch': 18.32}
>>> 2025-08-18 23:02:44,225 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.20807689428329468, 'learning_rate': 8.931247722524169e-05, 'epoch': 18.48}
>>> 2025-08-18 23:02:47,833 - INFO - >>> {'loss': 0.0068, 'grad_norm': 1.3376332521438599, 'learning_rate': 8.914989899415323e-05, 'epoch': 18.64}
>>> 2025-08-18 23:02:51,443 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.15156486630439758, 'learning_rate': 8.898624386213725e-05, 'epoch': 18.8}
>>> 2025-08-18 23:02:57,741 - INFO - >>> {'loss': 0.0095, 'grad_norm': 0.21964477002620697, 'learning_rate': 8.88215163308749e-05, 'epoch': 18.96}
>>> 2025-08-18 23:02:59,281 - INFO - >>> {'loss': 0.0144, 'grad_norm': 0.4994644522666931, 'learning_rate': 8.8655720931546e-05, 'epoch': 19.0}
>>> 2025-08-18 23:03:06,102 - INFO - >>> {'loss': 0.0086, 'grad_norm': 0.23954300582408905, 'learning_rate': 8.84888622247043e-05, 'epoch': 19.16}
>>> 2025-08-18 23:03:10,589 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.5450769662857056, 'learning_rate': 8.83209448001521e-05, 'epoch': 19.32}
>>> 2025-08-18 23:03:15,980 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.4193078279495239, 'learning_rate': 8.815197327681399e-05, 'epoch': 19.48}
>>> 2025-08-18 23:03:20,219 - INFO - >>> {'loss': 0.0068, 'grad_norm': 0.6384969353675842, 'learning_rate': 8.798195230260973e-05, 'epoch': 19.64}
>>> 2025-08-18 23:03:24,048 - INFO - >>> {'loss': 0.0186, 'grad_norm': 1.855926275253296, 'learning_rate': 8.781088655432648e-05, 'epoch': 19.8}
>>> 2025-08-18 23:03:28,928 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.6295043230056763, 'learning_rate': 8.763878073749012e-05, 'epoch': 19.96}
>>> 2025-08-18 23:03:29,854 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.388176292181015, 'learning_rate': 8.746563958623584e-05, 'epoch': 20.0}
>>> 2025-08-18 23:03:33,362 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.24108684062957764, 'learning_rate': 8.729146786317786e-05, 'epoch': 20.16}
>>> 2025-08-18 23:03:39,608 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.26688632369041443, 'learning_rate': 8.711627035927847e-05, 'epoch': 20.32}
>>> 2025-08-18 23:03:45,085 - INFO - >>> {'loss': 0.0065, 'grad_norm': 0.44204118847846985, 'learning_rate': 8.694005189371627e-05, 'epoch': 20.48}
>>> 2025-08-18 23:03:49,569 - INFO - >>> {'loss': 0.0103, 'grad_norm': 1.6523377895355225, 'learning_rate': 8.676281731375353e-05, 'epoch': 20.64}
>>> 2025-08-18 23:03:54,470 - INFO - >>> {'loss': 0.0075, 'grad_norm': 0.5310528874397278, 'learning_rate': 8.658457149460295e-05, 'epoch': 20.8}
>>> 2025-08-18 23:03:58,879 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.3437862992286682, 'learning_rate': 8.640531933929344e-05, 'epoch': 20.96}
>>> 2025-08-18 23:04:00,184 - INFO - >>> {'loss': 0.0263, 'grad_norm': 2.877471923828125, 'learning_rate': 8.622506577853538e-05, 'epoch': 21.0}
>>> 2025-08-18 23:04:04,955 - INFO - >>> {'loss': 0.0172, 'grad_norm': 1.939958095550537, 'learning_rate': 8.604381577058486e-05, 'epoch': 21.16}
>>> 2025-08-18 23:04:09,541 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.3517882823944092, 'learning_rate': 8.586157430110747e-05, 'epoch': 21.32}
>>> 2025-08-18 23:04:13,516 - INFO - >>> {'loss': 0.0113, 'grad_norm': 0.7493717074394226, 'learning_rate': 8.56783463830409e-05, 'epoch': 21.48}
>>> 2025-08-18 23:04:18,766 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.32853710651397705, 'learning_rate': 8.549413705645737e-05, 'epoch': 21.64}
>>> 2025-08-18 23:04:23,573 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.16678838431835175, 'learning_rate': 8.530895138842467e-05, 'epoch': 21.8}
>>> 2025-08-18 23:04:29,525 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.17991212010383606, 'learning_rate': 8.512279447286703e-05, 'epoch': 21.96}
>>> 2025-08-18 23:04:30,892 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.22661614418029785, 'learning_rate': 8.493567143042485e-05, 'epoch': 22.0}
>>> 2025-08-18 23:04:35,760 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.11305858939886093, 'learning_rate': 8.47475874083139e-05, 'epoch': 22.16}
>>> 2025-08-18 23:04:40,751 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.15679025650024414, 'learning_rate': 8.455854758018376e-05, 'epoch': 22.32}
>>> 2025-08-18 23:04:44,936 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.730616569519043, 'learning_rate': 8.436855714597546e-05, 'epoch': 22.48}
>>> 2025-08-18 23:04:48,337 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.11280831694602966, 'learning_rate': 8.417762133177848e-05, 'epoch': 22.64}
>>> 2025-08-18 23:04:52,982 - INFO - >>> {'loss': 0.013, 'grad_norm': 1.2798130512237549, 'learning_rate': 8.398574538968697e-05, 'epoch': 22.8}
>>> 2025-08-18 23:04:57,546 - INFO - >>> {'loss': 0.0094, 'grad_norm': 0.8199072480201721, 'learning_rate': 8.379293459765526e-05, 'epoch': 22.96}
>>> 2025-08-18 23:04:58,870 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.19160763919353485, 'learning_rate': 8.359919425935275e-05, 'epoch': 23.0}
>>> 2025-08-18 23:05:03,495 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.1489151120185852, 'learning_rate': 8.340452970401797e-05, 'epoch': 23.16}
>>> 2025-08-18 23:05:07,934 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.08717067539691925, 'learning_rate': 8.3208946286312e-05, 'epoch': 23.32}
>>> 2025-08-18 23:05:14,577 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.06755514442920685, 'learning_rate': 8.301244938617116e-05, 'epoch': 23.48}
>>> 2025-08-18 23:05:19,481 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.20558111369609833, 'learning_rate': 8.281504440865905e-05, 'epoch': 23.64}
>>> 2025-08-18 23:05:23,422 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.19218184053897858, 'learning_rate': 8.261673678381786e-05, 'epoch': 23.8}
>>> 2025-08-18 23:05:27,772 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.8058413863182068, 'learning_rate': 8.241753196651902e-05, 'epoch': 23.96}
>>> 2025-08-18 23:05:30,186 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.23953591287136078, 'learning_rate': 8.221743543631313e-05, 'epoch': 24.0}
>>> 2025-08-18 23:05:34,814 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.06891027092933655, 'learning_rate': 8.201645269727925e-05, 'epoch': 24.16}
>>> 2025-08-18 23:05:39,839 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.06586794555187225, 'learning_rate': 8.181458927787347e-05, 'epoch': 24.32}
>>> 2025-08-18 23:05:45,987 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.09760080277919769, 'learning_rate': 8.161185073077686e-05, 'epoch': 24.48}
>>> 2025-08-18 23:05:51,260 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.1895781010389328, 'learning_rate': 8.140824263274279e-05, 'epoch': 24.64}
>>> 2025-08-18 23:05:56,693 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.05413712188601494, 'learning_rate': 8.120377058444336e-05, 'epoch': 24.8}
>>> 2025-08-18 23:06:00,575 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.085154227912426, 'learning_rate': 8.09984402103156e-05, 'epoch': 24.96}
>>> 2025-08-18 23:06:01,044 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.079225715840646e-05, 'epoch': 25.0}
>>> 2025-08-18 23:06:05,509 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.03732124716043472, 'learning_rate': 8.058522710021772e-05, 'epoch': 25.16}
>>> 2025-08-18 23:06:10,670 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.11052514612674713, 'learning_rate': 8.037735573054979e-05, 'epoch': 25.32}
>>> 2025-08-18 23:06:14,657 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.04407893866300583, 'learning_rate': 8.016864876734514e-05, 'epoch': 25.48}
>>> 2025-08-18 23:06:19,056 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.11863203346729279, 'learning_rate': 7.995911195153105e-05, 'epoch': 25.64}
>>> 2025-08-18 23:06:24,457 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.11379394680261612, 'learning_rate': 7.974875104686163e-05, 'epoch': 25.8}
>>> 2025-08-18 23:06:30,638 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.5046608448028564, 'learning_rate': 7.95375718397593e-05, 'epoch': 25.96}
>>> 2025-08-18 23:06:31,108 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.932558013915562e-05, 'epoch': 26.0}
>>> 2025-08-18 23:06:34,133 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.1545914113521576, 'learning_rate': 7.911278177633151e-05, 'epoch': 26.16}
>>> 2025-08-18 23:06:36,993 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.03971666842699051, 'learning_rate': 7.889918260475685e-05, 'epoch': 26.32}
>>> 2025-08-18 23:06:42,703 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.20112298429012299, 'learning_rate': 7.868478849992945e-05, 'epoch': 26.48}
>>> 2025-08-18 23:06:48,161 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.13738222420215607, 'learning_rate': 7.846960535921344e-05, 'epoch': 26.64}
>>> 2025-08-18 23:06:53,744 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.03277839720249176, 'learning_rate': 7.825363910167708e-05, 'epoch': 26.8}
>>> 2025-08-18 23:06:59,476 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.06427926570177078, 'learning_rate': 7.803689566792989e-05, 'epoch': 26.96}
>>> 2025-08-18 23:07:00,233 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.05796707049012184, 'learning_rate': 7.781938101995927e-05, 'epoch': 27.0}
>>> 2025-08-18 23:07:04,715 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.03515656292438507, 'learning_rate': 7.76011011409665e-05, 'epoch': 27.16}
>>> 2025-08-18 23:07:09,986 - INFO - >>> {'loss': 0.007, 'grad_norm': 0.23092308640480042, 'learning_rate': 7.738206203520222e-05, 'epoch': 27.32}
>>> 2025-08-18 23:07:14,527 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.02417074516415596, 'learning_rate': 7.716226972780112e-05, 'epoch': 27.48}
>>> 2025-08-18 23:07:21,418 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.06944245845079422, 'learning_rate': 7.694173026461634e-05, 'epoch': 27.64}
>>> 2025-08-18 23:07:26,186 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.1582522839307785, 'learning_rate': 7.672044971205314e-05, 'epoch': 27.8}
>>> 2025-08-18 23:07:29,527 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.12012387067079544, 'learning_rate': 7.649843415690198e-05, 'epoch': 27.96}
>>> 2025-08-18 23:07:30,839 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.08132266998291016, 'learning_rate': 7.627568970617113e-05, 'epoch': 28.0}
>>> 2025-08-18 23:07:36,339 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.09255608916282654, 'learning_rate': 7.605222248691872e-05, 'epoch': 28.16}
>>> 2025-08-18 23:07:40,287 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.02673182263970375, 'learning_rate': 7.582803864608411e-05, 'epoch': 28.32}
>>> 2025-08-18 23:07:45,595 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.09066497534513474, 'learning_rate': 7.560314435031885e-05, 'epoch': 28.48}
>>> 2025-08-18 23:07:50,716 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.026935698464512825, 'learning_rate': 7.53775457858171e-05, 'epoch': 28.64}
>>> 2025-08-18 23:07:57,672 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.11782215535640717, 'learning_rate': 7.51512491581454e-05, 'epoch': 28.8}
>>> 2025-08-18 23:08:01,271 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.03283674269914627, 'learning_rate': 7.4924260692072e-05, 'epoch': 28.96}
>>> 2025-08-18 23:08:02,809 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.05563128739595413, 'learning_rate': 7.469658663139563e-05, 'epoch': 29.0}
>>> 2025-08-18 23:08:07,451 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.01392329391092062, 'learning_rate': 7.446823323877375e-05, 'epoch': 29.16}
>>> 2025-08-18 23:08:13,448 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.09992330521345139, 'learning_rate': 7.423920679555028e-05, 'epoch': 29.32}
>>> 2025-08-18 23:08:18,579 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.10740990936756134, 'learning_rate': 7.400951360158284e-05, 'epoch': 29.48}
>>> 2025-08-18 23:08:22,744 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.029460255056619644, 'learning_rate': 7.377915997506945e-05, 'epoch': 29.64}
>>> 2025-08-18 23:08:26,740 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.025172781199216843, 'learning_rate': 7.354815225237468e-05, 'epoch': 29.8}
>>> 2025-08-18 23:08:31,829 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.025933703407645226, 'learning_rate': 7.331649678785546e-05, 'epoch': 29.96}
>>> 2025-08-18 23:08:32,728 - INFO - >>> {'loss': 0.0146, 'grad_norm': 0.7354328036308289, 'learning_rate': 7.308419995368616e-05, 'epoch': 30.0}
>>> 2025-08-18 23:08:36,493 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.0232185460627079, 'learning_rate': 7.285126813968346e-05, 'epoch': 30.16}
>>> 2025-08-18 23:08:42,036 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.05397513136267662, 'learning_rate': 7.261770775313046e-05, 'epoch': 30.32}
>>> 2025-08-18 23:08:47,936 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.010718907229602337, 'learning_rate': 7.238352521860049e-05, 'epoch': 30.48}
>>> 2025-08-18 23:08:53,206 - INFO - >>> {'loss': 0.0091, 'grad_norm': 0.20613129436969757, 'learning_rate': 7.214872697778037e-05, 'epoch': 30.64}
>>> 2025-08-18 23:08:57,714 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.05453690141439438, 'learning_rate': 7.191331948929323e-05, 'epoch': 30.8}
>>> 2025-08-18 23:09:02,896 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.015524347312748432, 'learning_rate': 7.167730922852087e-05, 'epoch': 30.96}
>>> 2025-08-18 23:09:04,031 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.2672692537307739, 'learning_rate': 7.14407026874256e-05, 'epoch': 31.0}
>>> 2025-08-18 23:09:08,084 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.04520173370838165, 'learning_rate': 7.120350637437165e-05, 'epoch': 31.16}
>>> 2025-08-18 23:09:14,040 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.09607017785310745, 'learning_rate': 7.096572681394625e-05, 'epoch': 31.32}
>>> 2025-08-18 23:09:18,222 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.017822563648223877, 'learning_rate': 7.072737054678003e-05, 'epoch': 31.48}
>>> 2025-08-18 23:09:20,869 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.03463542088866234, 'learning_rate': 7.048844412936719e-05, 'epoch': 31.64}
>>> 2025-08-18 23:09:25,747 - INFO - >>> {'loss': 0.0124, 'grad_norm': 0.22105847299098969, 'learning_rate': 7.024895413388508e-05, 'epoch': 31.8}
>>> 2025-08-18 23:09:29,894 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.02972705103456974, 'learning_rate': 7.000890714801351e-05, 'epoch': 31.96}
>>> 2025-08-18 23:09:32,298 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.021341100335121155, 'learning_rate': 6.976830977475346e-05, 'epoch': 32.0}
>>> 2025-08-18 23:09:36,272 - INFO - >>> {'loss': 0.0105, 'grad_norm': 0.2170085906982422, 'learning_rate': 6.952716863224551e-05, 'epoch': 32.16}
>>> 2025-08-18 23:09:40,701 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.026288172230124474, 'learning_rate': 6.928549035358772e-05, 'epoch': 32.32}
>>> 2025-08-18 23:09:47,443 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.07394473999738693, 'learning_rate': 6.904328158665323e-05, 'epoch': 32.48}
>>> 2025-08-18 23:09:51,231 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.06796962022781372, 'learning_rate': 6.880054899390744e-05, 'epoch': 32.64}
>>> 2025-08-18 23:09:57,197 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.026827672496438026, 'learning_rate': 6.855729925222462e-05, 'epoch': 32.8}
>>> 2025-08-18 23:10:00,814 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.020442329347133636, 'learning_rate': 6.831353905270434e-05, 'epoch': 32.96}
>>> 2025-08-18 23:10:01,285 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.806927510048738e-05, 'epoch': 33.0}
>>> 2025-08-18 23:10:05,951 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.039962686598300934, 'learning_rate': 6.782451411457137e-05, 'epoch': 33.16}
>>> 2025-08-18 23:10:10,793 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.017809560522437096, 'learning_rate': 6.757926282762583e-05, 'epoch': 33.32}
>>> 2025-08-18 23:10:15,184 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.10867179930210114, 'learning_rate': 6.733352798580708e-05, 'epoch': 33.48}
>>> 2025-08-18 23:10:19,571 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.07418897747993469, 'learning_rate': 6.708731634857263e-05, 'epoch': 33.64}
>>> 2025-08-18 23:10:23,532 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.03248339146375656, 'learning_rate': 6.684063468849527e-05, 'epoch': 33.8}
>>> 2025-08-18 23:10:29,173 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.017204634845256805, 'learning_rate': 6.659348979107679e-05, 'epoch': 33.96}
>>> 2025-08-18 23:10:31,578 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.03452882170677185, 'learning_rate': 6.634588845456123e-05, 'epoch': 34.0}
>>> 2025-08-18 23:10:37,624 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.07385751605033875, 'learning_rate': 6.609783748974802e-05, 'epoch': 34.16}
>>> 2025-08-18 23:10:41,998 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.08872190862894058, 'learning_rate': 6.584934371980453e-05, 'epoch': 34.32}
>>> 2025-08-18 23:10:47,299 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.007497296668589115, 'learning_rate': 6.560041398007847e-05, 'epoch': 34.48}
>>> 2025-08-18 23:10:52,017 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.11004994809627533, 'learning_rate': 6.53510551179098e-05, 'epoch': 34.64}
>>> 2025-08-18 23:10:55,726 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.03283283859491348, 'learning_rate': 6.510127399244234e-05, 'epoch': 34.8}
>>> 2025-08-18 23:11:00,377 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.015244207344949245, 'learning_rate': 6.485107747443528e-05, 'epoch': 34.96}
>>> 2025-08-18 23:11:01,255 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.04960480332374573, 'learning_rate': 6.460047244607397e-05, 'epoch': 35.0}
>>> 2025-08-18 23:11:06,343 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.09486176073551178, 'learning_rate': 6.434946580078072e-05, 'epoch': 35.16}
>>> 2025-08-18 23:11:12,083 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.018561184406280518, 'learning_rate': 6.409806444302518e-05, 'epoch': 35.32}
>>> 2025-08-18 23:11:16,866 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.1108737364411354, 'learning_rate': 6.38462752881344e-05, 'epoch': 35.48}
>>> 2025-08-18 23:11:22,116 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.09548342972993851, 'learning_rate': 6.359410526210258e-05, 'epoch': 35.64}
>>> 2025-08-18 23:11:26,598 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.019287649542093277, 'learning_rate': 6.334156130140068e-05, 'epoch': 35.8}
>>> 2025-08-18 23:11:30,706 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.017511753365397453, 'learning_rate': 6.30886503527854e-05, 'epoch': 35.96}
>>> 2025-08-18 23:11:31,817 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.03342924267053604, 'learning_rate': 6.283537937310828e-05, 'epoch': 36.0}
>>> 2025-08-18 23:11:35,638 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.07192923128604889, 'learning_rate': 6.258175532912431e-05, 'epoch': 36.16}
>>> 2025-08-18 23:11:40,363 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.1354425996541977, 'learning_rate': 6.232778519730023e-05, 'epoch': 36.32}
>>> 2025-08-18 23:11:46,241 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.016192179173231125, 'learning_rate': 6.207347596362265e-05, 'epoch': 36.48}
>>> 2025-08-18 23:11:50,552 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.013161888346076012, 'learning_rate': 6.181883462340588e-05, 'epoch': 36.64}
>>> 2025-08-18 23:11:56,336 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.09415096044540405, 'learning_rate': 6.15638681810996e-05, 'epoch': 36.8}
>>> 2025-08-18 23:12:01,265 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.01341296173632145, 'learning_rate': 6.1308583650096e-05, 'epoch': 36.96}
>>> 2025-08-18 23:12:02,396 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.032469846308231354, 'learning_rate': 6.105298805253708e-05, 'epoch': 37.0}
>>> 2025-08-18 23:12:09,386 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.07422269135713577, 'learning_rate': 6.079708841912133e-05, 'epoch': 37.16}
>>> 2025-08-18 23:12:14,396 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.012089827097952366, 'learning_rate': 6.054089178891039e-05, 'epoch': 37.32}
>>> 2025-08-18 23:12:19,668 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.0685519427061081, 'learning_rate': 6.028440520913544e-05, 'epoch': 37.48}
>>> 2025-08-18 23:12:24,522 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.005144498776644468, 'learning_rate': 6.0027635735003316e-05, 'epoch': 37.64}
>>> 2025-08-18 23:12:29,679 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.13065829873085022, 'learning_rate': 5.9770590429502516e-05, 'epoch': 37.8}
>>> 2025-08-18 23:12:33,285 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.018640918657183647, 'learning_rate': 5.9513276363208784e-05, 'epoch': 37.96}
>>> 2025-08-18 23:12:33,756 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.925570061409077e-05, 'epoch': 38.0}
>>> 2025-08-18 23:12:38,146 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.07703384757041931, 'learning_rate': 5.8997870267315234e-05, 'epoch': 38.16}
>>> 2025-08-18 23:12:43,101 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.009370939806103706, 'learning_rate': 5.873979241505218e-05, 'epoch': 38.32}
>>> 2025-08-18 23:12:46,423 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.01394642237573862, 'learning_rate': 5.84814741562798e-05, 'epoch': 38.48}
>>> 2025-08-18 23:12:51,520 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.013226274400949478, 'learning_rate': 5.822292259658914e-05, 'epoch': 38.64}
>>> 2025-08-18 23:12:58,106 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.06863369047641754, 'learning_rate': 5.79641448479887e-05, 'epoch': 38.8}
>>> 2025-08-18 23:13:02,190 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0187369417399168, 'learning_rate': 5.770514802870879e-05, 'epoch': 38.96}
>>> 2025-08-18 23:13:03,326 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.027848688885569572, 'learning_rate': 5.7445939263005734e-05, 'epoch': 39.0}
>>> 2025-08-18 23:13:08,125 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.013313406147062778, 'learning_rate': 5.718652568096585e-05, 'epoch': 39.16}
>>> 2025-08-18 23:13:12,883 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.05108705163002014, 'learning_rate': 5.692691441830941e-05, 'epoch': 39.32}
>>> 2025-08-18 23:13:16,195 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.02439712919294834, 'learning_rate': 5.666711261619428e-05, 'epoch': 39.48}
>>> 2025-08-18 23:13:20,620 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.13807177543640137, 'learning_rate': 5.6407127421019534e-05, 'epoch': 39.64}
>>> 2025-08-18 23:13:24,134 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.016046566888689995, 'learning_rate': 5.614696598422885e-05, 'epoch': 39.8}
>>> 2025-08-18 23:13:30,771 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.07156931608915329, 'learning_rate': 5.5886635462113804e-05, 'epoch': 39.96}
>>> 2025-08-18 23:13:31,896 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.022516516968607903, 'learning_rate': 5.562614301561704e-05, 'epoch': 40.0}
>>> 2025-08-18 23:13:37,219 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.07772187888622284, 'learning_rate': 5.536549581013525e-05, 'epoch': 40.16}
>>> 2025-08-18 23:13:41,708 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.07602699846029282, 'learning_rate': 5.5104701015322125e-05, 'epoch': 40.32}
>>> 2025-08-18 23:13:47,927 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.003849644912406802, 'learning_rate': 5.48437658048911e-05, 'epoch': 40.48}
>>> 2025-08-18 23:13:51,979 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.02110930159687996, 'learning_rate': 5.4582697356418034e-05, 'epoch': 40.64}
>>> 2025-08-18 23:13:55,491 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.010424422100186348, 'learning_rate': 5.432150285114378e-05, 'epoch': 40.8}
>>> 2025-08-18 23:13:59,265 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.01273195818066597, 'learning_rate': 5.4060189473776676e-05, 'epoch': 40.96}
>>> 2025-08-18 23:13:59,885 - INFO - >>> {'loss': 0.0003, 'grad_norm': 0.033890191465616226, 'learning_rate': 5.379876441229486e-05, 'epoch': 41.0}
>>> 2025-08-18 23:14:03,621 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.015289274975657463, 'learning_rate': 5.3537234857748584e-05, 'epoch': 41.16}
>>> 2025-08-18 23:14:08,914 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.010172797366976738, 'learning_rate': 5.327560800406241e-05, 'epoch': 41.32}
>>> 2025-08-18 23:14:13,318 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.11168941855430603, 'learning_rate': 5.30138910478373e-05, 'epoch': 41.48}
>>> 2025-08-18 23:14:17,332 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.09011177718639374, 'learning_rate': 5.275209118815273e-05, 'epoch': 41.64}
>>> 2025-08-18 23:14:23,702 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.062407974153757095, 'learning_rate': 5.249021562636857e-05, 'epoch': 41.8}
>>> 2025-08-18 23:14:28,847 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.008598986081779003, 'learning_rate': 5.222827156592701e-05, 'epoch': 41.96}
>>> 2025-08-18 23:14:30,150 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.0219357218593359, 'learning_rate': 5.196626621215449e-05, 'epoch': 42.0}
>>> 2025-08-18 23:14:34,641 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.011959007941186428, 'learning_rate': 5.170420677206343e-05, 'epoch': 42.16}
>>> 2025-08-18 23:14:38,833 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.02086619846522808, 'learning_rate': 5.144210045415402e-05, 'epoch': 42.32}
>>> 2025-08-18 23:14:43,829 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.007073949556797743, 'learning_rate': 5.1179954468215915e-05, 'epoch': 42.48}
>>> 2025-08-18 23:14:48,769 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0032688353676348925, 'learning_rate': 5.0917776025129926e-05, 'epoch': 42.64}
>>> 2025-08-18 23:14:54,989 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.07227982580661774, 'learning_rate': 5.065557233666968e-05, 'epoch': 42.8}
>>> 2025-08-18 23:14:59,812 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.007676925044506788, 'learning_rate': 5.039335061530319e-05, 'epoch': 42.96}
>>> 2025-08-18 23:15:02,103 - INFO - >>> {'loss': 0.0071, 'grad_norm': 0.3363591134548187, 'learning_rate': 5.0131118073994556e-05, 'epoch': 43.0}
>>> 2025-08-18 23:15:05,960 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.015140675008296967, 'learning_rate': 4.986888192600546e-05, 'epoch': 43.16}
>>> 2025-08-18 23:15:12,095 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.005949033424258232, 'learning_rate': 4.9606649384696826e-05, 'epoch': 43.32}
>>> 2025-08-18 23:15:16,763 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.008990355767309666, 'learning_rate': 4.934442766333034e-05, 'epoch': 43.48}
>>> 2025-08-18 23:15:19,924 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.01705150119960308, 'learning_rate': 4.9082223974870086e-05, 'epoch': 43.64}
>>> 2025-08-18 23:15:25,282 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.03470096364617348, 'learning_rate': 4.8820045531784096e-05, 'epoch': 43.8}
>>> 2025-08-18 23:15:30,740 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.19851483404636383, 'learning_rate': 4.8557899545846e-05, 'epoch': 43.96}
>>> 2025-08-18 23:15:31,641 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.045459065586328506, 'learning_rate': 4.829579322793659e-05, 'epoch': 44.0}
>>> 2025-08-18 23:15:37,762 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.002166881924495101, 'learning_rate': 4.8033733787845535e-05, 'epoch': 44.16}
>>> 2025-08-18 23:15:41,125 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.02203340269625187, 'learning_rate': 4.7771728434073e-05, 'epoch': 44.32}
>>> 2025-08-18 23:15:45,969 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.05246927961707115, 'learning_rate': 4.7509784373631444e-05, 'epoch': 44.48}
>>> 2025-08-18 23:15:50,540 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.1416529268026352, 'learning_rate': 4.724790881184727e-05, 'epoch': 44.64}
>>> 2025-08-18 23:15:54,698 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.13306289911270142, 'learning_rate': 4.6986108952162695e-05, 'epoch': 44.8}
>>> 2025-08-18 23:16:00,094 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.005381083115935326, 'learning_rate': 4.6724391995937604e-05, 'epoch': 44.96}
>>> 2025-08-18 23:16:00,994 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.030954331159591675, 'learning_rate': 4.646276514225143e-05, 'epoch': 45.0}
>>> 2025-08-18 23:16:04,730 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.020105578005313873, 'learning_rate': 4.6201235587705154e-05, 'epoch': 45.16}
>>> 2025-08-18 23:16:08,911 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.11185196042060852, 'learning_rate': 4.5939810526223336e-05, 'epoch': 45.32}
>>> 2025-08-18 23:16:13,799 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.020133081823587418, 'learning_rate': 4.567849714885623e-05, 'epoch': 45.48}
>>> 2025-08-18 23:16:18,389 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.004711218643933535, 'learning_rate': 4.5417302643581985e-05, 'epoch': 45.64}
>>> 2025-08-18 23:16:24,237 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0010867643868550658, 'learning_rate': 4.5156234195108916e-05, 'epoch': 45.8}
>>> 2025-08-18 23:16:28,731 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.1750466525554657, 'learning_rate': 4.4895298984677886e-05, 'epoch': 45.96}
>>> 2025-08-18 23:16:30,737 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.12578780949115753, 'learning_rate': 4.4634504189864765e-05, 'epoch': 46.0}
>>> 2025-08-18 23:16:34,519 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.10036025941371918, 'learning_rate': 4.4373856984382984e-05, 'epoch': 46.16}
>>> 2025-08-18 23:16:39,813 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004683477804064751, 'learning_rate': 4.4113364537886215e-05, 'epoch': 46.32}
>>> 2025-08-18 23:16:44,890 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.007643351797014475, 'learning_rate': 4.385303401577118e-05, 'epoch': 46.48}
>>> 2025-08-18 23:16:48,546 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.006433811504393816, 'learning_rate': 4.359287257898049e-05, 'epoch': 46.64}
>>> 2025-08-18 23:16:53,522 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.10191885381937027, 'learning_rate': 4.333288738380573e-05, 'epoch': 46.8}
>>> 2025-08-18 23:16:58,174 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.009987132623791695, 'learning_rate': 4.3073085581690605e-05, 'epoch': 46.96}
>>> 2025-08-18 23:16:58,646 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.281347431903416e-05, 'epoch': 47.0}
>>> 2025-08-18 23:17:02,754 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.020400432869791985, 'learning_rate': 4.2554060736994284e-05, 'epoch': 47.16}
>>> 2025-08-18 23:17:06,663 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.014667509123682976, 'learning_rate': 4.229485197129122e-05, 'epoch': 47.32}
>>> 2025-08-18 23:17:12,863 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0022905562072992325, 'learning_rate': 4.203585515201131e-05, 'epoch': 47.48}
>>> 2025-08-18 23:17:17,248 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.08624711632728577, 'learning_rate': 4.177707740341087e-05, 'epoch': 47.64}
>>> 2025-08-18 23:17:22,531 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.09745950251817703, 'learning_rate': 4.1518525843720216e-05, 'epoch': 47.8}
>>> 2025-08-18 23:17:28,036 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.05047424137592316, 'learning_rate': 4.126020758494782e-05, 'epoch': 47.96}
>>> 2025-08-18 23:17:28,508 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.100212973268478e-05, 'epoch': 48.0}
>>> 2025-08-18 23:17:34,368 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.008252942003309727, 'learning_rate': 4.074429938590924e-05, 'epoch': 48.16}
>>> 2025-08-18 23:17:39,673 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0032488442957401276, 'learning_rate': 4.0486723636791234e-05, 'epoch': 48.32}
>>> 2025-08-18 23:17:42,807 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.01173826027661562, 'learning_rate': 4.022940957049751e-05, 'epoch': 48.48}
>>> 2025-08-18 23:17:46,797 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.1095028892159462, 'learning_rate': 3.9972364264996696e-05, 'epoch': 48.64}
>>> 2025-08-18 23:17:50,461 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.12894010543823242, 'learning_rate': 3.9715594790864586e-05, 'epoch': 48.8}
>>> 2025-08-18 23:17:55,767 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.07532110065221786, 'learning_rate': 3.945910821108963e-05, 'epoch': 48.96}
>>> 2025-08-18 23:17:57,031 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.036854393780231476, 'learning_rate': 3.920291158087869e-05, 'epoch': 49.0}
>>> 2025-08-18 23:18:01,521 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.012703105807304382, 'learning_rate': 3.894701194746291e-05, 'epoch': 49.16}
>>> 2025-08-18 23:18:07,844 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.003950438927859068, 'learning_rate': 3.869141634990399e-05, 'epoch': 49.32}
>>> 2025-08-18 23:18:13,131 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.04923038184642792, 'learning_rate': 3.8436131818900416e-05, 'epoch': 49.48}
>>> 2025-08-18 23:18:17,550 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.16553887724876404, 'learning_rate': 3.818116537659412e-05, 'epoch': 49.64}
>>> 2025-08-18 23:18:22,861 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.09152540564537048, 'learning_rate': 3.7926524036377364e-05, 'epoch': 49.8}
>>> 2025-08-18 23:18:26,161 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.007606880739331245, 'learning_rate': 3.767221480269978e-05, 'epoch': 49.96}
>>> 2025-08-18 23:18:26,920 - INFO - >>> {'loss': 0.0002, 'grad_norm': 0.01896461471915245, 'learning_rate': 3.741824467087569e-05, 'epoch': 50.0}
>>> 2025-08-18 23:18:32,283 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.10401087254285812, 'learning_rate': 3.716462062689172e-05, 'epoch': 50.16}
>>> 2025-08-18 23:18:37,898 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0049211555160582066, 'learning_rate': 3.691134964721462e-05, 'epoch': 50.32}
>>> 2025-08-18 23:18:42,768 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.08549652248620987, 'learning_rate': 3.665843869859934e-05, 'epoch': 50.48}
>>> 2025-08-18 23:18:47,955 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.08420350402593613, 'learning_rate': 3.6405894737897414e-05, 'epoch': 50.64}
>>> 2025-08-18 23:18:52,694 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0047166734002530575, 'learning_rate': 3.615372471186562e-05, 'epoch': 50.8}
>>> 2025-08-18 23:18:55,621 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.026511207222938538, 'learning_rate': 3.5901935556974834e-05, 'epoch': 50.96}
>>> 2025-08-18 23:18:56,893 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0226121935993433, 'learning_rate': 3.5650534199219296e-05, 'epoch': 51.0}
>>> 2025-08-18 23:19:00,603 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.09136425703763962, 'learning_rate': 3.539952755392605e-05, 'epoch': 51.16}
>>> 2025-08-18 23:19:05,188 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.09198153018951416, 'learning_rate': 3.514892252556474e-05, 'epoch': 51.32}
>>> 2025-08-18 23:19:10,372 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.005594566464424133, 'learning_rate': 3.489872600755765e-05, 'epoch': 51.48}
>>> 2025-08-18 23:19:14,835 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.007446436211466789, 'learning_rate': 3.464894488209022e-05, 'epoch': 51.64}
>>> 2025-08-18 23:19:20,016 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.005569907370954752, 'learning_rate': 3.4399586019921534e-05, 'epoch': 51.8}
>>> 2025-08-18 23:19:24,420 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.004945731721818447, 'learning_rate': 3.415065628019547e-05, 'epoch': 51.96}
>>> 2025-08-18 23:19:25,344 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.03125704079866409, 'learning_rate': 3.3902162510252e-05, 'epoch': 52.0}
>>> 2025-08-18 23:19:30,726 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0031676338985562325, 'learning_rate': 3.365411154543878e-05, 'epoch': 52.16}
>>> 2025-08-18 23:19:34,912 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.009780405089259148, 'learning_rate': 3.3406510208923224e-05, 'epoch': 52.32}
>>> 2025-08-18 23:19:39,474 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.10925029218196869, 'learning_rate': 3.315936531150473e-05, 'epoch': 52.48}
>>> 2025-08-18 23:19:44,435 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.009141850285232067, 'learning_rate': 3.291268365142738e-05, 'epoch': 52.64}
>>> 2025-08-18 23:19:49,448 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.008223462849855423, 'learning_rate': 3.266647201419294e-05, 'epoch': 52.8}
>>> 2025-08-18 23:19:55,521 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.10287006944417953, 'learning_rate': 3.242073717237418e-05, 'epoch': 52.96}
>>> 2025-08-18 23:19:56,654 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.028684232383966446, 'learning_rate': 3.217548588542864e-05, 'epoch': 53.0}
>>> 2025-08-18 23:20:00,114 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.008655928075313568, 'learning_rate': 3.193072489951263e-05, 'epoch': 53.16}
>>> 2025-08-18 23:20:05,912 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.04767279326915741, 'learning_rate': 3.1686460947295695e-05, 'epoch': 53.32}
>>> 2025-08-18 23:20:09,816 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.10958976298570633, 'learning_rate': 3.1442700747775414e-05, 'epoch': 53.48}
>>> 2025-08-18 23:20:13,683 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004106566775590181, 'learning_rate': 3.1199451006092584e-05, 'epoch': 53.64}
>>> 2025-08-18 23:20:19,484 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.00401054322719574, 'learning_rate': 3.095671841334678e-05, 'epoch': 53.8}
>>> 2025-08-18 23:20:22,796 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.015488306991755962, 'learning_rate': 3.0714509646412296e-05, 'epoch': 53.96}
>>> 2025-08-18 23:20:24,802 - INFO - >>> {'loss': 0.0078, 'grad_norm': 0.3863213062286377, 'learning_rate': 3.0472831367754494e-05, 'epoch': 54.0}
>>> 2025-08-18 23:20:27,435 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.030297592282295227, 'learning_rate': 3.0231690225246535e-05, 'epoch': 54.16}
>>> 2025-08-18 23:20:32,208 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.09776744991540909, 'learning_rate': 2.999109285198649e-05, 'epoch': 54.32}
>>> 2025-08-18 23:20:36,321 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.003818454220890999, 'learning_rate': 2.9751045866114922e-05, 'epoch': 54.48}
>>> 2025-08-18 23:20:41,335 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.05785239115357399, 'learning_rate': 2.9511555870632824e-05, 'epoch': 54.64}
>>> 2025-08-18 23:20:46,042 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0019847347866743803, 'learning_rate': 2.927262945321998e-05, 'epoch': 54.8}
>>> 2025-08-18 23:20:52,229 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.004348308779299259, 'learning_rate': 2.9034273186053755e-05, 'epoch': 54.96}
>>> 2025-08-18 23:20:53,803 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.023933786898851395, 'learning_rate': 2.8796493625628356e-05, 'epoch': 55.0}
>>> 2025-08-18 23:20:59,219 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.0315011702477932, 'learning_rate': 2.8559297312574417e-05, 'epoch': 55.16}
>>> 2025-08-18 23:21:04,480 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.009128526784479618, 'learning_rate': 2.832269077147913e-05, 'epoch': 55.32}
>>> 2025-08-18 23:21:08,950 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.006877806503325701, 'learning_rate': 2.8086680510706774e-05, 'epoch': 55.48}
>>> 2025-08-18 23:21:13,749 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.12256662547588348, 'learning_rate': 2.7851273022219644e-05, 'epoch': 55.64}
>>> 2025-08-18 23:21:17,882 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.01470860093832016, 'learning_rate': 2.7616474781399526e-05, 'epoch': 55.8}
>>> 2025-08-18 23:21:23,075 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.13231560587882996, 'learning_rate': 2.7382292246869547e-05, 'epoch': 55.96}
>>> 2025-08-18 23:21:25,474 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.027249814942479134, 'learning_rate': 2.7148731860316546e-05, 'epoch': 56.0}
>>> 2025-08-18 23:21:29,193 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.008665354922413826, 'learning_rate': 2.6915800046313848e-05, 'epoch': 56.16}
>>> 2025-08-18 23:21:32,786 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.016938848420977592, 'learning_rate': 2.6683503212144563e-05, 'epoch': 56.32}
>>> 2025-08-18 23:21:37,622 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.01104164868593216, 'learning_rate': 2.645184774762533e-05, 'epoch': 56.48}
>>> 2025-08-18 23:21:43,498 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.07561159878969193, 'learning_rate': 2.622084002493056e-05, 'epoch': 56.64}
>>> 2025-08-18 23:21:47,698 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.007749611511826515, 'learning_rate': 2.599048639841717e-05, 'epoch': 56.8}
>>> 2025-08-18 23:21:53,854 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.07736805826425552, 'learning_rate': 2.5760793204449735e-05, 'epoch': 56.96}
>>> 2025-08-18 23:21:54,775 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.022214755415916443, 'learning_rate': 2.5531766761226272e-05, 'epoch': 57.0}
>>> 2025-08-18 23:22:00,119 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.058853257447481155, 'learning_rate': 2.530341336860439e-05, 'epoch': 57.16}
>>> 2025-08-18 23:22:06,885 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0041630081832408905, 'learning_rate': 2.5075739307928014e-05, 'epoch': 57.32}
>>> 2025-08-18 23:22:10,249 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.017444437369704247, 'learning_rate': 2.4848750841854616e-05, 'epoch': 57.48}
>>> 2025-08-18 23:22:15,757 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.0835849791765213, 'learning_rate': 2.4622454214182917e-05, 'epoch': 57.64}
>>> 2025-08-18 23:22:18,507 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.021244481205940247, 'learning_rate': 2.4396855649681166e-05, 'epoch': 57.8}
>>> 2025-08-18 23:22:22,764 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.003018573857843876, 'learning_rate': 2.417196135391591e-05, 'epoch': 57.96}
>>> 2025-08-18 23:22:23,892 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.028449414297938347, 'learning_rate': 2.3947777513081292e-05, 'epoch': 58.0}
>>> 2025-08-18 23:22:28,314 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.06080387532711029, 'learning_rate': 2.372431029382888e-05, 'epoch': 58.16}
>>> 2025-08-18 23:22:33,830 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.002518431516364217, 'learning_rate': 2.350156584309804e-05, 'epoch': 58.32}
>>> 2025-08-18 23:22:36,996 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.24880103766918182, 'learning_rate': 2.327955028794688e-05, 'epoch': 58.48}
>>> 2025-08-18 23:22:41,278 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.003961901646107435, 'learning_rate': 2.305826973538366e-05, 'epoch': 58.64}
>>> 2025-08-18 23:22:45,515 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.010103500448167324, 'learning_rate': 2.2837730272198888e-05, 'epoch': 58.8}
>>> 2025-08-18 23:22:51,772 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.06926265358924866, 'learning_rate': 2.2617937964797785e-05, 'epoch': 58.96}
>>> 2025-08-18 23:22:52,894 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.021034495905041695, 'learning_rate': 2.2398898859033494e-05, 'epoch': 59.0}
>>> 2025-08-18 23:22:58,062 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.05239913612604141, 'learning_rate': 2.2180618980040747e-05, 'epoch': 59.16}
>>> 2025-08-18 23:23:03,290 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.006746627856045961, 'learning_rate': 2.1963104332070127e-05, 'epoch': 59.32}
>>> 2025-08-18 23:23:07,430 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.006736589130014181, 'learning_rate': 2.1746360898322933e-05, 'epoch': 59.48}
>>> 2025-08-18 23:23:13,326 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.003490633796900511, 'learning_rate': 2.1530394640786567e-05, 'epoch': 59.64}
>>> 2025-08-18 23:23:18,768 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.007120236288756132, 'learning_rate': 2.1315211500070558e-05, 'epoch': 59.8}
>>> 2025-08-18 23:23:22,484 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.015092043206095695, 'learning_rate': 2.1100817395243157e-05, 'epoch': 59.96}
>>> 2025-08-18 23:23:23,758 - INFO - >>> {'loss': 0.008, 'grad_norm': 0.38937950134277344, 'learning_rate': 2.088721822366849e-05, 'epoch': 60.0}
>>> 2025-08-18 23:23:27,335 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.01424422301352024, 'learning_rate': 2.0674419860844384e-05, 'epoch': 60.16}
>>> 2025-08-18 23:23:30,500 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.015150327235460281, 'learning_rate': 2.046242816024071e-05, 'epoch': 60.32}
>>> 2025-08-18 23:23:35,366 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.010817834176123142, 'learning_rate': 2.0251248953138374e-05, 'epoch': 60.48}
>>> 2025-08-18 23:23:40,609 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.06471984833478928, 'learning_rate': 2.0040888048468954e-05, 'epoch': 60.64}
>>> 2025-08-18 23:23:46,345 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.12280818074941635, 'learning_rate': 1.9831351232654872e-05, 'epoch': 60.8}
>>> 2025-08-18 23:23:52,842 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0013751363148912787, 'learning_rate': 1.962264426945023e-05, 'epoch': 60.96}
>>> 2025-08-18 23:23:53,742 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.030011771246790886, 'learning_rate': 1.9414772899782276e-05, 'epoch': 61.0}
>>> 2025-08-18 23:23:58,470 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.08800584077835083, 'learning_rate': 1.920774284159353e-05, 'epoch': 61.16}
>>> 2025-08-18 23:24:02,879 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.010932614095509052, 'learning_rate': 1.9001559789684404e-05, 'epoch': 61.32}
>>> 2025-08-18 23:24:09,210 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.08460935205221176, 'learning_rate': 1.8796229415556628e-05, 'epoch': 61.48}
>>> 2025-08-18 23:24:13,837 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.009618093259632587, 'learning_rate': 1.859175736725724e-05, 'epoch': 61.64}
>>> 2025-08-18 23:24:19,808 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.009221166372299194, 'learning_rate': 1.8388149269223153e-05, 'epoch': 61.8}
>>> 2025-08-18 23:24:24,178 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.06094168871641159, 'learning_rate': 1.8185410722126556e-05, 'epoch': 61.96}
>>> 2025-08-18 23:24:26,011 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.02435390278697014, 'learning_rate': 1.798354730272077e-05, 'epoch': 62.0}
>>> 2025-08-18 23:24:28,936 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.01845930889248848, 'learning_rate': 1.7782564563686884e-05, 'epoch': 62.16}
>>> 2025-08-18 23:24:33,489 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.006386721972376108, 'learning_rate': 1.7582468033480992e-05, 'epoch': 62.32}
>>> 2025-08-18 23:24:37,377 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.007958500646054745, 'learning_rate': 1.7383263216182157e-05, 'epoch': 62.48}
>>> 2025-08-18 23:24:43,061 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.0035728972870856524, 'learning_rate': 1.7184955591340974e-05, 'epoch': 62.64}
>>> 2025-08-18 23:24:47,850 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.07027547806501389, 'learning_rate': 1.6987550613828862e-05, 'epoch': 62.8}
>>> 2025-08-18 23:24:53,367 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.09729073941707611, 'learning_rate': 1.679105371368802e-05, 'epoch': 62.96}
>>> 2025-08-18 23:24:55,165 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.023822395130991936, 'learning_rate': 1.6595470295982045e-05, 'epoch': 63.0}
>>> 2025-08-18 23:24:58,326 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.019894642755389214, 'learning_rate': 1.6400805740647267e-05, 'epoch': 63.16}
>>> 2025-08-18 23:25:02,856 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.12326792627573013, 'learning_rate': 1.6207065402344747e-05, 'epoch': 63.32}
>>> 2025-08-18 23:25:09,201 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004140727687627077, 'learning_rate': 1.6014254610313033e-05, 'epoch': 63.48}
>>> 2025-08-18 23:25:14,257 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0031778442207723856, 'learning_rate': 1.582237866822151e-05, 'epoch': 63.64}
>>> 2025-08-18 23:25:19,707 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.07212559878826141, 'learning_rate': 1.563144285402453e-05, 'epoch': 63.8}
>>> 2025-08-18 23:25:24,448 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.003832305781543255, 'learning_rate': 1.5441452419816237e-05, 'epoch': 63.96}
>>> 2025-08-18 23:25:25,162 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.0279505867511034, 'learning_rate': 1.5252412591686105e-05, 'epoch': 64.0}
>>> 2025-08-18 23:25:30,550 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.0675617903470993, 'learning_rate': 1.5064328569575165e-05, 'epoch': 64.16}
>>> 2025-08-18 23:25:35,206 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.010759114287793636, 'learning_rate': 1.4877205527132982e-05, 'epoch': 64.32}
>>> 2025-08-18 23:25:40,523 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.002295452170073986, 'learning_rate': 1.4691048611575337e-05, 'epoch': 64.48}
>>> 2025-08-18 23:25:44,542 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.008399845100939274, 'learning_rate': 1.4505862943542642e-05, 'epoch': 64.64}
>>> 2025-08-18 23:25:48,598 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.1031566709280014, 'learning_rate': 1.4321653616959097e-05, 'epoch': 64.8}
>>> 2025-08-18 23:25:54,301 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.004158213268965483, 'learning_rate': 1.4138425698892555e-05, 'epoch': 64.96}
>>> 2025-08-18 23:25:55,363 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.03313802555203438, 'learning_rate': 1.3956184229415148e-05, 'epoch': 65.0}
>>> 2025-08-18 23:25:59,174 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.013536494225263596, 'learning_rate': 1.3774934221464642e-05, 'epoch': 65.16}
>>> 2025-08-18 23:26:03,999 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.008767371065914631, 'learning_rate': 1.359468066070657e-05, 'epoch': 65.32}
>>> 2025-08-18 23:26:07,954 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.009114250540733337, 'learning_rate': 1.341542850539706e-05, 'epoch': 65.48}
>>> 2025-08-18 23:26:13,010 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.09854069352149963, 'learning_rate': 1.3237182686246468e-05, 'epoch': 65.64}
>>> 2025-08-18 23:26:16,853 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.008624358102679253, 'learning_rate': 1.3059948106283725e-05, 'epoch': 65.8}
>>> 2025-08-18 23:26:23,896 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0011726479278877378, 'learning_rate': 1.2883729640721531e-05, 'epoch': 65.96}
>>> 2025-08-18 23:26:24,829 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.28316494822502136, 'learning_rate': 1.2708532136822155e-05, 'epoch': 66.0}
>>> 2025-08-18 23:26:28,968 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004200359340757132, 'learning_rate': 1.2534360413764169e-05, 'epoch': 66.16}
>>> 2025-08-18 23:26:33,382 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.007350051309913397, 'learning_rate': 1.2361219262509883e-05, 'epoch': 66.32}
>>> 2025-08-18 23:26:38,405 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.006483597680926323, 'learning_rate': 1.2189113445673528e-05, 'epoch': 66.48}
>>> 2025-08-18 23:26:43,464 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.09412561357021332, 'learning_rate': 1.2018047697390279e-05, 'epoch': 66.64}
>>> 2025-08-18 23:26:49,197 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.10470934957265854, 'learning_rate': 1.1848026723186012e-05, 'epoch': 66.8}
>>> 2025-08-18 23:26:54,287 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004741950426250696, 'learning_rate': 1.1679055199847893e-05, 'epoch': 66.96}
>>> 2025-08-18 23:26:55,655 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.19118908047676086, 'learning_rate': 1.1511137775295704e-05, 'epoch': 67.0}
>>> 2025-08-18 23:27:01,627 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.001981391105800867, 'learning_rate': 1.1344279068454011e-05, 'epoch': 67.16}
>>> 2025-08-18 23:27:05,904 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.011281667277216911, 'learning_rate': 1.1178483669125112e-05, 'epoch': 67.32}
>>> 2025-08-18 23:27:10,075 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.009347238577902317, 'learning_rate': 1.101375613786278e-05, 'epoch': 67.48}
>>> 2025-08-18 23:27:14,898 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0026519198436290026, 'learning_rate': 1.0850101005846786e-05, 'epoch': 67.64}
>>> 2025-08-18 23:27:18,662 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.18734996020793915, 'learning_rate': 1.0687522774758319e-05, 'epoch': 67.8}
>>> 2025-08-18 23:27:24,129 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.1168823093175888, 'learning_rate': 1.0526025916656119e-05, 'epoch': 67.96}
>>> 2025-08-18 23:27:25,402 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.11865725368261337, 'learning_rate': 1.0365614873853462e-05, 'epoch': 68.0}
>>> 2025-08-18 23:27:30,359 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.003922325558960438, 'learning_rate': 1.0206294058795973e-05, 'epoch': 68.16}
>>> 2025-08-18 23:27:35,032 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.007783702574670315, 'learning_rate': 1.0048067853940285e-05, 'epoch': 68.32}
>>> 2025-08-18 23:27:38,922 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004926962312310934, 'learning_rate': 9.890940611633414e-06, 'epoch': 68.48}
>>> 2025-08-18 23:27:45,097 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.08029253035783768, 'learning_rate': 9.734916653993103e-06, 'epoch': 68.64}
>>> 2025-08-18 23:27:50,075 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.09326094388961792, 'learning_rate': 9.580000272788914e-06, 'epoch': 68.8}
>>> 2025-08-18 23:27:54,987 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0039893873035907745, 'learning_rate': 9.426195729324161e-06, 'epoch': 68.96}
>>> 2025-08-18 23:27:55,457 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.27350725431872e-06, 'epoch': 69.0}
>>> 2025-08-18 23:27:59,669 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.04366350546479225, 'learning_rate': 9.121939047792621e-06, 'epoch': 69.16}
>>> 2025-08-18 23:28:04,060 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.005757212173193693, 'learning_rate': 8.971495278950559e-06, 'epoch': 69.32}
>>> 2025-08-18 23:28:09,454 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.109444759786129, 'learning_rate': 8.82218008606716e-06, 'epoch': 69.48}
>>> 2025-08-18 23:28:15,238 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0069655380211770535, 'learning_rate': 8.673997576373205e-06, 'epoch': 69.64}
>>> 2025-08-18 23:28:20,287 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0049010710790753365, 'learning_rate': 8.526951825942609e-06, 'epoch': 69.8}
>>> 2025-08-18 23:28:24,651 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.12912146747112274, 'learning_rate': 8.381046879580306e-06, 'epoch': 69.96}
>>> 2025-08-18 23:28:25,971 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.019533630460500717, 'learning_rate': 8.23628675071102e-06, 'epoch': 70.0}
>>> 2025-08-18 23:28:31,891 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.0050545730628073215, 'learning_rate': 8.092675421268826e-06, 'epoch': 70.16}
>>> 2025-08-18 23:28:36,745 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.08569394052028656, 'learning_rate': 7.950216841587638e-06, 'epoch': 70.32}
>>> 2025-08-18 23:28:42,486 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.08876031637191772, 'learning_rate': 7.808914930292543e-06, 'epoch': 70.48}
>>> 2025-08-18 23:28:46,112 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.009339418262243271, 'learning_rate': 7.66877357419204e-06, 'epoch': 70.64}
>>> 2025-08-18 23:28:51,846 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0017646568594500422, 'learning_rate': 7.5297966281710705e-06, 'epoch': 70.8}
>>> 2025-08-18 23:28:54,964 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.014719669707119465, 'learning_rate': 7.391987915085013e-06, 'epoch': 70.96}
>>> 2025-08-18 23:28:56,558 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.026792148128151894, 'learning_rate': 7.255351225654527e-06, 'epoch': 71.0}
>>> 2025-08-18 23:29:02,225 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0025009524542838335, 'learning_rate': 7.119890318361277e-06, 'epoch': 71.16}
>>> 2025-08-18 23:29:05,612 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.014401976019144058, 'learning_rate': 6.98560891934455e-06, 'epoch': 71.32}
>>> 2025-08-18 23:29:08,500 - INFO - >>> {'loss': 0.0003, 'grad_norm': 0.013641014695167542, 'learning_rate': 6.852510722298761e-06, 'epoch': 71.48}
>>> 2025-08-18 23:29:14,323 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.05227920413017273, 'learning_rate': 6.72059938837184e-06, 'epoch': 71.64}
>>> 2025-08-18 23:29:20,399 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.052947018295526505, 'learning_rate': 6.589878546064543e-06, 'epoch': 71.8}
>>> 2025-08-18 23:29:24,880 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.12690497934818268, 'learning_rate': 6.46035179113062e-06, 'epoch': 71.96}
>>> 2025-08-18 23:29:26,249 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.028972197324037552, 'learning_rate': 6.332022686477928e-06, 'epoch': 72.0}
>>> 2025-08-18 23:29:28,329 - INFO - >>> {'loss': 0.0003, 'grad_norm': 0.02766072191298008, 'learning_rate': 6.204894762070407e-06, 'epoch': 72.16}
>>> 2025-08-18 23:29:33,522 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004009603522717953, 'learning_rate': 6.078971514830989e-06, 'epoch': 72.32}
>>> 2025-08-18 23:29:38,638 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.10923086106777191, 'learning_rate': 5.9542564085454165e-06, 'epoch': 72.48}
>>> 2025-08-18 23:29:42,842 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.007445318624377251, 'learning_rate': 5.830752873766948e-06, 'epoch': 72.64}
>>> 2025-08-18 23:29:47,966 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.0032042977400124073, 'learning_rate': 5.708464307722006e-06, 'epoch': 72.8}
>>> 2025-08-18 23:29:53,940 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0009411493665538728, 'learning_rate': 5.587394074216712e-06, 'epoch': 72.96}
>>> 2025-08-18 23:29:54,841 - INFO - >>> {'loss': 0.0098, 'grad_norm': 0.559840738773346, 'learning_rate': 5.46754550354438e-06, 'epoch': 73.0}
>>> 2025-08-18 23:29:59,781 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.007131586782634258, 'learning_rate': 5.348921892393904e-06, 'epoch': 73.16}
>>> 2025-08-18 23:30:03,805 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.009562677703797817, 'learning_rate': 5.231526503759054e-06, 'epoch': 73.32}
>>> 2025-08-18 23:30:08,252 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.006855725310742855, 'learning_rate': 5.115362566848747e-06, 'epoch': 73.48}
>>> 2025-08-18 23:30:13,244 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.07663769274950027, 'learning_rate': 5.000433276998218e-06, 'epoch': 73.64}
>>> 2025-08-18 23:30:18,811 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.10022594034671783, 'learning_rate': 4.886741795581101e-06, 'epoch': 73.8}
>>> 2025-08-18 23:30:24,278 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.03847019746899605, 'learning_rate': 4.774291249922508e-06, 'epoch': 73.96}
>>> 2025-08-18 23:30:25,033 - INFO - >>> {'loss': 0.0002, 'grad_norm': 0.014926127158105373, 'learning_rate': 4.6630847332129575e-06, 'epoch': 74.0}
>>> 2025-08-18 23:30:29,889 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0058985063806176186, 'learning_rate': 4.553125304423339e-06, 'epoch': 74.16}
>>> 2025-08-18 23:30:34,326 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.11738493293523788, 'learning_rate': 4.44441598822074e-06, 'epoch': 74.32}
>>> 2025-08-18 23:30:40,393 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.09695232659578323, 'learning_rate': 4.336959774885241e-06, 'epoch': 74.48}
>>> 2025-08-18 23:30:44,859 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.008986622095108032, 'learning_rate': 4.2307596202276815e-06, 'epoch': 74.64}
>>> 2025-08-18 23:30:49,279 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0029709753580391407, 'learning_rate': 4.1258184455083505e-06, 'epoch': 74.8}
>>> 2025-08-18 23:30:53,860 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.0443987175822258, 'learning_rate': 4.022139137356623e-06, 'epoch': 74.96}
>>> 2025-08-18 23:30:55,573 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.034455131739377975, 'learning_rate': 3.919724547691556e-06, 'epoch': 75.0}
>>> 2025-08-18 23:31:01,855 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.07581954449415207, 'learning_rate': 3.818577493643444e-06, 'epoch': 75.16}
>>> 2025-08-18 23:31:06,000 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.0787229835987091, 'learning_rate': 3.7187007574763232e-06, 'epoch': 75.32}
>>> 2025-08-18 23:31:10,624 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.11214999109506607, 'learning_rate': 3.6200970865114704e-06, 'epoch': 75.48}
>>> 2025-08-18 23:31:14,269 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.01021606381982565, 'learning_rate': 3.522769193051789e-06, 'epoch': 75.64}
>>> 2025-08-18 23:31:20,360 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0014609472127631307, 'learning_rate': 3.426719754307206e-06, 'epoch': 75.8}
>>> 2025-08-18 23:31:23,496 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.007761560380458832, 'learning_rate': 3.331951412321066e-06, 'epoch': 75.96}
>>> 2025-08-18 23:31:25,908 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.023265060037374496, 'learning_rate': 3.2384667738974196e-06, 'epoch': 76.0}
>>> 2025-08-18 23:31:30,387 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.0023405267857015133, 'learning_rate': 3.1462684105293293e-06, 'epoch': 76.16}
>>> 2025-08-18 23:31:35,190 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0015553507255390286, 'learning_rate': 3.0553588583281444e-06, 'epoch': 76.32}
>>> 2025-08-18 23:31:40,793 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.005074668675661087, 'learning_rate': 2.965740617953733e-06, 'epoch': 76.48}
>>> 2025-08-18 23:31:45,206 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.1277151256799698, 'learning_rate': 2.877416154545681e-06, 'epoch': 76.64}
>>> 2025-08-18 23:31:48,868 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.011517118662595749, 'learning_rate': 2.7903878976555163e-06, 'epoch': 76.8}
>>> 2025-08-18 23:31:55,477 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.0636163204908371, 'learning_rate': 2.7046582411798473e-06, 'epoch': 76.96}
>>> 2025-08-18 23:31:56,543 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.02406676858663559, 'learning_rate': 2.620229543294528e-06, 'epoch': 77.0}
>>> 2025-08-18 23:32:01,787 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0031256824731826782, 'learning_rate': 2.537104126389794e-06, 'epoch': 77.16}
>>> 2025-08-18 23:32:06,372 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.008950839750468731, 'learning_rate': 2.4552842770063757e-06, 'epoch': 77.32}
>>> 2025-08-18 23:32:09,570 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.011056013405323029, 'learning_rate': 2.3747722457725996e-06, 'epoch': 77.48}
>>> 2025-08-18 23:32:14,920 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.01605074852705002, 'learning_rate': 2.2955702473424824e-06, 'epoch': 77.64}
>>> 2025-08-18 23:32:20,067 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.03137494996190071, 'learning_rate': 2.217680460334809e-06, 'epoch': 77.8}
>>> 2025-08-18 23:32:24,711 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.12763969600200653, 'learning_rate': 2.141105027273227e-06, 'epoch': 77.96}
>>> 2025-08-18 23:32:26,785 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.226125106215477, 'learning_rate': 2.065846054527265e-06, 'epoch': 78.0}
>>> 2025-08-18 23:32:31,573 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.014428003691136837, 'learning_rate': 1.9919056122544465e-06, 'epoch': 78.16}
>>> 2025-08-18 23:32:37,901 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.07358027249574661, 'learning_rate': 1.919285734343307e-06, 'epoch': 78.32}
>>> 2025-08-18 23:32:42,219 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.0015347019070759416, 'learning_rate': 1.8479884183574657e-06, 'epoch': 78.48}
>>> 2025-08-18 23:32:47,117 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.1414608359336853, 'learning_rate': 1.7780156254806779e-06, 'epoch': 78.64}
>>> 2025-08-18 23:32:50,946 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.08002154529094696, 'learning_rate': 1.7093692804628635e-06, 'epoch': 78.8}
>>> 2025-08-18 23:32:56,280 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0012557763839140534, 'learning_rate': 1.6420512715672131e-06, 'epoch': 78.96}
>>> 2025-08-18 23:32:58,198 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.029906217008829117, 'learning_rate': 1.5760634505182004e-06, 'epoch': 79.0}
>>> 2025-08-18 23:33:03,259 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.003511270275339484, 'learning_rate': 1.5114076324506565e-06, 'epoch': 79.16}
>>> 2025-08-18 23:33:08,295 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004169537220150232, 'learning_rate': 1.4480855958598715e-06, 'epoch': 79.32}
>>> 2025-08-18 23:33:13,017 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.08068802952766418, 'learning_rate': 1.3860990825526333e-06, 'epoch': 79.48}
>>> 2025-08-18 23:33:18,143 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.0790487602353096, 'learning_rate': 1.3254497975993264e-06, 'epoch': 79.64}
>>> 2025-08-18 23:33:22,403 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.01520640030503273, 'learning_rate': 1.2661394092870537e-06, 'epoch': 79.8}
>>> 2025-08-18 23:33:25,758 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.014693567529320717, 'learning_rate': 1.2081695490737178e-06, 'epoch': 79.96}
>>> 2025-08-18 23:33:27,592 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.02483856864273548, 'learning_rate': 1.1515418115431553e-06, 'epoch': 80.0}
>>> 2025-08-18 23:33:32,005 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.1246614158153534, 'learning_rate': 1.0962577543612795e-06, 'epoch': 80.16}
>>> 2025-08-18 23:33:37,898 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.002754403045400977, 'learning_rate': 1.04231889823323e-06, 'epoch': 80.32}
>>> 2025-08-18 23:33:43,423 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0007895840099081397, 'learning_rate': 9.897267268615284e-07, 'epoch': 80.48}
>>> 2025-08-18 23:33:48,213 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.009380772709846497, 'learning_rate': 9.384826869052898e-07, 'epoch': 80.64}
>>> 2025-08-18 23:33:52,301 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.014402750879526138, 'learning_rate': 8.885881879404201e-07, 'epoch': 80.8}
>>> 2025-08-18 23:33:56,008 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.12320474535226822, 'learning_rate': 8.400446024208309e-07, 'epoch': 80.96}
>>> 2025-08-18 23:33:57,581 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.18811608850955963, 'learning_rate': 7.928532656407029e-07, 'epoch': 81.0}
>>> 2025-08-18 23:34:02,242 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.05784808471798897, 'learning_rate': 7.470154756977543e-07, 'epoch': 81.16}
>>> 2025-08-18 23:34:07,841 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.10028359293937683, 'learning_rate': 7.025324934575139e-07, 'epoch': 81.32}
>>> 2025-08-18 23:34:13,057 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.006725553888827562, 'learning_rate': 6.594055425186763e-07, 'epoch': 81.48}
>>> 2025-08-18 23:34:17,171 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.007739493157714605, 'learning_rate': 6.176358091794011e-07, 'epoch': 81.64}
>>> 2025-08-18 23:34:22,875 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0017092523630708456, 'learning_rate': 5.772244424047169e-07, 'epoch': 81.8}
>>> 2025-08-18 23:34:27,540 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.007194116711616516, 'learning_rate': 5.381725537948856e-07, 'epoch': 81.96}
>>> 2025-08-18 23:34:28,440 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.029552021995186806, 'learning_rate': 5.004812175548656e-07, 'epoch': 82.0}
>>> 2025-08-18 23:34:32,397 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.012835017405450344, 'learning_rate': 4.641514704647132e-07, 'epoch': 82.16}
>>> 2025-08-18 23:34:37,232 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.005210192874073982, 'learning_rate': 4.2918431185110517e-07, 'epoch': 82.32}
>>> 2025-08-18 23:34:42,703 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.004064058419317007, 'learning_rate': 3.9558070355983357e-07, 'epoch': 82.48}
>>> 2025-08-18 23:34:47,860 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0030486932955682278, 'learning_rate': 3.6334156992935406e-07, 'epoch': 82.64}
>>> 2025-08-18 23:34:52,983 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0049436381086707115, 'learning_rate': 3.324677977653401e-07, 'epoch': 82.8}
>>> 2025-08-18 23:34:58,111 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.05957726761698723, 'learning_rate': 3.0296023631631865e-07, 'epoch': 82.96}
>>> 2025-08-18 23:34:59,241 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.021899938583374023, 'learning_rate': 2.748196972502892e-07, 'epoch': 83.0}
>>> 2025-08-18 23:35:04,929 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.004962593782693148, 'learning_rate': 2.4804695463240826e-07, 'epoch': 83.16}
>>> 2025-08-18 23:35:09,074 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0089088324457407, 'learning_rate': 2.226427449036894e-07, 'epoch': 83.32}
>>> 2025-08-18 23:35:13,232 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.06971079111099243, 'learning_rate': 1.9860776686075332e-07, 'epoch': 83.48}
>>> 2025-08-18 23:35:17,155 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.1236102432012558, 'learning_rate': 1.7594268163659278e-07, 'epoch': 83.64}
>>> 2025-08-18 23:35:23,397 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.0015684941317886114, 'learning_rate': 1.546481126824151e-07, 'epoch': 83.8}
>>> 2025-08-18 23:35:29,306 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.0748811736702919, 'learning_rate': 1.347246457504503e-07, 'epoch': 83.96}
>>> 2025-08-18 23:35:29,782 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.1617282887787518e-07, 'epoch': 84.0}
>>> 2025-08-18 23:35:35,477 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.09199398010969162, 'learning_rate': 9.899317237172523e-08, 'epoch': 84.16}
>>> 2025-08-18 23:35:39,761 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.0719766691327095, 'learning_rate': 8.318614879485043e-08, 'epoch': 84.32}
>>> 2025-08-18 23:35:43,573 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.014738624915480614, 'learning_rate': 6.875219295293111e-08, 'epoch': 84.48}
>>> 2025-08-18 23:35:47,704 - INFO - >>> {'loss': 0.0006, 'grad_norm': 0.008031872101128101, 'learning_rate': 5.569170188250983e-08, 'epoch': 84.64}
>>> 2025-08-18 23:35:53,509 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.055576857179403305, 'learning_rate': 4.400503484006113e-08, 'epoch': 84.8}
>>> 2025-08-18 23:35:58,103 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.0017734664725139737, 'learning_rate': 3.369251329213285e-08, 'epoch': 84.96}
>>> 2025-08-18 23:35:59,028 - INFO - >>> {'loss': 0.0003, 'grad_norm': 0.020600179210305214, 'learning_rate': 2.4754420906475396e-08, 'epoch': 85.0}
>>> 2025-08-18 23:36:03,736 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0007452918798662722, 'learning_rate': 1.7191003544259064e-08, 'epoch': 85.16}
>>> 2025-08-18 23:36:08,713 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.003833540016785264, 'learning_rate': 1.100246925331283e-08, 'epoch': 85.32}
>>> 2025-08-18 23:36:13,264 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.00748040247708559, 'learning_rate': 6.188988262373352e-09, 'epoch': 85.48}
>>> 2025-08-18 23:36:17,220 - INFO - >>> {'loss': 0.0004, 'grad_norm': 0.012610828503966331, 'learning_rate': 2.750692976444258e-09, 'epoch': 85.64}
>>> 2025-08-18 23:36:23,485 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.0515986792743206, 'learning_rate': 6.876779731213035e-10, 'epoch': 85.8}
>>> 2025-08-18 23:36:24,503 - INFO - >>> {'train_runtime': 2594.2329, 'train_samples_per_second': 3.855, 'train_steps_per_second': 0.231, 'train_loss': 0.19567491081543267, 'epoch': 85.8}
>>> 2025-08-18 23:36:24,505 - INFO - 训练成功！
>>> 2025-08-18 23:36:24,505 - INFO - 模型存放位置：./output/qwen202508182252
>>> 2025-08-18 23:58:03,394 - INFO - ========__main__  202508182358========
>>> 2025-08-18 23:58:03,395 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-18 23:58:03,396 - INFO - 开始进行模型测试
>>> 2025-08-18 23:58:45,203 - INFO - 已选择模型文件夹: qwen202508182252
>>> 2025-08-18 23:58:45,206 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508182252/checkpoint-600
>>> 2025-08-19 12:51:05,588 - INFO - ========__main__  202508191251========
>>> 2025-08-19 12:51:05,589 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 12:51:05,589 - INFO - 开始进行模型测试
>>> 2025-08-19 12:51:12,090 - INFO - 已选择模型文件夹: qwen202508182252
>>> 2025-08-19 12:51:12,093 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508182252/checkpoint-600
>>> 2025-08-19 13:15:19,279 - INFO - ========__main__  202508191315========
>>> 2025-08-19 13:15:19,279 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 13:15:19,280 - INFO - 开始进行模型测试
>>> 2025-08-19 13:15:23,402 - INFO - 已选择模型文件夹: qwen202508182252
>>> 2025-08-19 13:15:23,405 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508182252/checkpoint-600
>>> 2025-08-19 13:15:54,808 - INFO - ========__main__  202508191315========
>>> 2025-08-19 13:15:54,809 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 13:15:54,809 - INFO - 开始进行模型测试
>>> 2025-08-19 13:16:33,826 - INFO - ========__main__  202508191316========
>>> 2025-08-19 13:16:33,826 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 13:16:33,827 - INFO - 开始进行模型测试
>>> 2025-08-19 13:16:41,884 - INFO - 已选择模型文件夹: qwen202508182252
>>> 2025-08-19 13:16:41,888 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508182252/checkpoint-600
>>> 2025-08-19 13:17:53,507 - INFO - ========__main__  202508191317========
>>> 2025-08-19 13:17:53,508 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 13:17:53,508 - INFO - 开始进行模型测试
>>> 2025-08-19 13:19:36,886 - INFO - ========__main__  202508191319========
>>> 2025-08-19 13:19:36,887 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 13:19:36,888 - INFO - 开始进行模型测试
>>> 2025-08-19 13:19:41,769 - INFO - 已选择模型文件夹: qwen202508182252
>>> 2025-08-19 13:19:41,772 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508182252/checkpoint-600
>>> 2025-08-19 13:32:45,993 - INFO - ========train Qwen2ForCausalLM  202508191332========
>>> 2025-08-19 13:32:45,993 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 13:32:45,994 - INFO - 开始进行训练
>>> 2025-08-19 13:32:48,464 - INFO - 导入包完成
>>> 2025-08-19 13:32:48,480 - INFO - 配置文件读取完成
>>> 2025-08-19 13:32:48,480 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-19 13:32:48,481 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-14b
>>> 2025-08-19 13:32:48,938 - INFO - tokenizer读取完成
>>> 2025-08-19 13:32:49,162 - ERROR - 模型导入失败：FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100), actual = `7.0`
>>> 2025-08-19 13:33:21,584 - INFO - ========train Qwen2ForCausalLM  202508191333========
>>> 2025-08-19 13:33:21,584 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 13:33:21,585 - INFO - 开始进行训练
>>> 2025-08-19 13:33:23,939 - INFO - 导入包完成
>>> 2025-08-19 13:33:23,954 - INFO - 配置文件读取完成
>>> 2025-08-19 13:33:23,955 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-19 13:33:23,955 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-08-19 13:33:24,252 - INFO - tokenizer读取完成
>>> 2025-08-19 13:33:26,471 - INFO - model dtype:torch.float16
>>> 2025-08-19 13:33:26,471 - INFO - 模型导入完成
>>> 2025-08-19 13:33:27,251 - INFO - 读取数据集成功
>>> 2025-08-19 13:33:31,944 - INFO - 数据处理成功
>>> 2025-08-19 13:33:47,168 - INFO - 开始训练！
>>> 2025-08-19 13:33:47,169 - INFO - 批次大小  : 8
>>> 2025-08-19 13:33:47,169 - INFO - 训练轮数  : 100
>>> 2025-08-19 13:33:47,169 - INFO - 学习率    : 0.0001
>>> 2025-08-19 13:33:47,170 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-19 13:33:47,170 - INFO - 模型路径  : /home/liangshuqiao/models/qwen3-8b
>>> 2025-08-19 13:34:08,563 - INFO - >>> {'loss': 3.73, 'grad_norm': 2.679239511489868, 'learning_rate': 0.0, 'epoch': 0.6153846153846154}
>>> 2025-08-19 13:34:46,883 - INFO - >>> {'loss': 7.3034, 'grad_norm': 5.217886447906494, 'learning_rate': 0.0001, 'epoch': 1.6153846153846154}
>>> 2025-08-19 13:35:26,375 - INFO - >>> {'loss': 7.0448, 'grad_norm': 4.494332790374756, 'learning_rate': 9.997482711915927e-05, 'epoch': 2.6153846153846154}
>>> 2025-08-19 13:36:10,125 - INFO - >>> {'loss': 6.3615, 'grad_norm': 4.408072471618652, 'learning_rate': 9.989933382359422e-05, 'epoch': 3.6153846153846154}
>>> 2025-08-19 13:36:46,781 - INFO - >>> {'loss': 6.0066, 'grad_norm': 4.30141544342041, 'learning_rate': 9.977359612865423e-05, 'epoch': 4.615384615384615}
>>> 2025-08-19 13:37:22,991 - INFO - >>> {'loss': 5.7125, 'grad_norm': 3.8214030265808105, 'learning_rate': 9.959774064153977e-05, 'epoch': 5.615384615384615}
>>> 2025-08-19 13:37:57,472 - INFO - >>> {'loss': 5.2807, 'grad_norm': 2.8880324363708496, 'learning_rate': 9.937194443381972e-05, 'epoch': 6.615384615384615}
>>> 2025-08-19 13:38:35,005 - INFO - >>> {'loss': 5.0778, 'grad_norm': 2.283903121948242, 'learning_rate': 9.909643486313533e-05, 'epoch': 7.615384615384615}
>>> 2025-08-19 13:39:12,334 - INFO - >>> {'loss': 4.8367, 'grad_norm': 1.8459539413452148, 'learning_rate': 9.877148934427037e-05, 'epoch': 8.615384615384615}
>>> 2025-08-19 13:39:49,138 - INFO - >>> {'loss': 4.7035, 'grad_norm': 1.5910844802856445, 'learning_rate': 9.839743506981782e-05, 'epoch': 9.615384615384615}
>>> 2025-08-19 13:40:24,519 - INFO - >>> {'loss': 4.4329, 'grad_norm': 1.3121286630630493, 'learning_rate': 9.797464868072488e-05, 'epoch': 10.615384615384615}
>>> 2025-08-19 13:41:02,292 - INFO - >>> {'loss': 4.4943, 'grad_norm': 1.1351940631866455, 'learning_rate': 9.750355588704727e-05, 'epoch': 11.615384615384615}
>>> 2025-08-19 13:41:42,193 - INFO - >>> {'loss': 4.365, 'grad_norm': 1.2947834730148315, 'learning_rate': 9.698463103929542e-05, 'epoch': 12.615384615384615}
>>> 2025-08-19 13:42:15,841 - INFO - >>> {'loss': 4.3231, 'grad_norm': 1.38405442237854, 'learning_rate': 9.641839665080363e-05, 'epoch': 13.615384615384615}
>>> 2025-08-19 13:42:53,779 - INFO - >>> {'loss': 4.2759, 'grad_norm': 1.5579955577850342, 'learning_rate': 9.580542287160348e-05, 'epoch': 14.615384615384615}
>>> 2025-08-19 13:43:27,429 - INFO - >>> {'loss': 4.1747, 'grad_norm': 1.6610922813415527, 'learning_rate': 9.514632691433107e-05, 'epoch': 15.615384615384615}
>>> 2025-08-19 13:44:04,139 - INFO - >>> {'loss': 3.9279, 'grad_norm': 1.5170201063156128, 'learning_rate': 9.444177243274618e-05, 'epoch': 16.615384615384617}
>>> 2025-08-19 13:44:43,025 - INFO - >>> {'loss': 4.0483, 'grad_norm': 1.5556542873382568, 'learning_rate': 9.369246885348926e-05, 'epoch': 17.615384615384617}
>>> 2025-08-19 13:45:16,613 - INFO - >>> {'loss': 3.8814, 'grad_norm': 1.5455849170684814, 'learning_rate': 9.289917066174886e-05, 'epoch': 18.615384615384617}
>>> 2025-08-19 13:45:54,145 - INFO - >>> {'loss': 3.8563, 'grad_norm': 1.3530528545379639, 'learning_rate': 9.206267664155907e-05, 'epoch': 19.615384615384617}
>>> 2025-08-19 13:46:31,271 - INFO - >>> {'loss': 3.7593, 'grad_norm': 1.4175540208816528, 'learning_rate': 9.118382907149165e-05, 'epoch': 20.615384615384617}
>>> 2025-08-19 13:47:07,423 - INFO - >>> {'loss': 3.6351, 'grad_norm': 1.3358339071273804, 'learning_rate': 9.026351287655294e-05, 'epoch': 21.615384615384617}
>>> 2025-08-19 13:47:45,805 - INFO - >>> {'loss': 3.6876, 'grad_norm': 1.2302806377410889, 'learning_rate': 8.930265473713938e-05, 'epoch': 22.615384615384617}
>>> 2025-08-19 13:48:20,113 - INFO - >>> {'loss': 3.5451, 'grad_norm': 1.3283789157867432, 'learning_rate': 8.83022221559489e-05, 'epoch': 23.615384615384617}
>>> 2025-08-19 13:48:54,703 - INFO - >>> {'loss': 3.4399, 'grad_norm': 1.386202335357666, 'learning_rate': 8.726322248378775e-05, 'epoch': 24.615384615384617}
>>> 2025-08-19 13:49:30,136 - INFO - >>> {'loss': 3.3205, 'grad_norm': 1.3082530498504639, 'learning_rate': 8.618670190525352e-05, 'epoch': 25.615384615384617}
>>> 2025-08-19 13:50:05,324 - INFO - >>> {'loss': 3.3917, 'grad_norm': 1.2303752899169922, 'learning_rate': 8.507374438531607e-05, 'epoch': 26.615384615384617}
>>> 2025-08-19 13:50:46,950 - INFO - >>> {'loss': 3.411, 'grad_norm': 1.313825249671936, 'learning_rate': 8.392547057785661e-05, 'epoch': 27.615384615384617}
>>> 2025-08-19 13:51:22,063 - INFO - >>> {'loss': 3.0837, 'grad_norm': 1.375734567642212, 'learning_rate': 8.274303669726426e-05, 'epoch': 28.615384615384617}
>>> 2025-08-19 13:52:01,371 - INFO - >>> {'loss': 3.1146, 'grad_norm': 1.5182771682739258, 'learning_rate': 8.152763335422613e-05, 'epoch': 29.615384615384617}
>>> 2025-08-19 13:52:37,453 - INFO - >>> {'loss': 3.0146, 'grad_norm': 1.296080470085144, 'learning_rate': 8.028048435688333e-05, 'epoch': 30.615384615384617}
>>> 2025-08-19 13:53:10,927 - INFO - >>> {'loss': 2.8362, 'grad_norm': 1.3220046758651733, 'learning_rate': 7.900284547855991e-05, 'epoch': 31.615384615384617}
>>> 2025-08-19 13:53:47,932 - INFO - >>> {'loss': 2.8977, 'grad_norm': 1.3075295686721802, 'learning_rate': 7.769600319330552e-05, 'epoch': 32.61538461538461}
>>> 2025-08-19 13:54:22,529 - INFO - >>> {'loss': 2.8122, 'grad_norm': 1.2959767580032349, 'learning_rate': 7.636127338052512e-05, 'epoch': 33.61538461538461}
>>> 2025-08-19 13:54:59,416 - INFO - >>> {'loss': 2.7431, 'grad_norm': 1.5201975107192993, 'learning_rate': 7.500000000000001e-05, 'epoch': 34.61538461538461}
>>> 2025-08-19 13:55:37,177 - INFO - >>> {'loss': 2.7507, 'grad_norm': 1.5507457256317139, 'learning_rate': 7.361355373863414e-05, 'epoch': 35.61538461538461}
>>> 2025-08-19 13:56:12,373 - INFO - >>> {'loss': 2.5811, 'grad_norm': 1.446158766746521, 'learning_rate': 7.220333063028872e-05, 'epoch': 36.61538461538461}
>>> 2025-08-19 13:56:55,202 - INFO - >>> {'loss': 2.4706, 'grad_norm': 1.6373448371887207, 'learning_rate': 7.077075065009433e-05, 'epoch': 37.61538461538461}
>>> 2025-08-19 13:57:28,749 - INFO - >>> {'loss': 2.3221, 'grad_norm': 1.7704863548278809, 'learning_rate': 6.931725628465643e-05, 'epoch': 38.61538461538461}
>>> 2025-08-19 13:58:01,484 - INFO - >>> {'loss': 2.3244, 'grad_norm': 1.48126220703125, 'learning_rate': 6.784431107959359e-05, 'epoch': 39.61538461538461}
>>> 2025-08-19 13:58:40,615 - INFO - >>> {'loss': 2.3355, 'grad_norm': 1.574275016784668, 'learning_rate': 6.635339816587109e-05, 'epoch': 40.61538461538461}
>>> 2025-08-19 13:59:12,939 - INFO - >>> {'loss': 2.1197, 'grad_norm': 1.7214628458023071, 'learning_rate': 6.484601876641375e-05, 'epoch': 41.61538461538461}
>>> 2025-08-19 13:59:47,829 - INFO - >>> {'loss': 2.0981, 'grad_norm': 1.586639404296875, 'learning_rate': 6.332369068450174e-05, 'epoch': 42.61538461538461}
>>> 2025-08-19 14:00:22,308 - INFO - >>> {'loss': 1.9265, 'grad_norm': 1.9144917726516724, 'learning_rate': 6.178794677547137e-05, 'epoch': 43.61538461538461}
>>> 2025-08-19 14:00:59,024 - INFO - >>> {'loss': 1.9462, 'grad_norm': 1.543745994567871, 'learning_rate': 6.024033340325954e-05, 'epoch': 44.61538461538461}
>>> 2025-08-19 14:01:30,182 - INFO - >>> {'loss': 1.7187, 'grad_norm': 1.6188828945159912, 'learning_rate': 5.868240888334653e-05, 'epoch': 45.61538461538461}
>>> 2025-08-19 14:02:06,333 - INFO - >>> {'loss': 1.6478, 'grad_norm': 1.5348896980285645, 'learning_rate': 5.7115741913664264e-05, 'epoch': 46.61538461538461}
>>> 2025-08-19 14:02:43,027 - INFO - >>> {'loss': 1.6424, 'grad_norm': 1.6931072473526, 'learning_rate': 5.5541909995050554e-05, 'epoch': 47.61538461538461}
>>> 2025-08-19 14:03:20,885 - INFO - >>> {'loss': 1.4397, 'grad_norm': 1.575042486190796, 'learning_rate': 5.396249784283942e-05, 'epoch': 48.61538461538461}
>>> 2025-08-19 14:03:56,266 - INFO - >>> {'loss': 1.5108, 'grad_norm': 1.7685025930404663, 'learning_rate': 5.2379095791187124e-05, 'epoch': 49.61538461538461}
>>> 2025-08-19 14:04:28,978 - INFO - >>> {'loss': 1.2214, 'grad_norm': 1.7104754447937012, 'learning_rate': 5.0793298191740404e-05, 'epoch': 50.61538461538461}
>>> 2025-08-19 14:05:03,129 - INFO - >>> {'loss': 1.1862, 'grad_norm': 1.9139437675476074, 'learning_rate': 4.92067018082596e-05, 'epoch': 51.61538461538461}
>>> 2025-08-19 14:05:38,097 - INFO - >>> {'loss': 1.089, 'grad_norm': 1.647046685218811, 'learning_rate': 4.762090420881289e-05, 'epoch': 52.61538461538461}
>>> 2025-08-19 14:06:10,085 - INFO - >>> {'loss': 1.0328, 'grad_norm': 1.6707515716552734, 'learning_rate': 4.603750215716057e-05, 'epoch': 53.61538461538461}
>>> 2025-08-19 14:06:41,407 - INFO - >>> {'loss': 0.9138, 'grad_norm': 1.9002271890640259, 'learning_rate': 4.445809000494946e-05, 'epoch': 54.61538461538461}
>>> 2025-08-19 14:07:18,172 - INFO - >>> {'loss': 0.933, 'grad_norm': 1.6724313497543335, 'learning_rate': 4.288425808633575e-05, 'epoch': 55.61538461538461}
>>> 2025-08-19 14:07:50,615 - INFO - >>> {'loss': 0.807, 'grad_norm': 1.860741138458252, 'learning_rate': 4.131759111665349e-05, 'epoch': 56.61538461538461}
>>> 2025-08-19 14:08:28,226 - INFO - >>> {'loss': 0.7, 'grad_norm': 1.5458564758300781, 'learning_rate': 3.9759666596740476e-05, 'epoch': 57.61538461538461}
>>> 2025-08-19 14:09:00,052 - INFO - >>> {'loss': 0.7096, 'grad_norm': 2.0544636249542236, 'learning_rate': 3.821205322452863e-05, 'epoch': 58.61538461538461}
>>> 2025-08-19 14:09:36,665 - INFO - >>> {'loss': 0.5844, 'grad_norm': 1.506828784942627, 'learning_rate': 3.6676309315498256e-05, 'epoch': 59.61538461538461}
>>> 2025-08-19 14:10:07,663 - INFO - >>> {'loss': 0.4959, 'grad_norm': 1.670697569847107, 'learning_rate': 3.515398123358627e-05, 'epoch': 60.61538461538461}
>>> 2025-08-19 14:10:46,384 - INFO - >>> {'loss': 0.4878, 'grad_norm': 1.5313230752944946, 'learning_rate': 3.364660183412892e-05, 'epoch': 61.61538461538461}
>>> 2025-08-19 14:11:16,825 - INFO - >>> {'loss': 0.4356, 'grad_norm': 1.592433214187622, 'learning_rate': 3.215568892040641e-05, 'epoch': 62.61538461538461}
>>> 2025-08-19 14:11:53,575 - INFO - >>> {'loss': 0.3756, 'grad_norm': 1.416720986366272, 'learning_rate': 3.0682743715343564e-05, 'epoch': 63.61538461538461}
>>> 2025-08-19 14:12:27,113 - INFO - >>> {'loss': 0.3455, 'grad_norm': 1.3762496709823608, 'learning_rate': 2.9229249349905684e-05, 'epoch': 64.61538461538461}
>>> 2025-08-19 14:13:03,177 - INFO - >>> {'loss': 0.281, 'grad_norm': 1.1830974817276, 'learning_rate': 2.7796669369711294e-05, 'epoch': 65.61538461538461}
>>> 2025-08-19 14:13:37,514 - INFO - >>> {'loss': 0.2849, 'grad_norm': 1.136816143989563, 'learning_rate': 2.638644626136587e-05, 'epoch': 66.61538461538461}
>>> 2025-08-19 14:14:16,243 - INFO - >>> {'loss': 0.2265, 'grad_norm': 1.0355185270309448, 'learning_rate': 2.500000000000001e-05, 'epoch': 67.61538461538461}
>>> 2025-08-19 14:14:53,354 - INFO - >>> {'loss': 0.2147, 'grad_norm': 1.0757410526275635, 'learning_rate': 2.363872661947488e-05, 'epoch': 68.61538461538461}
>>> 2025-08-19 14:15:30,731 - INFO - >>> {'loss': 0.171, 'grad_norm': 0.9126036167144775, 'learning_rate': 2.2303996806694488e-05, 'epoch': 69.61538461538461}
>>> 2025-08-19 14:16:07,030 - INFO - >>> {'loss': 0.1711, 'grad_norm': 0.9779925346374512, 'learning_rate': 2.09971545214401e-05, 'epoch': 70.61538461538461}
>>> 2025-08-19 14:16:38,207 - INFO - >>> {'loss': 0.1283, 'grad_norm': 0.7957079410552979, 'learning_rate': 1.9719515643116674e-05, 'epoch': 71.61538461538461}
>>> 2025-08-19 14:17:11,184 - INFO - >>> {'loss': 0.1484, 'grad_norm': 0.890861988067627, 'learning_rate': 1.847236664577389e-05, 'epoch': 72.61538461538461}
>>> 2025-08-19 14:17:45,500 - INFO - >>> {'loss': 0.1271, 'grad_norm': 0.6172863245010376, 'learning_rate': 1.725696330273575e-05, 'epoch': 73.61538461538461}
>>> 2025-08-19 14:18:23,352 - INFO - >>> {'loss': 0.093, 'grad_norm': 0.6864464282989502, 'learning_rate': 1.60745294221434e-05, 'epoch': 74.61538461538461}
>>> 2025-08-19 14:18:55,897 - INFO - >>> {'loss': 0.113, 'grad_norm': 0.6107854843139648, 'learning_rate': 1.4926255614683932e-05, 'epoch': 75.61538461538461}
>>> 2025-08-19 14:19:28,775 - INFO - >>> {'loss': 0.0979, 'grad_norm': 0.6053614616394043, 'learning_rate': 1.3813298094746491e-05, 'epoch': 76.61538461538461}
>>> 2025-08-19 14:20:06,179 - INFO - >>> {'loss': 0.0719, 'grad_norm': 0.5293943285942078, 'learning_rate': 1.2736777516212266e-05, 'epoch': 77.61538461538461}
>>> 2025-08-19 14:20:42,465 - INFO - >>> {'loss': 0.0712, 'grad_norm': 0.5337038636207581, 'learning_rate': 1.1697777844051105e-05, 'epoch': 78.61538461538461}
>>> 2025-08-19 14:21:17,834 - INFO - >>> {'loss': 0.0906, 'grad_norm': 0.5154390931129456, 'learning_rate': 1.0697345262860636e-05, 'epoch': 79.61538461538461}
>>> 2025-08-19 14:21:54,323 - INFO - >>> {'loss': 0.072, 'grad_norm': 0.5806818604469299, 'learning_rate': 9.73648712344707e-06, 'epoch': 80.61538461538461}
>>> 2025-08-19 14:22:27,429 - INFO - >>> {'loss': 0.0618, 'grad_norm': 0.5036183595657349, 'learning_rate': 8.816170928508365e-06, 'epoch': 81.61538461538461}
>>> 2025-08-19 14:23:04,356 - INFO - >>> {'loss': 0.0718, 'grad_norm': 0.48383626341819763, 'learning_rate': 7.937323358440935e-06, 'epoch': 82.61538461538461}
>>> 2025-08-19 14:23:36,371 - INFO - >>> {'loss': 0.0646, 'grad_norm': 0.4430064260959625, 'learning_rate': 7.100829338251147e-06, 'epoch': 83.61538461538461}
>>> 2025-08-19 14:24:12,632 - INFO - >>> {'loss': 0.0651, 'grad_norm': 0.40839314460754395, 'learning_rate': 6.3075311465107535e-06, 'epoch': 84.61538461538461}
>>> 2025-08-19 14:24:42,895 - INFO - >>> {'loss': 0.0446, 'grad_norm': 0.3756992816925049, 'learning_rate': 5.558227567253832e-06, 'epoch': 85.61538461538461}
>>> 2025-08-19 14:25:18,318 - INFO - >>> {'loss': 0.0666, 'grad_norm': 0.4497193694114685, 'learning_rate': 4.853673085668947e-06, 'epoch': 86.61538461538461}
>>> 2025-08-19 14:25:52,206 - INFO - >>> {'loss': 0.06, 'grad_norm': 0.39147189259529114, 'learning_rate': 4.19457712839652e-06, 'epoch': 87.61538461538461}
>>> 2025-08-19 14:26:22,239 - INFO - >>> {'loss': 0.0406, 'grad_norm': 0.3916855454444885, 'learning_rate': 3.581603349196372e-06, 'epoch': 88.61538461538461}
>>> 2025-08-19 14:26:55,434 - INFO - >>> {'loss': 0.0555, 'grad_norm': 0.3805268108844757, 'learning_rate': 3.0153689607045845e-06, 'epoch': 89.61538461538461}
>>> 2025-08-19 14:27:29,609 - INFO - >>> {'loss': 0.041, 'grad_norm': 0.4124501943588257, 'learning_rate': 2.496444112952734e-06, 'epoch': 90.61538461538461}
>>> 2025-08-19 14:28:02,786 - INFO - >>> {'loss': 0.0613, 'grad_norm': 0.4203071594238281, 'learning_rate': 2.0253513192751373e-06, 'epoch': 91.61538461538461}
>>> 2025-08-19 14:28:35,976 - INFO - >>> {'loss': 0.0455, 'grad_norm': 0.35574978590011597, 'learning_rate': 1.6025649301821876e-06, 'epoch': 92.61538461538461}
>>> 2025-08-19 14:29:11,848 - INFO - >>> {'loss': 0.0498, 'grad_norm': 0.35088008642196655, 'learning_rate': 1.2285106557296477e-06, 'epoch': 93.61538461538461}
>>> 2025-08-19 14:29:44,262 - INFO - >>> {'loss': 0.0438, 'grad_norm': 0.3193173408508301, 'learning_rate': 9.035651368646648e-07, 'epoch': 94.61538461538461}
>>> 2025-08-19 14:30:20,202 - INFO - >>> {'loss': 0.0533, 'grad_norm': 0.40145936608314514, 'learning_rate': 6.280555661802856e-07, 'epoch': 95.61538461538461}
>>> 2025-08-19 14:30:53,965 - INFO - >>> {'loss': 0.046, 'grad_norm': 0.32515791058540344, 'learning_rate': 4.02259358460233e-07, 'epoch': 96.61538461538461}
>>> 2025-08-19 14:31:26,755 - INFO - >>> {'loss': 0.0469, 'grad_norm': 0.39581167697906494, 'learning_rate': 2.2640387134577058e-07, 'epoch': 97.61538461538461}
>>> 2025-08-19 14:32:02,339 - INFO - >>> {'loss': 0.0523, 'grad_norm': 0.36767053604125977, 'learning_rate': 1.0066617640578368e-07, 'epoch': 98.61538461538461}
>>> 2025-08-19 14:32:34,425 - INFO - >>> {'loss': 0.0421, 'grad_norm': 0.40421825647354126, 'learning_rate': 2.5172880840745873e-08, 'epoch': 99.61538461538461}
>>> 2025-08-19 14:32:35,392 - INFO - >>> {'train_runtime': 3527.9182, 'train_samples_per_second': 2.835, 'train_steps_per_second': 0.028, 'train_loss': 1.9205697426944972, 'epoch': 99.61538461538461}
>>> 2025-08-19 14:32:35,394 - INFO - 训练成功！
>>> 2025-08-19 14:32:35,394 - INFO - 模型存放位置：./output/qwen202508191333
>>> 2025-08-19 17:23:36,220 - INFO - ========__main__  202508191723========
>>> 2025-08-19 17:23:36,220 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 17:23:36,221 - INFO - 开始进行模型测试
>>> 2025-08-19 17:23:48,853 - INFO - 已选择模型文件夹: qwen202508191333
>>> 2025-08-19 17:23:48,856 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508191333/checkpoint-100
>>> 2025-08-19 17:27:09,710 - INFO - ========__main__  202508191727========
>>> 2025-08-19 17:27:09,711 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 17:27:09,711 - INFO - 开始进行模型测试
>>> 2025-08-19 17:27:46,631 - INFO - 已选择模型文件夹: qwen202508191333
>>> 2025-08-19 17:27:46,634 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508191333/checkpoint-100
>>> 2025-08-19 17:31:37,730 - INFO - ========__main__  202508191731========
>>> 2025-08-19 17:31:37,731 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 17:31:37,731 - INFO - 开始进行模型测试
>>> 2025-08-19 17:31:44,897 - INFO - 已选择模型文件夹: qwen202508191333
>>> 2025-08-19 17:31:44,900 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508191333/checkpoint-100
>>> 2025-08-19 17:34:12,537 - INFO - ========__main__  202508191734========
>>> 2025-08-19 17:34:12,537 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 17:34:12,538 - INFO - 开始进行模型测试
>>> 2025-08-19 17:34:15,585 - INFO - 已选择模型文件夹: qwen202508191333
>>> 2025-08-19 17:34:15,588 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508191333/checkpoint-100
>>> 2025-08-19 17:39:33,293 - INFO - ========__main__  202508191739========
>>> 2025-08-19 17:39:33,293 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 17:39:33,294 - INFO - 开始进行模型测试
>>> 2025-08-19 17:39:39,963 - INFO - 已选择模型文件夹: qwen202508182252
>>> 2025-08-19 17:39:39,966 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508182252/checkpoint-600
>>> 2025-08-19 17:41:41,693 - INFO - ========__main__  202508191741========
>>> 2025-08-19 17:41:41,694 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 17:41:41,695 - INFO - 开始进行模型测试
>>> 2025-08-19 17:41:44,534 - INFO - 已选择模型文件夹: qwen202508182252
>>> 2025-08-19 17:41:44,537 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508182252/checkpoint-600
>>> 2025-08-19 17:46:24,511 - INFO - ========__main__  202508191746========
>>> 2025-08-19 17:46:24,511 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 17:46:24,512 - INFO - 开始进行模型测试
>>> 2025-08-19 17:46:41,803 - INFO - 已选择模型文件夹: qwen202508182252
>>> 2025-08-19 17:46:41,806 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508182252/checkpoint-600
>>> 2025-08-19 20:47:13,937 - INFO - 开始进行原始模型对话测试
>>> 2025-08-19 20:47:14,385 - INFO - 导入包完成
>>> 2025-08-19 20:47:14,400 - INFO - 配置文件读取完成
>>> 2025-08-19 23:25:47,328 - INFO - 开始进行原始模型对话测试
>>> 2025-08-19 23:25:50,035 - INFO - 导入包完成
>>> 2025-08-19 23:25:50,052 - INFO - 配置文件读取完成
>>> 2025-08-19 23:27:50,126 - INFO - ========__main__  202508192327========
>>> 2025-08-19 23:27:50,127 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 23:27:50,127 - INFO - 开始进行模型测试
>>> 2025-08-19 23:27:55,230 - INFO - 已选择模型文件夹: qwen202508191333
>>> 2025-08-19 23:27:55,233 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508191333/checkpoint-100
>>> 2025-08-19 23:28:23,077 - INFO - ========__main__  202508192328========
>>> 2025-08-19 23:28:23,077 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-08-19 23:28:23,078 - INFO - 开始进行模型测试
>>> 2025-08-19 23:28:32,633 - INFO - 已选择模型文件夹: qwen202508191333
>>> 2025-08-19 23:28:32,636 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508191333/checkpoint-100
>>> 2025-08-20 15:00:03,722 - INFO - ========__main__  202508201500========
>>> 2025-08-20 15:00:03,723 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-20 15:00:03,725 - INFO - 开始进行模型测试
>>> 2025-08-20 15:00:15,032 - INFO - 已选择模型文件夹: qwen202508191333
>>> 2025-08-20 15:00:15,035 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508191333/checkpoint-100
>>> 2025-08-20 19:36:00,672 - INFO - 正在读取配置文件: config.yaml
>>> 2025-08-20 19:36:00,678 - INFO - 从配置文件中提取到模型路径: /home/liangshuqiao/models/gpt-oss
>>> 2025-08-20 19:36:00,678 - INFO - 开始加载模型配置文件...
>>> 2025-08-20 19:36:00,678 - INFO - 正在加载模型: /home/liangshuqiao/models/gpt-oss
>>> 2025-08-20 19:36:33,567 - ERROR - 加载模型时出错: CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 31.73 GiB of which 430.19 MiB is free. Including non-PyTorch memory, this process has 31.31 GiB memory in use. Of the allocated memory 28.10 GiB is allocated by PyTorch, and 2.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-08-20 19:36:34,642 - ERROR - 无法获取模型信息
>>> 2025-08-20 20:04:45,769 - INFO - 导入包完成
>>> 2025-08-20 20:04:45,769 - INFO - ========train Qwen2ForCausalLM  202508202004========
>>> 2025-08-20 20:04:45,770 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-20 20:04:45,770 - INFO - 开始进行训练
>>> 2025-08-20 20:04:45,784 - INFO - 配置文件读取完成
>>> 2025-08-20 20:04:45,784 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-20 20:04:45,785 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-08-20 20:04:46,272 - INFO - tokenizer读取完成
>>> 2025-08-20 20:04:49,613 - INFO - model dtype:torch.float16
>>> 2025-08-20 20:04:49,613 - INFO - 模型导入完成
>>> 2025-08-20 20:04:49,614 - INFO - 数据读取开始
>>> 2025-08-20 20:04:50,853 - INFO - 数据下载完成
>>> 2025-08-20 20:05:10,687 - INFO - 开始训练！
>>> 2025-08-20 20:05:10,688 - INFO - 批次大小  : 8
>>> 2025-08-20 20:05:10,688 - INFO - 训练轮数  : 100
>>> 2025-08-20 20:05:10,688 - INFO - 学习率    : 0.0001
>>> 2025-08-20 20:05:10,689 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-20 20:05:10,689 - INFO - 模型路径  : /home/liangshuqiao/models/qwen3-8b
>>> 2025-08-20 20:19:19,622 - INFO - 导入包完成
>>> 2025-08-20 20:19:19,623 - INFO - ========train Qwen2ForCausalLM  202508202019========
>>> 2025-08-20 20:19:19,623 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-20 20:19:19,624 - INFO - 开始进行训练
>>> 2025-08-20 20:19:19,636 - INFO - 基础配置文件读取完成
>>> 2025-08-20 20:19:19,643 - INFO - 训练配置读取完成
>>> 2025-08-20 20:19:19,644 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-20 20:19:19,644 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-08-20 20:19:20,105 - INFO - tokenizer读取完成
>>> 2025-08-20 20:19:22,238 - INFO - model dtype:torch.float16
>>> 2025-08-20 20:19:22,238 - INFO - 模型导入完成
>>> 2025-08-20 20:19:22,239 - INFO - 数据读取开始
>>> 2025-08-20 20:19:23,185 - INFO - 数据下载完成
>>> 2025-08-20 20:19:27,384 - INFO - 数据映射完成
>>> 2025-08-20 20:21:49,418 - INFO - 导入包完成
>>> 2025-08-20 20:21:49,418 - INFO - ========train Qwen2ForCausalLM  202508202021========
>>> 2025-08-20 20:21:49,419 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-20 20:21:49,419 - INFO - 开始进行训练
>>> 2025-08-20 20:21:49,432 - INFO - 基础配置文件读取完成
>>> 2025-08-20 20:21:49,439 - INFO - 训练配置读取完成
>>> 2025-08-20 20:21:49,440 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-20 20:21:49,440 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-08-20 20:21:49,902 - INFO - tokenizer读取完成
>>> 2025-08-20 20:21:52,101 - INFO - model dtype:torch.float16
>>> 2025-08-20 20:21:52,102 - INFO - 模型导入完成
>>> 2025-08-20 20:21:52,102 - INFO - 数据读取开始
>>> 2025-08-20 20:21:53,106 - INFO - 数据下载完成
>>> 2025-08-20 20:21:57,272 - INFO - 数据映射完成
>>> 2025-08-20 20:22:55,276 - INFO - 导入包完成
>>> 2025-08-20 20:22:55,277 - INFO - ========train Qwen2ForCausalLM  202508202022========
>>> 2025-08-20 20:22:55,277 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-20 20:22:55,278 - INFO - 开始进行训练
>>> 2025-08-20 20:22:55,291 - INFO - 基础配置文件读取完成
>>> 2025-08-20 20:22:55,298 - INFO - 训练配置读取完成
>>> 2025-08-20 20:22:55,298 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-20 20:22:55,299 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-08-20 20:22:55,760 - INFO - tokenizer读取完成
>>> 2025-08-20 20:22:57,819 - INFO - model dtype:torch.float16
>>> 2025-08-20 20:22:57,820 - INFO - 模型导入完成
>>> 2025-08-20 20:22:57,820 - INFO - 数据读取开始
>>> 2025-08-20 20:22:58,595 - INFO - 数据下载完成
>>> 2025-08-20 20:23:02,785 - INFO - 数据映射完成
>>> 2025-08-20 20:23:15,086 - INFO - 开始训练！
>>> 2025-08-20 20:23:15,087 - INFO - 批次大小  : 8
>>> 2025-08-20 20:23:15,087 - INFO - 训练轮数  : 100
>>> 2025-08-20 20:23:15,087 - INFO - 学习率    : 0.0001
>>> 2025-08-20 20:23:15,088 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-20 20:23:15,088 - INFO - 模型路径  : /home/liangshuqiao/models/qwen3-8b
>>> 2025-08-21 12:13:12,334 - INFO - ==========gpt oss模型微调脚本启动==========
>>> 2025-08-21 12:13:12,335 - INFO - 模块导入完成
>>> 2025-08-21 12:13:12,355 - INFO - 配置导入完成
>>> 2025-08-21 12:14:10,415 - INFO - ==========gpt oss模型微调脚本启动==========
>>> 2025-08-21 12:14:10,415 - INFO - 模块导入完成
>>> 2025-08-21 12:14:10,435 - INFO - 配置导入完成
>>> 2025-08-21 12:19:06,763 - INFO - ==========gpt oss模型微调脚本启动==========
>>> 2025-08-21 12:19:06,764 - INFO - 模块导入完成
>>> 2025-08-21 12:19:06,784 - INFO - 配置导入完成
>>> 2025-08-21 12:19:10,721 - INFO - 数据读取开始
>>> 2025-08-21 12:21:45,570 - INFO - ==========gpt oss模型微调脚本启动==========
>>> 2025-08-21 12:21:45,571 - INFO - 模块导入完成
>>> 2025-08-21 12:21:45,590 - INFO - 配置导入完成
>>> 2025-08-21 12:21:48,463 - INFO - 模型和分词器加载完成
>>> 2025-08-21 12:21:48,463 - INFO - 数据读取开始
>>> 2025-08-21 12:21:49,273 - INFO - 数据下载完成
>>> 2025-08-21 12:21:49,476 - INFO - 数据映射完成
>>> 2025-08-21 12:21:49,477 - INFO - 数据集加载完成
>>> 2025-08-21 12:28:57,053 - INFO - ==========gpt oss模型微调脚本启动==========
>>> 2025-08-21 12:28:57,054 - INFO - 模块导入完成
>>> 2025-08-21 12:28:57,073 - INFO - 配置导入完成
>>> 2025-08-21 12:28:59,795 - INFO - 模型和分词器加载完成
>>> 2025-08-21 12:28:59,795 - INFO - 数据读取开始
>>> 2025-08-21 12:29:00,617 - INFO - 数据下载完成
>>> 2025-08-21 12:29:00,723 - INFO - 数据映射完成
>>> 2025-08-21 12:29:00,724 - INFO - 数据集加载完成
>>> 2025-08-21 12:29:00,724 - INFO - 打印训练参数如下
>>> 2025-08-21 12:29:00,725 - INFO - task_type >>> CAUSAL_LM
>>> 2025-08-21 12:29:00,725 - INFO - dtype >>> torch.float16
>>> 2025-08-21 12:29:00,725 - INFO - load_in_4bit >>> True
>>> 2025-08-21 12:29:00,726 - INFO - batch_size >>> 8
>>> 2025-08-21 12:29:00,726 - INFO - gradient_accumulator_steps >>> 8
>>> 2025-08-21 12:29:00,726 - INFO - warmup_steps >>> 1
>>> 2025-08-21 12:29:00,727 - INFO - epoch >>> 100
>>> 2025-08-21 12:29:00,727 - INFO - eval_steps >>> 10
>>> 2025-08-21 12:29:00,727 - INFO - learning_rate >>> 0.0001
>>> 2025-08-21 12:29:00,728 - INFO - lr_scheduler_type >>> cosine
>>> 2025-08-21 12:29:00,728 - INFO - max_seq_length >>> 4096
>>> 2025-08-21 12:29:00,729 - INFO - use_history >>> False
>>> 2025-08-21 12:29:00,729 - INFO - r >>> 8
>>> 2025-08-21 12:29:00,729 - INFO - interface_mode >>> False
>>> 2025-08-21 12:29:00,730 - INFO - target_modules >>> ['q_proj', 'v_proj']
>>> 2025-08-21 12:29:00,730 - INFO - lora_alpha >>> 16
>>> 2025-08-21 12:29:00,730 - INFO - lora_dropout >>> 0.05
>>> 2025-08-21 12:29:00,731 - INFO - bias >>> none
>>> 2025-08-21 12:29:00,731 - INFO - use_gradient_checkpointing >>> unsloth
>>> 2025-08-21 12:29:00,731 - INFO - random_state >>> 3407
>>> 2025-08-21 12:29:00,732 - INFO - use_rslora >>> True
>>> 2025-08-21 12:29:00,732 - INFO - loftq_config >>> None
>>> 2025-08-21 12:50:56,058 - INFO - 导入包完成
>>> 2025-08-21 12:50:56,059 - INFO - ========train Qwen2ForCausalLM  202508211250========
>>> 2025-08-21 12:50:56,059 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-21 12:50:56,060 - INFO - 开始进行训练
>>> 2025-08-21 12:50:56,072 - INFO - 基础配置文件读取完成
>>> 2025-08-21 12:50:56,078 - INFO - 训练配置读取完成
>>> 2025-08-21 12:50:56,079 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-21 12:50:56,079 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-08-21 12:50:56,525 - INFO - tokenizer读取完成
>>> 2025-08-21 12:50:58,670 - INFO - model dtype:torch.float16
>>> 2025-08-21 12:50:58,671 - INFO - 模型导入完成
>>> 2025-08-21 12:50:58,671 - INFO - 数据读取开始
>>> 2025-08-21 12:50:59,479 - INFO - 数据下载完成
>>> 2025-08-21 12:51:03,704 - INFO - 数据映射完成
>>> 2025-08-21 12:51:03,705 - INFO - 打印训练参数如下
>>> 2025-08-21 12:51:03,705 - INFO - task_type >>> CAUSAL_LM
>>> 2025-08-21 12:51:03,705 - INFO - dtype >>> torch.float16
>>> 2025-08-21 12:51:03,706 - INFO - load_in_4bit >>> True
>>> 2025-08-21 12:51:03,706 - INFO - batch_size >>> 8
>>> 2025-08-21 12:51:03,706 - INFO - gradient_accumulator_steps >>> 8
>>> 2025-08-21 12:51:03,707 - INFO - warmup_steps >>> 1
>>> 2025-08-21 12:51:03,707 - INFO - epoch >>> 100
>>> 2025-08-21 12:51:03,707 - INFO - eval_steps >>> 10
>>> 2025-08-21 12:51:03,708 - INFO - learning_rate >>> 0.0001
>>> 2025-08-21 12:51:03,708 - INFO - lr_scheduler_type >>> cosine
>>> 2025-08-21 12:51:03,708 - INFO - max_seq_length >>> 4096
>>> 2025-08-21 12:51:03,709 - INFO - r >>> 8
>>> 2025-08-21 12:51:03,709 - INFO - interface_mode >>> False
>>> 2025-08-21 12:51:03,709 - INFO - target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-08-21 12:51:03,710 - INFO - lora_alpha >>> 16
>>> 2025-08-21 12:51:03,710 - INFO - lora_dropout >>> 0.05
>>> 2025-08-21 12:51:54,762 - INFO - 导入包完成
>>> 2025-08-21 12:51:54,762 - INFO - ========train Qwen2ForCausalLM  202508211251========
>>> 2025-08-21 12:51:54,763 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-21 12:51:54,763 - INFO - 开始进行训练
>>> 2025-08-21 12:51:54,775 - INFO - 基础配置文件读取完成
>>> 2025-08-21 12:51:54,782 - INFO - 训练配置读取完成
>>> 2025-08-21 12:51:54,782 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-21 12:51:54,782 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-08-21 12:51:55,228 - INFO - tokenizer读取完成
>>> 2025-08-21 12:51:57,345 - INFO - model dtype:torch.float16
>>> 2025-08-21 12:51:57,346 - INFO - 模型导入完成
>>> 2025-08-21 12:51:57,346 - INFO - 数据读取开始
>>> 2025-08-21 12:51:58,105 - INFO - 数据下载完成
>>> 2025-08-21 12:52:02,430 - INFO - 数据映射完成
>>> 2025-08-21 12:52:02,430 - INFO - 打印训练参数如下
>>> 2025-08-21 12:52:02,431 - INFO - task_type >>> CAUSAL_LM
>>> 2025-08-21 12:52:02,431 - INFO - dtype >>> torch.float16
>>> 2025-08-21 12:52:02,431 - INFO - load_in_4bit >>> True
>>> 2025-08-21 12:52:02,432 - INFO - batch_size >>> 8
>>> 2025-08-21 12:52:02,432 - INFO - gradient_accumulator_steps >>> 8
>>> 2025-08-21 12:52:02,432 - INFO - warmup_steps >>> 1
>>> 2025-08-21 12:52:02,433 - INFO - epoch >>> 100
>>> 2025-08-21 12:52:02,433 - INFO - eval_steps >>> 10
>>> 2025-08-21 12:52:02,433 - INFO - learning_rate >>> 0.0001
>>> 2025-08-21 12:52:02,434 - INFO - lr_scheduler_type >>> cosine
>>> 2025-08-21 12:52:02,434 - INFO - max_seq_length >>> 4096
>>> 2025-08-21 12:52:02,435 - INFO - r >>> 8
>>> 2025-08-21 12:52:02,435 - INFO - interface_mode >>> False
>>> 2025-08-21 12:52:02,435 - INFO - target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-08-21 12:52:02,436 - INFO - lora_alpha >>> 16
>>> 2025-08-21 12:52:02,436 - INFO - lora_dropout >>> 0.05
>>> 2025-08-21 12:52:02,436 - INFO - use_gradient_checkpointing >>> True
>>> 2025-08-21 12:52:25,982 - INFO - 导入包完成
>>> 2025-08-21 12:52:25,983 - INFO - ========train Qwen2ForCausalLM  202508211252========
>>> 2025-08-21 12:52:25,983 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-21 12:52:25,984 - INFO - 开始进行训练
>>> 2025-08-21 12:52:25,996 - INFO - 基础配置文件读取完成
>>> 2025-08-21 12:52:26,003 - INFO - 训练配置读取完成
>>> 2025-08-21 12:52:26,003 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-21 12:52:26,004 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-08-21 12:52:26,460 - INFO - tokenizer读取完成
>>> 2025-08-21 12:52:28,589 - INFO - model dtype:torch.float16
>>> 2025-08-21 12:52:28,590 - INFO - 模型导入完成
>>> 2025-08-21 12:52:28,590 - INFO - 数据读取开始
>>> 2025-08-21 12:52:29,547 - INFO - 数据下载完成
>>> 2025-08-21 12:52:33,622 - INFO - 数据映射完成
>>> 2025-08-21 12:52:33,623 - INFO - 打印训练参数如下
>>> 2025-08-21 12:52:33,623 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-08-21 12:52:33,624 - INFO -   dtype >>> torch.float16
>>> 2025-08-21 12:52:33,624 - INFO -   load_in_4bit >>> True
>>> 2025-08-21 12:52:33,624 - INFO -   batch_size >>> 8
>>> 2025-08-21 12:52:33,625 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-08-21 12:52:33,625 - INFO -   warmup_steps >>> 1
>>> 2025-08-21 12:52:33,625 - INFO -   epoch >>> 100
>>> 2025-08-21 12:52:33,626 - INFO -   eval_steps >>> 10
>>> 2025-08-21 12:52:33,626 - INFO -   learning_rate >>> 0.0001
>>> 2025-08-21 12:52:33,627 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-08-21 12:52:33,627 - INFO -   max_seq_length >>> 4096
>>> 2025-08-21 12:52:33,627 - INFO -   r >>> 8
>>> 2025-08-21 12:52:33,628 - INFO -   interface_mode >>> False
>>> 2025-08-21 12:52:33,628 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-08-21 12:52:33,628 - INFO -   lora_alpha >>> 16
>>> 2025-08-21 12:52:33,629 - INFO -   lora_dropout >>> 0.05
>>> 2025-08-21 12:52:33,629 - INFO -   use_gradient_checkpointing >>> True
>>> 2025-08-21 12:52:40,772 - INFO - 开始训练！
>>> 2025-08-21 12:52:40,772 - INFO - 批次大小  : 8
>>> 2025-08-21 12:52:40,773 - INFO - 训练轮数  : 100
>>> 2025-08-21 12:52:40,773 - INFO - 学习率    : 0.0001
>>> 2025-08-21 12:52:40,773 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-21 12:52:40,774 - INFO - 模型路径  : /home/liangshuqiao/models/qwen3-8b
>>> 2025-08-21 12:52:58,272 - INFO - >>> {'loss': 3.73, 'grad_norm': 1.7611726522445679, 'learning_rate': 0.0, 'epoch': 0.6153846153846154}
>>> 2025-08-21 12:53:10,348 - INFO - >>> {'loss': 3.662, 'grad_norm': 1.7281666994094849, 'learning_rate': 0.0001, 'epoch': 1.0}
>>> 2025-08-21 12:53:29,892 - INFO - >>> {'loss': 3.6581, 'grad_norm': 1.7684705257415771, 'learning_rate': 9.999376947588288e-05, 'epoch': 1.6153846153846154}
>>> 2025-08-21 12:53:40,401 - INFO - >>> {'loss': 3.5416, 'grad_norm': 1.5348093509674072, 'learning_rate': 9.99750794563087e-05, 'epoch': 2.0}
>>> 2025-08-21 12:53:59,138 - INFO - >>> {'loss': 3.4945, 'grad_norm': 1.6653978824615479, 'learning_rate': 9.994393459922218e-05, 'epoch': 2.6153846153846154}
>>> 2025-08-21 12:54:10,658 - INFO - >>> {'loss': 3.3041, 'grad_norm': 1.8730970621109009, 'learning_rate': 9.990034266657467e-05, 'epoch': 3.0}
>>> 2025-08-21 12:54:32,837 - INFO - >>> {'loss': 3.2738, 'grad_norm': 1.7950289249420166, 'learning_rate': 9.984431452238967e-05, 'epoch': 3.6153846153846154}
>>> 2025-08-21 12:54:40,233 - INFO - >>> {'loss': 3.2025, 'grad_norm': 1.9112640619277954, 'learning_rate': 9.977586413005531e-05, 'epoch': 4.0}
>>> 2025-08-21 12:54:59,680 - INFO - >>> {'loss': 3.0919, 'grad_norm': 1.6180789470672607, 'learning_rate': 9.96950085488444e-05, 'epoch': 4.615384615384615}
>>> 2025-08-21 12:55:08,959 - INFO - >>> {'loss': 3.0397, 'grad_norm': 1.5451222658157349, 'learning_rate': 9.960176792966289e-05, 'epoch': 5.0}
>>> 2025-08-21 12:55:26,371 - INFO - >>> {'loss': 2.9455, 'grad_norm': 1.5447489023208618, 'learning_rate': 9.949616551002787e-05, 'epoch': 5.615384615384615}
>>> 2025-08-21 12:55:36,188 - INFO - >>> {'loss': 2.8219, 'grad_norm': 1.3461918830871582, 'learning_rate': 9.93782276082762e-05, 'epoch': 6.0}
>>> 2025-08-21 12:55:52,346 - INFO - >>> {'loss': 2.7662, 'grad_norm': 1.2539334297180176, 'learning_rate': 9.924798361700553e-05, 'epoch': 6.615384615384615}
>>> 2025-08-21 12:56:02,742 - INFO - >>> {'loss': 2.6922, 'grad_norm': 1.1172631978988647, 'learning_rate': 9.910546599574902e-05, 'epoch': 7.0}
>>> 2025-08-21 12:56:20,694 - INFO - >>> {'loss': 2.6844, 'grad_norm': 0.9681398272514343, 'learning_rate': 9.895071026288574e-05, 'epoch': 7.615384615384615}
>>> 2025-08-21 12:56:30,762 - INFO - >>> {'loss': 2.5409, 'grad_norm': 0.8553614616394043, 'learning_rate': 9.87837549867887e-05, 'epoch': 8.0}
>>> 2025-08-21 12:56:48,954 - INFO - >>> {'loss': 2.5479, 'grad_norm': 0.7973280549049377, 'learning_rate': 9.860464177621284e-05, 'epoch': 8.615384615384615}
>>> 2025-08-21 12:56:59,723 - INFO - >>> {'loss': 2.5019, 'grad_norm': 0.8054794669151306, 'learning_rate': 9.841341526992536e-05, 'epoch': 9.0}
>>> 2025-08-21 12:57:16,226 - INFO - >>> {'loss': 2.459, 'grad_norm': 0.6530501842498779, 'learning_rate': 9.821012312558058e-05, 'epoch': 9.615384615384615}
>>> 2025-08-21 12:57:24,717 - INFO - >>> {'loss': 2.444, 'grad_norm': 0.7391039729118347, 'learning_rate': 9.799481600784286e-05, 'epoch': 10.0}
>>> 2025-08-21 12:57:43,421 - INFO - >>> {'loss': 2.3693, 'grad_norm': 0.5826030969619751, 'learning_rate': 9.776754757575975e-05, 'epoch': 10.615384615384615}
>>> 2025-08-21 12:57:53,032 - INFO - >>> {'loss': 2.4286, 'grad_norm': 0.6300196051597595, 'learning_rate': 9.752837446938915e-05, 'epoch': 11.0}
>>> 2025-08-21 12:58:12,365 - INFO - >>> {'loss': 2.3232, 'grad_norm': 0.5197663307189941, 'learning_rate': 9.727735629568336e-05, 'epoch': 11.615384615384615}
>>> 2025-08-21 12:58:21,735 - INFO - >>> {'loss': 2.3585, 'grad_norm': 0.6248791813850403, 'learning_rate': 9.701455561363379e-05, 'epoch': 12.0}
>>> 2025-08-21 12:58:42,020 - INFO - >>> {'loss': 2.2476, 'grad_norm': 0.5572148561477661, 'learning_rate': 9.674003791867991e-05, 'epoch': 12.615384615384615}
>>> 2025-08-21 12:58:51,092 - INFO - >>> {'loss': 2.3746, 'grad_norm': 0.614099383354187, 'learning_rate': 9.645387162638652e-05, 'epoch': 13.0}
>>> 2025-08-21 12:59:07,676 - INFO - >>> {'loss': 2.2737, 'grad_norm': 0.627761960029602, 'learning_rate': 9.615612805539305e-05, 'epoch': 13.615384615384615}
>>> 2025-08-21 12:59:18,782 - INFO - >>> {'loss': 2.2492, 'grad_norm': 0.6148277521133423, 'learning_rate': 9.584688140963944e-05, 'epoch': 14.0}
>>> 2025-08-21 12:59:36,024 - INFO - >>> {'loss': 2.2564, 'grad_norm': 0.6118592619895935, 'learning_rate': 9.552620875987311e-05, 'epoch': 14.615384615384615}
>>> 2025-08-21 12:59:45,402 - INFO - >>> {'loss': 2.1772, 'grad_norm': 0.7514955997467041, 'learning_rate': 9.51941900244412e-05, 'epoch': 15.0}
>>> 2025-08-21 13:00:02,154 - INFO - >>> {'loss': 2.3278, 'grad_norm': 0.6658194065093994, 'learning_rate': 9.485090794937319e-05, 'epoch': 15.615384615384615}
>>> 2025-08-21 13:00:12,808 - INFO - >>> {'loss': 2.0559, 'grad_norm': 0.6075881719589233, 'learning_rate': 9.449644808775902e-05, 'epoch': 16.0}
>>> 2025-08-21 13:00:30,582 - INFO - >>> {'loss': 2.1906, 'grad_norm': 0.6314740180969238, 'learning_rate': 9.413089877842736e-05, 'epoch': 16.615384615384617}
>>> 2025-08-21 13:00:40,913 - INFO - >>> {'loss': 2.1297, 'grad_norm': 0.6167408227920532, 'learning_rate': 9.375435112392969e-05, 'epoch': 17.0}
>>> 2025-08-21 13:00:59,333 - INFO - >>> {'loss': 2.155, 'grad_norm': 0.6248344779014587, 'learning_rate': 9.336689896783573e-05, 'epoch': 17.615384615384617}
>>> 2025-08-21 13:01:07,995 - INFO - >>> {'loss': 2.1159, 'grad_norm': 0.6694375276565552, 'learning_rate': 9.29686388713456e-05, 'epoch': 18.0}
>>> 2025-08-21 13:01:24,850 - INFO - >>> {'loss': 2.1645, 'grad_norm': 0.671242892742157, 'learning_rate': 9.255967008922474e-05, 'epoch': 18.615384615384617}
>>> 2025-08-21 13:01:35,633 - INFO - >>> {'loss': 2.0494, 'grad_norm': 0.6421876549720764, 'learning_rate': 9.214009454506753e-05, 'epoch': 19.0}
>>> 2025-08-21 13:01:53,486 - INFO - >>> {'loss': 2.1158, 'grad_norm': 0.6436594724655151, 'learning_rate': 9.171001680589588e-05, 'epoch': 19.615384615384617}
>>> 2025-08-21 13:02:02,084 - INFO - >>> {'loss': 2.0386, 'grad_norm': 0.7394345998764038, 'learning_rate': 9.126954405609882e-05, 'epoch': 20.0}
>>> 2025-08-21 13:02:21,646 - INFO - >>> {'loss': 2.0943, 'grad_norm': 0.6385970711708069, 'learning_rate': 9.081878607071996e-05, 'epoch': 20.615384615384617}
>>> 2025-08-21 13:02:32,134 - INFO - >>> {'loss': 2.0085, 'grad_norm': 0.6702353954315186, 'learning_rate': 9.035785518809927e-05, 'epoch': 21.0}
>>> 2025-08-21 13:02:49,849 - INFO - >>> {'loss': 2.0438, 'grad_norm': 0.6581701636314392, 'learning_rate': 8.988686628187597e-05, 'epoch': 21.615384615384617}
>>> 2025-08-21 13:03:01,531 - INFO - >>> {'loss': 2.013, 'grad_norm': 0.5902714133262634, 'learning_rate': 8.940593673235962e-05, 'epoch': 22.0}
>>> 2025-08-21 13:03:19,014 - INFO - >>> {'loss': 2.0351, 'grad_norm': 0.5972193479537964, 'learning_rate': 8.891518639727649e-05, 'epoch': 22.615384615384617}
>>> 2025-08-21 13:03:27,934 - INFO - >>> {'loss': 1.965, 'grad_norm': 0.6542006134986877, 'learning_rate': 8.841473758189854e-05, 'epoch': 23.0}
>>> 2025-08-21 13:03:45,501 - INFO - >>> {'loss': 2.0312, 'grad_norm': 0.6550471782684326, 'learning_rate': 8.790471500856228e-05, 'epoch': 23.615384615384617}
>>> 2025-08-21 13:03:54,378 - INFO - >>> {'loss': 1.8922, 'grad_norm': 0.6302747130393982, 'learning_rate': 8.738524578558547e-05, 'epoch': 24.0}
>>> 2025-08-21 13:04:12,875 - INFO - >>> {'loss': 1.9754, 'grad_norm': 0.6683315634727478, 'learning_rate': 8.685645937558896e-05, 'epoch': 24.615384615384617}
>>> 2025-08-21 13:04:22,462 - INFO - >>> {'loss': 1.9117, 'grad_norm': 0.63786381483078, 'learning_rate': 8.631848756323197e-05, 'epoch': 25.0}
>>> 2025-08-21 13:04:39,247 - INFO - >>> {'loss': 1.8806, 'grad_norm': 0.626786470413208, 'learning_rate': 8.577146442236857e-05, 'epoch': 25.615384615384617}
>>> 2025-08-21 13:04:50,921 - INFO - >>> {'loss': 2.0117, 'grad_norm': 0.6349155306816101, 'learning_rate': 8.521552628263362e-05, 'epoch': 26.0}
>>> 2025-08-21 13:05:05,385 - INFO - >>> {'loss': 1.8781, 'grad_norm': 0.6754645109176636, 'learning_rate': 8.465081169546659e-05, 'epoch': 26.615384615384617}
>>> 2025-08-21 13:05:16,818 - INFO - >>> {'loss': 1.939, 'grad_norm': 0.6116978526115417, 'learning_rate': 8.40774613995817e-05, 'epoch': 27.0}
>>> 2025-08-21 13:05:36,257 - INFO - >>> {'loss': 1.9143, 'grad_norm': 0.6869587898254395, 'learning_rate': 8.349561828589277e-05, 'epoch': 27.615384615384617}
>>> 2025-08-21 13:05:44,155 - INFO - >>> {'loss': 1.807, 'grad_norm': 0.6852778196334839, 'learning_rate': 8.290542736190188e-05, 'epoch': 28.0}
>>> 2025-08-21 13:06:02,718 - INFO - >>> {'loss': 1.8988, 'grad_norm': 0.6782939434051514, 'learning_rate': 8.230703571556048e-05, 'epoch': 28.615384615384617}
>>> 2025-08-21 13:06:13,254 - INFO - >>> {'loss': 1.7839, 'grad_norm': 0.6461690664291382, 'learning_rate': 8.170059247861194e-05, 'epoch': 29.0}
>>> 2025-08-21 13:06:32,606 - INFO - >>> {'loss': 1.8935, 'grad_norm': 0.6765950918197632, 'learning_rate': 8.108624878942477e-05, 'epoch': 29.615384615384617}
>>> 2025-08-21 13:06:39,923 - INFO - >>> {'loss': 1.7037, 'grad_norm': 0.7754513025283813, 'learning_rate': 8.046415775532585e-05, 'epoch': 30.0}
>>> 2025-08-21 13:06:59,398 - INFO - >>> {'loss': 1.8432, 'grad_norm': 0.6450155973434448, 'learning_rate': 7.983447441444281e-05, 'epoch': 30.615384615384617}
>>> 2025-08-21 13:07:08,560 - INFO - >>> {'loss': 1.7631, 'grad_norm': 0.655302107334137, 'learning_rate': 7.919735569706533e-05, 'epoch': 31.0}
>>> 2025-08-21 13:07:24,403 - INFO - >>> {'loss': 1.7352, 'grad_norm': 0.6282192468643188, 'learning_rate': 7.855296038653475e-05, 'epoch': 31.615384615384617}
>>> 2025-08-21 13:07:35,165 - INFO - >>> {'loss': 1.8917, 'grad_norm': 0.6498188972473145, 'learning_rate': 7.790144907967201e-05, 'epoch': 32.0}
>>> 2025-08-21 13:07:53,257 - INFO - >>> {'loss': 1.7438, 'grad_norm': 0.5832468271255493, 'learning_rate': 7.724298414675353e-05, 'epoch': 32.61538461538461}
>>> 2025-08-21 13:08:02,492 - INFO - >>> {'loss': 1.8238, 'grad_norm': 0.7434511184692383, 'learning_rate': 7.657772969104508e-05, 'epoch': 33.0}
>>> 2025-08-21 13:08:18,878 - INFO - >>> {'loss': 1.7482, 'grad_norm': 0.6160987019538879, 'learning_rate': 7.590585150790389e-05, 'epoch': 33.61538461538461}
>>> 2025-08-21 13:08:28,926 - INFO - >>> {'loss': 1.7454, 'grad_norm': 0.7913209199905396, 'learning_rate': 7.522751704345887e-05, 'epoch': 34.0}
>>> 2025-08-21 13:08:47,690 - INFO - >>> {'loss': 1.7105, 'grad_norm': 0.573047935962677, 'learning_rate': 7.454289535287968e-05, 'epoch': 34.61538461538461}
>>> 2025-08-21 13:08:56,767 - INFO - >>> {'loss': 1.7553, 'grad_norm': 0.9400175213813782, 'learning_rate': 7.385215705824449e-05, 'epoch': 35.0}
>>> 2025-08-21 13:09:16,316 - INFO - >>> {'loss': 1.7285, 'grad_norm': 0.5779616236686707, 'learning_rate': 7.31554743060174e-05, 'epoch': 35.61538461538461}
>>> 2025-08-21 13:09:25,709 - INFO - >>> {'loss': 1.5993, 'grad_norm': 0.8615336418151855, 'learning_rate': 7.245302072414601e-05, 'epoch': 36.0}
>>> 2025-08-21 13:09:43,114 - INFO - >>> {'loss': 1.6871, 'grad_norm': 0.6030092239379883, 'learning_rate': 7.174497137878966e-05, 'epoch': 36.61538461538461}
>>> 2025-08-21 13:09:54,919 - INFO - >>> {'loss': 1.6023, 'grad_norm': 0.7519347071647644, 'learning_rate': 7.103150273068921e-05, 'epoch': 37.0}
>>> 2025-08-21 13:10:15,290 - INFO - >>> {'loss': 1.6288, 'grad_norm': 0.5779029130935669, 'learning_rate': 7.031279259118946e-05, 'epoch': 37.61538461538461}
>>> 2025-08-21 13:10:23,279 - INFO - >>> {'loss': 1.614, 'grad_norm': 0.953682541847229, 'learning_rate': 6.958902007792466e-05, 'epoch': 38.0}
>>> 2025-08-21 13:10:40,263 - INFO - >>> {'loss': 1.5468, 'grad_norm': 0.6001282334327698, 'learning_rate': 6.886036557017881e-05, 'epoch': 38.61538461538461}
>>> 2025-08-21 13:10:50,118 - INFO - >>> {'loss': 1.6399, 'grad_norm': 0.6812637448310852, 'learning_rate': 6.812701066393124e-05, 'epoch': 39.0}
>>> 2025-08-21 13:11:05,280 - INFO - >>> {'loss': 1.5369, 'grad_norm': 0.6096909046173096, 'learning_rate': 6.738913812659912e-05, 'epoch': 39.61538461538461}
>>> 2025-08-21 13:11:16,064 - INFO - >>> {'loss': 1.5768, 'grad_norm': 0.6897392272949219, 'learning_rate': 6.664693185148807e-05, 'epoch': 40.0}
>>> 2025-08-21 13:11:34,817 - INFO - >>> {'loss': 1.524, 'grad_norm': 0.5978816151618958, 'learning_rate': 6.590057681196191e-05, 'epoch': 40.61538461538461}
>>> 2025-08-21 13:11:41,916 - INFO - >>> {'loss': 1.5192, 'grad_norm': 1.0997109413146973, 'learning_rate': 6.515025901534364e-05, 'epoch': 41.0}
>>> 2025-08-21 13:11:59,427 - INFO - >>> {'loss': 1.4393, 'grad_norm': 0.6490558385848999, 'learning_rate': 6.439616545655834e-05, 'epoch': 41.61538461538461}
>>> 2025-08-21 13:12:10,041 - INFO - >>> {'loss': 1.5395, 'grad_norm': 0.6897649765014648, 'learning_rate': 6.363848407153016e-05, 'epoch': 42.0}
>>> 2025-08-21 13:12:26,534 - INFO - >>> {'loss': 1.4276, 'grad_norm': 0.6302342414855957, 'learning_rate': 6.287740369034485e-05, 'epoch': 42.61538461538461}
>>> 2025-08-21 13:12:37,299 - INFO - >>> {'loss': 1.4837, 'grad_norm': 0.8705406785011292, 'learning_rate': 6.211311399018916e-05, 'epoch': 43.0}
>>> 2025-08-21 13:12:53,462 - INFO - >>> {'loss': 1.3814, 'grad_norm': 0.7038429975509644, 'learning_rate': 6.13458054480795e-05, 'epoch': 43.61538461538461}
>>> 2025-08-21 13:13:04,265 - INFO - >>> {'loss': 1.4544, 'grad_norm': 0.7868169546127319, 'learning_rate': 6.0575669293390954e-05, 'epoch': 44.0}
>>> 2025-08-21 13:13:21,586 - INFO - >>> {'loss': 1.4285, 'grad_norm': 0.6411401033401489, 'learning_rate': 5.980289746019892e-05, 'epoch': 44.61538461538461}
>>> 2025-08-21 13:13:30,080 - INFO - >>> {'loss': 1.2921, 'grad_norm': 0.887295126914978, 'learning_rate': 5.9027682539445104e-05, 'epoch': 45.0}
>>> 2025-08-21 13:13:45,939 - INFO - >>> {'loss': 1.3675, 'grad_norm': 0.7596606016159058, 'learning_rate': 5.8250217730939973e-05, 'epoch': 45.61538461538461}
>>> 2025-08-21 13:13:57,832 - INFO - >>> {'loss': 1.3056, 'grad_norm': 0.8386614322662354, 'learning_rate': 5.747069679521305e-05, 'epoch': 46.0}
>>> 2025-08-21 13:14:14,702 - INFO - >>> {'loss': 1.3026, 'grad_norm': 0.6898871660232544, 'learning_rate': 5.668931400522396e-05, 'epoch': 46.61538461538461}
>>> 2025-08-21 13:14:24,682 - INFO - >>> {'loss': 1.2992, 'grad_norm': 1.0151607990264893, 'learning_rate': 5.5906264097945407e-05, 'epoch': 47.0}
>>> 2025-08-21 13:14:42,410 - INFO - >>> {'loss': 1.2755, 'grad_norm': 0.7495226860046387, 'learning_rate': 5.5121742225830665e-05, 'epoch': 47.61538461538461}
>>> 2025-08-21 13:14:52,881 - INFO - >>> {'loss': 1.2382, 'grad_norm': 0.8880992531776428, 'learning_rate': 5.433594390817756e-05, 'epoch': 48.0}
>>> 2025-08-21 13:15:11,103 - INFO - >>> {'loss': 1.1778, 'grad_norm': 0.7151917219161987, 'learning_rate': 5.35490649824008e-05, 'epoch': 48.61538461538461}
>>> 2025-08-21 13:15:20,659 - INFO - >>> {'loss': 1.3237, 'grad_norm': 1.1045633554458618, 'learning_rate': 5.276130155522541e-05, 'epoch': 49.0}
>>> 2025-08-21 13:15:38,603 - INFO - >>> {'loss': 1.257, 'grad_norm': 0.7450249195098877, 'learning_rate': 5.1972849953812644e-05, 'epoch': 49.61538461538461}
>>> 2025-08-21 13:15:46,773 - INFO - >>> {'loss': 1.0107, 'grad_norm': 1.0790517330169678, 'learning_rate': 5.1183906676831197e-05, 'epoch': 50.0}
>>> 2025-08-21 13:16:04,549 - INFO - >>> {'loss': 1.1321, 'grad_norm': 0.7406512498855591, 'learning_rate': 5.039466834548568e-05, 'epoch': 50.61538461538461}
>>> 2025-08-21 13:16:12,656 - INFO - >>> {'loss': 1.1506, 'grad_norm': 1.293278455734253, 'learning_rate': 4.960533165451435e-05, 'epoch': 51.0}
>>> 2025-08-21 13:16:31,200 - INFO - >>> {'loss': 1.133, 'grad_norm': 0.8130970001220703, 'learning_rate': 4.8816093323168815e-05, 'epoch': 51.61538461538461}
>>> 2025-08-21 13:16:40,311 - INFO - >>> {'loss': 1.0285, 'grad_norm': 1.1012672185897827, 'learning_rate': 4.802715004618737e-05, 'epoch': 52.0}
>>> 2025-08-21 13:16:58,211 - INFO - >>> {'loss': 1.049, 'grad_norm': 0.8546715974807739, 'learning_rate': 4.7238698444774595e-05, 'epoch': 52.61538461538461}
>>> 2025-08-21 13:17:09,037 - INFO - >>> {'loss': 1.0566, 'grad_norm': 1.2035167217254639, 'learning_rate': 4.64509350175992e-05, 'epoch': 53.0}
>>> 2025-08-21 13:17:23,313 - INFO - >>> {'loss': 1.0142, 'grad_norm': 0.8471351861953735, 'learning_rate': 4.566405609182247e-05, 'epoch': 53.61538461538461}
>>> 2025-08-21 13:17:33,350 - INFO - >>> {'loss': 0.9994, 'grad_norm': 1.2965524196624756, 'learning_rate': 4.4878257774169346e-05, 'epoch': 54.0}
>>> 2025-08-21 13:17:48,692 - INFO - >>> {'loss': 0.8989, 'grad_norm': 1.0316578149795532, 'learning_rate': 4.4093735902054605e-05, 'epoch': 54.61538461538461}
>>> 2025-08-21 13:18:00,200 - INFO - >>> {'loss': 1.0337, 'grad_norm': 1.0974535942077637, 'learning_rate': 4.331068599477605e-05, 'epoch': 55.0}
>>> 2025-08-21 13:18:17,854 - INFO - >>> {'loss': 0.9136, 'grad_norm': 0.9299582839012146, 'learning_rate': 4.2529303204786953e-05, 'epoch': 55.61538461538461}
>>> 2025-08-21 13:18:28,027 - INFO - >>> {'loss': 0.9458, 'grad_norm': 1.3418357372283936, 'learning_rate': 4.1749782269060045e-05, 'epoch': 56.0}
>>> 2025-08-21 13:18:44,474 - INFO - >>> {'loss': 0.8925, 'grad_norm': 0.9939422011375427, 'learning_rate': 4.097231746055491e-05, 'epoch': 56.61538461538461}
>>> 2025-08-21 13:18:54,305 - INFO - >>> {'loss': 0.8594, 'grad_norm': 1.253443717956543, 'learning_rate': 4.01971025398011e-05, 'epoch': 57.0}
>>> 2025-08-21 13:19:14,012 - INFO - >>> {'loss': 0.8335, 'grad_norm': 0.9073976278305054, 'learning_rate': 3.942433070660905e-05, 'epoch': 57.61538461538461}
>>> 2025-08-21 13:19:21,484 - INFO - >>> {'loss': 0.8604, 'grad_norm': 1.6998459100723267, 'learning_rate': 3.8654194551920485e-05, 'epoch': 58.0}
>>> 2025-08-21 13:19:39,050 - INFO - >>> {'loss': 0.837, 'grad_norm': 1.105123519897461, 'learning_rate': 3.788688600981085e-05, 'epoch': 58.61538461538461}
>>> 2025-08-21 13:19:50,006 - INFO - >>> {'loss': 0.7406, 'grad_norm': 1.3204132318496704, 'learning_rate': 3.712259630965518e-05, 'epoch': 59.0}
>>> 2025-08-21 13:20:07,904 - INFO - >>> {'loss': 0.7573, 'grad_norm': 0.9657199382781982, 'learning_rate': 3.636151592846985e-05, 'epoch': 59.61538461538461}
>>> 2025-08-21 13:20:16,427 - INFO - >>> {'loss': 0.7564, 'grad_norm': 1.5899055004119873, 'learning_rate': 3.560383454344168e-05, 'epoch': 60.0}
>>> 2025-08-21 13:20:32,440 - INFO - >>> {'loss': 0.7512, 'grad_norm': 1.170590877532959, 'learning_rate': 3.484974098465636e-05, 'epoch': 60.61538461538461}
>>> 2025-08-21 13:20:45,155 - INFO - >>> {'loss': 0.676, 'grad_norm': 1.2651251554489136, 'learning_rate': 3.409942318803809e-05, 'epoch': 61.0}
>>> 2025-08-21 13:21:02,825 - INFO - >>> {'loss': 0.6662, 'grad_norm': 1.0033864974975586, 'learning_rate': 3.335306814851196e-05, 'epoch': 61.61538461538461}
>>> 2025-08-21 13:21:13,294 - INFO - >>> {'loss': 0.7033, 'grad_norm': 1.6756870746612549, 'learning_rate': 3.261086187340088e-05, 'epoch': 62.0}
>>> 2025-08-21 13:21:27,654 - INFO - >>> {'loss': 0.6866, 'grad_norm': 1.125396728515625, 'learning_rate': 3.187298933606878e-05, 'epoch': 62.61538461538461}
>>> 2025-08-21 13:21:38,346 - INFO - >>> {'loss': 0.5842, 'grad_norm': 1.409475564956665, 'learning_rate': 3.11396344298212e-05, 'epoch': 63.0}
>>> 2025-08-21 13:21:57,026 - INFO - >>> {'loss': 0.6189, 'grad_norm': 1.1396859884262085, 'learning_rate': 3.0410979922075343e-05, 'epoch': 63.61538461538461}
>>> 2025-08-21 13:22:06,036 - INFO - >>> {'loss': 0.5928, 'grad_norm': 1.3700330257415771, 'learning_rate': 2.9687207408810557e-05, 'epoch': 64.0}
>>> 2025-08-21 13:22:22,964 - INFO - >>> {'loss': 0.6143, 'grad_norm': 1.077088713645935, 'learning_rate': 2.8968497269310803e-05, 'epoch': 64.61538461538461}
>>> 2025-08-21 13:22:33,906 - INFO - >>> {'loss': 0.4994, 'grad_norm': 1.4568840265274048, 'learning_rate': 2.8255028621210355e-05, 'epoch': 65.0}
>>> 2025-08-21 13:22:51,345 - INFO - >>> {'loss': 0.4918, 'grad_norm': 1.1831822395324707, 'learning_rate': 2.754697927585399e-05, 'epoch': 65.61538461538461}
>>> 2025-08-21 13:23:01,884 - INFO - >>> {'loss': 0.5925, 'grad_norm': 1.315808892250061, 'learning_rate': 2.6844525693982613e-05, 'epoch': 66.0}
>>> 2025-08-21 13:23:18,334 - INFO - >>> {'loss': 0.5616, 'grad_norm': 1.077126145362854, 'learning_rate': 2.614784294175554e-05, 'epoch': 66.61538461538461}
>>> 2025-08-21 13:23:30,441 - INFO - >>> {'loss': 0.4294, 'grad_norm': 1.4443631172180176, 'learning_rate': 2.5457104647120322e-05, 'epoch': 67.0}
>>> 2025-08-21 13:23:49,139 - INFO - >>> {'loss': 0.5003, 'grad_norm': 1.0141081809997559, 'learning_rate': 2.4772482956541132e-05, 'epoch': 67.61538461538461}
>>> 2025-08-21 13:23:59,411 - INFO - >>> {'loss': 0.4522, 'grad_norm': 1.604263186454773, 'learning_rate': 2.4094148492096125e-05, 'epoch': 68.0}
>>> 2025-08-21 13:24:18,843 - INFO - >>> {'loss': 0.483, 'grad_norm': 1.0370279550552368, 'learning_rate': 2.3422270308954934e-05, 'epoch': 68.61538461538461}
>>> 2025-08-21 13:24:28,215 - INFO - >>> {'loss': 0.4114, 'grad_norm': 1.5168609619140625, 'learning_rate': 2.2757015853246493e-05, 'epoch': 69.0}
>>> 2025-08-21 13:24:47,816 - INFO - >>> {'loss': 0.4072, 'grad_norm': 0.9959152936935425, 'learning_rate': 2.2098550920327998e-05, 'epoch': 69.61538461538461}
>>> 2025-08-21 13:24:58,572 - INFO - >>> {'loss': 0.4783, 'grad_norm': 1.58380925655365, 'learning_rate': 2.1447039613465265e-05, 'epoch': 70.0}
>>> 2025-08-21 13:25:16,876 - INFO - >>> {'loss': 0.4101, 'grad_norm': 1.0063362121582031, 'learning_rate': 2.0802644302934683e-05, 'epoch': 70.61538461538461}
>>> 2025-08-21 13:25:26,275 - INFO - >>> {'loss': 0.4156, 'grad_norm': 1.4893722534179688, 'learning_rate': 2.0165525585557204e-05, 'epoch': 71.0}
>>> 2025-08-21 13:25:42,217 - INFO - >>> {'loss': 0.3505, 'grad_norm': 1.1462535858154297, 'learning_rate': 1.953584224467418e-05, 'epoch': 71.61538461538461}
>>> 2025-08-21 13:25:53,360 - INFO - >>> {'loss': 0.4318, 'grad_norm': 1.2034801244735718, 'learning_rate': 1.8913751210575248e-05, 'epoch': 72.0}
>>> 2025-08-21 13:26:08,287 - INFO - >>> {'loss': 0.3746, 'grad_norm': 1.207518219947815, 'learning_rate': 1.8299407521388067e-05, 'epoch': 72.61538461538461}
>>> 2025-08-21 13:26:19,725 - INFO - >>> {'loss': 0.3657, 'grad_norm': 1.114702820777893, 'learning_rate': 1.7692964284439505e-05, 'epoch': 73.0}
>>> 2025-08-21 13:26:35,304 - INFO - >>> {'loss': 0.3646, 'grad_norm': 0.9997194409370422, 'learning_rate': 1.7094572638098123e-05, 'epoch': 73.61538461538461}
>>> 2025-08-21 13:26:46,413 - INFO - >>> {'loss': 0.3348, 'grad_norm': 1.4014168977737427, 'learning_rate': 1.6504381714107252e-05, 'epoch': 74.0}
>>> 2025-08-21 13:27:04,634 - INFO - >>> {'loss': 0.3055, 'grad_norm': 1.014390230178833, 'learning_rate': 1.5922538600418318e-05, 'epoch': 74.61538461538461}
>>> 2025-08-21 13:27:13,854 - INFO - >>> {'loss': 0.3797, 'grad_norm': 1.3246656656265259, 'learning_rate': 1.5349188304533413e-05, 'epoch': 75.0}
>>> 2025-08-21 13:27:30,747 - INFO - >>> {'loss': 0.3065, 'grad_norm': 0.9703415632247925, 'learning_rate': 1.4784473717366387e-05, 'epoch': 75.61538461538461}
>>> 2025-08-21 13:27:41,002 - INFO - >>> {'loss': 0.3434, 'grad_norm': 1.33357572555542, 'learning_rate': 1.4228535577631442e-05, 'epoch': 76.0}
>>> 2025-08-21 13:27:57,701 - INFO - >>> {'loss': 0.3411, 'grad_norm': 0.9535523653030396, 'learning_rate': 1.3681512436768045e-05, 'epoch': 76.61538461538461}
>>> 2025-08-21 13:28:08,698 - INFO - >>> {'loss': 0.2416, 'grad_norm': 1.3417870998382568, 'learning_rate': 1.314354062441106e-05, 'epoch': 77.0}
>>> 2025-08-21 13:28:27,113 - INFO - >>> {'loss': 0.2909, 'grad_norm': 1.0136195421218872, 'learning_rate': 1.2614754214414548e-05, 'epoch': 77.61538461538461}
>>> 2025-08-21 13:28:36,618 - INFO - >>> {'loss': 0.299, 'grad_norm': 1.1844820976257324, 'learning_rate': 1.2095284991437733e-05, 'epoch': 78.0}
>>> 2025-08-21 13:28:56,369 - INFO - >>> {'loss': 0.2425, 'grad_norm': 0.960906445980072, 'learning_rate': 1.1585262418101467e-05, 'epoch': 78.61538461538461}
>>> 2025-08-21 13:29:05,717 - INFO - >>> {'loss': 0.3504, 'grad_norm': 1.271132469177246, 'learning_rate': 1.1084813602723515e-05, 'epoch': 79.0}
>>> 2025-08-21 13:29:24,425 - INFO - >>> {'loss': 0.2609, 'grad_norm': 0.8407022953033447, 'learning_rate': 1.0594063267640386e-05, 'epoch': 79.61538461538461}
>>> 2025-08-21 13:29:33,305 - INFO - >>> {'loss': 0.3101, 'grad_norm': 1.7012252807617188, 'learning_rate': 1.0113133718124035e-05, 'epoch': 80.0}
>>> 2025-08-21 13:29:53,736 - INFO - >>> {'loss': 0.2805, 'grad_norm': 0.8733201622962952, 'learning_rate': 9.642144811900739e-06, 'epoch': 80.61538461538461}
>>> 2025-08-21 13:30:02,166 - INFO - >>> {'loss': 0.2266, 'grad_norm': 1.3184653520584106, 'learning_rate': 9.181213929280046e-06, 'epoch': 81.0}
>>> 2025-08-21 13:30:19,991 - INFO - >>> {'loss': 0.2137, 'grad_norm': 0.8902402520179749, 'learning_rate': 8.7304559439012e-06, 'epoch': 81.61538461538461}
>>> 2025-08-21 13:30:30,422 - INFO - >>> {'loss': 0.3282, 'grad_norm': 1.2652102708816528, 'learning_rate': 8.28998319410413e-06, 'epoch': 82.0}
>>> 2025-08-21 13:30:49,226 - INFO - >>> {'loss': 0.236, 'grad_norm': 0.8859483599662781, 'learning_rate': 7.859905454932471e-06, 'epoch': 82.61538461538461}
>>> 2025-08-21 13:30:58,865 - INFO - >>> {'loss': 0.2707, 'grad_norm': 1.240654706954956, 'learning_rate': 7.440329910775273e-06, 'epoch': 83.0}
>>> 2025-08-21 13:31:14,944 - INFO - >>> {'loss': 0.2489, 'grad_norm': 0.9930007457733154, 'learning_rate': 7.031361128654401e-06, 'epoch': 83.61538461538461}
>>> 2025-08-21 13:31:25,422 - INFO - >>> {'loss': 0.235, 'grad_norm': 1.0283880233764648, 'learning_rate': 6.633101032164274e-06, 'epoch': 84.0}
>>> 2025-08-21 13:31:43,667 - INFO - >>> {'loss': 0.2746, 'grad_norm': 0.984508752822876, 'learning_rate': 6.2456488760703205e-06, 'epoch': 84.61538461538461}
>>> 2025-08-21 13:31:53,635 - INFO - >>> {'loss': 0.1917, 'grad_norm': 1.0600039958953857, 'learning_rate': 5.869101221572654e-06, 'epoch': 85.0}
>>> 2025-08-21 13:32:08,032 - INFO - >>> {'loss': 0.199, 'grad_norm': 0.8543765544891357, 'learning_rate': 5.5035519122409895e-06, 'epoch': 85.61538461538461}
>>> 2025-08-21 13:32:19,059 - INFO - >>> {'loss': 0.2867, 'grad_norm': 1.252030611038208, 'learning_rate': 5.149092050626825e-06, 'epoch': 86.0}
>>> 2025-08-21 13:32:36,265 - INFO - >>> {'loss': 0.2169, 'grad_norm': 0.8839917182922363, 'learning_rate': 4.805809975558828e-06, 'epoch': 86.61538461538461}
>>> 2025-08-21 13:32:45,890 - INFO - >>> {'loss': 0.2443, 'grad_norm': 1.1275601387023926, 'learning_rate': 4.47379124012689e-06, 'epoch': 87.0}
>>> 2025-08-21 13:33:03,624 - INFO - >>> {'loss': 0.2432, 'grad_norm': 0.8427793979644775, 'learning_rate': 4.153118590360561e-06, 'epoch': 87.61538461538461}
>>> 2025-08-21 13:33:12,777 - INFO - >>> {'loss': 0.1797, 'grad_norm': 1.2405248880386353, 'learning_rate': 3.843871944606969e-06, 'epoch': 88.0}
>>> 2025-08-21 13:33:28,608 - INFO - >>> {'loss': 0.2034, 'grad_norm': 0.8700474500656128, 'learning_rate': 3.5461283736134722e-06, 'epoch': 88.61538461538461}
>>> 2025-08-21 13:33:39,338 - INFO - >>> {'loss': 0.2503, 'grad_norm': 1.177566409111023, 'learning_rate': 3.2599620813200837e-06, 'epoch': 89.0}
>>> 2025-08-21 13:33:55,202 - INFO - >>> {'loss': 0.1838, 'grad_norm': 0.8211973309516907, 'learning_rate': 2.9854443863662262e-06, 'epoch': 89.61538461538461}
>>> 2025-08-21 13:34:05,656 - INFO - >>> {'loss': 0.2923, 'grad_norm': 1.363012671470642, 'learning_rate': 2.722643704316652e-06, 'epoch': 90.0}
>>> 2025-08-21 13:34:23,205 - INFO - >>> {'loss': 0.1873, 'grad_norm': 0.8271744847297668, 'learning_rate': 2.4716255306108605e-06, 'epoch': 90.61538461538461}
>>> 2025-08-21 13:34:32,158 - INFO - >>> {'loss': 0.2682, 'grad_norm': 1.2144695520401, 'learning_rate': 2.2324524242402613e-06, 'epoch': 91.0}
>>> 2025-08-21 13:34:49,827 - INFO - >>> {'loss': 0.2299, 'grad_norm': 0.7922514081001282, 'learning_rate': 2.0051839921571448e-06, 'epoch': 91.61538461538461}
>>> 2025-08-21 13:34:59,431 - INFO - >>> {'loss': 0.1674, 'grad_norm': 1.3678085803985596, 'learning_rate': 1.7898768744194162e-06, 'epoch': 92.0}
>>> 2025-08-21 13:35:16,169 - INFO - >>> {'loss': 0.2236, 'grad_norm': 0.8371128439903259, 'learning_rate': 1.5865847300746417e-06, 'epoch': 92.61538461538461}
>>> 2025-08-21 13:35:27,360 - INFO - >>> {'loss': 0.1937, 'grad_norm': 1.138611078262329, 'learning_rate': 1.3953582237871521e-06, 'epoch': 93.0}
>>> 2025-08-21 13:35:44,142 - INFO - >>> {'loss': 0.2238, 'grad_norm': 0.8585594296455383, 'learning_rate': 1.2162450132113201e-06, 'epoch': 93.61538461538461}
>>> 2025-08-21 13:35:53,469 - INFO - >>> {'loss': 0.1891, 'grad_norm': 1.0792115926742554, 'learning_rate': 1.049289737114273e-06, 'epoch': 94.0}
>>> 2025-08-21 13:36:10,085 - INFO - >>> {'loss': 0.204, 'grad_norm': 0.8748236298561096, 'learning_rate': 8.945340042509797e-07, 'epoch': 94.61538461538461}
>>> 2025-08-21 13:36:20,683 - INFO - >>> {'loss': 0.2169, 'grad_norm': 1.0769917964935303, 'learning_rate': 7.520163829944804e-07, 'epoch': 95.0}
>>> 2025-08-21 13:36:38,550 - INFO - >>> {'loss': 0.2036, 'grad_norm': 0.8758822679519653, 'learning_rate': 6.217723917238128e-07, 'epoch': 95.61538461538461}
>>> 2025-08-21 13:36:48,397 - INFO - >>> {'loss': 0.2182, 'grad_norm': 1.0889655351638794, 'learning_rate': 5.038344899721436e-07, 'epoch': 96.0}
>>> 2025-08-21 13:37:06,185 - INFO - >>> {'loss': 0.2134, 'grad_norm': 0.792594313621521, 'learning_rate': 3.9823207033710676e-07, 'epoch': 96.61538461538461}
>>> 2025-08-21 13:37:16,539 - INFO - >>> {'loss': 0.2002, 'grad_norm': 1.2997617721557617, 'learning_rate': 3.0499145115561176e-07, 'epoch': 97.0}
>>> 2025-08-21 13:37:32,451 - INFO - >>> {'loss': 0.2034, 'grad_norm': 0.851608157157898, 'learning_rate': 2.2413586994470825e-07, 'epoch': 97.61538461538461}
>>> 2025-08-21 13:37:43,146 - INFO - >>> {'loss': 0.219, 'grad_norm': 1.132721185684204, 'learning_rate': 1.5568547761034004e-07, 'epoch': 98.0}
>>> 2025-08-21 13:38:00,331 - INFO - >>> {'loss': 0.2136, 'grad_norm': 0.8189862370491028, 'learning_rate': 9.965733342532924e-08, 'epoch': 98.61538461538461}
>>> 2025-08-21 13:38:10,117 - INFO - >>> {'loss': 0.1997, 'grad_norm': 1.178524136543274, 'learning_rate': 5.606540077782163e-08, 'epoch': 99.0}
>>> 2025-08-21 13:38:26,077 - INFO - >>> {'loss': 0.1897, 'grad_norm': 0.8486526012420654, 'learning_rate': 2.4920543691309138e-08, 'epoch': 99.61538461538461}
>>> 2025-08-21 13:38:35,853 - INFO - >>> {'loss': 0.2401, 'grad_norm': 1.1752676963806152, 'learning_rate': 6.2305241171345395e-09, 'epoch': 100.0}
>>> 2025-08-21 13:38:36,498 - INFO - >>> {'train_runtime': 2755.4462, 'train_samples_per_second': 3.629, 'train_steps_per_second': 0.073, 'train_loss': 1.251376223191619, 'epoch': 100.0}
>>> 2025-08-21 13:38:36,500 - INFO - 训练成功！
>>> 2025-08-21 13:38:36,500 - INFO - 模型存放位置：./output/qwen3-8b202508211252
>>> 2025-08-24 11:19:09,393 - INFO - ==========gpt oss模型微调脚本启动==========
>>> 2025-08-24 11:19:11,321 - INFO - 模块导入完成
>>> 2025-08-24 11:19:12,706 - INFO - 配置导入完成
>>> 2025-08-24 11:19:17,552 - INFO - 模型和分词器加载完成
>>> 2025-08-24 11:19:19,066 - INFO - 数据读取开始
>>> 2025-08-24 11:19:21,262 - INFO - 数据下载完成
>>> 2025-08-24 11:19:22,732 - INFO - 数据映射完成
>>> 2025-08-24 11:19:24,218 - INFO - 数据集加载完成
>>> 2025-08-24 11:19:25,586 - INFO - 打印训练参数如下
>>> 2025-08-24 11:19:27,053 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-08-24 11:19:28,882 - INFO -   dtype >>> torch.float16
>>> 2025-08-24 11:19:30,712 - INFO -   load_in_4bit >>> True
>>> 2025-08-24 11:19:32,500 - INFO -   batch_size >>> 8
>>> 2025-08-24 11:19:34,188 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-08-24 11:19:36,197 - INFO -   warmup_steps >>> 1
>>> 2025-08-24 11:19:37,924 - INFO -   epoch >>> 10
>>> 2025-08-24 11:19:39,531 - INFO -   eval_steps >>> 10
>>> 2025-08-24 11:19:41,238 - INFO -   learning_rate >>> 5e-05
>>> 2025-08-24 11:19:43,075 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-08-24 11:19:45,003 - INFO -   max_seq_length >>> 4096
>>> 2025-08-24 11:19:46,831 - INFO -   r >>> 8
>>> 2025-08-24 11:19:48,338 - INFO -   interface_mode >>> False
>>> 2025-08-24 11:19:50,187 - INFO -   target_modules >>> ['q_proj', 'v_proj']
>>> 2025-08-24 11:19:52,336 - INFO -   lora_alpha >>> 16
>>> 2025-08-24 11:19:54,043 - INFO -   lora_dropout >>> 0.05
>>> 2025-08-24 11:20:57,110 - INFO - ==========模型微调脚本启动==========
>>> 2025-08-24 11:20:58,899 - INFO - 模块导入完成
>>> 2025-08-24 11:21:00,285 - INFO - 配置导入完成
>>> 2025-08-24 11:21:04,347 - INFO - 模型和分词器加载完成
>>> 2025-08-24 11:21:05,773 - INFO - 数据读取开始
>>> 2025-08-24 11:21:07,972 - INFO - 数据下载完成
>>> 2025-08-24 11:21:09,539 - INFO - 数据映射完成
>>> 2025-08-24 11:21:11,025 - INFO - 数据集加载完成
>>> 2025-08-24 11:21:12,391 - INFO - 打印训练参数如下
>>> 2025-08-24 11:21:13,858 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-08-24 11:21:15,687 - INFO -   dtype >>> torch.float16
>>> 2025-08-24 11:21:17,515 - INFO -   load_in_4bit >>> True
>>> 2025-08-24 11:21:19,303 - INFO -   batch_size >>> 8
>>> 2025-08-24 11:21:20,990 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-08-24 11:21:22,999 - INFO -   warmup_steps >>> 1
>>> 2025-08-24 11:21:24,727 - INFO -   epoch >>> 10
>>> 2025-08-24 11:21:26,334 - INFO -   eval_steps >>> 10
>>> 2025-08-24 11:21:28,041 - INFO -   learning_rate >>> 5e-05
>>> 2025-08-24 11:21:29,869 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-08-24 11:21:31,798 - INFO -   max_seq_length >>> 4096
>>> 2025-08-24 11:21:33,626 - INFO -   r >>> 8
>>> 2025-08-24 11:21:35,132 - INFO -   interface_mode >>> False
>>> 2025-08-24 11:21:36,980 - INFO -   target_modules >>> ['q_proj', 'v_proj']
>>> 2025-08-24 11:21:39,130 - INFO -   lora_alpha >>> 16
>>> 2025-08-24 11:21:40,837 - INFO -   lora_dropout >>> 0.05
>>> 2025-08-24 11:22:55,118 - INFO - ==========模型微调脚本启动==========
>>> 2025-08-24 11:22:56,905 - INFO - 模块导入完成
>>> 2025-08-24 11:22:58,289 - INFO - 配置导入完成
>>> 2025-08-24 11:23:02,516 - INFO - 模型和分词器加载完成
>>> 2025-08-24 11:23:03,943 - INFO - 数据读取开始
>>> 2025-08-24 11:23:06,181 - INFO - 数据下载完成
>>> 2025-08-24 11:23:31,203 - INFO - ==========模型微调脚本启动==========
>>> 2025-08-24 11:23:32,990 - INFO - 模块导入完成
>>> 2025-08-24 11:23:34,375 - INFO - 配置导入完成
>>> 2025-08-24 11:23:38,667 - INFO - 模型和分词器加载完成
>>> 2025-08-24 11:23:40,093 - INFO - 数据读取开始
>>> 2025-08-24 11:23:42,270 - INFO - 数据下载完成
>>> 2025-08-24 11:23:43,747 - INFO - 数据映射完成
>>> 2025-08-24 11:23:45,234 - INFO - 数据集加载完成
>>> 2025-08-24 11:23:46,602 - INFO - 打印训练参数如下
>>> 2025-08-24 11:23:48,069 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-08-24 11:23:49,898 - INFO -   dtype >>> torch.float16
>>> 2025-08-24 11:23:51,726 - INFO -   load_in_4bit >>> True
>>> 2025-08-24 11:23:53,515 - INFO -   batch_size >>> 8
>>> 2025-08-24 11:23:55,203 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-08-24 11:23:57,213 - INFO -   warmup_steps >>> 1
>>> 2025-08-24 11:23:58,941 - INFO -   epoch >>> 100
>>> 2025-08-24 11:24:00,569 - INFO -   eval_steps >>> 10
>>> 2025-08-24 11:24:02,277 - INFO -   learning_rate >>> 0.0001
>>> 2025-08-24 11:24:04,125 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-08-24 11:24:06,053 - INFO -   max_seq_length >>> 4096
>>> 2025-08-24 11:24:07,882 - INFO -   r >>> 8
>>> 2025-08-24 11:24:09,391 - INFO -   interface_mode >>> False
>>> 2025-08-24 11:24:11,239 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-08-24 11:24:13,590 - INFO -   lora_alpha >>> 16
>>> 2025-08-24 11:24:15,298 - INFO -   lora_dropout >>> 0.05
>>> 2025-08-24 11:24:17,086 - INFO -   use_gradient_checkpointing >>> True
>>> 2025-08-24 11:24:28,710 - INFO - 开始训练！
>>> 2025-08-24 11:24:31,523 - ERROR - 训练失败：element 0 of tensors does not require grad and does not have a grad_fn
>>> 2025-08-24 11:25:37,158 - INFO - ==========模型微调脚本启动==========
>>> 2025-08-24 11:25:38,945 - INFO - 模块导入完成
>>> 2025-08-24 11:25:40,330 - INFO - 配置导入完成
>>> 2025-08-24 11:25:44,435 - INFO - 模型和分词器加载完成
>>> 2025-08-24 11:25:45,861 - INFO - 数据读取开始
>>> 2025-08-24 11:25:48,057 - INFO - 数据下载完成
>>> 2025-08-24 11:25:49,535 - INFO - 数据映射完成
>>> 2025-08-24 11:25:51,021 - INFO - 数据集加载完成
>>> 2025-08-24 11:25:52,388 - INFO - 打印训练参数如下
>>> 2025-08-24 11:25:53,855 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-08-24 11:25:55,682 - INFO -   dtype >>> torch.float16
>>> 2025-08-24 11:25:57,510 - INFO -   load_in_4bit >>> True
>>> 2025-08-24 11:25:59,299 - INFO -   batch_size >>> 8
>>> 2025-08-24 11:26:00,986 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-08-24 11:26:02,995 - INFO -   warmup_steps >>> 1
>>> 2025-08-24 11:26:04,722 - INFO -   epoch >>> 100
>>> 2025-08-24 11:26:06,349 - INFO -   eval_steps >>> 10
>>> 2025-08-24 11:26:08,057 - INFO -   learning_rate >>> 0.0001
>>> 2025-08-24 11:26:09,905 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-08-24 11:26:11,834 - INFO -   max_seq_length >>> 4096
>>> 2025-08-24 11:26:13,662 - INFO -   r >>> 8
>>> 2025-08-24 11:26:15,169 - INFO -   interface_mode >>> False
>>> 2025-08-24 11:26:17,018 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-08-24 11:26:19,369 - INFO -   lora_alpha >>> 16
>>> 2025-08-24 11:26:21,077 - INFO -   lora_dropout >>> 0.05
>>> 2025-08-24 11:26:22,865 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-08-24 11:26:34,482 - INFO - 开始训练！
>>> 2025-08-24 11:26:37,276 - ERROR - 训练失败：element 0 of tensors does not require grad and does not have a grad_fn
>>> 2025-08-24 11:27:29,116 - INFO - 导入包完成
>>> 2025-08-24 11:27:30,683 - INFO - ========train Qwen2ForCausalLM  202508241127========
>>> 2025-08-24 11:27:33,195 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-24 11:27:36,348 - INFO - 开始进行训练
>>> 2025-08-24 11:27:37,965 - INFO - 基础配置文件读取完成
>>> 2025-08-24 11:27:39,652 - INFO - 训练配置读取完成
>>> 2025-08-24 11:27:41,279 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-24 11:27:43,529 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-08-24 11:27:46,248 - INFO - tokenizer读取完成
>>> 2025-08-24 11:27:50,157 - INFO - model dtype:torch.float16
>>> 2025-08-24 11:27:52,125 - INFO - 模型导入完成
>>> 2025-08-24 11:27:53,712 - INFO - 数据读取开始
>>> 2025-08-24 11:27:55,907 - INFO - 数据下载完成
>>> 2025-08-24 11:28:01,498 - INFO - 数据映射完成
>>> 2025-08-24 11:28:02,985 - INFO - 打印训练参数如下
>>> 2025-08-24 11:28:04,451 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-08-24 11:28:06,279 - INFO -   dtype >>> torch.float16
>>> 2025-08-24 11:28:08,107 - INFO -   load_in_4bit >>> True
>>> 2025-08-24 11:28:09,894 - INFO -   batch_size >>> 8
>>> 2025-08-24 11:28:11,581 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-08-24 11:28:13,591 - INFO -   warmup_steps >>> 1
>>> 2025-08-24 11:28:15,319 - INFO -   epoch >>> 100
>>> 2025-08-24 11:28:16,946 - INFO -   eval_steps >>> 10
>>> 2025-08-24 11:28:18,653 - INFO -   learning_rate >>> 0.0001
>>> 2025-08-24 11:28:20,501 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-08-24 11:28:22,430 - INFO -   max_seq_length >>> 4096
>>> 2025-08-24 11:28:24,257 - INFO -   r >>> 8
>>> 2025-08-24 11:28:25,764 - INFO -   interface_mode >>> False
>>> 2025-08-24 11:28:27,612 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-08-24 11:28:29,961 - INFO -   lora_alpha >>> 16
>>> 2025-08-24 11:28:31,669 - INFO -   lora_dropout >>> 0.05
>>> 2025-08-24 11:28:33,461 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-08-24 11:28:49,466 - INFO - 开始训练！
>>> 2025-08-24 11:28:51,053 - INFO - 批次大小  : 8
>>> 2025-08-24 11:28:52,720 - INFO - 训练轮数  : 100
>>> 2025-08-24 11:28:54,428 - INFO - 学习率    : 0.0001
>>> 2025-08-24 11:28:56,216 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-24 11:28:58,507 - INFO - 模型路径  : /home/liangshuqiao/models/qwen3-8b
>>> 2025-08-24 11:29:18,081 - INFO - >>> {'loss': 3.73, 'grad_norm': 1.9258376359939575, 'learning_rate': 0.0, 'epoch': 0.6153846153846154}
>>> 2025-08-24 11:29:30,148 - INFO - >>> {'loss': 3.662, 'grad_norm': 1.8892629146575928, 'learning_rate': 0.0001, 'epoch': 1.0}
>>> 2025-08-24 11:29:49,705 - INFO - >>> {'loss': 3.653, 'grad_norm': 1.9250199794769287, 'learning_rate': 9.999376947588288e-05, 'epoch': 1.6153846153846154}
>>> 2025-08-24 11:30:00,219 - INFO - >>> {'loss': 3.5335, 'grad_norm': 1.6435669660568237, 'learning_rate': 9.99750794563087e-05, 'epoch': 2.0}
>>> 2025-08-24 11:30:19,437 - INFO - >>> {'loss': 3.4826, 'grad_norm': 1.754421353340149, 'learning_rate': 9.994393459922218e-05, 'epoch': 2.6153846153846154}
>>> 2025-08-24 11:44:10,595 - INFO - 导入包完成
>>> 2025-08-24 11:44:12,164 - INFO - ========train Qwen2ForCausalLM  202508241144========
>>> 2025-08-24 11:44:14,676 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-24 11:44:17,829 - INFO - 开始进行训练
>>> 2025-08-24 11:44:19,447 - INFO - 基础配置文件读取完成
>>> 2025-08-24 11:44:21,136 - INFO - 训练配置读取完成
>>> 2025-08-24 11:44:22,764 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-08-24 11:44:25,013 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-08-24 11:44:27,736 - INFO - tokenizer读取完成
>>> 2025-08-24 11:44:31,565 - INFO - model dtype:torch.float16
>>> 2025-08-24 11:44:33,534 - INFO - 模型导入完成
>>> 2025-08-24 11:44:35,122 - INFO - 数据读取开始
>>> 2025-08-24 11:44:37,306 - INFO - 数据下载完成
>>> 2025-08-24 11:44:42,767 - INFO - 数据映射完成
>>> 2025-08-24 11:44:44,253 - INFO - 打印训练参数如下
>>> 2025-08-24 11:44:45,720 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-08-24 11:44:47,548 - INFO -   dtype >>> torch.float16
>>> 2025-08-24 11:44:49,376 - INFO -   load_in_4bit >>> True
>>> 2025-08-24 11:44:51,165 - INFO -   batch_size >>> 8
>>> 2025-08-24 11:44:52,852 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-08-24 11:44:54,860 - INFO -   warmup_steps >>> 1
>>> 2025-08-24 11:44:56,588 - INFO -   epoch >>> 100
>>> 2025-08-24 11:44:58,221 - INFO -   eval_steps >>> 10
>>> 2025-08-24 11:44:59,928 - INFO -   learning_rate >>> 0.0001
>>> 2025-08-24 11:45:01,776 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-08-24 11:45:03,707 - INFO -   max_seq_length >>> 4096
>>> 2025-08-24 11:45:05,536 - INFO -   r >>> 8
>>> 2025-08-24 11:45:07,043 - INFO -   interface_mode >>> False
>>> 2025-08-24 11:45:08,891 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-08-24 11:45:11,241 - INFO -   lora_alpha >>> 16
>>> 2025-08-24 11:45:12,949 - INFO -   lora_dropout >>> 0.05
>>> 2025-08-24 11:45:14,737 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-08-24 11:45:25,571 - INFO - 开始训练！
>>> 2025-08-24 11:45:27,158 - INFO - 批次大小  : 8
>>> 2025-08-24 11:45:28,826 - INFO - 训练轮数  : 100
>>> 2025-08-24 11:45:30,533 - INFO - 学习率    : 0.0001
>>> 2025-08-24 11:45:32,321 - INFO - 数据集路径: dataset/own/Medical_20250816.json
>>> 2025-08-24 11:45:34,816 - INFO - 模型路径  : /home/liangshuqiao/models/qwen3-8b
>>> 2025-08-24 11:45:54,300 - INFO - >>> {'loss': 3.73, 'grad_norm': 1.9392122030258179, 'learning_rate': 0.0, 'epoch': 0.6153846153846154}
>>> 2025-08-24 11:46:06,355 - INFO - >>> {'loss': 3.662, 'grad_norm': 1.896055817604065, 'learning_rate': 0.0001, 'epoch': 1.0}
>>> 2025-08-24 11:46:25,871 - INFO - >>> {'loss': 3.6537, 'grad_norm': 1.9360990524291992, 'learning_rate': 9.999376947588288e-05, 'epoch': 1.6153846153846154}
>>> 2025-08-24 11:46:36,361 - INFO - >>> {'loss': 3.5338, 'grad_norm': 1.6704819202423096, 'learning_rate': 9.99750794563087e-05, 'epoch': 2.0}
>>> 2025-08-24 11:46:55,106 - INFO - >>> {'loss': 3.4828, 'grad_norm': 1.7846671342849731, 'learning_rate': 9.994393459922218e-05, 'epoch': 2.6153846153846154}
>>> 2025-08-24 11:47:05,774 - INFO - >>> {'loss': 3.2899, 'grad_norm': 2.0349032878875732, 'learning_rate': 9.990034266657467e-05, 'epoch': 3.0}
>>> 2025-08-24 11:47:25,763 - INFO - >>> {'loss': 3.2596, 'grad_norm': 1.7223531007766724, 'learning_rate': 9.984431452238967e-05, 'epoch': 3.6153846153846154}
>>> 2025-08-24 11:47:33,028 - INFO - >>> {'loss': 3.1817, 'grad_norm': 1.9879833459854126, 'learning_rate': 9.977586413005531e-05, 'epoch': 4.0}
>>> 2025-08-24 11:47:52,454 - INFO - >>> {'loss': 3.0688, 'grad_norm': 1.6732429265975952, 'learning_rate': 9.96950085488444e-05, 'epoch': 4.615384615384615}
>>> 2025-08-24 11:48:01,682 - INFO - >>> {'loss': 3.0179, 'grad_norm': 1.5910590887069702, 'learning_rate': 9.960176792966289e-05, 'epoch': 5.0}
>>> 2025-08-24 11:48:19,093 - INFO - >>> {'loss': 2.9178, 'grad_norm': 1.566517949104309, 'learning_rate': 9.949616551002787e-05, 'epoch': 5.615384615384615}
>>> 2025-08-24 11:48:28,915 - INFO - >>> {'loss': 2.7971, 'grad_norm': 1.3573718070983887, 'learning_rate': 9.93782276082762e-05, 'epoch': 6.0}
>>> 2025-08-24 11:48:45,079 - INFO - >>> {'loss': 2.7404, 'grad_norm': 1.21498441696167, 'learning_rate': 9.924798361700553e-05, 'epoch': 6.615384615384615}
>>> 2025-08-24 11:48:55,484 - INFO - >>> {'loss': 2.6703, 'grad_norm': 1.0783864259719849, 'learning_rate': 9.910546599574902e-05, 'epoch': 7.0}
>>> 2025-08-24 11:49:13,408 - INFO - >>> {'loss': 2.6655, 'grad_norm': 0.9560243487358093, 'learning_rate': 9.895071026288574e-05, 'epoch': 7.615384615384615}
>>> 2025-08-24 11:49:23,512 - INFO - >>> {'loss': 2.5246, 'grad_norm': 0.8380343914031982, 'learning_rate': 9.87837549867887e-05, 'epoch': 8.0}
>>> 2025-08-24 11:49:41,704 - INFO - >>> {'loss': 2.5322, 'grad_norm': 0.7838985323905945, 'learning_rate': 9.860464177621284e-05, 'epoch': 8.615384615384615}
>>> 2025-08-24 11:49:52,489 - INFO - >>> {'loss': 2.486, 'grad_norm': 0.7760108113288879, 'learning_rate': 9.841341526992536e-05, 'epoch': 9.0}
>>> 2025-08-24 11:50:08,962 - INFO - >>> {'loss': 2.4463, 'grad_norm': 0.6431402564048767, 'learning_rate': 9.821012312558058e-05, 'epoch': 9.615384615384615}
>>> 2025-08-24 11:50:17,462 - INFO - >>> {'loss': 2.4294, 'grad_norm': 0.7189927697181702, 'learning_rate': 9.799481600784286e-05, 'epoch': 10.0}
>>> 2025-08-24 11:50:36,185 - INFO - >>> {'loss': 2.3582, 'grad_norm': 0.5808456540107727, 'learning_rate': 9.776754757575975e-05, 'epoch': 10.615384615384615}
>>> 2025-08-24 11:50:45,792 - INFO - >>> {'loss': 2.419, 'grad_norm': 0.6234794855117798, 'learning_rate': 9.752837446938915e-05, 'epoch': 11.0}
>>> 2025-08-24 11:51:05,097 - INFO - >>> {'loss': 2.3159, 'grad_norm': 0.5375956892967224, 'learning_rate': 9.727735629568336e-05, 'epoch': 11.615384615384615}
>>> 2025-08-24 11:51:14,453 - INFO - >>> {'loss': 2.3507, 'grad_norm': 0.6298753619194031, 'learning_rate': 9.701455561363379e-05, 'epoch': 12.0}
>>> 2025-08-24 11:51:34,712 - INFO - >>> {'loss': 2.2418, 'grad_norm': 0.5833466053009033, 'learning_rate': 9.674003791867991e-05, 'epoch': 12.615384615384615}
>>> 2025-08-24 11:51:43,787 - INFO - >>> {'loss': 2.3674, 'grad_norm': 0.6300576329231262, 'learning_rate': 9.645387162638652e-05, 'epoch': 13.0}
>>> 2025-08-24 11:52:00,348 - INFO - >>> {'loss': 2.266, 'grad_norm': 0.6931204199790955, 'learning_rate': 9.615612805539305e-05, 'epoch': 13.615384615384615}
>>> 2025-08-24 11:52:11,481 - INFO - >>> {'loss': 2.2442, 'grad_norm': 0.6324782371520996, 'learning_rate': 9.584688140963944e-05, 'epoch': 14.0}
>>> 2025-08-24 11:52:28,778 - INFO - >>> {'loss': 2.2497, 'grad_norm': 0.6282979249954224, 'learning_rate': 9.552620875987311e-05, 'epoch': 14.615384615384615}
>>> 2025-08-24 11:52:38,175 - INFO - >>> {'loss': 2.1708, 'grad_norm': 0.7485424280166626, 'learning_rate': 9.51941900244412e-05, 'epoch': 15.0}
>>> 2025-08-24 11:52:54,974 - INFO - >>> {'loss': 2.321, 'grad_norm': 0.6680889129638672, 'learning_rate': 9.485090794937319e-05, 'epoch': 15.615384615384615}
>>> 2025-08-24 11:53:05,653 - INFO - >>> {'loss': 2.0506, 'grad_norm': 0.6179815530776978, 'learning_rate': 9.449644808775902e-05, 'epoch': 16.0}
>>> 2025-08-24 11:53:23,479 - INFO - >>> {'loss': 2.1851, 'grad_norm': 0.6468100547790527, 'learning_rate': 9.413089877842736e-05, 'epoch': 16.615384615384617}
>>> 2025-08-24 11:53:33,846 - INFO - >>> {'loss': 2.1256, 'grad_norm': 0.6268352270126343, 'learning_rate': 9.375435112392969e-05, 'epoch': 17.0}
>>> 2025-08-24 11:53:52,270 - INFO - >>> {'loss': 2.151, 'grad_norm': 0.6548537015914917, 'learning_rate': 9.336689896783573e-05, 'epoch': 17.615384615384617}
>>> 2025-08-24 11:54:00,948 - INFO - >>> {'loss': 2.112, 'grad_norm': 0.692197859287262, 'learning_rate': 9.29686388713456e-05, 'epoch': 18.0}
>>> 2025-08-24 11:54:17,808 - INFO - >>> {'loss': 2.1617, 'grad_norm': 0.7003070116043091, 'learning_rate': 9.255967008922474e-05, 'epoch': 18.615384615384617}
>>> 2025-08-24 11:54:28,565 - INFO - >>> {'loss': 2.0457, 'grad_norm': 0.6509020924568176, 'learning_rate': 9.214009454506753e-05, 'epoch': 19.0}
>>> 2025-08-24 11:54:46,395 - INFO - >>> {'loss': 2.1135, 'grad_norm': 0.6724352836608887, 'learning_rate': 9.171001680589588e-05, 'epoch': 19.615384615384617}
>>> 2025-08-24 11:54:54,964 - INFO - >>> {'loss': 2.0332, 'grad_norm': 0.7499798536300659, 'learning_rate': 9.126954405609882e-05, 'epoch': 20.0}
>>> 2025-08-24 11:55:14,483 - INFO - >>> {'loss': 2.091, 'grad_norm': 0.6598544716835022, 'learning_rate': 9.081878607071996e-05, 'epoch': 20.615384615384617}
>>> 2025-08-24 11:55:24,903 - INFO - >>> {'loss': 2.0039, 'grad_norm': 0.6765710115432739, 'learning_rate': 9.035785518809927e-05, 'epoch': 21.0}
>>> 2025-08-24 11:55:42,101 - INFO - >>> {'loss': 2.0396, 'grad_norm': 0.6789984703063965, 'learning_rate': 8.988686628187597e-05, 'epoch': 21.615384615384617}
>>> 2025-08-24 11:55:53,748 - INFO - >>> {'loss': 2.0112, 'grad_norm': 0.5904083251953125, 'learning_rate': 8.940593673235962e-05, 'epoch': 22.0}
>>> 2025-08-24 11:56:11,139 - INFO - >>> {'loss': 2.0323, 'grad_norm': 0.6181341409683228, 'learning_rate': 8.891518639727649e-05, 'epoch': 22.615384615384617}
>>> 2025-08-24 11:56:20,037 - INFO - >>> {'loss': 1.9633, 'grad_norm': 0.6795833706855774, 'learning_rate': 8.841473758189854e-05, 'epoch': 23.0}
>>> 2025-08-24 11:56:37,505 - INFO - >>> {'loss': 2.0293, 'grad_norm': 0.6780845522880554, 'learning_rate': 8.790471500856228e-05, 'epoch': 23.615384615384617}
>>> 2025-08-24 11:56:46,390 - INFO - >>> {'loss': 1.8899, 'grad_norm': 0.6443747282028198, 'learning_rate': 8.738524578558547e-05, 'epoch': 24.0}
>>> 2025-08-24 11:57:05,167 - INFO - >>> {'loss': 1.9723, 'grad_norm': 0.6795402765274048, 'learning_rate': 8.685645937558896e-05, 'epoch': 24.615384615384617}
>>> 2025-08-24 11:57:14,770 - INFO - >>> {'loss': 1.9106, 'grad_norm': 0.648615300655365, 'learning_rate': 8.631848756323197e-05, 'epoch': 25.0}
>>> 2025-08-24 11:57:31,619 - INFO - >>> {'loss': 1.8771, 'grad_norm': 0.6442487835884094, 'learning_rate': 8.577146442236857e-05, 'epoch': 25.615384615384617}
>>> 2025-08-24 11:57:43,327 - INFO - >>> {'loss': 2.011, 'grad_norm': 0.6395440697669983, 'learning_rate': 8.521552628263362e-05, 'epoch': 26.0}
>>> 2025-08-24 11:57:57,814 - INFO - >>> {'loss': 1.8739, 'grad_norm': 0.7007648944854736, 'learning_rate': 8.465081169546659e-05, 'epoch': 26.615384615384617}
>>> 2025-08-24 11:58:09,279 - INFO - >>> {'loss': 1.9403, 'grad_norm': 0.6176594495773315, 'learning_rate': 8.40774613995817e-05, 'epoch': 27.0}
>>> 2025-08-24 11:58:28,747 - INFO - >>> {'loss': 1.9128, 'grad_norm': 0.6947966814041138, 'learning_rate': 8.349561828589277e-05, 'epoch': 27.615384615384617}
>>> 2025-08-24 11:58:36,684 - INFO - >>> {'loss': 1.8048, 'grad_norm': 0.689856767654419, 'learning_rate': 8.290542736190188e-05, 'epoch': 28.0}
>>> 2025-08-24 11:58:55,302 - INFO - >>> {'loss': 1.8969, 'grad_norm': 0.6747297048568726, 'learning_rate': 8.230703571556048e-05, 'epoch': 28.615384615384617}
>>> 2025-08-24 11:59:05,840 - INFO - >>> {'loss': 1.7832, 'grad_norm': 0.630537211894989, 'learning_rate': 8.170059247861194e-05, 'epoch': 29.0}
>>> 2025-08-24 11:59:25,198 - INFO - >>> {'loss': 1.8911, 'grad_norm': 0.6490577459335327, 'learning_rate': 8.108624878942477e-05, 'epoch': 29.615384615384617}
>>> 2025-08-24 11:59:32,519 - INFO - >>> {'loss': 1.7032, 'grad_norm': 0.8356537818908691, 'learning_rate': 8.046415775532585e-05, 'epoch': 30.0}
>>> 2025-08-24 11:59:52,017 - INFO - >>> {'loss': 1.842, 'grad_norm': 0.6414859890937805, 'learning_rate': 7.983447441444281e-05, 'epoch': 30.615384615384617}
>>> 2025-08-24 12:00:01,178 - INFO - >>> {'loss': 1.7599, 'grad_norm': 0.6604441404342651, 'learning_rate': 7.919735569706533e-05, 'epoch': 31.0}
>>> 2025-08-24 12:00:16,968 - INFO - >>> {'loss': 1.732, 'grad_norm': 0.6329131126403809, 'learning_rate': 7.855296038653475e-05, 'epoch': 31.615384615384617}
>>> 2025-08-24 12:00:27,710 - INFO - >>> {'loss': 1.8908, 'grad_norm': 0.6613389253616333, 'learning_rate': 7.790144907967201e-05, 'epoch': 32.0}
>>> 2025-08-24 12:00:45,761 - INFO - >>> {'loss': 1.7401, 'grad_norm': 0.5797635316848755, 'learning_rate': 7.724298414675353e-05, 'epoch': 32.61538461538461}
>>> 2025-08-24 12:00:54,966 - INFO - >>> {'loss': 1.8214, 'grad_norm': 0.7469627261161804, 'learning_rate': 7.657772969104508e-05, 'epoch': 33.0}
>>> 2025-08-24 12:01:11,293 - INFO - >>> {'loss': 1.7443, 'grad_norm': 0.6182936429977417, 'learning_rate': 7.590585150790389e-05, 'epoch': 33.61538461538461}
>>> 2025-08-24 12:01:21,290 - INFO - >>> {'loss': 1.7402, 'grad_norm': 0.7978143095970154, 'learning_rate': 7.522751704345887e-05, 'epoch': 34.0}
>>> 2025-08-24 12:01:40,049 - INFO - >>> {'loss': 1.7057, 'grad_norm': 0.570589542388916, 'learning_rate': 7.454289535287968e-05, 'epoch': 34.61538461538461}
>>> 2025-08-24 12:01:49,090 - INFO - >>> {'loss': 1.7483, 'grad_norm': 0.938433051109314, 'learning_rate': 7.385215705824449e-05, 'epoch': 35.0}
>>> 2025-08-24 12:02:08,613 - INFO - >>> {'loss': 1.7234, 'grad_norm': 0.5656577348709106, 'learning_rate': 7.31554743060174e-05, 'epoch': 35.61538461538461}
>>> 2025-08-24 12:02:18,020 - INFO - >>> {'loss': 1.5886, 'grad_norm': 0.8531631827354431, 'learning_rate': 7.245302072414601e-05, 'epoch': 36.0}
>>> 2025-08-24 12:02:35,416 - INFO - >>> {'loss': 1.6779, 'grad_norm': 0.5993920564651489, 'learning_rate': 7.174497137878966e-05, 'epoch': 36.61538461538461}
>>> 2025-08-24 12:02:47,241 - INFO - >>> {'loss': 1.6003, 'grad_norm': 0.7347621321678162, 'learning_rate': 7.103150273068921e-05, 'epoch': 37.0}
>>> 2025-08-24 12:03:07,660 - INFO - >>> {'loss': 1.6223, 'grad_norm': 0.5632477402687073, 'learning_rate': 7.031279259118946e-05, 'epoch': 37.61538461538461}
>>> 2025-08-24 12:03:15,644 - INFO - >>> {'loss': 1.6045, 'grad_norm': 0.9562720060348511, 'learning_rate': 6.958902007792466e-05, 'epoch': 38.0}
>>> 2025-08-24 12:03:32,647 - INFO - >>> {'loss': 1.5356, 'grad_norm': 0.6042262315750122, 'learning_rate': 6.886036557017881e-05, 'epoch': 38.61538461538461}
>>> 2025-08-24 12:03:42,535 - INFO - >>> {'loss': 1.6375, 'grad_norm': 0.6785352826118469, 'learning_rate': 6.812701066393124e-05, 'epoch': 39.0}
>>> 2025-08-24 12:03:57,633 - INFO - >>> {'loss': 1.526, 'grad_norm': 0.6168742179870605, 'learning_rate': 6.738913812659912e-05, 'epoch': 39.61538461538461}
>>> 2025-08-24 12:04:08,409 - INFO - >>> {'loss': 1.5729, 'grad_norm': 0.6902583241462708, 'learning_rate': 6.664693185148807e-05, 'epoch': 40.0}
>>> 2025-08-24 12:04:27,192 - INFO - >>> {'loss': 1.5166, 'grad_norm': 0.6053202152252197, 'learning_rate': 6.590057681196191e-05, 'epoch': 40.61538461538461}
>>> 2025-08-24 12:04:34,295 - INFO - >>> {'loss': 1.5074, 'grad_norm': 1.1026688814163208, 'learning_rate': 6.515025901534364e-05, 'epoch': 41.0}
>>> 2025-08-24 12:04:51,771 - INFO - >>> {'loss': 1.427, 'grad_norm': 0.6620516777038574, 'learning_rate': 6.439616545655834e-05, 'epoch': 41.61538461538461}
>>> 2025-08-24 12:05:02,353 - INFO - >>> {'loss': 1.534, 'grad_norm': 0.6954311728477478, 'learning_rate': 6.363848407153016e-05, 'epoch': 42.0}
>>> 2025-08-24 12:05:18,825 - INFO - >>> {'loss': 1.418, 'grad_norm': 0.626167356967926, 'learning_rate': 6.287740369034485e-05, 'epoch': 42.61538461538461}
>>> 2025-08-24 12:05:29,567 - INFO - >>> {'loss': 1.474, 'grad_norm': 0.8963415026664734, 'learning_rate': 6.211311399018916e-05, 'epoch': 43.0}
>>> 2025-08-24 12:05:45,655 - INFO - >>> {'loss': 1.3706, 'grad_norm': 0.7174260020256042, 'learning_rate': 6.13458054480795e-05, 'epoch': 43.61538461538461}
>>> 2025-08-24 12:05:56,452 - INFO - >>> {'loss': 1.4448, 'grad_norm': 0.7964538931846619, 'learning_rate': 6.0575669293390954e-05, 'epoch': 44.0}
>>> 2025-08-24 12:06:13,737 - INFO - >>> {'loss': 1.4177, 'grad_norm': 0.6518447995185852, 'learning_rate': 5.980289746019892e-05, 'epoch': 44.61538461538461}
>>> 2025-08-24 12:06:22,231 - INFO - >>> {'loss': 1.2806, 'grad_norm': 0.8987723588943481, 'learning_rate': 5.9027682539445104e-05, 'epoch': 45.0}
>>> 2025-08-24 12:06:38,019 - INFO - >>> {'loss': 1.354, 'grad_norm': 0.7678878307342529, 'learning_rate': 5.8250217730939973e-05, 'epoch': 45.61538461538461}
>>> 2025-08-24 12:06:49,911 - INFO - >>> {'loss': 1.2948, 'grad_norm': 0.8508551716804504, 'learning_rate': 5.747069679521305e-05, 'epoch': 46.0}
>>> 2025-08-24 12:07:06,788 - INFO - >>> {'loss': 1.2926, 'grad_norm': 0.6994855403900146, 'learning_rate': 5.668931400522396e-05, 'epoch': 46.61538461538461}
>>> 2025-08-24 12:07:16,738 - INFO - >>> {'loss': 1.281, 'grad_norm': 1.036634922027588, 'learning_rate': 5.5906264097945407e-05, 'epoch': 47.0}
>>> 2025-08-24 12:07:34,483 - INFO - >>> {'loss': 1.2625, 'grad_norm': 0.7593111991882324, 'learning_rate': 5.5121742225830665e-05, 'epoch': 47.61538461538461}
>>> 2025-08-24 12:07:44,886 - INFO - >>> {'loss': 1.2224, 'grad_norm': 0.9155545830726624, 'learning_rate': 5.433594390817756e-05, 'epoch': 48.0}
>>> 2025-08-24 12:08:03,143 - INFO - >>> {'loss': 1.1602, 'grad_norm': 0.7344493269920349, 'learning_rate': 5.35490649824008e-05, 'epoch': 48.61538461538461}
>>> 2025-08-24 12:08:12,732 - INFO - >>> {'loss': 1.3182, 'grad_norm': 1.118806004524231, 'learning_rate': 5.276130155522541e-05, 'epoch': 49.0}
>>> 2025-08-24 12:08:30,714 - INFO - >>> {'loss': 1.2418, 'grad_norm': 0.7601959109306335, 'learning_rate': 5.1972849953812644e-05, 'epoch': 49.61538461538461}
>>> 2025-08-24 12:08:38,914 - INFO - >>> {'loss': 0.9951, 'grad_norm': 1.1163990497589111, 'learning_rate': 5.1183906676831197e-05, 'epoch': 50.0}
>>> 2025-08-24 12:08:56,643 - INFO - >>> {'loss': 1.1163, 'grad_norm': 0.7541620135307312, 'learning_rate': 5.039466834548568e-05, 'epoch': 50.61538461538461}
>>> 2025-08-24 12:09:04,748 - INFO - >>> {'loss': 1.1345, 'grad_norm': 1.31440269947052, 'learning_rate': 4.960533165451435e-05, 'epoch': 51.0}
>>> 2025-08-24 12:09:23,355 - INFO - >>> {'loss': 1.1219, 'grad_norm': 0.8248103857040405, 'learning_rate': 4.8816093323168815e-05, 'epoch': 51.61538461538461}
>>> 2025-08-24 12:09:32,488 - INFO - >>> {'loss': 1.0029, 'grad_norm': 1.1601041555404663, 'learning_rate': 4.802715004618737e-05, 'epoch': 52.0}
>>> 2025-08-24 12:09:50,434 - INFO - >>> {'loss': 1.0333, 'grad_norm': 0.8718932271003723, 'learning_rate': 4.7238698444774595e-05, 'epoch': 52.61538461538461}
>>> 2025-08-24 12:10:01,303 - INFO - >>> {'loss': 1.0362, 'grad_norm': 1.2095019817352295, 'learning_rate': 4.64509350175992e-05, 'epoch': 53.0}
>>> 2025-08-24 12:10:15,621 - INFO - >>> {'loss': 0.9921, 'grad_norm': 0.8773055672645569, 'learning_rate': 4.566405609182247e-05, 'epoch': 53.61538461538461}
>>> 2025-08-24 12:10:25,600 - INFO - >>> {'loss': 0.9904, 'grad_norm': 1.3165949583053589, 'learning_rate': 4.4878257774169346e-05, 'epoch': 54.0}
>>> 2025-08-24 12:10:40,885 - INFO - >>> {'loss': 0.879, 'grad_norm': 1.0507209300994873, 'learning_rate': 4.4093735902054605e-05, 'epoch': 54.61538461538461}
>>> 2025-08-24 12:10:52,429 - INFO - >>> {'loss': 1.0155, 'grad_norm': 1.1176791191101074, 'learning_rate': 4.331068599477605e-05, 'epoch': 55.0}
>>> 2025-08-24 12:11:10,112 - INFO - >>> {'loss': 0.8864, 'grad_norm': 0.9577789306640625, 'learning_rate': 4.2529303204786953e-05, 'epoch': 55.61538461538461}
>>> 2025-08-24 12:11:20,294 - INFO - >>> {'loss': 0.9419, 'grad_norm': 1.377012014389038, 'learning_rate': 4.1749782269060045e-05, 'epoch': 56.0}
>>> 2025-08-24 12:11:36,769 - INFO - >>> {'loss': 0.8749, 'grad_norm': 1.0187121629714966, 'learning_rate': 4.097231746055491e-05, 'epoch': 56.61538461538461}
>>> 2025-08-24 12:11:46,603 - INFO - >>> {'loss': 0.8343, 'grad_norm': 1.3244290351867676, 'learning_rate': 4.01971025398011e-05, 'epoch': 57.0}
>>> 2025-08-24 12:12:06,338 - INFO - >>> {'loss': 0.8127, 'grad_norm': 0.9212909936904907, 'learning_rate': 3.942433070660905e-05, 'epoch': 57.61538461538461}
>>> 2025-08-24 12:12:13,796 - INFO - >>> {'loss': 0.8324, 'grad_norm': 1.799425482749939, 'learning_rate': 3.8654194551920485e-05, 'epoch': 58.0}
>>> 2025-08-24 12:12:31,307 - INFO - >>> {'loss': 0.8113, 'grad_norm': 1.1288542747497559, 'learning_rate': 3.788688600981085e-05, 'epoch': 58.61538461538461}
>>> 2025-08-24 12:12:42,234 - INFO - >>> {'loss': 0.7192, 'grad_norm': 1.3103772401809692, 'learning_rate': 3.712259630965518e-05, 'epoch': 59.0}
>>> 2025-08-24 12:13:00,065 - INFO - >>> {'loss': 0.7304, 'grad_norm': 0.9962290525436401, 'learning_rate': 3.636151592846985e-05, 'epoch': 59.61538461538461}
>>> 2025-08-24 12:13:08,507 - INFO - >>> {'loss': 0.7398, 'grad_norm': 1.6673115491867065, 'learning_rate': 3.560383454344168e-05, 'epoch': 60.0}
>>> 2025-08-24 12:13:24,498 - INFO - >>> {'loss': 0.7235, 'grad_norm': 1.2061865329742432, 'learning_rate': 3.484974098465636e-05, 'epoch': 60.61538461538461}
>>> 2025-08-24 12:13:37,174 - INFO - >>> {'loss': 0.6546, 'grad_norm': 1.2872148752212524, 'learning_rate': 3.409942318803809e-05, 'epoch': 61.0}
>>> 2025-08-24 12:13:54,768 - INFO - >>> {'loss': 0.6452, 'grad_norm': 1.0251303911209106, 'learning_rate': 3.335306814851196e-05, 'epoch': 61.61538461538461}
>>> 2025-08-24 12:14:05,228 - INFO - >>> {'loss': 0.671, 'grad_norm': 1.5578725337982178, 'learning_rate': 3.261086187340088e-05, 'epoch': 62.0}
>>> 2025-08-24 12:14:19,465 - INFO - >>> {'loss': 0.6583, 'grad_norm': 1.127974033355713, 'learning_rate': 3.187298933606878e-05, 'epoch': 62.61538461538461}
>>> 2025-08-24 12:14:30,172 - INFO - >>> {'loss': 0.5656, 'grad_norm': 1.3772649765014648, 'learning_rate': 3.11396344298212e-05, 'epoch': 63.0}
>>> 2025-08-24 12:14:48,745 - INFO - >>> {'loss': 0.5956, 'grad_norm': 1.121029257774353, 'learning_rate': 3.0410979922075343e-05, 'epoch': 63.61538461538461}
>>> 2025-08-24 12:14:57,754 - INFO - >>> {'loss': 0.5676, 'grad_norm': 1.3742072582244873, 'learning_rate': 2.9687207408810557e-05, 'epoch': 64.0}
>>> 2025-08-24 12:15:14,684 - INFO - >>> {'loss': 0.5906, 'grad_norm': 1.0623221397399902, 'learning_rate': 2.8968497269310803e-05, 'epoch': 64.61538461538461}
>>> 2025-08-24 12:15:25,642 - INFO - >>> {'loss': 0.4784, 'grad_norm': 1.4441670179367065, 'learning_rate': 2.8255028621210355e-05, 'epoch': 65.0}
>>> 2025-08-24 12:15:43,065 - INFO - >>> {'loss': 0.4708, 'grad_norm': 1.1570541858673096, 'learning_rate': 2.754697927585399e-05, 'epoch': 65.61538461538461}
>>> 2025-08-24 12:15:53,589 - INFO - >>> {'loss': 0.5687, 'grad_norm': 1.309243083000183, 'learning_rate': 2.6844525693982613e-05, 'epoch': 66.0}
>>> 2025-08-24 12:16:10,013 - INFO - >>> {'loss': 0.5361, 'grad_norm': 1.052319884300232, 'learning_rate': 2.614784294175554e-05, 'epoch': 66.61538461538461}
>>> 2025-08-24 12:16:22,102 - INFO - >>> {'loss': 0.4166, 'grad_norm': 1.4416172504425049, 'learning_rate': 2.5457104647120322e-05, 'epoch': 67.0}
>>> 2025-08-24 12:16:40,794 - INFO - >>> {'loss': 0.4798, 'grad_norm': 1.0029127597808838, 'learning_rate': 2.4772482956541132e-05, 'epoch': 67.61538461538461}
>>> 2025-08-24 12:16:51,047 - INFO - >>> {'loss': 0.4345, 'grad_norm': 1.6010226011276245, 'learning_rate': 2.4094148492096125e-05, 'epoch': 68.0}
>>> 2025-08-24 12:17:10,435 - INFO - >>> {'loss': 0.4612, 'grad_norm': 1.002724051475525, 'learning_rate': 2.3422270308954934e-05, 'epoch': 68.61538461538461}
>>> 2025-08-24 12:17:19,804 - INFO - >>> {'loss': 0.3974, 'grad_norm': 1.4760427474975586, 'learning_rate': 2.2757015853246493e-05, 'epoch': 69.0}
>>> 2025-08-24 12:17:39,358 - INFO - >>> {'loss': 0.3956, 'grad_norm': 0.9654204845428467, 'learning_rate': 2.2098550920327998e-05, 'epoch': 69.61538461538461}
>>> 2025-08-24 12:17:50,118 - INFO - >>> {'loss': 0.4501, 'grad_norm': 1.5505322217941284, 'learning_rate': 2.1447039613465265e-05, 'epoch': 70.0}
>>> 2025-08-24 12:18:08,430 - INFO - >>> {'loss': 0.3893, 'grad_norm': 1.0033990144729614, 'learning_rate': 2.0802644302934683e-05, 'epoch': 70.61538461538461}
>>> 2025-08-24 12:18:17,822 - INFO - >>> {'loss': 0.4056, 'grad_norm': 1.4640446901321411, 'learning_rate': 2.0165525585557204e-05, 'epoch': 71.0}
>>> 2025-08-24 12:18:33,753 - INFO - >>> {'loss': 0.3354, 'grad_norm': 1.1328037977218628, 'learning_rate': 1.953584224467418e-05, 'epoch': 71.61538461538461}
>>> 2025-08-24 12:18:44,918 - INFO - >>> {'loss': 0.4148, 'grad_norm': 1.1835893392562866, 'learning_rate': 1.8913751210575248e-05, 'epoch': 72.0}
>>> 2025-08-24 12:18:59,876 - INFO - >>> {'loss': 0.3514, 'grad_norm': 1.1666314601898193, 'learning_rate': 1.8299407521388067e-05, 'epoch': 72.61538461538461}
>>> 2025-08-24 12:19:11,347 - INFO - >>> {'loss': 0.3572, 'grad_norm': 1.1227059364318848, 'learning_rate': 1.7692964284439505e-05, 'epoch': 73.0}
>>> 2025-08-24 12:19:26,948 - INFO - >>> {'loss': 0.3458, 'grad_norm': 0.9966959953308105, 'learning_rate': 1.7094572638098123e-05, 'epoch': 73.61538461538461}
>>> 2025-08-24 12:19:38,081 - INFO - >>> {'loss': 0.3263, 'grad_norm': 1.3409450054168701, 'learning_rate': 1.6504381714107252e-05, 'epoch': 74.0}
>>> 2025-08-24 12:19:56,404 - INFO - >>> {'loss': 0.2892, 'grad_norm': 0.9926612377166748, 'learning_rate': 1.5922538600418318e-05, 'epoch': 74.61538461538461}
>>> 2025-08-24 12:20:05,623 - INFO - >>> {'loss': 0.3684, 'grad_norm': 1.2845985889434814, 'learning_rate': 1.5349188304533413e-05, 'epoch': 75.0}
>>> 2025-08-24 12:20:22,594 - INFO - >>> {'loss': 0.2927, 'grad_norm': 0.940920889377594, 'learning_rate': 1.4784473717366387e-05, 'epoch': 75.61538461538461}
>>> 2025-08-24 12:20:32,896 - INFO - >>> {'loss': 0.3281, 'grad_norm': 1.2977032661437988, 'learning_rate': 1.4228535577631442e-05, 'epoch': 76.0}
>>> 2025-08-24 12:20:49,615 - INFO - >>> {'loss': 0.3262, 'grad_norm': 0.9263949394226074, 'learning_rate': 1.3681512436768045e-05, 'epoch': 76.61538461538461}
>>> 2025-08-24 12:21:00,617 - INFO - >>> {'loss': 0.2297, 'grad_norm': 1.3320146799087524, 'learning_rate': 1.314354062441106e-05, 'epoch': 77.0}
>>> 2025-08-24 12:21:19,027 - INFO - >>> {'loss': 0.2781, 'grad_norm': 1.003759741783142, 'learning_rate': 1.2614754214414548e-05, 'epoch': 77.61538461538461}
>>> 2025-08-24 12:21:28,502 - INFO - >>> {'loss': 0.2851, 'grad_norm': 1.1271748542785645, 'learning_rate': 1.2095284991437733e-05, 'epoch': 78.0}
>>> 2025-08-24 12:21:48,235 - INFO - >>> {'loss': 0.2317, 'grad_norm': 0.9437592625617981, 'learning_rate': 1.1585262418101467e-05, 'epoch': 78.61538461538461}
>>> 2025-08-24 12:21:57,558 - INFO - >>> {'loss': 0.3325, 'grad_norm': 1.223781704902649, 'learning_rate': 1.1084813602723515e-05, 'epoch': 79.0}
>>> 2025-08-24 12:22:16,212 - INFO - >>> {'loss': 0.2505, 'grad_norm': 0.8237596750259399, 'learning_rate': 1.0594063267640386e-05, 'epoch': 79.61538461538461}
>>> 2025-08-24 12:22:25,060 - INFO - >>> {'loss': 0.2908, 'grad_norm': 1.6478041410446167, 'learning_rate': 1.0113133718124035e-05, 'epoch': 80.0}
>>> 2025-08-24 12:22:45,431 - INFO - >>> {'loss': 0.266, 'grad_norm': 0.8733077049255371, 'learning_rate': 9.642144811900739e-06, 'epoch': 80.61538461538461}
>>> 2025-08-24 12:22:53,854 - INFO - >>> {'loss': 0.2182, 'grad_norm': 1.3333802223205566, 'learning_rate': 9.181213929280046e-06, 'epoch': 81.0}
>>> 2025-08-24 12:23:11,650 - INFO - >>> {'loss': 0.202, 'grad_norm': 0.883886456489563, 'learning_rate': 8.7304559439012e-06, 'epoch': 81.61538461538461}
>>> 2025-08-24 12:23:22,089 - INFO - >>> {'loss': 0.3135, 'grad_norm': 1.2461739778518677, 'learning_rate': 8.28998319410413e-06, 'epoch': 82.0}
>>> 2025-08-24 12:23:40,916 - INFO - >>> {'loss': 0.2214, 'grad_norm': 0.8719131350517273, 'learning_rate': 7.859905454932471e-06, 'epoch': 82.61538461538461}
>>> 2025-08-29 22:25:30,848 - INFO - 导入包完成
>>> 2025-08-29 22:25:32,414 - INFO - ========train Qwen2ForCausalLM  202508292225========
>>> 2025-08-29 22:25:34,925 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-29 22:25:38,078 - INFO - 开始进行训练
>>> 2025-08-29 22:25:39,695 - INFO - 基础配置文件读取完成
>>> 2025-08-29 22:25:41,383 - INFO - 训练配置读取完成
>>> 2025-08-29 22:25:43,011 - INFO - 数据集路径：dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-08-29 22:25:45,823 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-08-29 22:25:48,551 - INFO - tokenizer读取完成
>>> 2025-08-29 22:25:53,123 - INFO - model dtype:torch.float16
>>> 2025-08-29 22:25:55,092 - INFO - 模型导入完成
>>> 2025-08-29 22:25:56,679 - INFO - 数据读取开始
>>> 2025-08-29 22:25:59,779 - INFO - 数据下载完成
>>> 2025-08-29 22:35:01,786 - INFO - 导入包完成
>>> 2025-08-29 22:35:03,352 - INFO - ========train Qwen2ForCausalLM  202508292235========
>>> 2025-08-29 22:35:05,863 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-29 22:35:09,016 - INFO - 开始进行训练
>>> 2025-08-29 22:35:10,632 - INFO - 基础配置文件读取完成
>>> 2025-08-29 22:35:12,320 - INFO - 训练配置读取完成
>>> 2025-08-29 22:35:13,947 - INFO - 数据集路径：dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-08-29 22:35:16,760 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-08-29 22:35:19,504 - INFO - tokenizer读取完成
>>> 2025-08-29 22:35:23,656 - INFO - model dtype:torch.float16
>>> 2025-08-29 22:35:25,625 - INFO - 模型导入完成
>>> 2025-08-29 22:35:27,212 - INFO - 数据读取开始
>>> 2025-08-29 22:35:29,421 - INFO - 数据下载完成
>>> 2025-08-29 22:40:47,340 - INFO - 数据映射完成
>>> 2025-08-29 22:40:48,826 - INFO - 打印训练参数如下
>>> 2025-08-29 22:40:50,293 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-08-29 22:40:52,121 - INFO -   dtype >>> torch.float16
>>> 2025-08-29 22:40:53,949 - INFO -   load_in_4bit >>> True
>>> 2025-08-29 22:40:55,736 - INFO -   batch_size >>> 8
>>> 2025-08-29 22:40:57,424 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-08-29 22:40:59,432 - INFO -   warmup_steps >>> 1
>>> 2025-08-29 22:41:01,159 - INFO -   epoch >>> 100
>>> 2025-08-29 22:41:02,786 - INFO -   eval_steps >>> 10
>>> 2025-08-29 22:41:04,494 - INFO -   learning_rate >>> 0.0001
>>> 2025-08-29 22:41:06,341 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-08-29 22:41:08,270 - INFO -   max_seq_length >>> 4096
>>> 2025-08-29 22:41:10,097 - INFO -   r >>> 8
>>> 2025-08-29 22:41:11,604 - INFO -   interface_mode >>> False
>>> 2025-08-29 22:41:13,452 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-08-29 22:41:15,802 - INFO -   lora_alpha >>> 16
>>> 2025-08-29 22:41:17,510 - INFO -   lora_dropout >>> 0.05
>>> 2025-08-29 22:41:19,298 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-08-29 22:41:34,997 - INFO - 开始训练！
>>> 2025-08-29 22:41:36,585 - INFO - 批次大小  : 8
>>> 2025-08-29 22:41:38,253 - INFO - 训练轮数  : 100
>>> 2025-08-29 22:41:39,960 - INFO - 学习率    : 0.0001
>>> 2025-08-29 22:41:41,748 - INFO - 数据集路径: dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-08-29 22:41:44,601 - INFO - 模型路径  : /home/liangshuqiao/models/qwen3-8b
>>> 2025-08-29 22:41:51,752 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacity of 31.73 GiB of which 7.41 GiB is free. Including non-PyTorch memory, this process has 24.32 GiB memory in use. Of the allocated memory 23.48 GiB is allocated by PyTorch, and 488.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-08-30 11:58:07,430 - INFO - 导入包完成
>>> 2025-08-30 11:58:08,997 - INFO - ========train Qwen2ForCausalLM  202508301158========
>>> 2025-08-30 11:58:11,507 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-30 11:58:14,661 - INFO - 开始进行训练
>>> 2025-08-30 11:58:16,279 - INFO - 基础配置文件读取完成
>>> 2025-08-30 11:58:17,967 - INFO - 训练配置读取完成
>>> 2025-08-30 11:58:19,594 - INFO - 数据集路径：dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-08-30 11:58:22,406 - INFO - 模型路径:/home/liangshuqiao/models/Qwen2-7B
>>> 2025-08-30 11:58:25,237 - INFO - tokenizer读取完成
>>> 2025-08-30 11:59:59,598 - INFO - model dtype:torch.float16
>>> 2025-08-30 12:00:01,567 - INFO - 模型导入完成
>>> 2025-08-30 12:00:03,154 - INFO - 数据读取开始
>>> 2025-08-30 12:00:05,378 - INFO - 数据下载完成
>>> 2025-08-30 12:05:24,944 - INFO - 数据映射完成
>>> 2025-08-30 12:05:26,431 - INFO - 打印训练参数如下
>>> 2025-08-30 12:05:27,898 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-08-30 12:05:29,726 - INFO -   dtype >>> torch.float16
>>> 2025-08-30 12:05:31,554 - INFO -   load_in_4bit >>> True
>>> 2025-08-30 12:05:33,342 - INFO -   batch_size >>> 8
>>> 2025-08-30 12:05:35,030 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-08-30 12:05:37,038 - INFO -   warmup_steps >>> 1
>>> 2025-08-30 12:05:38,765 - INFO -   epoch >>> 100
>>> 2025-08-30 12:05:40,392 - INFO -   eval_steps >>> 10
>>> 2025-08-30 12:05:42,099 - INFO -   learning_rate >>> 0.0001
>>> 2025-08-30 12:05:43,947 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-08-30 12:05:45,875 - INFO -   max_seq_length >>> 4096
>>> 2025-08-30 12:05:47,703 - INFO -   r >>> 8
>>> 2025-08-30 12:05:49,210 - INFO -   interface_mode >>> False
>>> 2025-08-30 12:05:51,058 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-08-30 12:05:53,408 - INFO -   lora_alpha >>> 16
>>> 2025-08-30 12:05:55,116 - INFO -   lora_dropout >>> 0.05
>>> 2025-08-30 12:05:56,904 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-08-30 12:06:12,260 - INFO - 开始训练！
>>> 2025-08-30 12:06:13,846 - INFO - 批次大小  : 8
>>> 2025-08-30 12:06:15,513 - INFO - 训练轮数  : 100
>>> 2025-08-30 12:06:17,220 - INFO - 学习率    : 0.0001
>>> 2025-08-30 12:06:19,008 - INFO - 数据集路径: dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-08-30 12:06:21,860 - INFO - 模型路径  : /home/liangshuqiao/models/Qwen2-7B
>>> 2025-08-30 12:06:28,459 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 7.96 GiB. GPU 0 has a total capacity of 31.73 GiB of which 1.69 GiB is free. Including non-PyTorch memory, this process has 30.04 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 561.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-08-30 12:08:29,905 - INFO - 导入包完成
>>> 2025-08-30 12:08:31,473 - INFO - ========train Qwen2ForCausalLM  202508301208========
>>> 2025-08-30 12:08:33,984 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-30 12:08:37,139 - INFO - 开始进行训练
>>> 2025-08-30 12:08:38,758 - INFO - 基础配置文件读取完成
>>> 2025-08-30 12:08:40,447 - INFO - 训练配置读取完成
>>> 2025-08-30 12:08:42,074 - INFO - 数据集路径：dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-08-30 12:08:44,887 - INFO - 模型路径:/home/liangshuqiao/models/Qwen2-7B
>>> 2025-08-30 12:08:47,622 - INFO - tokenizer读取完成
>>> 2025-08-30 12:08:59,953 - INFO - model dtype:torch.float16
>>> 2025-08-30 12:09:01,924 - INFO - 模型导入完成
>>> 2025-08-30 12:09:03,511 - INFO - 数据读取开始
>>> 2025-08-30 12:09:05,795 - INFO - 数据下载完成
>>> 2025-08-30 12:09:11,374 - INFO - 数据映射完成
>>> 2025-08-30 12:09:12,861 - INFO - 打印训练参数如下
>>> 2025-08-30 12:09:14,328 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-08-30 12:09:16,157 - INFO -   dtype >>> torch.float16
>>> 2025-08-30 12:09:17,986 - INFO -   load_in_4bit >>> True
>>> 2025-08-30 12:09:19,775 - INFO -   batch_size >>> 8
>>> 2025-08-30 12:09:21,464 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-08-30 12:09:23,474 - INFO -   warmup_steps >>> 1
>>> 2025-08-30 12:09:25,203 - INFO -   epoch >>> 20
>>> 2025-08-30 12:09:26,811 - INFO -   eval_steps >>> 10
>>> 2025-08-30 12:09:28,519 - INFO -   learning_rate >>> 2e-05
>>> 2025-08-30 12:09:30,348 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-08-30 12:09:32,277 - INFO -   max_seq_length >>> 4096
>>> 2025-08-30 12:09:34,111 - INFO -   r >>> 8
>>> 2025-08-30 12:09:35,619 - INFO -   interface_mode >>> False
>>> 2025-08-30 12:09:37,468 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-08-30 12:09:39,820 - INFO -   lora_alpha >>> 16
>>> 2025-08-30 12:09:41,527 - INFO -   lora_dropout >>> 0.05
>>> 2025-08-30 12:09:43,316 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-08-30 12:09:58,353 - INFO - 开始训练！
>>> 2025-08-30 12:10:04,150 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 7.96 GiB. GPU 0 has a total capacity of 31.73 GiB of which 1.70 GiB is free. Including non-PyTorch memory, this process has 30.03 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 561.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-08-30 12:15:15,943 - INFO - 导入包完成
>>> 2025-08-30 12:15:17,510 - INFO - ========train Qwen2ForCausalLM  202508301215========
>>> 2025-08-30 12:15:20,022 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-30 12:15:23,177 - INFO - 开始进行训练
>>> 2025-08-30 12:15:24,794 - INFO - 基础配置文件读取完成
>>> 2025-08-30 12:15:26,482 - INFO - 训练配置读取完成
>>> 2025-08-30 12:15:28,110 - INFO - 数据集路径：dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-08-30 12:15:30,922 - INFO - 模型路径:/home/liangshuqiao/models/Qwen2-7B
>>> 2025-08-30 12:15:33,652 - INFO - tokenizer读取完成
>>> 2025-08-30 12:15:37,678 - INFO - model dtype:torch.float16
>>> 2025-08-30 12:15:39,646 - INFO - 模型导入完成
>>> 2025-08-30 12:15:41,233 - INFO - 数据读取开始
>>> 2025-08-30 12:15:43,838 - INFO - 数据下载完成
>>> 2025-08-30 12:15:49,372 - INFO - 数据映射完成
>>> 2025-08-30 12:15:50,859 - INFO - 打印训练参数如下
>>> 2025-08-30 12:15:52,326 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-08-30 12:15:54,154 - INFO -   dtype >>> torch.float16
>>> 2025-08-30 12:15:55,983 - INFO -   load_in_4bit >>> True
>>> 2025-08-30 12:15:57,771 - INFO -   batch_size >>> 8
>>> 2025-08-30 12:15:59,459 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-08-30 12:16:01,468 - INFO -   warmup_steps >>> 1
>>> 2025-08-30 12:16:03,196 - INFO -   epoch >>> 20
>>> 2025-08-30 12:16:04,803 - INFO -   eval_steps >>> 10
>>> 2025-08-30 12:16:06,510 - INFO -   learning_rate >>> 2e-05
>>> 2025-08-30 12:16:08,338 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-08-30 12:16:10,267 - INFO -   max_seq_length >>> 4096
>>> 2025-08-30 12:16:12,095 - INFO -   r >>> 8
>>> 2025-08-30 12:16:13,602 - INFO -   interface_mode >>> False
>>> 2025-08-30 12:16:15,450 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-08-30 12:16:17,801 - INFO -   lora_alpha >>> 16
>>> 2025-08-30 12:16:19,509 - INFO -   lora_dropout >>> 0.05
>>> 2025-08-30 12:16:21,297 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-08-30 12:16:36,142 - INFO - 开始训练！
>>> 2025-08-30 12:16:41,857 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 7.96 GiB. GPU 0 has a total capacity of 31.73 GiB of which 1.70 GiB is free. Including non-PyTorch memory, this process has 30.03 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 561.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-08-30 12:21:28,838 - INFO - 导入包完成
>>> 2025-08-30 12:21:30,405 - INFO - ========train Qwen2ForCausalLM  202508301221========
>>> 2025-08-30 12:21:32,916 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-30 12:21:36,070 - INFO - 开始进行训练
>>> 2025-08-30 12:21:37,689 - INFO - 基础配置文件读取完成
>>> 2025-08-30 12:21:39,377 - INFO - 训练配置读取完成
>>> 2025-08-30 12:21:41,004 - INFO - 数据集路径：dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-08-30 12:21:43,816 - INFO - 模型路径:/home/liangshuqiao/models/Qwen2-7B
>>> 2025-08-30 12:21:46,550 - INFO - tokenizer读取完成
>>> 2025-08-30 12:21:48,417 - INFO - model dtype:torch.bfloat16
>>> 2025-08-30 12:21:50,405 - INFO - 模型导入完成
>>> 2025-08-30 12:21:51,993 - INFO - 数据读取开始
>>> 2025-08-30 12:21:54,256 - INFO - 数据下载完成
>>> 2025-08-30 12:21:59,669 - INFO - 数据映射完成
>>> 2025-08-30 12:22:01,155 - INFO - 打印训练参数如下
>>> 2025-08-30 12:22:02,622 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-08-30 12:22:04,450 - INFO -   dtype >>> torch.bfloat16
>>> 2025-08-30 12:22:06,298 - INFO -   load_in_4bit >>> True
>>> 2025-08-30 12:22:08,086 - INFO -   batch_size >>> 8
>>> 2025-08-30 12:22:09,773 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-08-30 12:22:11,783 - INFO -   warmup_steps >>> 1
>>> 2025-08-30 12:22:13,511 - INFO -   epoch >>> 20
>>> 2025-08-30 12:22:15,118 - INFO -   eval_steps >>> 10
>>> 2025-08-30 12:22:16,825 - INFO -   learning_rate >>> 2e-05
>>> 2025-08-30 12:22:18,653 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-08-30 12:22:20,581 - INFO -   max_seq_length >>> 4096
>>> 2025-08-30 12:22:22,409 - INFO -   r >>> 8
>>> 2025-08-30 12:22:23,915 - INFO -   interface_mode >>> False
>>> 2025-08-30 12:22:25,763 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-08-30 12:22:28,113 - INFO -   lora_alpha >>> 16
>>> 2025-08-30 12:22:29,820 - INFO -   lora_dropout >>> 0.05
>>> 2025-08-30 12:22:31,607 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-08-30 12:22:38,937 - INFO - 开始训练！
>>> 2025-08-30 12:23:04,111 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 7.96 GiB. GPU 0 has a total capacity of 31.73 GiB of which 1.49 GiB is free. Including non-PyTorch memory, this process has 30.24 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 777.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-08-30 12:23:37,661 - INFO - 导入包完成
>>> 2025-08-30 12:23:39,228 - INFO - ========train Qwen2ForCausalLM  202508301223========
>>> 2025-08-30 12:23:41,738 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-30 12:23:44,891 - INFO - 开始进行训练
>>> 2025-08-30 12:23:46,507 - INFO - 基础配置文件读取完成
>>> 2025-08-30 12:23:48,194 - INFO - 训练配置读取完成
>>> 2025-08-30 12:23:49,820 - INFO - 数据集路径：dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-08-30 12:23:52,632 - INFO - 模型路径:/home/liangshuqiao/models/Qwen2-7B
>>> 2025-08-30 12:23:55,353 - INFO - tokenizer读取完成
>>> 2025-08-30 12:23:57,225 - INFO - model dtype:torch.bfloat16
>>> 2025-08-30 12:23:59,212 - INFO - 模型导入完成
>>> 2025-08-30 12:24:00,799 - INFO - 数据读取开始
>>> 2025-08-30 12:24:02,998 - INFO - 数据下载完成
>>> 2025-08-30 12:24:08,485 - INFO - 数据映射完成
>>> 2025-08-30 12:24:09,972 - INFO - 打印训练参数如下
>>> 2025-08-30 12:24:11,439 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-08-30 12:24:13,267 - INFO -   dtype >>> torch.bfloat16
>>> 2025-08-30 12:24:15,115 - INFO -   load_in_4bit >>> True
>>> 2025-08-30 12:24:16,904 - INFO -   batch_size >>> 4
>>> 2025-08-30 12:24:18,592 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-08-30 12:24:20,600 - INFO -   warmup_steps >>> 1
>>> 2025-08-30 12:24:22,328 - INFO -   epoch >>> 20
>>> 2025-08-30 12:24:23,935 - INFO -   eval_steps >>> 10
>>> 2025-08-30 12:24:25,642 - INFO -   learning_rate >>> 2e-05
>>> 2025-08-30 12:24:27,470 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-08-30 12:24:29,398 - INFO -   max_seq_length >>> 4096
>>> 2025-08-30 12:24:31,227 - INFO -   r >>> 8
>>> 2025-08-30 12:24:32,733 - INFO -   interface_mode >>> False
>>> 2025-08-30 12:24:34,582 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-08-30 12:24:36,933 - INFO -   lora_alpha >>> 16
>>> 2025-08-30 12:24:38,640 - INFO -   lora_dropout >>> 0.05
>>> 2025-08-30 12:24:40,428 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-08-30 12:24:47,650 - INFO - 开始训练！
>>> 2025-08-30 12:27:14,471 - INFO - >>> {'loss': 2.3029, 'grad_norm': 0.9173410534858704, 'learning_rate': 0.0, 'epoch': 0.0033613445378151263}
>>> 2025-08-30 12:29:34,955 - INFO - >>> {'loss': 2.3415, 'grad_norm': 0.9432646632194519, 'learning_rate': 2e-05, 'epoch': 0.0067226890756302525}
>>> 2025-08-30 12:29:47,732 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 4.25 GiB. GPU 0 has a total capacity of 31.73 GiB of which 2.66 GiB is free. Including non-PyTorch memory, this process has 29.07 GiB memory in use. Of the allocated memory 24.30 GiB is allocated by PyTorch, and 4.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-08-30 14:39:39,286 - INFO - 导入包完成
>>> 2025-08-30 14:39:40,853 - INFO - ========train Qwen2ForCausalLM  202508301439========
>>> 2025-08-30 14:39:43,364 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-30 14:39:46,518 - INFO - 开始进行训练
>>> 2025-08-30 14:39:48,137 - INFO - 基础配置文件读取完成
>>> 2025-08-30 14:39:49,825 - INFO - 训练配置读取完成
>>> 2025-08-30 14:39:51,452 - INFO - 数据集路径：dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-08-30 14:39:54,265 - INFO - 模型路径:/home/liangshuqiao/models/Qwen2-1.5b
>>> 2025-08-30 14:39:56,555 - ERROR - 分词表导入失败：Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/liangshuqiao/models/Qwen2-1.5b'. Use `repo_type` argument if needed.
>>> 2025-08-30 14:40:50,067 - INFO - 导入包完成
>>> 2025-08-30 14:40:51,634 - INFO - ========train Qwen2ForCausalLM  202508301440========
>>> 2025-08-30 14:40:54,145 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-30 14:40:57,298 - INFO - 开始进行训练
>>> 2025-08-30 14:40:58,917 - INFO - 基础配置文件读取完成
>>> 2025-08-30 14:41:00,605 - INFO - 训练配置读取完成
>>> 2025-08-30 14:41:02,232 - INFO - 数据集路径：dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-08-30 14:41:05,043 - INFO - 模型路径:/home/liangshuqiao/models/qwen-1.5b
>>> 2025-08-30 14:41:07,314 - ERROR - 分词表导入失败：Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/liangshuqiao/models/qwen-1.5b'. Use `repo_type` argument if needed.
>>> 2025-08-30 14:42:00,916 - INFO - 导入包完成
>>> 2025-08-30 14:42:02,483 - INFO - ========train Qwen2ForCausalLM  202508301442========
>>> 2025-08-30 14:42:04,994 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-30 14:42:08,148 - INFO - 开始进行训练
>>> 2025-08-30 14:42:09,795 - INFO - 基础配置文件读取完成
>>> 2025-08-30 14:42:11,506 - INFO - 训练配置读取完成
>>> 2025-08-30 14:42:13,133 - INFO - 数据集路径：dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-08-30 14:42:15,944 - INFO - 模型路径:/home/liangshuqiao/models/qwen-1.5b
>>> 2025-08-30 14:42:18,217 - ERROR - 分词表导入失败：Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/liangshuqiao/models/qwen-1.5b'. Use `repo_type` argument if needed.
>>> 2025-08-30 14:43:14,051 - INFO - 导入包完成
>>> 2025-08-30 14:43:15,618 - INFO - ========train Qwen2ForCausalLM  202508301443========
>>> 2025-08-30 14:43:18,129 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-30 14:43:21,283 - INFO - 开始进行训练
>>> 2025-08-30 14:43:22,900 - INFO - 基础配置文件读取完成
>>> 2025-08-30 14:43:24,588 - INFO - 训练配置读取完成
>>> 2025-08-30 14:43:26,215 - INFO - 数据集路径：dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-08-30 14:43:29,027 - INFO - 模型路径:/home/liangshuqiao/models/qwen-1.5
>>> 2025-08-30 14:43:31,758 - INFO - tokenizer读取完成
>>> 2025-08-30 14:43:33,667 - INFO - model dtype:torch.bfloat16
>>> 2025-08-30 14:43:35,656 - INFO - 模型导入完成
>>> 2025-08-30 14:43:37,243 - INFO - 数据读取开始
>>> 2025-08-30 14:43:40,847 - INFO - 数据下载完成
>>> 2025-08-30 14:49:00,928 - INFO - 数据映射完成
>>> 2025-08-30 14:49:03,471 - INFO - 数据下载完成
>>> 2025-08-30 14:49:09,116 - INFO - 数据映射完成
>>> 2025-08-30 14:49:10,602 - INFO - 打印训练参数如下
>>> 2025-08-30 14:49:12,069 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-08-30 14:49:13,896 - INFO -   dtype >>> torch.bfloat16
>>> 2025-08-30 14:49:15,744 - INFO -   load_in_4bit >>> True
>>> 2025-08-30 14:49:17,532 - INFO -   batch_size >>> 4
>>> 2025-08-30 14:49:19,219 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-08-30 14:49:21,227 - INFO -   warmup_steps >>> 1
>>> 2025-08-30 14:49:22,955 - INFO -   epoch >>> 20
>>> 2025-08-30 14:49:24,562 - INFO -   eval_steps >>> 10
>>> 2025-08-30 14:49:26,269 - INFO -   learning_rate >>> 2e-05
>>> 2025-08-30 14:49:28,096 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-08-30 14:49:30,025 - INFO -   max_seq_length >>> 4096
>>> 2025-08-30 14:49:31,852 - INFO -   r >>> 8
>>> 2025-08-30 14:49:33,359 - INFO -   interface_mode >>> False
>>> 2025-08-30 14:49:35,207 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-08-30 14:49:37,557 - INFO -   lora_alpha >>> 16
>>> 2025-08-30 14:49:39,265 - INFO -   lora_dropout >>> 0.05
>>> 2025-08-30 14:49:41,053 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-08-30 14:49:44,777 - INFO - 开始训练！
>>> 2025-08-30 14:50:17,453 - INFO - >>> {'loss': 2.1394, 'grad_norm': 0.2522106468677521, 'learning_rate': 0.0, 'epoch': 0.0033613445378151263}
>>> 2025-08-30 14:50:50,989 - INFO - >>> {'loss': 2.173, 'grad_norm': 0.24773851037025452, 'learning_rate': 2e-05, 'epoch': 0.0067226890756302525}
>>> 2025-08-30 14:51:26,532 - INFO - >>> {'loss': 2.0885, 'grad_norm': 0.2369876503944397, 'learning_rate': 1.9999998610293885e-05, 'epoch': 0.010084033613445379}
>>> 2025-08-30 14:52:00,102 - INFO - >>> {'loss': 2.1227, 'grad_norm': 0.2350810021162033, 'learning_rate': 1.9999994441175927e-05, 'epoch': 0.013445378151260505}
>>> 2025-08-30 14:52:34,355 - INFO - >>> {'loss': 2.1037, 'grad_norm': 0.24361097812652588, 'learning_rate': 1.9999987492647274e-05, 'epoch': 0.01680672268907563}
>>> 2025-08-30 14:53:09,064 - INFO - >>> {'loss': 2.15, 'grad_norm': 0.2389388531446457, 'learning_rate': 1.999997776470987e-05, 'epoch': 0.020168067226890758}
>>> 2025-08-30 14:53:44,323 - INFO - >>> {'loss': 2.0693, 'grad_norm': 0.23536016047000885, 'learning_rate': 1.9999965257366413e-05, 'epoch': 0.023529411764705882}
>>> 2025-08-30 14:55:18,149 - INFO - 导入包完成
>>> 2025-08-30 14:55:19,715 - INFO - ========train Qwen2ForCausalLM  202508301455========
>>> 2025-08-30 14:55:22,225 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-08-30 14:55:25,378 - INFO - 开始进行训练
>>> 2025-08-30 14:55:26,992 - INFO - 基础配置文件读取完成
>>> 2025-08-30 14:55:28,678 - INFO - 训练配置读取完成
>>> 2025-08-30 14:55:30,305 - INFO - 已启用验证数据集
>>> 2025-08-30 14:55:31,932 - INFO - 数据集路径：dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-08-30 14:55:34,744 - INFO - 模型路径:/home/liangshuqiao/models/qwen-1.5
>>> 2025-08-30 14:55:37,477 - INFO - tokenizer读取完成
>>> 2025-08-30 14:55:39,407 - INFO - model dtype:torch.bfloat16
>>> 2025-08-30 14:55:41,396 - INFO - 模型导入完成
>>> 2025-08-30 14:55:42,983 - INFO - 数据读取开始
>>> 2025-08-30 14:55:48,582 - INFO - 数据下载完成
>>> 2025-08-30 14:55:54,030 - INFO - 数据映射完成
>>> 2025-08-30 14:55:57,495 - INFO - 数据下载完成
>>> 2025-08-30 14:56:03,036 - INFO - 数据映射完成
>>> 2025-08-30 14:56:04,522 - INFO - 验证数据集处理完成
>>> 2025-08-30 14:56:06,191 - INFO - 打印训练参数如下
>>> 2025-08-30 14:56:07,657 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-08-30 14:56:09,485 - INFO -   dtype >>> torch.bfloat16
>>> 2025-08-30 14:56:11,333 - INFO -   load_in_4bit >>> True
>>> 2025-08-30 14:56:13,120 - INFO -   batch_size >>> 8
>>> 2025-08-30 14:56:14,807 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-08-30 14:56:16,816 - INFO -   warmup_steps >>> 1
>>> 2025-08-30 14:56:18,543 - INFO -   epoch >>> 20
>>> 2025-08-30 14:56:20,150 - INFO -   eval_steps >>> 10
>>> 2025-08-30 14:56:21,857 - INFO -   learning_rate >>> 0.0001
>>> 2025-08-30 14:56:23,705 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-08-30 14:56:25,632 - INFO -   max_seq_length >>> 4096
>>> 2025-08-30 14:56:27,460 - INFO -   r >>> 8
>>> 2025-08-30 14:56:28,966 - INFO -   interface_mode >>> False
>>> 2025-08-30 14:56:30,814 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-08-30 14:56:33,164 - INFO -   lora_alpha >>> 16
>>> 2025-08-30 14:56:34,872 - INFO -   lora_dropout >>> 0.05
>>> 2025-08-30 14:56:36,659 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-08-30 14:56:40,410 - INFO - 开始训练！
>>> 2025-08-30 14:57:57,779 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 31.73 GiB of which 1.61 GiB is free. Including non-PyTorch memory, this process has 30.12 GiB memory in use. Of the allocated memory 21.21 GiB is allocated by PyTorch, and 8.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-09-08 15:34:39,609 - INFO - 正在读取配置文件: config.yaml
>>> 2025-09-08 15:34:41,243 - INFO - 从配置文件中提取到模型路径: /home/liangshuqiao/models/qwen-05
>>> 2025-09-08 15:34:43,412 - INFO - 开始加载模型配置文件...
>>> 2025-09-08 15:34:45,059 - ERROR - 模型路径不存在: /home/liangshuqiao/models/qwen-05
>>> 2025-09-08 15:34:47,328 - ERROR - 无法获取模型信息
>>> 2025-09-08 15:39:41,202 - INFO - 正在读取配置文件: config.yaml
>>> 2025-09-08 15:39:42,836 - INFO - 从配置文件中提取到模型路径: /home/liangshuqiao/models/qwen-05
>>> 2025-09-08 15:39:45,005 - INFO - 开始加载模型配置文件...
>>> 2025-09-08 15:39:46,651 - ERROR - 模型路径不存在: /home/liangshuqiao/models/qwen-05
>>> 2025-09-08 15:39:48,921 - ERROR - 无法获取模型信息
>>> 2025-09-08 15:40:20,768 - INFO - 正在读取配置文件: config.yaml
>>> 2025-09-08 15:40:22,404 - INFO - 从配置文件中提取到模型路径: /home/liangshuqiao/models/qwen05
>>> 2025-09-08 15:40:24,553 - INFO - 开始加载模型配置文件...
>>> 2025-09-08 15:40:26,201 - INFO - 正在加载模型: /home/liangshuqiao/models/qwen05
>>> 2025-09-08 15:40:29,751 - INFO - 模型加载完成
>>> 2025-09-08 15:40:31,346 - INFO - ============================================================
>>> 2025-09-08 15:40:33,997 - INFO - 模型详细信息
>>> 2025-09-08 15:40:35,565 - INFO - ============================================================
>>> 2025-09-08 15:40:38,218 - INFO - 模型路径: /home/liangshuqiao/models/qwen05
>>> 2025-09-08 15:40:40,428 - INFO - 模型类型: qwen2
>>> 2025-09-08 15:40:42,095 - INFO - 模型架构: ['Qwen2ForCausalLM']
>>> 2025-09-08 15:40:44,065 - INFO - ----------------------------------------
>>> 2025-09-08 15:40:46,316 - INFO - 参数信息:
>>> 2025-09-08 15:40:47,864 - INFO -   总参数量: 494.03M (494,032,768)
>>> 2025-09-08 15:40:49,893 - INFO -   可训练参数: 494.03M (494,032,768)
>>> 2025-09-08 15:40:51,942 - INFO -   冻结参数: 0 (0)
>>> 2025-09-08 15:40:53,650 - INFO -   可训练参数比例: 100.00%
>>> 2025-09-08 15:40:55,458 - INFO - ----------------------------------------
>>> 2025-09-08 15:40:57,709 - INFO - 模型结构信息:
>>> 2025-09-08 15:40:59,297 - INFO -   层数: 24
>>> 2025-09-08 15:41:00,904 - INFO -   隐藏层大小: 896
>>> 2025-09-08 15:41:02,592 - INFO -   注意力头数: 14
>>> 2025-09-08 15:41:04,260 - INFO -   总层数量: 290
>>> 2025-09-08 15:41:05,928 - INFO -   可训练层数量: 290
>>> 2025-09-08 15:41:07,636 - INFO - ----------------------------------------
>>> 2025-09-08 15:41:09,885 - INFO - 特殊Token:
>>> 2025-09-08 15:41:11,493 - INFO -   bos_token: None
>>> 2025-09-08 15:41:13,281 - INFO -   eos_token: <|im_end|>
>>> 2025-09-08 15:41:15,189 - INFO -   unk_token: None
>>> 2025-09-08 15:41:16,977 - INFO -   pad_token: <|endoftext|>
>>> 2025-09-08 15:41:18,945 - INFO -   sep_token: None
>>> 2025-09-08 15:41:20,733 - INFO -   mask_token: None
>>> 2025-09-08 15:41:22,541 - INFO -   vocab_size: 151643
>>> 2025-09-08 15:41:24,389 - INFO - ----------------------------------------
>>> 2025-09-08 15:41:26,639 - INFO - 量化信息:
>>> 2025-09-08 15:41:28,185 - INFO -   数据类型: torch.float32
>>> 2025-09-08 15:41:30,054 - INFO -   量化方式: None
>>> 2025-09-08 15:41:31,741 - INFO - ----------------------------------------
>>> 2025-09-08 15:41:33,991 - INFO - 可训练层 (前10个):
>>> 2025-09-08 15:41:35,678 - INFO -   1. model.embed_tokens.weight
>>> 2025-09-08 15:41:37,727 - INFO -   2. model.layers.0.self_attn.q_proj.weight
>>> 2025-09-08 15:41:40,037 - INFO -   3. model.layers.0.self_attn.q_proj.bias
>>> 2025-09-08 15:41:42,307 - INFO -   4. model.layers.0.self_attn.k_proj.weight
>>> 2025-09-08 15:41:44,618 - INFO -   5. model.layers.0.self_attn.k_proj.bias
>>> 2025-09-08 15:41:46,888 - INFO -   6. model.layers.0.self_attn.v_proj.weight
>>> 2025-09-08 15:41:49,198 - INFO -   7. model.layers.0.self_attn.v_proj.bias
>>> 2025-09-08 15:41:51,468 - INFO -   8. model.layers.0.self_attn.o_proj.weight
>>> 2025-09-08 15:41:53,779 - INFO -   9. model.layers.0.mlp.gate_proj.weight
>>> 2025-09-08 15:41:56,029 - INFO -   10. model.layers.0.mlp.up_proj.weight
>>> 2025-09-08 15:41:58,260 - INFO -   ... 还有 280 个可训练层
>>> 2025-09-08 15:42:26,661 - INFO - 开始进行原始模型对话测试
>>> 2025-09-08 15:42:28,737 - INFO - 导入包完成
>>> 2025-09-08 15:42:30,218 - INFO - 配置文件读取完成
>>> 2025-09-08 15:46:05,682 - INFO - 导入包完成
>>> 2025-09-08 15:46:07,248 - INFO - ========train Qwen2ForCausalLM  202509081546========
>>> 2025-09-08 15:46:09,758 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-09-08 15:46:12,790 - INFO - 开始进行训练
>>> 2025-09-08 15:46:14,394 - INFO - 基础配置文件读取完成
>>> 2025-09-08 15:46:16,082 - INFO - 训练配置读取完成
>>> 2025-09-08 15:46:17,708 - INFO - 已启用验证数据集
>>> 2025-09-08 15:46:19,335 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/own/Medical_20250816.json
>>> 2025-09-08 15:46:22,768 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-08 15:46:25,294 - INFO - tokenizer读取完成
>>> 2025-09-08 15:46:27,272 - INFO - model dtype:torch.bfloat16
>>> 2025-09-08 15:46:29,259 - INFO - 模型导入完成
>>> 2025-09-08 15:46:30,846 - INFO - 数据读取开始
>>> 2025-09-08 15:46:33,271 - INFO - 数据下载完成
>>> 2025-09-08 15:48:04,351 - INFO - 导入包完成
>>> 2025-09-08 15:48:05,918 - INFO - ========train Qwen2ForCausalLM  202509081548========
>>> 2025-09-08 15:48:08,428 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-09-08 15:48:11,460 - INFO - 开始进行训练
>>> 2025-09-08 15:48:13,064 - INFO - 基础配置文件读取完成
>>> 2025-09-08 15:48:14,753 - INFO - 训练配置读取完成
>>> 2025-09-08 15:48:16,379 - INFO - 已启用验证数据集
>>> 2025-09-08 15:48:18,006 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/own/Medical_20250816.json
>>> 2025-09-08 15:48:21,440 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-08 15:48:23,974 - INFO - tokenizer读取完成
>>> 2025-09-08 15:48:25,952 - INFO - model dtype:torch.bfloat16
>>> 2025-09-08 15:48:27,940 - INFO - 模型导入完成
>>> 2025-09-08 15:48:29,526 - INFO - 数据读取开始
>>> 2025-09-08 15:48:31,915 - INFO - 数据下载完成
>>> 2025-09-08 15:48:37,826 - INFO - 数据映射完成
>>> 2025-09-08 15:48:40,287 - INFO - 数据下载完成
>>> 2025-09-08 15:51:14,801 - INFO - 导入包完成
>>> 2025-09-08 15:51:14,801 - INFO - ========train Qwen2ForCausalLM  202509081551========
>>> 2025-09-08 15:51:14,802 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-09-08 15:51:14,802 - INFO - 开始进行训练
>>> 2025-09-08 15:51:14,808 - INFO - 基础配置文件读取完成
>>> 2025-09-08 15:51:14,816 - INFO - 训练配置读取完成
>>> 2025-09-08 15:51:14,816 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/own/Medical_20250816.json
>>> 2025-09-08 15:51:14,817 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-08 15:51:15,110 - INFO - tokenizer读取完成
>>> 2025-09-08 15:51:15,313 - INFO - model dtype:torch.bfloat16
>>> 2025-09-08 15:51:15,314 - INFO - 模型导入完成
>>> 2025-09-08 15:51:15,314 - INFO - 数据读取开始
>>> 2025-09-08 15:51:16,323 - INFO - 数据下载完成
>>> 2025-09-08 15:51:20,946 - INFO - 数据映射完成
>>> 2025-09-08 15:51:20,946 - INFO - 打印训练参数如下
>>> 2025-09-08 15:51:20,947 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-08 15:51:20,947 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-08 15:51:20,948 - INFO -   load_in_4bit >>> True
>>> 2025-09-08 15:51:20,948 - INFO -   batch_size >>> 8
>>> 2025-09-08 15:51:20,948 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-09-08 15:51:20,949 - INFO -   warmup_steps >>> 1
>>> 2025-09-08 15:51:20,949 - INFO -   epoch >>> 20
>>> 2025-09-08 15:51:20,949 - INFO -   eval_steps >>> 10
>>> 2025-09-08 15:51:20,950 - INFO -   learning_rate >>> 0.0001
>>> 2025-09-08 15:51:20,950 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-08 15:51:20,950 - INFO -   max_seq_length >>> 4096
>>> 2025-09-08 15:51:20,951 - INFO -   r >>> 8
>>> 2025-09-08 15:51:20,951 - INFO -   interface_mode >>> False
>>> 2025-09-08 15:51:20,951 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-09-08 15:51:20,952 - INFO -   lora_alpha >>> 16
>>> 2025-09-08 15:51:20,952 - INFO -   lora_dropout >>> 0.05
>>> 2025-09-08 15:51:20,952 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-08 15:51:21,735 - INFO - 开始训练！
>>> 2025-09-08 15:51:30,266 - INFO - >>> {'loss': 3.4057, 'grad_norm': 2.991401433944702, 'learning_rate': 0.0, 'epoch': 0.6153846153846154}
>>> 2025-09-08 15:51:44,907 - INFO - >>> {'loss': 6.6686, 'grad_norm': 5.293604850769043, 'learning_rate': 0.0001, 'epoch': 1.6153846153846154}
>>> 2025-09-08 15:51:58,087 - INFO - >>> {'loss': 6.7431, 'grad_norm': 5.256581783294678, 'learning_rate': 9.931806517013612e-05, 'epoch': 2.6153846153846154}
>>> 2025-09-08 15:52:11,705 - INFO - >>> {'loss': 6.4768, 'grad_norm': 4.77231502532959, 'learning_rate': 9.729086208503174e-05, 'epoch': 3.6153846153846154}
>>> 2025-09-08 15:52:24,297 - INFO - >>> {'loss': 6.4861, 'grad_norm': 4.737471103668213, 'learning_rate': 9.397368756032445e-05, 'epoch': 4.615384615384615}
>>> 2025-09-08 15:52:36,679 - INFO - >>> {'loss': 6.5548, 'grad_norm': 4.268071174621582, 'learning_rate': 8.945702546981969e-05, 'epoch': 5.615384615384615}
>>> 2025-09-08 15:52:48,623 - INFO - >>> {'loss': 6.3998, 'grad_norm': 3.8932886123657227, 'learning_rate': 8.386407858128706e-05, 'epoch': 6.615384615384615}
>>> 2025-09-08 15:53:01,912 - INFO - >>> {'loss': 6.3428, 'grad_norm': 4.1412153244018555, 'learning_rate': 7.734740790612136e-05, 'epoch': 7.615384615384615}
>>> 2025-09-08 15:53:16,090 - INFO - >>> {'loss': 6.3128, 'grad_norm': 3.5668766498565674, 'learning_rate': 7.008477123264848e-05, 'epoch': 8.615384615384615}
>>> 2025-09-08 15:53:29,158 - INFO - >>> {'loss': 6.2392, 'grad_norm': 3.7072861194610596, 'learning_rate': 6.227427435703997e-05, 'epoch': 9.615384615384615}
>>> 2025-09-08 15:53:41,396 - INFO - >>> {'loss': 6.112, 'grad_norm': 3.5033187866210938, 'learning_rate': 5.4128967273616625e-05, 'epoch': 10.615384615384615}
>>> 2025-09-08 15:53:54,391 - INFO - >>> {'loss': 6.2251, 'grad_norm': 3.228714942932129, 'learning_rate': 4.5871032726383386e-05, 'epoch': 11.615384615384615}
>>> 2025-09-08 15:54:08,013 - INFO - >>> {'loss': 6.0914, 'grad_norm': 3.353549003601074, 'learning_rate': 3.772572564296005e-05, 'epoch': 12.615384615384615}
>>> 2025-09-08 15:54:19,827 - INFO - >>> {'loss': 6.2696, 'grad_norm': 3.1546547412872314, 'learning_rate': 2.991522876735154e-05, 'epoch': 13.615384615384615}
>>> 2025-09-08 15:54:32,901 - INFO - >>> {'loss': 6.0767, 'grad_norm': 3.0570905208587646, 'learning_rate': 2.2652592093878666e-05, 'epoch': 14.615384615384615}
>>> 2025-09-08 15:54:44,432 - INFO - >>> {'loss': 6.1996, 'grad_norm': 3.299652338027954, 'learning_rate': 1.6135921418712956e-05, 'epoch': 15.615384615384615}
>>> 2025-09-08 15:54:57,079 - INFO - >>> {'loss': 5.9159, 'grad_norm': 3.187647819519043, 'learning_rate': 1.0542974530180327e-05, 'epoch': 16.615384615384617}
>>> 2025-09-08 15:55:10,486 - INFO - >>> {'loss': 6.0692, 'grad_norm': 2.9600846767425537, 'learning_rate': 6.026312439675552e-06, 'epoch': 17.615384615384617}
>>> 2025-09-08 15:55:22,373 - INFO - >>> {'loss': 6.1404, 'grad_norm': 3.2453930377960205, 'learning_rate': 2.7091379149682685e-06, 'epoch': 18.615384615384617}
>>> 2025-09-08 15:55:35,407 - INFO - >>> {'loss': 6.0568, 'grad_norm': 2.828608274459839, 'learning_rate': 6.819348298638839e-07, 'epoch': 19.615384615384617}
>>> 2025-09-08 15:55:35,940 - INFO - >>> {'train_runtime': 253.969, 'train_samples_per_second': 7.875, 'train_steps_per_second': 0.079, 'train_loss': 6.139307069778442, 'epoch': 19.615384615384617}
>>> 2025-09-08 15:55:35,942 - INFO - 训练成功！
>>> 2025-09-08 15:55:35,943 - INFO - 模型存放位置：./output/qwen2-0.5b202509081551
>>> 2025-09-08 15:57:44,968 - INFO - ========__main__  202509081557========
>>> 2025-09-08 15:57:44,968 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-09-08 15:57:44,969 - INFO - 开始进行模型测试
>>> 2025-09-08 15:57:51,775 - INFO - 已选择模型文件夹: qwen2-0.5b202509081551
>>> 2025-09-08 15:57:51,778 - INFO - 最新的 LoRA checkpoint 路径:output/qwen2-0.5b202509081551/checkpoint-20
>>> 2025-09-08 15:59:11,450 - INFO - 导入包完成
>>> 2025-09-08 15:59:11,450 - INFO - ========train Qwen2ForCausalLM  202509081559========
>>> 2025-09-08 15:59:11,451 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-09-08 15:59:11,451 - INFO - 开始进行训练
>>> 2025-09-08 15:59:11,457 - INFO - 基础配置文件读取完成
>>> 2025-09-08 15:59:11,466 - INFO - 训练配置读取完成
>>> 2025-09-08 15:59:11,466 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/own/Medical_20250816.json
>>> 2025-09-08 15:59:11,467 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-08 15:59:11,762 - INFO - tokenizer读取完成
>>> 2025-09-08 15:59:12,098 - INFO - model dtype:torch.float16
>>> 2025-09-08 15:59:12,099 - INFO - 模型导入完成
>>> 2025-09-08 15:59:12,099 - INFO - 数据读取开始
>>> 2025-09-08 15:59:13,113 - INFO - 数据下载完成
>>> 2025-09-08 15:59:17,295 - INFO - 数据映射完成
>>> 2025-09-08 15:59:17,296 - INFO - 打印训练参数如下
>>> 2025-09-08 15:59:17,297 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-08 15:59:17,297 - INFO -   dtype >>> torch.float16
>>> 2025-09-08 15:59:17,297 - INFO -   load_in_4bit >>> True
>>> 2025-09-08 15:59:17,298 - INFO -   batch_size >>> 8
>>> 2025-09-08 15:59:17,298 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-09-08 15:59:17,298 - INFO -   warmup_steps >>> 1
>>> 2025-09-08 15:59:17,299 - INFO -   epoch >>> 100
>>> 2025-09-08 15:59:17,299 - INFO -   eval_steps >>> 10
>>> 2025-09-08 15:59:17,299 - INFO -   learning_rate >>> 0.0001
>>> 2025-09-08 15:59:17,300 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-08 15:59:17,300 - INFO -   max_seq_length >>> 4096
>>> 2025-09-08 15:59:17,300 - INFO -   use_history >>> False
>>> 2025-09-08 15:59:17,301 - INFO -   r >>> 8
>>> 2025-09-08 15:59:17,301 - INFO -   interface_mode >>> False
>>> 2025-09-08 15:59:17,301 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
>>> 2025-09-08 15:59:17,302 - INFO -   lora_alpha >>> 16
>>> 2025-09-08 15:59:17,302 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-08 15:59:17,302 - INFO -   bias >>> none
>>> 2025-09-08 15:59:17,303 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-08 15:59:17,303 - INFO -   random_state >>> 3407
>>> 2025-09-08 15:59:17,303 - INFO -   use_rslora >>> True
>>> 2025-09-08 15:59:17,304 - INFO -   loftq_config >>> None
>>> 2025-09-08 15:59:18,843 - INFO - 开始训练！
>>> 2025-09-08 15:59:23,106 - INFO - >>> {'loss': 3.4046, 'grad_norm': 4.242115497589111, 'learning_rate': 0.0, 'epoch': 0.6153846153846154}
>>> 2025-09-08 15:59:29,019 - INFO - >>> {'loss': 6.6673, 'grad_norm': 7.568901062011719, 'learning_rate': 0.0001, 'epoch': 1.6153846153846154}
>>> 2025-09-08 15:59:34,386 - INFO - >>> {'loss': 6.3785, 'grad_norm': 5.222240447998047, 'learning_rate': 9.997482711915927e-05, 'epoch': 2.6153846153846154}
>>> 2025-09-08 15:59:39,936 - INFO - >>> {'loss': 5.8928, 'grad_norm': 3.936692476272583, 'learning_rate': 9.989933382359422e-05, 'epoch': 3.6153846153846154}
>>> 2025-09-08 15:59:45,049 - INFO - >>> {'loss': 5.7099, 'grad_norm': 3.6369941234588623, 'learning_rate': 9.977359612865423e-05, 'epoch': 4.615384615384615}
>>> 2025-09-08 15:59:50,190 - INFO - >>> {'loss': 5.7168, 'grad_norm': 2.7854857444763184, 'learning_rate': 9.959774064153977e-05, 'epoch': 5.615384615384615}
>>> 2025-09-08 15:59:55,287 - INFO - >>> {'loss': 5.5633, 'grad_norm': 2.390430212020874, 'learning_rate': 9.937194443381972e-05, 'epoch': 6.615384615384615}
>>> 2025-09-08 16:00:00,615 - INFO - >>> {'loss': 5.3914, 'grad_norm': 2.2909555435180664, 'learning_rate': 9.909643486313533e-05, 'epoch': 7.615384615384615}
>>> 2025-09-08 16:00:05,977 - INFO - >>> {'loss': 5.3553, 'grad_norm': 2.1536014080047607, 'learning_rate': 9.877148934427037e-05, 'epoch': 8.615384615384615}
>>> 2025-09-08 16:00:11,299 - INFO - >>> {'loss': 5.2383, 'grad_norm': 2.2680423259735107, 'learning_rate': 9.839743506981782e-05, 'epoch': 9.615384615384615}
>>> 2025-09-08 16:00:16,478 - INFO - >>> {'loss': 5.0394, 'grad_norm': 2.476022958755493, 'learning_rate': 9.797464868072488e-05, 'epoch': 10.615384615384615}
>>> 2025-09-08 16:00:21,831 - INFO - >>> {'loss': 5.1576, 'grad_norm': 2.381404161453247, 'learning_rate': 9.750355588704727e-05, 'epoch': 11.615384615384615}
>>> 2025-09-08 16:00:27,418 - INFO - >>> {'loss': 4.9139, 'grad_norm': 2.430863380432129, 'learning_rate': 9.698463103929542e-05, 'epoch': 12.615384615384615}
>>> 2025-09-08 16:00:32,286 - INFO - >>> {'loss': 5.0095, 'grad_norm': 2.273574113845825, 'learning_rate': 9.641839665080363e-05, 'epoch': 13.615384615384615}
>>> 2025-09-08 16:00:37,715 - INFO - >>> {'loss': 4.8267, 'grad_norm': 2.2590525150299072, 'learning_rate': 9.580542287160348e-05, 'epoch': 14.615384615384615}
>>> 2025-09-08 16:00:42,826 - INFO - >>> {'loss': 4.7391, 'grad_norm': 2.213984251022339, 'learning_rate': 9.514632691433107e-05, 'epoch': 15.615384615384615}
>>> 2025-09-08 16:00:48,581 - INFO - >>> {'loss': 4.4587, 'grad_norm': 1.809232473373413, 'learning_rate': 9.444177243274618e-05, 'epoch': 16.615384615384617}
>>> 2025-09-08 16:00:55,066 - INFO - >>> {'loss': 4.5902, 'grad_norm': 1.9813754558563232, 'learning_rate': 9.369246885348926e-05, 'epoch': 17.615384615384617}
>>> 2025-09-08 16:01:00,940 - INFO - >>> {'loss': 4.4067, 'grad_norm': 2.3189456462860107, 'learning_rate': 9.289917066174886e-05, 'epoch': 18.615384615384617}
>>> 2025-09-08 16:01:07,449 - INFO - >>> {'loss': 4.4805, 'grad_norm': 2.175800085067749, 'learning_rate': 9.206267664155907e-05, 'epoch': 19.615384615384617}
>>> 2025-09-08 16:01:14,076 - INFO - >>> {'loss': 4.3145, 'grad_norm': 2.342374563217163, 'learning_rate': 9.118382907149165e-05, 'epoch': 20.615384615384617}
>>> 2025-09-08 16:01:20,646 - INFO - >>> {'loss': 4.1332, 'grad_norm': 2.369633436203003, 'learning_rate': 9.026351287655294e-05, 'epoch': 21.615384615384617}
>>> 2025-09-08 16:01:27,829 - INFO - >>> {'loss': 4.3515, 'grad_norm': 2.270127773284912, 'learning_rate': 8.930265473713938e-05, 'epoch': 22.615384615384617}
>>> 2025-09-08 16:01:34,283 - INFO - >>> {'loss': 4.0675, 'grad_norm': 2.1042075157165527, 'learning_rate': 8.83022221559489e-05, 'epoch': 23.615384615384617}
>>> 2025-09-08 16:01:40,765 - INFO - >>> {'loss': 3.9875, 'grad_norm': 2.1646926403045654, 'learning_rate': 8.726322248378775e-05, 'epoch': 24.615384615384617}
>>> 2025-09-08 16:01:47,490 - INFO - >>> {'loss': 3.8893, 'grad_norm': 2.2990949153900146, 'learning_rate': 8.618670190525352e-05, 'epoch': 25.615384615384617}
>>> 2025-09-08 16:01:54,326 - INFO - >>> {'loss': 3.9424, 'grad_norm': 2.265127182006836, 'learning_rate': 8.507374438531607e-05, 'epoch': 26.615384615384617}
>>> 2025-09-08 16:02:02,353 - INFO - >>> {'loss': 3.9987, 'grad_norm': 2.280592679977417, 'learning_rate': 8.392547057785661e-05, 'epoch': 27.615384615384617}
>>> 2025-09-08 16:02:09,224 - INFO - >>> {'loss': 3.6175, 'grad_norm': 2.39058518409729, 'learning_rate': 8.274303669726426e-05, 'epoch': 28.615384615384617}
>>> 2025-09-08 16:02:16,961 - INFO - >>> {'loss': 3.6903, 'grad_norm': 2.2570841312408447, 'learning_rate': 8.152763335422613e-05, 'epoch': 29.615384615384617}
>>> 2025-09-08 16:02:24,327 - INFO - >>> {'loss': 3.535, 'grad_norm': 2.3712656497955322, 'learning_rate': 8.028048435688333e-05, 'epoch': 30.615384615384617}
>>> 2025-09-08 16:02:30,725 - INFO - >>> {'loss': 3.3873, 'grad_norm': 2.4621200561523438, 'learning_rate': 7.900284547855991e-05, 'epoch': 31.615384615384617}
>>> 2025-09-08 16:02:37,739 - INFO - >>> {'loss': 3.4506, 'grad_norm': 2.400059700012207, 'learning_rate': 7.769600319330552e-05, 'epoch': 32.61538461538461}
>>> 2025-09-08 16:02:44,023 - INFO - >>> {'loss': 3.4072, 'grad_norm': 2.5187816619873047, 'learning_rate': 7.636127338052512e-05, 'epoch': 33.61538461538461}
>>> 2025-09-08 16:02:50,906 - INFO - >>> {'loss': 3.3169, 'grad_norm': 2.60971999168396, 'learning_rate': 7.500000000000001e-05, 'epoch': 34.61538461538461}
>>> 2025-09-08 16:02:57,983 - INFO - >>> {'loss': 3.4077, 'grad_norm': 3.069154977798462, 'learning_rate': 7.361355373863414e-05, 'epoch': 35.61538461538461}
>>> 2025-09-08 16:03:04,397 - INFO - >>> {'loss': 3.1385, 'grad_norm': 2.54599666595459, 'learning_rate': 7.220333063028872e-05, 'epoch': 36.61538461538461}
>>> 2025-09-08 16:03:12,292 - INFO - >>> {'loss': 3.1011, 'grad_norm': 2.41739559173584, 'learning_rate': 7.077075065009433e-05, 'epoch': 37.61538461538461}
>>> 2025-09-08 16:03:18,391 - INFO - >>> {'loss': 2.9519, 'grad_norm': 2.9551596641540527, 'learning_rate': 6.931725628465643e-05, 'epoch': 38.61538461538461}
>>> 2025-09-08 16:03:24,372 - INFO - >>> {'loss': 2.9498, 'grad_norm': 2.4811041355133057, 'learning_rate': 6.784431107959359e-05, 'epoch': 39.61538461538461}
>>> 2025-09-08 16:03:31,698 - INFO - >>> {'loss': 3.038, 'grad_norm': 2.559722661972046, 'learning_rate': 6.635339816587109e-05, 'epoch': 40.61538461538461}
>>> 2025-09-08 16:03:37,461 - INFO - >>> {'loss': 2.7549, 'grad_norm': 3.0537354946136475, 'learning_rate': 6.484601876641375e-05, 'epoch': 41.61538461538461}
>>> 2025-09-08 16:03:43,905 - INFO - >>> {'loss': 2.7839, 'grad_norm': 2.593203544616699, 'learning_rate': 6.332369068450174e-05, 'epoch': 42.61538461538461}
>>> 2025-09-08 16:03:50,468 - INFO - >>> {'loss': 2.6485, 'grad_norm': 2.7386844158172607, 'learning_rate': 6.178794677547137e-05, 'epoch': 43.61538461538461}
>>> 2025-09-08 16:03:57,085 - INFO - >>> {'loss': 2.7193, 'grad_norm': 2.750154733657837, 'learning_rate': 6.024033340325954e-05, 'epoch': 44.61538461538461}
>>> 2025-09-08 16:04:02,998 - INFO - >>> {'loss': 2.4417, 'grad_norm': 2.853167772293091, 'learning_rate': 5.868240888334653e-05, 'epoch': 45.61538461538461}
>>> 2025-09-08 16:04:09,824 - INFO - >>> {'loss': 2.4377, 'grad_norm': 2.64374041557312, 'learning_rate': 5.7115741913664264e-05, 'epoch': 46.61538461538461}
>>> 2025-09-08 16:04:16,560 - INFO - >>> {'loss': 2.4465, 'grad_norm': 2.7402021884918213, 'learning_rate': 5.5541909995050554e-05, 'epoch': 47.61538461538461}
>>> 2025-09-08 16:04:23,430 - INFO - >>> {'loss': 2.2208, 'grad_norm': 2.508888006210327, 'learning_rate': 5.396249784283942e-05, 'epoch': 48.61538461538461}
>>> 2025-09-08 16:04:30,006 - INFO - >>> {'loss': 2.3907, 'grad_norm': 2.948843002319336, 'learning_rate': 5.2379095791187124e-05, 'epoch': 49.61538461538461}
>>> 2025-09-08 16:04:36,373 - INFO - >>> {'loss': 2.0186, 'grad_norm': 2.7597784996032715, 'learning_rate': 5.0793298191740404e-05, 'epoch': 50.61538461538461}
>>> 2025-09-08 16:04:42,833 - INFO - >>> {'loss': 2.1119, 'grad_norm': 3.338109016418457, 'learning_rate': 4.92067018082596e-05, 'epoch': 51.61538461538461}
>>> 2025-09-08 16:04:49,201 - INFO - >>> {'loss': 1.9823, 'grad_norm': 2.7131471633911133, 'learning_rate': 4.762090420881289e-05, 'epoch': 52.61538461538461}
>>> 2025-09-08 16:04:55,271 - INFO - >>> {'loss': 1.934, 'grad_norm': 2.751068353652954, 'learning_rate': 4.603750215716057e-05, 'epoch': 53.61538461538461}
>>> 2025-09-08 16:05:01,301 - INFO - >>> {'loss': 1.862, 'grad_norm': 3.1864867210388184, 'learning_rate': 4.445809000494946e-05, 'epoch': 54.61538461538461}
>>> 2025-09-08 16:05:08,259 - INFO - >>> {'loss': 1.9214, 'grad_norm': 2.6217968463897705, 'learning_rate': 4.288425808633575e-05, 'epoch': 55.61538461538461}
>>> 2025-09-08 16:05:14,561 - INFO - >>> {'loss': 1.8066, 'grad_norm': 3.5876612663269043, 'learning_rate': 4.131759111665349e-05, 'epoch': 56.61538461538461}
>>> 2025-09-08 16:05:21,638 - INFO - >>> {'loss': 1.7283, 'grad_norm': 2.7236716747283936, 'learning_rate': 3.9759666596740476e-05, 'epoch': 57.61538461538461}
>>> 2025-09-08 16:05:27,598 - INFO - >>> {'loss': 1.7947, 'grad_norm': 3.9444968700408936, 'learning_rate': 3.821205322452863e-05, 'epoch': 58.61538461538461}
>>> 2025-09-08 16:05:34,643 - INFO - >>> {'loss': 1.6054, 'grad_norm': 2.75545072555542, 'learning_rate': 3.6676309315498256e-05, 'epoch': 59.61538461538461}
>>> 2025-09-08 16:05:40,638 - INFO - >>> {'loss': 1.6285, 'grad_norm': 3.433464288711548, 'learning_rate': 3.515398123358627e-05, 'epoch': 60.61538461538461}
>>> 2025-09-08 16:05:48,012 - INFO - >>> {'loss': 1.5367, 'grad_norm': 2.810697078704834, 'learning_rate': 3.364660183412892e-05, 'epoch': 61.61538461538461}
>>> 2025-09-08 16:05:53,906 - INFO - >>> {'loss': 1.5333, 'grad_norm': 3.1887006759643555, 'learning_rate': 3.215568892040641e-05, 'epoch': 62.61538461538461}
>>> 2025-09-08 16:06:00,994 - INFO - >>> {'loss': 1.4469, 'grad_norm': 2.98496413230896, 'learning_rate': 3.0682743715343564e-05, 'epoch': 63.61538461538461}
>>> 2025-09-08 16:06:07,389 - INFO - >>> {'loss': 1.3757, 'grad_norm': 2.924281120300293, 'learning_rate': 2.9229249349905684e-05, 'epoch': 64.61538461538461}
>>> 2025-09-08 16:06:14,304 - INFO - >>> {'loss': 1.2648, 'grad_norm': 3.029667615890503, 'learning_rate': 2.7796669369711294e-05, 'epoch': 65.61538461538461}
>>> 2025-09-08 16:06:20,959 - INFO - >>> {'loss': 1.4609, 'grad_norm': 2.7024893760681152, 'learning_rate': 2.638644626136587e-05, 'epoch': 66.61538461538461}
>>> 2025-09-08 16:06:28,124 - INFO - >>> {'loss': 1.234, 'grad_norm': 2.9137649536132812, 'learning_rate': 2.500000000000001e-05, 'epoch': 67.61538461538461}
>>> 2025-09-08 16:06:35,017 - INFO - >>> {'loss': 1.2409, 'grad_norm': 3.0065627098083496, 'learning_rate': 2.363872661947488e-05, 'epoch': 68.61538461538461}
>>> 2025-09-08 16:06:41,899 - INFO - >>> {'loss': 1.1046, 'grad_norm': 2.8451247215270996, 'learning_rate': 2.2303996806694488e-05, 'epoch': 69.61538461538461}
>>> 2025-09-08 16:06:48,847 - INFO - >>> {'loss': 1.2477, 'grad_norm': 3.134324550628662, 'learning_rate': 2.09971545214401e-05, 'epoch': 70.61538461538461}
>>> 2025-09-08 16:06:54,539 - INFO - >>> {'loss': 1.0927, 'grad_norm': 2.971220016479492, 'learning_rate': 1.9719515643116674e-05, 'epoch': 71.61538461538461}
>>> 2025-09-08 16:07:00,945 - INFO - >>> {'loss': 1.1293, 'grad_norm': 2.76387357711792, 'learning_rate': 1.847236664577389e-05, 'epoch': 72.61538461538461}
>>> 2025-09-08 16:07:07,581 - INFO - >>> {'loss': 1.1414, 'grad_norm': 2.512441396713257, 'learning_rate': 1.725696330273575e-05, 'epoch': 73.61538461538461}
>>> 2025-09-08 16:07:14,708 - INFO - >>> {'loss': 1.0695, 'grad_norm': 2.871476650238037, 'learning_rate': 1.60745294221434e-05, 'epoch': 74.61538461538461}
>>> 2025-09-08 16:07:20,961 - INFO - >>> {'loss': 1.021, 'grad_norm': 2.5717780590057373, 'learning_rate': 1.4926255614683932e-05, 'epoch': 75.61538461538461}
>>> 2025-09-08 16:07:27,083 - INFO - >>> {'loss': 1.0473, 'grad_norm': 2.758901834487915, 'learning_rate': 1.3813298094746491e-05, 'epoch': 76.61538461538461}
>>> 2025-09-08 16:07:34,147 - INFO - >>> {'loss': 0.9135, 'grad_norm': 3.0135343074798584, 'learning_rate': 1.2736777516212266e-05, 'epoch': 77.61538461538461}
>>> 2025-09-08 16:07:40,980 - INFO - >>> {'loss': 0.9671, 'grad_norm': 2.8517568111419678, 'learning_rate': 1.1697777844051105e-05, 'epoch': 78.61538461538461}
>>> 2025-09-08 16:07:47,644 - INFO - >>> {'loss': 0.9775, 'grad_norm': 2.64589524269104, 'learning_rate': 1.0697345262860636e-05, 'epoch': 79.61538461538461}
>>> 2025-09-08 16:07:54,495 - INFO - >>> {'loss': 0.964, 'grad_norm': 3.1360385417938232, 'learning_rate': 9.73648712344707e-06, 'epoch': 80.61538461538461}
>>> 2025-09-08 16:08:00,674 - INFO - >>> {'loss': 0.8505, 'grad_norm': 2.8090410232543945, 'learning_rate': 8.816170928508365e-06, 'epoch': 81.61538461538461}
>>> 2025-09-08 16:08:07,591 - INFO - >>> {'loss': 0.9867, 'grad_norm': 2.6177330017089844, 'learning_rate': 7.937323358440935e-06, 'epoch': 82.61538461538461}
>>> 2025-09-08 16:08:13,459 - INFO - >>> {'loss': 0.9575, 'grad_norm': 2.573639392852783, 'learning_rate': 7.100829338251147e-06, 'epoch': 83.61538461538461}
>>> 2025-09-08 16:08:20,361 - INFO - >>> {'loss': 0.8988, 'grad_norm': 2.5677542686462402, 'learning_rate': 6.3075311465107535e-06, 'epoch': 84.61538461538461}
>>> 2025-09-08 16:08:26,092 - INFO - >>> {'loss': 0.8252, 'grad_norm': 2.449681043624878, 'learning_rate': 5.558227567253832e-06, 'epoch': 85.61538461538461}
>>> 2025-09-08 16:08:33,032 - INFO - >>> {'loss': 0.9434, 'grad_norm': 2.8263180255889893, 'learning_rate': 4.853673085668947e-06, 'epoch': 86.61538461538461}
>>> 2025-09-08 16:08:39,625 - INFO - >>> {'loss': 0.8913, 'grad_norm': 2.470705509185791, 'learning_rate': 4.19457712839652e-06, 'epoch': 87.61538461538461}
>>> 2025-09-08 16:08:45,556 - INFO - >>> {'loss': 0.8027, 'grad_norm': 3.06359601020813, 'learning_rate': 3.581603349196372e-06, 'epoch': 88.61538461538461}
>>> 2025-09-08 16:08:51,964 - INFO - >>> {'loss': 0.8083, 'grad_norm': 2.4248578548431396, 'learning_rate': 3.0153689607045845e-06, 'epoch': 89.61538461538461}
>>> 2025-09-08 16:08:58,854 - INFO - >>> {'loss': 0.8966, 'grad_norm': 3.019824266433716, 'learning_rate': 2.496444112952734e-06, 'epoch': 90.61538461538461}
>>> 2025-09-08 16:09:05,087 - INFO - >>> {'loss': 0.9051, 'grad_norm': 2.828665256500244, 'learning_rate': 2.0253513192751373e-06, 'epoch': 91.61538461538461}
>>> 2025-09-08 16:09:11,556 - INFO - >>> {'loss': 0.7855, 'grad_norm': 2.6522068977355957, 'learning_rate': 1.6025649301821876e-06, 'epoch': 92.61538461538461}
>>> 2025-09-08 16:09:18,319 - INFO - >>> {'loss': 0.8803, 'grad_norm': 2.5728447437286377, 'learning_rate': 1.2285106557296477e-06, 'epoch': 93.61538461538461}
>>> 2025-09-08 16:09:24,450 - INFO - >>> {'loss': 0.7942, 'grad_norm': 2.576223850250244, 'learning_rate': 9.035651368646648e-07, 'epoch': 94.61538461538461}
>>> 2025-09-08 16:09:31,234 - INFO - >>> {'loss': 0.8725, 'grad_norm': 2.7661361694335938, 'learning_rate': 6.280555661802856e-07, 'epoch': 95.61538461538461}
>>> 2025-09-08 16:09:37,507 - INFO - >>> {'loss': 0.8114, 'grad_norm': 2.410371780395508, 'learning_rate': 4.02259358460233e-07, 'epoch': 96.61538461538461}
>>> 2025-09-08 16:09:43,516 - INFO - >>> {'loss': 0.9512, 'grad_norm': 2.9543344974517822, 'learning_rate': 2.2640387134577058e-07, 'epoch': 97.61538461538461}
>>> 2025-09-08 16:09:50,335 - INFO - >>> {'loss': 0.8381, 'grad_norm': 2.7568211555480957, 'learning_rate': 1.0066617640578368e-07, 'epoch': 98.61538461538461}
>>> 2025-09-08 16:09:56,362 - INFO - >>> {'loss': 0.8165, 'grad_norm': 2.913492441177368, 'learning_rate': 2.5172880840745873e-08, 'epoch': 99.61538461538461}
>>> 2025-09-08 16:09:57,005 - INFO - >>> {'train_runtime': 637.8933, 'train_samples_per_second': 15.677, 'train_steps_per_second': 0.157, 'train_loss': 2.621392326951027, 'epoch': 99.61538461538461}
>>> 2025-09-08 16:09:57,007 - INFO - 训练成功！
>>> 2025-09-08 16:09:57,008 - INFO - 模型存放位置：./output/qwen2-0.5b202509081559
>>> 2025-09-08 16:11:01,370 - INFO - ========__main__  202509081611========
>>> 2025-09-08 16:11:01,370 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/env/bin/python
>>> 2025-09-08 16:11:01,371 - INFO - 开始进行模型测试
>>> 2025-09-08 16:11:04,559 - INFO - 已选择模型文件夹: qwen2-0.5b202509081559
>>> 2025-09-08 16:11:04,562 - INFO - 最新的 LoRA checkpoint 路径:output/qwen2-0.5b202509081559/checkpoint-100
>>> 2025-09-08 16:14:54,098 - INFO - 导入包完成
>>> 2025-09-08 16:14:54,098 - INFO - ========train Qwen2ForCausalLM  202509081614========
>>> 2025-09-08 16:14:54,099 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-08 16:14:54,099 - INFO - 开始进行训练
>>> 2025-09-08 16:14:54,112 - INFO - 基础配置文件读取完成
>>> 2025-09-08 16:14:54,120 - INFO - 训练配置读取完成
>>> 2025-09-08 16:14:54,121 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/own/Medical_20250816.json
>>> 2025-09-08 16:14:54,121 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-08 16:14:54,565 - INFO - tokenizer读取完成
>>> 2025-09-08 16:14:54,869 - INFO - model dtype:torch.float16
>>> 2025-09-08 16:14:54,870 - INFO - 模型导入完成
>>> 2025-09-08 16:14:54,870 - INFO - 数据读取开始
>>> 2025-09-08 16:14:55,924 - INFO - 数据下载完成
>>> 2025-09-08 16:15:00,134 - INFO - 数据映射完成
>>> 2025-09-08 16:15:00,135 - INFO - 打印训练参数如下
>>> 2025-09-08 16:15:00,136 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-08 16:15:00,136 - INFO -   dtype >>> torch.float16
>>> 2025-09-08 16:15:00,136 - INFO -   load_in_4bit >>> True
>>> 2025-09-08 16:15:00,137 - INFO -   batch_size >>> 8
>>> 2025-09-08 16:15:00,137 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-09-08 16:15:00,137 - INFO -   warmup_steps >>> 1
>>> 2025-09-08 16:15:00,138 - INFO -   epoch >>> 100
>>> 2025-09-08 16:15:00,138 - INFO -   eval_steps >>> 10
>>> 2025-09-08 16:15:00,138 - INFO -   learning_rate >>> 0.0001
>>> 2025-09-08 16:15:00,139 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-08 16:15:00,139 - INFO -   max_seq_length >>> 4096
>>> 2025-09-08 16:15:00,139 - INFO -   use_history >>> False
>>> 2025-09-08 16:15:00,140 - INFO -   r >>> 8
>>> 2025-09-08 16:15:00,140 - INFO -   interface_mode >>> False
>>> 2025-09-08 16:15:00,140 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
>>> 2025-09-08 16:15:00,141 - INFO -   lora_alpha >>> 16
>>> 2025-09-08 16:15:00,141 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-08 16:15:00,142 - INFO -   bias >>> none
>>> 2025-09-08 16:15:00,142 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-08 16:15:00,142 - INFO -   random_state >>> 3407
>>> 2025-09-08 16:15:00,143 - INFO -   use_rslora >>> True
>>> 2025-09-08 16:15:00,143 - INFO -   loftq_config >>> None
>>> 2025-09-08 16:15:01,786 - INFO - 开始训练！
>>> 2025-09-08 16:15:05,794 - INFO - >>> {'loss': 3.4046, 'grad_norm': 4.081578731536865, 'learning_rate': 0.0, 'epoch': 0.6153846153846154}
>>> 2025-09-08 16:15:08,205 - INFO - >>> {'loss': 3.342, 'grad_norm': 3.5934994220733643, 'learning_rate': 0.0001, 'epoch': 1.0}
>>> 2025-09-08 16:15:11,972 - INFO - >>> {'loss': 3.1762, 'grad_norm': 2.5992274284362793, 'learning_rate': 9.999376947588288e-05, 'epoch': 1.6153846153846154}
>>> 2025-09-08 16:15:14,066 - INFO - >>> {'loss': 3.0355, 'grad_norm': 2.050671100616455, 'learning_rate': 9.99750794563087e-05, 'epoch': 2.0}
>>> 2025-09-08 16:15:17,584 - INFO - >>> {'loss': 2.9843, 'grad_norm': 1.7225133180618286, 'learning_rate': 9.994393459922218e-05, 'epoch': 2.6153846153846154}
>>> 2025-09-08 16:15:19,666 - INFO - >>> {'loss': 2.6882, 'grad_norm': 1.3941751718521118, 'learning_rate': 9.990034266657467e-05, 'epoch': 3.0}
>>> 2025-09-08 16:15:23,384 - INFO - >>> {'loss': 2.8144, 'grad_norm': 1.2295246124267578, 'learning_rate': 9.984431452238967e-05, 'epoch': 3.6153846153846154}
>>> 2025-09-08 16:15:25,103 - INFO - >>> {'loss': 2.604, 'grad_norm': 1.5038994550704956, 'learning_rate': 9.977586413005531e-05, 'epoch': 4.0}
>>> 2025-09-08 16:15:28,791 - INFO - >>> {'loss': 2.65, 'grad_norm': 1.120152473449707, 'learning_rate': 9.96950085488444e-05, 'epoch': 4.615384615384615}
>>> 2025-09-08 16:15:30,806 - INFO - >>> {'loss': 2.7094, 'grad_norm': 1.2260535955429077, 'learning_rate': 9.960176792966289e-05, 'epoch': 5.0}
>>> 2025-09-08 16:15:34,215 - INFO - >>> {'loss': 2.5671, 'grad_norm': 1.2893884181976318, 'learning_rate': 9.949616551002787e-05, 'epoch': 5.615384615384615}
>>> 2025-09-08 16:15:36,288 - INFO - >>> {'loss': 2.6173, 'grad_norm': 1.368642807006836, 'learning_rate': 9.93782276082762e-05, 'epoch': 6.0}
>>> 2025-09-08 16:15:39,674 - INFO - >>> {'loss': 2.5277, 'grad_norm': 1.3935219049453735, 'learning_rate': 9.924798361700553e-05, 'epoch': 6.615384615384615}
>>> 2025-09-08 16:15:41,791 - INFO - >>> {'loss': 2.4597, 'grad_norm': 1.2763023376464844, 'learning_rate': 9.910546599574902e-05, 'epoch': 7.0}
>>> 2025-09-08 16:15:45,272 - INFO - >>> {'loss': 2.4529, 'grad_norm': 1.1144287586212158, 'learning_rate': 9.895071026288574e-05, 'epoch': 7.615384615384615}
>>> 2025-09-08 16:15:47,452 - INFO - >>> {'loss': 2.3462, 'grad_norm': 1.2418915033340454, 'learning_rate': 9.87837549867887e-05, 'epoch': 8.0}
>>> 2025-09-08 16:15:50,911 - INFO - >>> {'loss': 2.3496, 'grad_norm': 1.0740801095962524, 'learning_rate': 9.860464177621284e-05, 'epoch': 8.615384615384615}
>>> 2025-09-08 16:15:53,238 - INFO - >>> {'loss': 2.275, 'grad_norm': 1.2076513767242432, 'learning_rate': 9.841341526992536e-05, 'epoch': 9.0}
>>> 2025-09-08 16:15:56,524 - INFO - >>> {'loss': 2.2537, 'grad_norm': 1.1079742908477783, 'learning_rate': 9.821012312558058e-05, 'epoch': 9.615384615384615}
>>> 2025-09-08 16:15:58,402 - INFO - >>> {'loss': 2.2444, 'grad_norm': 1.3950868844985962, 'learning_rate': 9.799481600784286e-05, 'epoch': 10.0}
>>> 2025-09-08 16:16:01,948 - INFO - >>> {'loss': 2.1743, 'grad_norm': 1.1211661100387573, 'learning_rate': 9.776754757575975e-05, 'epoch': 10.615384615384615}
>>> 2025-09-08 16:16:03,894 - INFO - >>> {'loss': 2.1761, 'grad_norm': 1.3298510313034058, 'learning_rate': 9.752837446938915e-05, 'epoch': 11.0}
>>> 2025-09-08 16:16:07,557 - INFO - >>> {'loss': 2.1485, 'grad_norm': 1.1803648471832275, 'learning_rate': 9.727735629568336e-05, 'epoch': 11.615384615384615}
>>> 2025-09-08 16:16:09,531 - INFO - >>> {'loss': 2.0113, 'grad_norm': 1.332028865814209, 'learning_rate': 9.701455561363379e-05, 'epoch': 12.0}
>>> 2025-09-08 16:16:13,351 - INFO - >>> {'loss': 1.9899, 'grad_norm': 1.1677205562591553, 'learning_rate': 9.674003791867991e-05, 'epoch': 12.615384615384615}
>>> 2025-09-08 16:16:15,230 - INFO - >>> {'loss': 2.0809, 'grad_norm': 1.3779444694519043, 'learning_rate': 9.645387162638652e-05, 'epoch': 13.0}
>>> 2025-09-08 16:16:18,462 - INFO - >>> {'loss': 1.9447, 'grad_norm': 1.1478230953216553, 'learning_rate': 9.615612805539305e-05, 'epoch': 13.615384615384615}
>>> 2025-09-08 16:16:20,707 - INFO - >>> {'loss': 1.9687, 'grad_norm': 1.5197478532791138, 'learning_rate': 9.584688140963944e-05, 'epoch': 14.0}
>>> 2025-09-08 16:16:24,036 - INFO - >>> {'loss': 1.9309, 'grad_norm': 1.1725679636001587, 'learning_rate': 9.552620875987311e-05, 'epoch': 14.615384615384615}
>>> 2025-09-08 16:16:26,204 - INFO - >>> {'loss': 1.7768, 'grad_norm': 1.6332950592041016, 'learning_rate': 9.51941900244412e-05, 'epoch': 15.0}
>>> 2025-09-08 16:16:29,714 - INFO - >>> {'loss': 1.9149, 'grad_norm': 1.2145640850067139, 'learning_rate': 9.485090794937319e-05, 'epoch': 15.615384615384615}
>>> 2025-09-08 16:16:31,994 - INFO - >>> {'loss': 1.6719, 'grad_norm': 1.3271746635437012, 'learning_rate': 9.449644808775902e-05, 'epoch': 16.0}
>>> 2025-09-08 16:16:35,713 - INFO - >>> {'loss': 1.765, 'grad_norm': 1.2857825756072998, 'learning_rate': 9.413089877842736e-05, 'epoch': 16.615384615384617}
>>> 2025-09-08 16:16:38,086 - INFO - >>> {'loss': 1.6376, 'grad_norm': 1.6113568544387817, 'learning_rate': 9.375435112392969e-05, 'epoch': 17.0}
>>> 2025-09-08 16:16:42,345 - INFO - >>> {'loss': 1.6864, 'grad_norm': 1.144437313079834, 'learning_rate': 9.336689896783573e-05, 'epoch': 17.615384615384617}
>>> 2025-09-08 16:16:44,328 - INFO - >>> {'loss': 1.5195, 'grad_norm': 1.79757821559906, 'learning_rate': 9.29686388713456e-05, 'epoch': 18.0}
>>> 2025-09-08 16:16:48,263 - INFO - >>> {'loss': 1.5492, 'grad_norm': 1.2524967193603516, 'learning_rate': 9.255967008922474e-05, 'epoch': 18.615384615384617}
>>> 2025-09-08 16:16:50,770 - INFO - >>> {'loss': 1.5526, 'grad_norm': 1.6413642168045044, 'learning_rate': 9.214009454506753e-05, 'epoch': 19.0}
>>> 2025-09-08 16:16:54,805 - INFO - >>> {'loss': 1.5542, 'grad_norm': 1.2900183200836182, 'learning_rate': 9.171001680589588e-05, 'epoch': 19.615384615384617}
>>> 2025-09-08 16:16:56,992 - INFO - >>> {'loss': 1.3064, 'grad_norm': 1.7925834655761719, 'learning_rate': 9.126954405609882e-05, 'epoch': 20.0}
>>> 2025-09-08 16:17:01,680 - INFO - >>> {'loss': 1.3984, 'grad_norm': 1.331845760345459, 'learning_rate': 9.081878607071996e-05, 'epoch': 20.615384615384617}
>>> 2025-09-08 16:17:04,177 - INFO - >>> {'loss': 1.3533, 'grad_norm': 1.8307101726531982, 'learning_rate': 9.035785518809927e-05, 'epoch': 21.0}
>>> 2025-09-08 16:17:08,346 - INFO - >>> {'loss': 1.2718, 'grad_norm': 1.3867107629776, 'learning_rate': 8.988686628187597e-05, 'epoch': 21.615384615384617}
>>> 2025-09-08 16:17:11,334 - INFO - >>> {'loss': 1.3183, 'grad_norm': 1.743468165397644, 'learning_rate': 8.940593673235962e-05, 'epoch': 22.0}
>>> 2025-09-08 16:17:16,044 - INFO - >>> {'loss': 1.2852, 'grad_norm': 1.434459924697876, 'learning_rate': 8.891518639727649e-05, 'epoch': 22.615384615384617}
>>> 2025-09-08 16:17:18,258 - INFO - >>> {'loss': 1.082, 'grad_norm': 1.8479816913604736, 'learning_rate': 8.841473758189854e-05, 'epoch': 23.0}
>>> 2025-09-08 16:20:47,103 - INFO - 导入包完成
>>> 2025-09-08 16:20:47,103 - INFO - ========train Qwen2ForCausalLM  202509081620========
>>> 2025-09-08 16:20:47,104 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-08 16:20:47,104 - INFO - 开始进行训练
>>> 2025-09-08 16:20:47,109 - INFO - 基础配置文件读取完成
>>> 2025-09-08 16:20:47,117 - INFO - 训练配置读取完成
>>> 2025-09-08 16:20:47,118 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/own/Medical_20250816.json
>>> 2025-09-08 16:20:47,118 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-08 16:20:47,563 - INFO - tokenizer读取完成
>>> 2025-09-08 16:20:47,899 - INFO - model dtype:torch.float16
>>> 2025-09-08 16:20:47,899 - INFO - 模型导入完成
>>> 2025-09-08 16:20:47,899 - INFO - 数据读取开始
>>> 2025-09-08 16:20:48,999 - INFO - 数据下载完成
>>> 2025-09-08 16:20:53,377 - INFO - 数据映射完成
>>> 2025-09-08 16:20:53,378 - INFO - 打印训练参数如下
>>> 2025-09-08 16:20:53,378 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-08 16:20:53,378 - INFO -   dtype >>> torch.float16
>>> 2025-09-08 16:20:53,379 - INFO -   load_in_4bit >>> True
>>> 2025-09-08 16:20:53,379 - INFO -   batch_size >>> 8
>>> 2025-09-08 16:20:53,379 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-09-08 16:20:53,380 - INFO -   warmup_steps >>> 1
>>> 2025-09-08 16:20:53,380 - INFO -   epoch >>> 100
>>> 2025-09-08 16:20:53,380 - INFO -   eval_steps >>> 10
>>> 2025-09-08 16:20:53,381 - INFO -   learning_rate >>> 0.0001
>>> 2025-09-08 16:20:53,381 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-08 16:20:53,381 - INFO -   max_seq_length >>> 4096
>>> 2025-09-08 16:20:53,382 - INFO -   use_history >>> False
>>> 2025-09-08 16:20:53,382 - INFO -   r >>> 8
>>> 2025-09-08 16:20:53,382 - INFO -   interface_mode >>> False
>>> 2025-09-08 16:20:53,383 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
>>> 2025-09-08 16:20:53,383 - INFO -   lora_alpha >>> 16
>>> 2025-09-08 16:20:53,384 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-08 16:20:53,384 - INFO -   bias >>> none
>>> 2025-09-08 16:20:53,384 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-08 16:20:53,385 - INFO -   random_state >>> 3407
>>> 2025-09-08 16:20:53,385 - INFO -   use_rslora >>> True
>>> 2025-09-08 16:20:53,385 - INFO -   loftq_config >>> None
>>> 2025-09-08 16:20:54,987 - INFO - 开始训练！
>>> 2025-09-08 16:20:58,901 - INFO - >>> {'loss': 3.4046, 'grad_norm': 4.112236022949219, 'learning_rate': 0.0, 'epoch': 0.6153846153846154}
>>> 2025-09-08 16:21:01,285 - INFO - >>> {'loss': 3.342, 'grad_norm': 3.6410977840423584, 'learning_rate': 0.0001, 'epoch': 1.0}
>>> 2025-09-08 16:21:05,029 - INFO - >>> {'loss': 3.176, 'grad_norm': 2.6736233234405518, 'learning_rate': 9.999376947588288e-05, 'epoch': 1.6153846153846154}
>>> 2025-09-08 16:21:07,101 - INFO - >>> {'loss': 3.0334, 'grad_norm': 2.089937448501587, 'learning_rate': 9.99750794563087e-05, 'epoch': 2.0}
>>> 2025-09-08 16:21:10,613 - INFO - >>> {'loss': 2.9811, 'grad_norm': 1.746726393699646, 'learning_rate': 9.994393459922218e-05, 'epoch': 2.6153846153846154}
>>> 2025-09-08 16:21:12,673 - INFO - >>> {'loss': 2.6852, 'grad_norm': 1.3866260051727295, 'learning_rate': 9.990034266657467e-05, 'epoch': 3.0}
>>> 2025-09-08 16:21:16,382 - INFO - >>> {'loss': 2.8116, 'grad_norm': 1.2373565435409546, 'learning_rate': 9.984431452238967e-05, 'epoch': 3.6153846153846154}
>>> 2025-09-08 16:21:18,052 - INFO - >>> {'loss': 2.5994, 'grad_norm': 1.5101308822631836, 'learning_rate': 9.977586413005531e-05, 'epoch': 4.0}
>>> 2025-09-08 16:21:21,728 - INFO - >>> {'loss': 2.6461, 'grad_norm': 1.1271941661834717, 'learning_rate': 9.96950085488444e-05, 'epoch': 4.615384615384615}
>>> 2025-09-08 16:21:23,723 - INFO - >>> {'loss': 2.7054, 'grad_norm': 1.2435277700424194, 'learning_rate': 9.960176792966289e-05, 'epoch': 5.0}
>>> 2025-09-08 16:21:27,106 - INFO - >>> {'loss': 2.5605, 'grad_norm': 1.2609812021255493, 'learning_rate': 9.949616551002787e-05, 'epoch': 5.615384615384615}
>>> 2025-09-08 16:21:29,121 - INFO - >>> {'loss': 2.6119, 'grad_norm': 1.3343969583511353, 'learning_rate': 9.93782276082762e-05, 'epoch': 6.0}
>>> 2025-09-08 16:21:32,437 - INFO - >>> {'loss': 2.5212, 'grad_norm': 1.3732277154922485, 'learning_rate': 9.924798361700553e-05, 'epoch': 6.615384615384615}
>>> 2025-09-08 16:21:34,526 - INFO - >>> {'loss': 2.4537, 'grad_norm': 1.2635629177093506, 'learning_rate': 9.910546599574902e-05, 'epoch': 7.0}
>>> 2025-09-08 16:21:37,982 - INFO - >>> {'loss': 2.4473, 'grad_norm': 1.1386469602584839, 'learning_rate': 9.895071026288574e-05, 'epoch': 7.615384615384615}
>>> 2025-09-08 16:21:40,114 - INFO - >>> {'loss': 2.3377, 'grad_norm': 1.2484668493270874, 'learning_rate': 9.87837549867887e-05, 'epoch': 8.0}
>>> 2025-09-08 16:21:43,566 - INFO - >>> {'loss': 2.3434, 'grad_norm': 1.050511121749878, 'learning_rate': 9.860464177621284e-05, 'epoch': 8.615384615384615}
>>> 2025-09-08 16:21:45,847 - INFO - >>> {'loss': 2.2686, 'grad_norm': 1.2324802875518799, 'learning_rate': 9.841341526992536e-05, 'epoch': 9.0}
>>> 2025-09-08 16:21:49,116 - INFO - >>> {'loss': 2.2463, 'grad_norm': 1.1322190761566162, 'learning_rate': 9.821012312558058e-05, 'epoch': 9.615384615384615}
>>> 2025-09-08 16:21:50,993 - INFO - >>> {'loss': 2.24, 'grad_norm': 1.3837629556655884, 'learning_rate': 9.799481600784286e-05, 'epoch': 10.0}
>>> 2025-09-08 16:21:54,533 - INFO - >>> {'loss': 2.167, 'grad_norm': 1.1194069385528564, 'learning_rate': 9.776754757575975e-05, 'epoch': 10.615384615384615}
>>> 2025-09-08 16:21:56,504 - INFO - >>> {'loss': 2.1724, 'grad_norm': 1.3444087505340576, 'learning_rate': 9.752837446938915e-05, 'epoch': 11.0}
>>> 2025-09-08 16:22:00,166 - INFO - >>> {'loss': 2.1425, 'grad_norm': 1.1688458919525146, 'learning_rate': 9.727735629568336e-05, 'epoch': 11.615384615384615}
>>> 2025-09-08 16:22:02,155 - INFO - >>> {'loss': 2.0053, 'grad_norm': 1.3421738147735596, 'learning_rate': 9.701455561363379e-05, 'epoch': 12.0}
>>> 2025-09-08 16:22:06,029 - INFO - >>> {'loss': 1.9852, 'grad_norm': 1.1344834566116333, 'learning_rate': 9.674003791867991e-05, 'epoch': 12.615384615384615}
>>> 2025-09-08 16:22:07,988 - INFO - >>> {'loss': 2.0746, 'grad_norm': 1.3802242279052734, 'learning_rate': 9.645387162638652e-05, 'epoch': 13.0}
>>> 2025-09-08 16:22:11,336 - INFO - >>> {'loss': 1.9375, 'grad_norm': 1.1441453695297241, 'learning_rate': 9.615612805539305e-05, 'epoch': 13.615384615384615}
>>> 2025-09-08 16:22:13,693 - INFO - >>> {'loss': 1.9636, 'grad_norm': 1.4531956911087036, 'learning_rate': 9.584688140963944e-05, 'epoch': 14.0}
>>> 2025-09-08 16:22:17,484 - INFO - >>> {'loss': 1.9256, 'grad_norm': 1.1112573146820068, 'learning_rate': 9.552620875987311e-05, 'epoch': 14.615384615384615}
>>> 2025-09-08 16:22:19,501 - INFO - >>> {'loss': 1.7717, 'grad_norm': 1.6295820474624634, 'learning_rate': 9.51941900244412e-05, 'epoch': 15.0}
>>> 2025-09-08 16:22:23,184 - INFO - >>> {'loss': 1.9097, 'grad_norm': 1.271017074584961, 'learning_rate': 9.485090794937319e-05, 'epoch': 15.615384615384615}
>>> 2025-09-08 16:22:25,654 - INFO - >>> {'loss': 1.6641, 'grad_norm': 1.3112447261810303, 'learning_rate': 9.449644808775902e-05, 'epoch': 16.0}
>>> 2025-09-08 16:22:29,685 - INFO - >>> {'loss': 1.7567, 'grad_norm': 1.284874439239502, 'learning_rate': 9.413089877842736e-05, 'epoch': 16.615384615384617}
>>> 2025-09-08 16:22:32,164 - INFO - >>> {'loss': 1.6358, 'grad_norm': 1.6202036142349243, 'learning_rate': 9.375435112392969e-05, 'epoch': 17.0}
>>> 2025-09-08 16:22:36,603 - INFO - >>> {'loss': 1.6797, 'grad_norm': 1.1229041814804077, 'learning_rate': 9.336689896783573e-05, 'epoch': 17.615384615384617}
>>> 2025-09-08 16:22:38,708 - INFO - >>> {'loss': 1.5146, 'grad_norm': 1.741707682609558, 'learning_rate': 9.29686388713456e-05, 'epoch': 18.0}
>>> 2025-09-08 16:22:42,730 - INFO - >>> {'loss': 1.5437, 'grad_norm': 1.2257945537567139, 'learning_rate': 9.255967008922474e-05, 'epoch': 18.615384615384617}
>>> 2025-09-08 16:22:45,373 - INFO - >>> {'loss': 1.5485, 'grad_norm': 1.6259771585464478, 'learning_rate': 9.214009454506753e-05, 'epoch': 19.0}
>>> 2025-09-08 16:22:49,695 - INFO - >>> {'loss': 1.5457, 'grad_norm': 1.299100637435913, 'learning_rate': 9.171001680589588e-05, 'epoch': 19.615384615384617}
>>> 2025-09-08 16:22:51,969 - INFO - >>> {'loss': 1.305, 'grad_norm': 1.8141013383865356, 'learning_rate': 9.126954405609882e-05, 'epoch': 20.0}
>>> 2025-09-08 16:22:56,721 - INFO - >>> {'loss': 1.3926, 'grad_norm': 1.336161494255066, 'learning_rate': 9.081878607071996e-05, 'epoch': 20.615384615384617}
>>> 2025-09-08 16:22:59,404 - INFO - >>> {'loss': 1.3473, 'grad_norm': 1.8375638723373413, 'learning_rate': 9.035785518809927e-05, 'epoch': 21.0}
>>> 2025-09-08 16:23:03,566 - INFO - >>> {'loss': 1.2642, 'grad_norm': 1.4586355686187744, 'learning_rate': 8.988686628187597e-05, 'epoch': 21.615384615384617}
>>> 2025-09-08 16:23:06,675 - INFO - >>> {'loss': 1.3197, 'grad_norm': 1.7356609106063843, 'learning_rate': 8.940593673235962e-05, 'epoch': 22.0}
>>> 2025-09-08 16:23:11,278 - INFO - >>> {'loss': 1.279, 'grad_norm': 1.417258620262146, 'learning_rate': 8.891518639727649e-05, 'epoch': 22.615384615384617}
>>> 2025-09-08 16:23:13,666 - INFO - >>> {'loss': 1.0808, 'grad_norm': 1.7747304439544678, 'learning_rate': 8.841473758189854e-05, 'epoch': 23.0}
>>> 2025-09-08 16:23:18,026 - INFO - >>> {'loss': 1.1356, 'grad_norm': 1.409256100654602, 'learning_rate': 8.790471500856228e-05, 'epoch': 23.615384615384617}
>>> 2025-09-08 16:23:20,371 - INFO - >>> {'loss': 1.0396, 'grad_norm': 2.2022385597229004, 'learning_rate': 8.738524578558547e-05, 'epoch': 24.0}
>>> 2025-09-08 16:23:25,351 - INFO - >>> {'loss': 1.0365, 'grad_norm': 1.353580355644226, 'learning_rate': 8.685645937558896e-05, 'epoch': 24.615384615384617}
>>> 2025-09-08 16:23:27,955 - INFO - >>> {'loss': 0.9486, 'grad_norm': 2.2834627628326416, 'learning_rate': 8.631848756323197e-05, 'epoch': 25.0}
>>> 2025-09-08 16:23:32,309 - INFO - >>> {'loss': 0.8622, 'grad_norm': 1.478149652481079, 'learning_rate': 8.577146442236857e-05, 'epoch': 25.615384615384617}
>>> 2025-09-08 16:23:35,794 - INFO - >>> {'loss': 0.9958, 'grad_norm': 2.1151275634765625, 'learning_rate': 8.521552628263362e-05, 'epoch': 26.0}
>>> 2025-09-08 16:23:39,561 - INFO - >>> {'loss': 0.763, 'grad_norm': 2.0170726776123047, 'learning_rate': 8.465081169546659e-05, 'epoch': 26.615384615384617}
>>> 2025-09-08 16:23:42,945 - INFO - >>> {'loss': 0.8806, 'grad_norm': 1.9327857494354248, 'learning_rate': 8.40774613995817e-05, 'epoch': 27.0}
>>> 2025-09-08 16:23:47,964 - INFO - >>> {'loss': 0.7518, 'grad_norm': 1.685855507850647, 'learning_rate': 8.349561828589277e-05, 'epoch': 27.615384615384617}
>>> 2025-09-08 16:23:50,200 - INFO - >>> {'loss': 0.6852, 'grad_norm': 2.3655574321746826, 'learning_rate': 8.290542736190188e-05, 'epoch': 28.0}
>>> 2025-09-08 16:23:55,098 - INFO - >>> {'loss': 0.6676, 'grad_norm': 2.083977460861206, 'learning_rate': 8.230703571556048e-05, 'epoch': 28.615384615384617}
>>> 2025-09-08 16:23:58,040 - INFO - >>> {'loss': 0.6098, 'grad_norm': 2.1337928771972656, 'learning_rate': 8.170059247861194e-05, 'epoch': 29.0}
>>> 2025-09-08 16:24:03,321 - INFO - >>> {'loss': 0.5933, 'grad_norm': 1.7640862464904785, 'learning_rate': 8.108624878942477e-05, 'epoch': 29.615384615384617}
>>> 2025-09-08 16:24:05,389 - INFO - >>> {'loss': 0.5094, 'grad_norm': 2.209901809692383, 'learning_rate': 8.046415775532585e-05, 'epoch': 30.0}
>>> 2025-09-08 16:24:10,972 - INFO - >>> {'loss': 0.5225, 'grad_norm': 1.9615788459777832, 'learning_rate': 7.983447441444281e-05, 'epoch': 30.615384615384617}
>>> 2025-09-08 16:24:13,655 - INFO - >>> {'loss': 0.4334, 'grad_norm': 2.6860389709472656, 'learning_rate': 7.919735569706533e-05, 'epoch': 31.0}
>>> 2025-09-08 16:24:17,989 - INFO - >>> {'loss': 0.3744, 'grad_norm': 1.9615906476974487, 'learning_rate': 7.855296038653475e-05, 'epoch': 31.615384615384617}
>>> 2025-09-08 16:24:20,823 - INFO - >>> {'loss': 0.5099, 'grad_norm': 2.43245005607605, 'learning_rate': 7.790144907967201e-05, 'epoch': 32.0}
>>> 2025-09-08 16:24:25,647 - INFO - >>> {'loss': 0.3819, 'grad_norm': 1.416100263595581, 'learning_rate': 7.724298414675353e-05, 'epoch': 32.61538461538461}
>>> 2025-09-08 16:24:28,253 - INFO - >>> {'loss': 0.3166, 'grad_norm': 2.095196485519409, 'learning_rate': 7.657772969104508e-05, 'epoch': 33.0}
>>> 2025-09-08 16:24:32,344 - INFO - >>> {'loss': 0.3129, 'grad_norm': 1.2437512874603271, 'learning_rate': 7.590585150790389e-05, 'epoch': 33.61538461538461}
>>> 2025-09-08 16:24:34,945 - INFO - >>> {'loss': 0.2779, 'grad_norm': 2.0419604778289795, 'learning_rate': 7.522751704345887e-05, 'epoch': 34.0}
>>> 2025-09-08 16:24:40,001 - INFO - >>> {'loss': 0.2536, 'grad_norm': 1.2538502216339111, 'learning_rate': 7.454289535287968e-05, 'epoch': 34.61538461538461}
>>> 2025-09-08 16:24:42,460 - INFO - >>> {'loss': 0.2415, 'grad_norm': 2.900974750518799, 'learning_rate': 7.385215705824449e-05, 'epoch': 35.0}
>>> 2025-09-08 16:24:47,476 - INFO - >>> {'loss': 0.2287, 'grad_norm': 1.6953374147415161, 'learning_rate': 7.31554743060174e-05, 'epoch': 35.61538461538461}
>>> 2025-09-08 16:24:49,862 - INFO - >>> {'loss': 0.1498, 'grad_norm': 1.5825321674346924, 'learning_rate': 7.245302072414601e-05, 'epoch': 36.0}
>>> 2025-09-08 16:24:54,431 - INFO - >>> {'loss': 0.1626, 'grad_norm': 1.5205585956573486, 'learning_rate': 7.174497137878966e-05, 'epoch': 36.61538461538461}
>>> 2025-09-08 16:24:57,505 - INFO - >>> {'loss': 0.1714, 'grad_norm': 1.5624654293060303, 'learning_rate': 7.103150273068921e-05, 'epoch': 37.0}
>>> 2025-09-08 16:25:02,709 - INFO - >>> {'loss': 0.1516, 'grad_norm': 0.9010241627693176, 'learning_rate': 7.031279259118946e-05, 'epoch': 37.61538461538461}
>>> 2025-09-08 16:25:04,875 - INFO - >>> {'loss': 0.0851, 'grad_norm': 2.13268780708313, 'learning_rate': 6.958902007792466e-05, 'epoch': 38.0}
>>> 2025-09-08 16:25:09,112 - INFO - >>> {'loss': 0.1056, 'grad_norm': 0.8352938890457153, 'learning_rate': 6.886036557017881e-05, 'epoch': 38.61538461538461}
>>> 2025-09-08 16:25:11,782 - INFO - >>> {'loss': 0.1021, 'grad_norm': 1.0348321199417114, 'learning_rate': 6.812701066393124e-05, 'epoch': 39.0}
>>> 2025-09-08 16:25:15,557 - INFO - >>> {'loss': 0.0949, 'grad_norm': 0.8603613376617432, 'learning_rate': 6.738913812659912e-05, 'epoch': 39.61538461538461}
>>> 2025-09-08 16:25:18,454 - INFO - >>> {'loss': 0.0706, 'grad_norm': 0.9281290173530579, 'learning_rate': 6.664693185148807e-05, 'epoch': 40.0}
>>> 2025-09-08 16:25:23,040 - INFO - >>> {'loss': 0.0723, 'grad_norm': 0.7138845324516296, 'learning_rate': 6.590057681196191e-05, 'epoch': 40.61538461538461}
>>> 2025-09-08 16:25:24,986 - INFO - >>> {'loss': 0.0521, 'grad_norm': 0.9347865581512451, 'learning_rate': 6.515025901534364e-05, 'epoch': 41.0}
>>> 2025-09-08 16:25:29,438 - INFO - >>> {'loss': 0.0426, 'grad_norm': 0.515579879283905, 'learning_rate': 6.439616545655834e-05, 'epoch': 41.61538461538461}
>>> 2025-09-08 16:25:32,170 - INFO - >>> {'loss': 0.0634, 'grad_norm': 0.8626475930213928, 'learning_rate': 6.363848407153016e-05, 'epoch': 42.0}
>>> 2025-09-08 16:25:36,266 - INFO - >>> {'loss': 0.0396, 'grad_norm': 0.46717336773872375, 'learning_rate': 6.287740369034485e-05, 'epoch': 42.61538461538461}
>>> 2025-09-08 16:25:39,041 - INFO - >>> {'loss': 0.0457, 'grad_norm': 0.5690860748291016, 'learning_rate': 6.211311399018916e-05, 'epoch': 43.0}
>>> 2025-09-08 16:25:43,218 - INFO - >>> {'loss': 0.0261, 'grad_norm': 0.42753922939300537, 'learning_rate': 6.13458054480795e-05, 'epoch': 43.61538461538461}
>>> 2025-09-08 16:25:45,997 - INFO - >>> {'loss': 0.0439, 'grad_norm': 0.7897441983222961, 'learning_rate': 6.0575669293390954e-05, 'epoch': 44.0}
>>> 2025-09-08 16:25:50,187 - INFO - >>> {'loss': 0.0292, 'grad_norm': 0.4035695791244507, 'learning_rate': 5.980289746019892e-05, 'epoch': 44.61538461538461}
>>> 2025-09-08 16:25:52,399 - INFO - >>> {'loss': 0.0254, 'grad_norm': 0.6615421175956726, 'learning_rate': 5.9027682539445104e-05, 'epoch': 45.0}
>>> 2025-09-08 16:25:56,423 - INFO - >>> {'loss': 0.0269, 'grad_norm': 0.38689926266670227, 'learning_rate': 5.8250217730939973e-05, 'epoch': 45.61538461538461}
>>> 2025-09-08 16:25:59,397 - INFO - >>> {'loss': 0.0185, 'grad_norm': 0.4418387711048126, 'learning_rate': 5.747069679521305e-05, 'epoch': 46.0}
>>> 2025-09-08 16:26:03,643 - INFO - >>> {'loss': 0.0201, 'grad_norm': 0.32657209038734436, 'learning_rate': 5.668931400522396e-05, 'epoch': 46.61538461538461}
>>> 2025-09-08 16:26:06,434 - INFO - >>> {'loss': 0.0172, 'grad_norm': 0.5191202759742737, 'learning_rate': 5.5906264097945407e-05, 'epoch': 47.0}
>>> 2025-09-08 16:26:10,704 - INFO - >>> {'loss': 0.0143, 'grad_norm': 0.3663175702095032, 'learning_rate': 5.5121742225830665e-05, 'epoch': 47.61538461538461}
>>> 2025-09-08 16:26:13,624 - INFO - >>> {'loss': 0.0195, 'grad_norm': 0.3556227385997772, 'learning_rate': 5.433594390817756e-05, 'epoch': 48.0}
>>> 2025-09-08 16:26:18,027 - INFO - >>> {'loss': 0.0142, 'grad_norm': 0.22863252460956573, 'learning_rate': 5.35490649824008e-05, 'epoch': 48.61538461538461}
>>> 2025-09-08 16:26:20,387 - INFO - >>> {'loss': 0.0154, 'grad_norm': 0.6581822633743286, 'learning_rate': 5.276130155522541e-05, 'epoch': 49.0}
>>> 2025-09-08 16:26:24,727 - INFO - >>> {'loss': 0.0139, 'grad_norm': 0.26160910725593567, 'learning_rate': 5.1972849953812644e-05, 'epoch': 49.61538461538461}
>>> 2025-09-08 16:26:26,885 - INFO - >>> {'loss': 0.0108, 'grad_norm': 0.5037664175033569, 'learning_rate': 5.1183906676831197e-05, 'epoch': 50.0}
>>> 2025-09-08 16:26:31,661 - INFO - >>> {'loss': 0.0109, 'grad_norm': 0.2444269359111786, 'learning_rate': 5.039466834548568e-05, 'epoch': 50.61538461538461}
>>> 2025-09-08 16:26:33,853 - INFO - >>> {'loss': 0.013, 'grad_norm': 0.3689153790473938, 'learning_rate': 4.960533165451435e-05, 'epoch': 51.0}
>>> 2025-09-08 16:26:38,456 - INFO - >>> {'loss': 0.0121, 'grad_norm': 0.1721024513244629, 'learning_rate': 4.8816093323168815e-05, 'epoch': 51.61538461538461}
>>> 2025-09-08 16:26:40,864 - INFO - >>> {'loss': 0.0071, 'grad_norm': 0.1482519954442978, 'learning_rate': 4.802715004618737e-05, 'epoch': 52.0}
>>> 2025-09-08 16:26:45,287 - INFO - >>> {'loss': 0.0104, 'grad_norm': 0.1865694522857666, 'learning_rate': 4.7238698444774595e-05, 'epoch': 52.61538461538461}
>>> 2025-09-08 16:26:47,990 - INFO - >>> {'loss': 0.0088, 'grad_norm': 0.40177789330482483, 'learning_rate': 4.64509350175992e-05, 'epoch': 53.0}
>>> 2025-09-08 16:26:51,564 - INFO - >>> {'loss': 0.0094, 'grad_norm': 0.23434136807918549, 'learning_rate': 4.566405609182247e-05, 'epoch': 53.61538461538461}
>>> 2025-09-08 16:26:54,004 - INFO - >>> {'loss': 0.0086, 'grad_norm': 0.32965555787086487, 'learning_rate': 4.4878257774169346e-05, 'epoch': 54.0}
>>> 2025-09-08 16:26:58,025 - INFO - >>> {'loss': 0.008, 'grad_norm': 0.1678168773651123, 'learning_rate': 4.4093735902054605e-05, 'epoch': 54.61538461538461}
>>> 2025-09-08 16:27:01,020 - INFO - >>> {'loss': 0.0083, 'grad_norm': 0.14134055376052856, 'learning_rate': 4.331068599477605e-05, 'epoch': 55.0}
>>> 2025-09-08 16:27:05,447 - INFO - >>> {'loss': 0.0065, 'grad_norm': 0.12990976870059967, 'learning_rate': 4.2529303204786953e-05, 'epoch': 55.61538461538461}
>>> 2025-09-08 16:27:07,974 - INFO - >>> {'loss': 0.0101, 'grad_norm': 0.5680553913116455, 'learning_rate': 4.1749782269060045e-05, 'epoch': 56.0}
>>> 2025-09-08 16:27:12,150 - INFO - >>> {'loss': 0.0072, 'grad_norm': 0.18205875158309937, 'learning_rate': 4.097231746055491e-05, 'epoch': 56.61538461538461}
>>> 2025-09-08 16:27:14,755 - INFO - >>> {'loss': 0.0084, 'grad_norm': 0.3351564109325409, 'learning_rate': 4.01971025398011e-05, 'epoch': 57.0}
>>> 2025-09-08 16:27:19,601 - INFO - >>> {'loss': 0.0076, 'grad_norm': 0.1423037052154541, 'learning_rate': 3.942433070660905e-05, 'epoch': 57.61538461538461}
>>> 2025-09-08 16:27:21,659 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.14101089537143707, 'learning_rate': 3.8654194551920485e-05, 'epoch': 58.0}
>>> 2025-09-08 16:27:26,059 - INFO - >>> {'loss': 0.0074, 'grad_norm': 0.24498796463012695, 'learning_rate': 3.788688600981085e-05, 'epoch': 58.61538461538461}
>>> 2025-09-08 16:27:28,778 - INFO - >>> {'loss': 0.006, 'grad_norm': 0.1693122237920761, 'learning_rate': 3.712259630965518e-05, 'epoch': 59.0}
>>> 2025-09-08 16:27:33,201 - INFO - >>> {'loss': 0.0065, 'grad_norm': 0.15875905752182007, 'learning_rate': 3.636151592846985e-05, 'epoch': 59.61538461538461}
>>> 2025-09-08 16:27:35,614 - INFO - >>> {'loss': 0.0062, 'grad_norm': 0.23158441483974457, 'learning_rate': 3.560383454344168e-05, 'epoch': 60.0}
>>> 2025-09-08 16:27:39,532 - INFO - >>> {'loss': 0.0064, 'grad_norm': 0.15325704216957092, 'learning_rate': 3.484974098465636e-05, 'epoch': 60.61538461538461}
>>> 2025-09-08 16:27:42,682 - INFO - >>> {'loss': 0.0061, 'grad_norm': 0.1705220341682434, 'learning_rate': 3.409942318803809e-05, 'epoch': 61.0}
>>> 2025-09-08 16:27:46,990 - INFO - >>> {'loss': 0.0063, 'grad_norm': 0.195700541138649, 'learning_rate': 3.335306814851196e-05, 'epoch': 61.61538461538461}
>>> 2025-09-08 16:27:49,427 - INFO - >>> {'loss': 0.0062, 'grad_norm': 0.23207548260688782, 'learning_rate': 3.261086187340088e-05, 'epoch': 62.0}
>>> 2025-09-08 16:27:53,117 - INFO - >>> {'loss': 0.0057, 'grad_norm': 0.0963149443268776, 'learning_rate': 3.187298933606878e-05, 'epoch': 62.61538461538461}
>>> 2025-09-08 16:27:55,574 - INFO - >>> {'loss': 0.006, 'grad_norm': 0.12129085510969162, 'learning_rate': 3.11396344298212e-05, 'epoch': 63.0}
>>> 2025-09-08 16:27:59,937 - INFO - >>> {'loss': 0.0058, 'grad_norm': 0.17938339710235596, 'learning_rate': 3.0410979922075343e-05, 'epoch': 63.61538461538461}
>>> 2025-09-08 16:28:02,188 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.22458499670028687, 'learning_rate': 2.9687207408810557e-05, 'epoch': 64.0}
>>> 2025-09-08 16:28:06,004 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.1502159833908081, 'learning_rate': 2.8968497269310803e-05, 'epoch': 64.61538461538461}
>>> 2025-09-08 16:28:08,566 - INFO - >>> {'loss': 0.0063, 'grad_norm': 0.2546912431716919, 'learning_rate': 2.8255028621210355e-05, 'epoch': 65.0}
>>> 2025-09-08 16:28:12,325 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.14398469030857086, 'learning_rate': 2.754697927585399e-05, 'epoch': 65.61538461538461}
>>> 2025-09-08 16:28:14,721 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.163047194480896, 'learning_rate': 2.6844525693982613e-05, 'epoch': 66.0}
>>> 2025-09-08 16:28:18,257 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.13115017116069794, 'learning_rate': 2.614784294175554e-05, 'epoch': 66.61538461538461}
>>> 2025-09-08 16:28:20,897 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.19774229824543, 'learning_rate': 2.5457104647120322e-05, 'epoch': 67.0}
>>> 2025-09-08 16:28:24,783 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.08513940125703812, 'learning_rate': 2.4772482956541132e-05, 'epoch': 67.61538461538461}
>>> 2025-09-08 16:28:27,097 - INFO - >>> {'loss': 0.0066, 'grad_norm': 0.20597253739833832, 'learning_rate': 2.4094148492096125e-05, 'epoch': 68.0}
>>> 2025-09-08 16:28:31,120 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.16332730650901794, 'learning_rate': 2.3422270308954934e-05, 'epoch': 68.61538461538461}
>>> 2025-09-08 16:28:33,242 - INFO - >>> {'loss': 0.006, 'grad_norm': 0.29518651962280273, 'learning_rate': 2.2757015853246493e-05, 'epoch': 69.0}
>>> 2025-09-08 16:28:37,256 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.1198042780160904, 'learning_rate': 2.2098550920327998e-05, 'epoch': 69.61538461538461}
>>> 2025-09-08 16:28:39,548 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.20657317340373993, 'learning_rate': 2.1447039613465265e-05, 'epoch': 70.0}
>>> 2025-09-08 16:28:43,363 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.09436926245689392, 'learning_rate': 2.0802644302934683e-05, 'epoch': 70.61538461538461}
>>> 2025-09-08 16:28:45,367 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.0900910273194313, 'learning_rate': 2.0165525585557204e-05, 'epoch': 71.0}
>>> 2025-09-08 16:28:48,553 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.19810850918293, 'learning_rate': 1.953584224467418e-05, 'epoch': 71.61538461538461}
>>> 2025-09-08 16:28:51,132 - INFO - >>> {'loss': 0.0058, 'grad_norm': 0.1951262205839157, 'learning_rate': 1.8913751210575248e-05, 'epoch': 72.0}
>>> 2025-09-08 16:28:54,450 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.20334604382514954, 'learning_rate': 1.8299407521388067e-05, 'epoch': 72.61538461538461}
>>> 2025-09-08 16:28:56,893 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.1705251932144165, 'learning_rate': 1.7692964284439505e-05, 'epoch': 73.0}
>>> 2025-09-08 16:29:00,139 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.09966213256120682, 'learning_rate': 1.7094572638098123e-05, 'epoch': 73.61538461538461}
>>> 2025-09-08 16:29:02,718 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.3059545159339905, 'learning_rate': 1.6504381714107252e-05, 'epoch': 74.0}
>>> 2025-09-08 16:29:06,529 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.13484689593315125, 'learning_rate': 1.5922538600418318e-05, 'epoch': 74.61538461538461}
>>> 2025-09-08 16:29:08,527 - INFO - >>> {'loss': 0.0057, 'grad_norm': 0.2369936853647232, 'learning_rate': 1.5349188304533413e-05, 'epoch': 75.0}
>>> 2025-09-08 16:29:11,992 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.09796665608882904, 'learning_rate': 1.4784473717366387e-05, 'epoch': 75.61538461538461}
>>> 2025-09-08 16:29:14,038 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.08581649512052536, 'learning_rate': 1.4228535577631442e-05, 'epoch': 76.0}
>>> 2025-09-08 16:29:17,417 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.13413956761360168, 'learning_rate': 1.3681512436768045e-05, 'epoch': 76.61538461538461}
>>> 2025-09-08 16:29:19,814 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.16372109949588776, 'learning_rate': 1.314354062441106e-05, 'epoch': 77.0}
>>> 2025-09-08 16:29:23,647 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.09637536108493805, 'learning_rate': 1.2614754214414548e-05, 'epoch': 77.61538461538461}
>>> 2025-09-08 16:29:25,603 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.1874656230211258, 'learning_rate': 1.2095284991437733e-05, 'epoch': 78.0}
>>> 2025-09-08 16:29:29,668 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.1906915307044983, 'learning_rate': 1.1585262418101467e-05, 'epoch': 78.61538461538461}
>>> 2025-09-08 16:29:31,637 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.22743846476078033, 'learning_rate': 1.1084813602723515e-05, 'epoch': 79.0}
>>> 2025-09-08 16:29:35,616 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.10974080115556717, 'learning_rate': 1.0594063267640386e-05, 'epoch': 79.61538461538461}
>>> 2025-09-08 16:29:37,523 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.08861440420150757, 'learning_rate': 1.0113133718124035e-05, 'epoch': 80.0}
>>> 2025-09-08 16:29:41,883 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.12773239612579346, 'learning_rate': 9.642144811900739e-06, 'epoch': 80.61538461538461}
>>> 2025-09-08 16:29:43,724 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.3255966305732727, 'learning_rate': 9.181213929280046e-06, 'epoch': 81.0}
>>> 2025-09-08 16:29:47,416 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.10275100916624069, 'learning_rate': 8.7304559439012e-06, 'epoch': 81.61538461538461}
>>> 2025-09-08 16:29:49,916 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.09079213440418243, 'learning_rate': 8.28998319410413e-06, 'epoch': 82.0}
>>> 2025-09-08 16:29:53,892 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.07524208724498749, 'learning_rate': 7.859905454932471e-06, 'epoch': 82.61538461538461}
>>> 2025-09-08 16:29:56,052 - INFO - >>> {'loss': 0.0059, 'grad_norm': 0.15729454159736633, 'learning_rate': 7.440329910775273e-06, 'epoch': 83.0}
>>> 2025-09-08 16:29:59,337 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.10733156651258469, 'learning_rate': 7.031361128654401e-06, 'epoch': 83.61538461538461}
>>> 2025-09-08 16:30:01,832 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.2509911060333252, 'learning_rate': 6.633101032164274e-06, 'epoch': 84.0}
>>> 2025-09-08 16:30:05,559 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.12934674322605133, 'learning_rate': 6.2456488760703205e-06, 'epoch': 84.61538461538461}
>>> 2025-09-08 16:30:07,775 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.218195378780365, 'learning_rate': 5.869101221572654e-06, 'epoch': 85.0}
>>> 2025-09-08 16:30:10,798 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.07542919367551804, 'learning_rate': 5.5035519122409895e-06, 'epoch': 85.61538461538461}
>>> 2025-09-08 16:30:13,409 - INFO - >>> {'loss': 0.0059, 'grad_norm': 0.12372343987226486, 'learning_rate': 5.149092050626825e-06, 'epoch': 86.0}
>>> 2025-09-08 16:30:17,198 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.10533229261636734, 'learning_rate': 4.805809975558828e-06, 'epoch': 86.61538461538461}
>>> 2025-09-08 16:30:19,345 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.09243912249803543, 'learning_rate': 4.47379124012689e-06, 'epoch': 87.0}
>>> 2025-09-08 16:30:23,130 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.13701534271240234, 'learning_rate': 4.153118590360561e-06, 'epoch': 87.61538461538461}
>>> 2025-09-08 16:30:25,053 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.2240765392780304, 'learning_rate': 3.843871944606969e-06, 'epoch': 88.0}
>>> 2025-09-08 16:30:28,428 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.13993334770202637, 'learning_rate': 3.5461283736134722e-06, 'epoch': 88.61538461538461}
>>> 2025-09-08 16:30:30,854 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.24244216084480286, 'learning_rate': 3.2599620813200837e-06, 'epoch': 89.0}
>>> 2025-09-08 16:30:34,468 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.11593177169561386, 'learning_rate': 2.9854443863662262e-06, 'epoch': 89.61538461538461}
>>> 2025-09-08 16:30:36,782 - INFO - >>> {'loss': 0.0059, 'grad_norm': 0.25645551085472107, 'learning_rate': 2.722643704316652e-06, 'epoch': 90.0}
>>> 2025-09-08 16:30:40,569 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.15906116366386414, 'learning_rate': 2.4716255306108605e-06, 'epoch': 90.61538461538461}
>>> 2025-09-08 16:30:42,643 - INFO - >>> {'loss': 0.0057, 'grad_norm': 0.17113271355628967, 'learning_rate': 2.2324524242402613e-06, 'epoch': 91.0}
>>> 2025-09-08 16:30:46,351 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.10835038125514984, 'learning_rate': 2.0051839921571448e-06, 'epoch': 91.61538461538461}
>>> 2025-09-08 16:30:48,598 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.08628121018409729, 'learning_rate': 1.7898768744194162e-06, 'epoch': 92.0}
>>> 2025-09-08 16:30:52,161 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.0791933462023735, 'learning_rate': 1.5865847300746417e-06, 'epoch': 92.61538461538461}
>>> 2025-09-08 16:30:54,711 - INFO - >>> {'loss': 0.0057, 'grad_norm': 0.11126277595758438, 'learning_rate': 1.3953582237871521e-06, 'epoch': 93.0}
>>> 2025-09-08 16:30:58,317 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.07739583402872086, 'learning_rate': 1.2162450132113201e-06, 'epoch': 93.61538461538461}
>>> 2025-09-08 16:31:00,371 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.15782691538333893, 'learning_rate': 1.049289737114273e-06, 'epoch': 94.0}
>>> 2025-09-08 16:31:03,966 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.15494364500045776, 'learning_rate': 8.945340042509797e-07, 'epoch': 94.61538461538461}
>>> 2025-09-08 16:31:06,392 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.16026021540164948, 'learning_rate': 7.520163829944804e-07, 'epoch': 95.0}
>>> 2025-09-08 16:31:10,188 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.09485828131437302, 'learning_rate': 6.217723917238128e-07, 'epoch': 95.61538461538461}
>>> 2025-09-08 16:31:12,289 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.0832587480545044, 'learning_rate': 5.038344899721436e-07, 'epoch': 96.0}
>>> 2025-09-08 16:31:16,006 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.08580049872398376, 'learning_rate': 3.9823207033710676e-07, 'epoch': 96.61538461538461}
>>> 2025-09-08 16:31:18,421 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.08346246927976608, 'learning_rate': 3.0499145115561176e-07, 'epoch': 97.0}
>>> 2025-09-08 16:31:21,747 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.09578610211610794, 'learning_rate': 2.2413586994470825e-07, 'epoch': 97.61538461538461}
>>> 2025-09-08 16:31:24,253 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.08284702897071838, 'learning_rate': 1.5568547761034004e-07, 'epoch': 98.0}
>>> 2025-09-08 16:31:27,721 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.13849914073944092, 'learning_rate': 9.965733342532924e-08, 'epoch': 98.61538461538461}
>>> 2025-09-08 16:31:29,894 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.17670349776744843, 'learning_rate': 5.606540077782163e-08, 'epoch': 99.0}
>>> 2025-09-08 16:31:33,286 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.1533728390932083, 'learning_rate': 2.4920543691309138e-08, 'epoch': 99.61538461538461}
>>> 2025-09-08 16:31:35,542 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.2253085970878601, 'learning_rate': 6.2305241171345395e-09, 'epoch': 100.0}
>>> 2025-09-08 16:31:36,198 - INFO - >>> {'train_runtime': 640.9002, 'train_samples_per_second': 15.603, 'train_steps_per_second': 0.312, 'train_loss': 0.5747399942332413, 'epoch': 100.0}
>>> 2025-09-08 16:31:36,200 - INFO - 训练成功！
>>> 2025-09-08 16:31:36,200 - INFO - 模型存放位置：./output/qwen2-0.5b202509081620
>>> 2025-09-08 16:31:54,821 - INFO - ========__main__  202509081631========
>>> 2025-09-08 16:31:54,822 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-08 16:31:54,823 - INFO - 开始进行模型测试
>>> 2025-09-08 16:31:58,153 - INFO - 已选择模型文件夹: qwen2-0.5b202509081620
>>> 2025-09-08 16:31:58,155 - INFO - 最新的 LoRA checkpoint 路径:output/qwen2-0.5b202509081620/checkpoint-200
>>> 2025-09-08 16:46:57,079 - INFO - 导入包完成
>>> 2025-09-08 16:46:57,080 - INFO - ========train Qwen2ForCausalLM  202509081646========
>>> 2025-09-08 16:46:57,080 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-08 16:46:57,081 - INFO - 开始进行训练
>>> 2025-09-08 16:46:57,086 - INFO - 基础配置文件读取完成
>>> 2025-09-08 16:46:57,094 - INFO - 训练配置读取完成
>>> 2025-09-08 16:46:57,094 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/own/Medical_20250816.json
>>> 2025-09-08 16:46:57,095 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-08 16:46:57,539 - INFO - tokenizer读取完成
>>> 2025-09-08 16:46:57,888 - INFO - model dtype:torch.float16
>>> 2025-09-08 16:46:57,888 - INFO - 模型导入完成
>>> 2025-09-08 16:46:57,889 - INFO - 数据读取开始
>>> 2025-09-08 16:46:58,926 - INFO - 数据下载完成
>>> 2025-09-08 16:47:03,155 - INFO - 数据映射完成
>>> 2025-09-08 16:47:03,156 - INFO - 打印训练参数如下
>>> 2025-09-08 16:47:03,157 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-08 16:47:03,157 - INFO -   dtype >>> torch.float16
>>> 2025-09-08 16:47:03,157 - INFO -   load_in_4bit >>> True
>>> 2025-09-08 16:47:03,158 - INFO -   batch_size >>> 8
>>> 2025-09-08 16:47:03,158 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-09-08 16:47:03,158 - INFO -   warmup_steps >>> 1
>>> 2025-09-08 16:47:03,159 - INFO -   epoch >>> 100
>>> 2025-09-08 16:47:03,159 - INFO -   eval_steps >>> 10
>>> 2025-09-08 16:47:03,159 - INFO -   learning_rate >>> 0.0001
>>> 2025-09-08 16:47:03,160 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-08 16:47:03,160 - INFO -   max_seq_length >>> 4096
>>> 2025-09-08 16:47:03,161 - INFO -   use_history >>> False
>>> 2025-09-08 16:47:03,161 - INFO -   r >>> 8
>>> 2025-09-08 16:47:03,161 - INFO -   interface_mode >>> False
>>> 2025-09-08 16:47:03,162 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
>>> 2025-09-08 16:47:03,162 - INFO -   lora_alpha >>> 16
>>> 2025-09-08 16:47:03,162 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-08 16:47:03,163 - INFO -   bias >>> none
>>> 2025-09-08 16:47:03,163 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-08 16:47:03,164 - INFO -   random_state >>> 3407
>>> 2025-09-08 16:47:03,164 - INFO -   use_rslora >>> True
>>> 2025-09-08 16:47:03,164 - INFO -   loftq_config >>> None
>>> 2025-09-08 16:47:04,700 - INFO - 开始训练！
>>> 2025-09-08 16:47:08,661 - INFO - >>> {'loss': 3.4046, 'grad_norm': 4.1806640625, 'learning_rate': 0.0, 'epoch': 0.6153846153846154}
>>> 2025-09-08 16:47:11,078 - INFO - >>> {'loss': 3.342, 'grad_norm': 3.689016103744507, 'learning_rate': 0.0001, 'epoch': 1.0}
>>> 2025-09-08 16:47:14,838 - INFO - >>> {'loss': 3.1754, 'grad_norm': 2.664807081222534, 'learning_rate': 9.999376947588288e-05, 'epoch': 1.6153846153846154}
>>> 2025-09-08 16:47:16,935 - INFO - >>> {'loss': 3.033, 'grad_norm': 2.09419584274292, 'learning_rate': 9.99750794563087e-05, 'epoch': 2.0}
>>> 2025-09-08 16:47:20,453 - INFO - >>> {'loss': 2.9823, 'grad_norm': 1.747077465057373, 'learning_rate': 9.994393459922218e-05, 'epoch': 2.6153846153846154}
>>> 2025-09-08 16:47:22,537 - INFO - >>> {'loss': 2.6858, 'grad_norm': 1.3790837526321411, 'learning_rate': 9.990034266657467e-05, 'epoch': 3.0}
>>> 2025-09-08 16:47:26,262 - INFO - >>> {'loss': 2.8127, 'grad_norm': 1.233031988143921, 'learning_rate': 9.984431452238967e-05, 'epoch': 3.6153846153846154}
>>> 2025-09-08 16:47:27,968 - INFO - >>> {'loss': 2.6028, 'grad_norm': 1.5079714059829712, 'learning_rate': 9.977586413005531e-05, 'epoch': 4.0}
>>> 2025-09-08 16:47:31,652 - INFO - >>> {'loss': 2.648, 'grad_norm': 1.1192511320114136, 'learning_rate': 9.96950085488444e-05, 'epoch': 4.615384615384615}
>>> 2025-09-08 16:47:33,666 - INFO - >>> {'loss': 2.7067, 'grad_norm': 1.2345824241638184, 'learning_rate': 9.960176792966289e-05, 'epoch': 5.0}
>>> 2025-09-08 16:47:37,073 - INFO - >>> {'loss': 2.564, 'grad_norm': 1.2465214729309082, 'learning_rate': 9.949616551002787e-05, 'epoch': 5.615384615384615}
>>> 2025-09-08 16:47:39,134 - INFO - >>> {'loss': 2.6131, 'grad_norm': 1.3307987451553345, 'learning_rate': 9.93782276082762e-05, 'epoch': 6.0}
>>> 2025-09-08 16:47:42,503 - INFO - >>> {'loss': 2.5251, 'grad_norm': 1.3679718971252441, 'learning_rate': 9.924798361700553e-05, 'epoch': 6.615384615384615}
>>> 2025-09-08 16:47:44,613 - INFO - >>> {'loss': 2.4546, 'grad_norm': 1.269324541091919, 'learning_rate': 9.910546599574902e-05, 'epoch': 7.0}
>>> 2025-09-08 16:47:48,086 - INFO - >>> {'loss': 2.4493, 'grad_norm': 1.1427001953125, 'learning_rate': 9.895071026288574e-05, 'epoch': 7.615384615384615}
>>> 2025-09-08 16:47:50,236 - INFO - >>> {'loss': 2.3426, 'grad_norm': 1.2745672464370728, 'learning_rate': 9.87837549867887e-05, 'epoch': 8.0}
>>> 2025-09-08 16:47:53,686 - INFO - >>> {'loss': 2.3462, 'grad_norm': 1.0675640106201172, 'learning_rate': 9.860464177621284e-05, 'epoch': 8.615384615384615}
>>> 2025-09-08 16:47:55,963 - INFO - >>> {'loss': 2.2689, 'grad_norm': 1.2229743003845215, 'learning_rate': 9.841341526992536e-05, 'epoch': 9.0}
>>> 2025-09-08 16:47:59,228 - INFO - >>> {'loss': 2.2467, 'grad_norm': 1.1227291822433472, 'learning_rate': 9.821012312558058e-05, 'epoch': 9.615384615384615}
>>> 2025-09-08 16:48:01,104 - INFO - >>> {'loss': 2.2417, 'grad_norm': 1.3817527294158936, 'learning_rate': 9.799481600784286e-05, 'epoch': 10.0}
>>> 2025-09-08 16:48:04,646 - INFO - >>> {'loss': 2.1695, 'grad_norm': 1.1162443161010742, 'learning_rate': 9.776754757575975e-05, 'epoch': 10.615384615384615}
>>> 2025-09-08 16:48:06,602 - INFO - >>> {'loss': 2.1703, 'grad_norm': 1.352489948272705, 'learning_rate': 9.752837446938915e-05, 'epoch': 11.0}
>>> 2025-09-08 16:48:10,259 - INFO - >>> {'loss': 2.1452, 'grad_norm': 1.1861296892166138, 'learning_rate': 9.727735629568336e-05, 'epoch': 11.615384615384615}
>>> 2025-09-08 16:48:12,236 - INFO - >>> {'loss': 2.0041, 'grad_norm': 1.338084101676941, 'learning_rate': 9.701455561363379e-05, 'epoch': 12.0}
>>> 2025-09-08 16:48:16,053 - INFO - >>> {'loss': 1.987, 'grad_norm': 1.1398215293884277, 'learning_rate': 9.674003791867991e-05, 'epoch': 12.615384615384615}
>>> 2025-09-08 16:48:17,926 - INFO - >>> {'loss': 2.0715, 'grad_norm': 1.3873618841171265, 'learning_rate': 9.645387162638652e-05, 'epoch': 13.0}
>>> 2025-09-08 16:48:21,159 - INFO - >>> {'loss': 1.9377, 'grad_norm': 1.1322705745697021, 'learning_rate': 9.615612805539305e-05, 'epoch': 13.615384615384615}
>>> 2025-09-08 16:48:23,426 - INFO - >>> {'loss': 1.9617, 'grad_norm': 1.4551572799682617, 'learning_rate': 9.584688140963944e-05, 'epoch': 14.0}
>>> 2025-09-08 16:48:26,754 - INFO - >>> {'loss': 1.9226, 'grad_norm': 1.1292736530303955, 'learning_rate': 9.552620875987311e-05, 'epoch': 14.615384615384615}
>>> 2025-09-08 16:48:28,734 - INFO - >>> {'loss': 1.771, 'grad_norm': 1.6121753454208374, 'learning_rate': 9.51941900244412e-05, 'epoch': 15.0}
>>> 2025-09-08 16:48:32,121 - INFO - >>> {'loss': 1.9055, 'grad_norm': 1.3021197319030762, 'learning_rate': 9.485090794937319e-05, 'epoch': 15.615384615384615}
>>> 2025-09-08 16:48:34,192 - INFO - >>> {'loss': 1.6648, 'grad_norm': 1.3342028856277466, 'learning_rate': 9.449644808775902e-05, 'epoch': 16.0}
>>> 2025-09-08 16:48:37,660 - INFO - >>> {'loss': 1.7545, 'grad_norm': 1.265695571899414, 'learning_rate': 9.413089877842736e-05, 'epoch': 16.615384615384617}
>>> 2025-09-08 16:48:39,809 - INFO - >>> {'loss': 1.6365, 'grad_norm': 1.6327897310256958, 'learning_rate': 9.375435112392969e-05, 'epoch': 17.0}
>>> 2025-09-08 16:48:43,590 - INFO - >>> {'loss': 1.6782, 'grad_norm': 1.1360583305358887, 'learning_rate': 9.336689896783573e-05, 'epoch': 17.615384615384617}
>>> 2025-09-08 16:48:45,429 - INFO - >>> {'loss': 1.5053, 'grad_norm': 1.7706531286239624, 'learning_rate': 9.29686388713456e-05, 'epoch': 18.0}
>>> 2025-09-08 16:48:48,876 - INFO - >>> {'loss': 1.5393, 'grad_norm': 1.240496039390564, 'learning_rate': 9.255967008922474e-05, 'epoch': 18.615384615384617}
>>> 2025-09-08 16:48:51,126 - INFO - >>> {'loss': 1.5399, 'grad_norm': 1.6432418823242188, 'learning_rate': 9.214009454506753e-05, 'epoch': 19.0}
>>> 2025-09-08 16:48:54,840 - INFO - >>> {'loss': 1.5411, 'grad_norm': 1.3086916208267212, 'learning_rate': 9.171001680589588e-05, 'epoch': 19.615384615384617}
>>> 2025-09-08 16:48:57,025 - INFO - >>> {'loss': 1.2946, 'grad_norm': 1.922285795211792, 'learning_rate': 9.126954405609882e-05, 'epoch': 20.0}
>>> 2025-09-08 16:49:01,340 - INFO - >>> {'loss': 1.3843, 'grad_norm': 1.3633884191513062, 'learning_rate': 9.081878607071996e-05, 'epoch': 20.615384615384617}
>>> 2025-09-08 16:49:03,709 - INFO - >>> {'loss': 1.3433, 'grad_norm': 1.811002492904663, 'learning_rate': 9.035785518809927e-05, 'epoch': 21.0}
>>> 2025-09-08 16:49:07,573 - INFO - >>> {'loss': 1.2569, 'grad_norm': 1.4016566276550293, 'learning_rate': 8.988686628187597e-05, 'epoch': 21.615384615384617}
>>> 2025-09-08 16:49:10,321 - INFO - >>> {'loss': 1.3081, 'grad_norm': 1.7536989450454712, 'learning_rate': 8.940593673235962e-05, 'epoch': 22.0}
>>> 2025-09-08 16:49:14,390 - INFO - >>> {'loss': 1.2677, 'grad_norm': 1.4287034273147583, 'learning_rate': 8.891518639727649e-05, 'epoch': 22.615384615384617}
>>> 2025-09-08 16:49:16,447 - INFO - >>> {'loss': 1.0734, 'grad_norm': 1.7823883295059204, 'learning_rate': 8.841473758189854e-05, 'epoch': 23.0}
>>> 2025-09-08 16:49:20,696 - INFO - >>> {'loss': 1.128, 'grad_norm': 1.4680492877960205, 'learning_rate': 8.790471500856228e-05, 'epoch': 23.615384615384617}
>>> 2025-09-08 16:49:22,877 - INFO - >>> {'loss': 1.0234, 'grad_norm': 2.242260694503784, 'learning_rate': 8.738524578558547e-05, 'epoch': 24.0}
>>> 2025-09-08 16:49:27,368 - INFO - >>> {'loss': 1.0258, 'grad_norm': 1.3577877283096313, 'learning_rate': 8.685645937558896e-05, 'epoch': 24.615384615384617}
>>> 2025-09-08 16:49:29,907 - INFO - >>> {'loss': 0.9401, 'grad_norm': 2.286707878112793, 'learning_rate': 8.631848756323197e-05, 'epoch': 25.0}
>>> 2025-09-08 16:49:33,856 - INFO - >>> {'loss': 0.8496, 'grad_norm': 1.4739514589309692, 'learning_rate': 8.577146442236857e-05, 'epoch': 25.615384615384617}
>>> 2025-09-08 16:49:36,858 - INFO - >>> {'loss': 0.9878, 'grad_norm': 2.112489700317383, 'learning_rate': 8.521552628263362e-05, 'epoch': 26.0}
>>> 2025-09-08 16:49:40,383 - INFO - >>> {'loss': 0.7484, 'grad_norm': 1.7402052879333496, 'learning_rate': 8.465081169546659e-05, 'epoch': 26.615384615384617}
>>> 2025-09-08 16:49:43,400 - INFO - >>> {'loss': 0.8715, 'grad_norm': 1.8903566598892212, 'learning_rate': 8.40774613995817e-05, 'epoch': 27.0}
>>> 2025-09-08 16:49:48,131 - INFO - >>> {'loss': 0.7435, 'grad_norm': 1.625299096107483, 'learning_rate': 8.349561828589277e-05, 'epoch': 27.615384615384617}
>>> 2025-09-08 16:49:50,238 - INFO - >>> {'loss': 0.657, 'grad_norm': 2.3589890003204346, 'learning_rate': 8.290542736190188e-05, 'epoch': 28.0}
>>> 2025-09-08 16:49:54,771 - INFO - >>> {'loss': 0.6515, 'grad_norm': 2.589667797088623, 'learning_rate': 8.230703571556048e-05, 'epoch': 28.615384615384617}
>>> 2025-09-08 16:49:57,389 - INFO - >>> {'loss': 0.5989, 'grad_norm': 2.034001111984253, 'learning_rate': 8.170059247861194e-05, 'epoch': 29.0}
>>> 2025-09-08 16:50:02,319 - INFO - >>> {'loss': 0.5773, 'grad_norm': 1.864859700202942, 'learning_rate': 8.108624878942477e-05, 'epoch': 29.615384615384617}
>>> 2025-09-08 16:50:04,325 - INFO - >>> {'loss': 0.5012, 'grad_norm': 2.20463228225708, 'learning_rate': 8.046415775532585e-05, 'epoch': 30.0}
>>> 2025-09-08 16:50:09,392 - INFO - >>> {'loss': 0.5077, 'grad_norm': 1.729616403579712, 'learning_rate': 7.983447441444281e-05, 'epoch': 30.615384615384617}
>>> 2025-09-08 16:50:11,791 - INFO - >>> {'loss': 0.425, 'grad_norm': 2.165405035018921, 'learning_rate': 7.919735569706533e-05, 'epoch': 31.0}
>>> 2025-09-08 16:50:15,854 - INFO - >>> {'loss': 0.36, 'grad_norm': 1.692306399345398, 'learning_rate': 7.855296038653475e-05, 'epoch': 31.615384615384617}
>>> 2025-09-08 16:50:18,656 - INFO - >>> {'loss': 0.4916, 'grad_norm': 2.2358546257019043, 'learning_rate': 7.790144907967201e-05, 'epoch': 32.0}
>>> 2025-09-08 16:50:23,178 - INFO - >>> {'loss': 0.3668, 'grad_norm': 1.5779180526733398, 'learning_rate': 7.724298414675353e-05, 'epoch': 32.61538461538461}
>>> 2025-09-08 16:50:25,817 - INFO - >>> {'loss': 0.3053, 'grad_norm': 2.4240338802337646, 'learning_rate': 7.657772969104508e-05, 'epoch': 33.0}
>>> 2025-09-08 16:50:29,790 - INFO - >>> {'loss': 0.3017, 'grad_norm': 1.3086881637573242, 'learning_rate': 7.590585150790389e-05, 'epoch': 33.61538461538461}
>>> 2025-09-08 16:50:32,302 - INFO - >>> {'loss': 0.2564, 'grad_norm': 1.940604567527771, 'learning_rate': 7.522751704345887e-05, 'epoch': 34.0}
>>> 2025-09-08 16:50:36,973 - INFO - >>> {'loss': 0.2422, 'grad_norm': 1.176567554473877, 'learning_rate': 7.454289535287968e-05, 'epoch': 34.61538461538461}
>>> 2025-09-08 16:50:39,385 - INFO - >>> {'loss': 0.2223, 'grad_norm': 2.643486738204956, 'learning_rate': 7.385215705824449e-05, 'epoch': 35.0}
>>> 2025-09-08 16:50:44,211 - INFO - >>> {'loss': 0.2079, 'grad_norm': 1.18243408203125, 'learning_rate': 7.31554743060174e-05, 'epoch': 35.61538461538461}
>>> 2025-09-08 16:50:46,659 - INFO - >>> {'loss': 0.1453, 'grad_norm': 1.4503333568572998, 'learning_rate': 7.245302072414601e-05, 'epoch': 36.0}
>>> 2025-09-08 16:50:50,903 - INFO - >>> {'loss': 0.1435, 'grad_norm': 1.1419897079467773, 'learning_rate': 7.174497137878966e-05, 'epoch': 36.61538461538461}
>>> 2025-09-08 16:50:53,893 - INFO - >>> {'loss': 0.1601, 'grad_norm': 1.375554084777832, 'learning_rate': 7.103150273068921e-05, 'epoch': 37.0}
>>> 2025-09-08 16:50:59,111 - INFO - >>> {'loss': 0.1374, 'grad_norm': 0.987358570098877, 'learning_rate': 7.031279259118946e-05, 'epoch': 37.61538461538461}
>>> 2025-09-08 16:51:01,285 - INFO - >>> {'loss': 0.0709, 'grad_norm': 1.3924275636672974, 'learning_rate': 6.958902007792466e-05, 'epoch': 38.0}
>>> 2025-09-08 16:51:05,557 - INFO - >>> {'loss': 0.0959, 'grad_norm': 0.992689847946167, 'learning_rate': 6.886036557017881e-05, 'epoch': 38.61538461538461}
>>> 2025-09-08 16:51:08,069 - INFO - >>> {'loss': 0.0901, 'grad_norm': 1.3211277723312378, 'learning_rate': 6.812701066393124e-05, 'epoch': 39.0}
>>> 2025-09-08 16:51:11,545 - INFO - >>> {'loss': 0.0838, 'grad_norm': 0.7579180598258972, 'learning_rate': 6.738913812659912e-05, 'epoch': 39.61538461538461}
>>> 2025-09-08 16:51:14,049 - INFO - >>> {'loss': 0.0633, 'grad_norm': 1.3896944522857666, 'learning_rate': 6.664693185148807e-05, 'epoch': 40.0}
>>> 2025-09-08 16:51:18,304 - INFO - >>> {'loss': 0.0642, 'grad_norm': 0.7217958569526672, 'learning_rate': 6.590057681196191e-05, 'epoch': 40.61538461538461}
>>> 2025-09-08 16:51:20,083 - INFO - >>> {'loss': 0.0436, 'grad_norm': 0.7120168209075928, 'learning_rate': 6.515025901534364e-05, 'epoch': 41.0}
>>> 2025-09-08 16:51:23,869 - INFO - >>> {'loss': 0.04, 'grad_norm': 1.0645266771316528, 'learning_rate': 6.439616545655834e-05, 'epoch': 41.61538461538461}
>>> 2025-09-08 16:51:26,248 - INFO - >>> {'loss': 0.0575, 'grad_norm': 0.9534891843795776, 'learning_rate': 6.363848407153016e-05, 'epoch': 42.0}
>>> 2025-09-08 16:51:29,721 - INFO - >>> {'loss': 0.0353, 'grad_norm': 0.6169402599334717, 'learning_rate': 6.287740369034485e-05, 'epoch': 42.61538461538461}
>>> 2025-09-08 16:51:32,109 - INFO - >>> {'loss': 0.0422, 'grad_norm': 0.5826928615570068, 'learning_rate': 6.211311399018916e-05, 'epoch': 43.0}
>>> 2025-09-08 16:51:35,752 - INFO - >>> {'loss': 0.023, 'grad_norm': 0.5104666352272034, 'learning_rate': 6.13458054480795e-05, 'epoch': 43.61538461538461}
>>> 2025-09-08 16:51:38,114 - INFO - >>> {'loss': 0.041, 'grad_norm': 0.6511726975440979, 'learning_rate': 6.0575669293390954e-05, 'epoch': 44.0}
>>> 2025-09-08 16:51:41,656 - INFO - >>> {'loss': 0.0267, 'grad_norm': 0.36999285221099854, 'learning_rate': 5.980289746019892e-05, 'epoch': 44.61538461538461}
>>> 2025-09-08 16:51:43,593 - INFO - >>> {'loss': 0.0231, 'grad_norm': 0.6120643019676208, 'learning_rate': 5.9027682539445104e-05, 'epoch': 45.0}
>>> 2025-09-08 16:51:46,982 - INFO - >>> {'loss': 0.0238, 'grad_norm': 0.39316606521606445, 'learning_rate': 5.8250217730939973e-05, 'epoch': 45.61538461538461}
>>> 2025-09-08 16:51:49,461 - INFO - >>> {'loss': 0.0168, 'grad_norm': 0.38123393058776855, 'learning_rate': 5.747069679521305e-05, 'epoch': 46.0}
>>> 2025-09-08 16:51:52,996 - INFO - >>> {'loss': 0.018, 'grad_norm': 0.27888619899749756, 'learning_rate': 5.668931400522396e-05, 'epoch': 46.61538461538461}
>>> 2025-09-08 16:51:55,320 - INFO - >>> {'loss': 0.0157, 'grad_norm': 0.3770868182182312, 'learning_rate': 5.5906264097945407e-05, 'epoch': 47.0}
>>> 2025-09-08 16:51:59,000 - INFO - >>> {'loss': 0.0134, 'grad_norm': 0.33581647276878357, 'learning_rate': 5.5121742225830665e-05, 'epoch': 47.61538461538461}
>>> 2025-09-08 16:52:01,404 - INFO - >>> {'loss': 0.0172, 'grad_norm': 0.36424607038497925, 'learning_rate': 5.433594390817756e-05, 'epoch': 48.0}
>>> 2025-09-08 16:52:05,084 - INFO - >>> {'loss': 0.0126, 'grad_norm': 0.2317061424255371, 'learning_rate': 5.35490649824008e-05, 'epoch': 48.61538461538461}
>>> 2025-09-08 16:52:07,190 - INFO - >>> {'loss': 0.0147, 'grad_norm': 0.683896005153656, 'learning_rate': 5.276130155522541e-05, 'epoch': 49.0}
>>> 2025-09-08 16:52:10,992 - INFO - >>> {'loss': 0.0124, 'grad_norm': 0.23927614092826843, 'learning_rate': 5.1972849953812644e-05, 'epoch': 49.61538461538461}
>>> 2025-09-08 16:52:12,975 - INFO - >>> {'loss': 0.0097, 'grad_norm': 0.4872860610485077, 'learning_rate': 5.1183906676831197e-05, 'epoch': 50.0}
>>> 2025-09-08 16:52:17,213 - INFO - >>> {'loss': 0.0102, 'grad_norm': 0.2501969635486603, 'learning_rate': 5.039466834548568e-05, 'epoch': 50.61538461538461}
>>> 2025-09-08 16:52:19,085 - INFO - >>> {'loss': 0.0114, 'grad_norm': 0.20593048632144928, 'learning_rate': 4.960533165451435e-05, 'epoch': 51.0}
>>> 2025-09-08 16:52:22,951 - INFO - >>> {'loss': 0.012, 'grad_norm': 0.34472861886024475, 'learning_rate': 4.8816093323168815e-05, 'epoch': 51.61538461538461}
>>> 2025-09-08 16:52:25,037 - INFO - >>> {'loss': 0.007, 'grad_norm': 0.21365474164485931, 'learning_rate': 4.802715004618737e-05, 'epoch': 52.0}
>>> 2025-09-08 16:52:28,633 - INFO - >>> {'loss': 0.0096, 'grad_norm': 0.23444287478923798, 'learning_rate': 4.7238698444774595e-05, 'epoch': 52.61538461538461}
>>> 2025-09-08 16:52:31,014 - INFO - >>> {'loss': 0.0081, 'grad_norm': 0.3824880123138428, 'learning_rate': 4.64509350175992e-05, 'epoch': 53.0}
>>> 2025-09-08 16:52:34,061 - INFO - >>> {'loss': 0.0081, 'grad_norm': 0.2052256017923355, 'learning_rate': 4.566405609182247e-05, 'epoch': 53.61538461538461}
>>> 2025-09-08 16:52:36,377 - INFO - >>> {'loss': 0.0096, 'grad_norm': 0.4833623766899109, 'learning_rate': 4.4878257774169346e-05, 'epoch': 54.0}
>>> 2025-09-08 16:52:39,657 - INFO - >>> {'loss': 0.0086, 'grad_norm': 0.29902777075767517, 'learning_rate': 4.4093735902054605e-05, 'epoch': 54.61538461538461}
>>> 2025-09-08 16:52:42,180 - INFO - >>> {'loss': 0.0074, 'grad_norm': 0.14733250439167023, 'learning_rate': 4.331068599477605e-05, 'epoch': 55.0}
>>> 2025-09-08 16:52:45,882 - INFO - >>> {'loss': 0.0062, 'grad_norm': 0.1623067408800125, 'learning_rate': 4.2529303204786953e-05, 'epoch': 55.61538461538461}
>>> 2025-09-08 16:52:48,050 - INFO - >>> {'loss': 0.0098, 'grad_norm': 0.35146045684814453, 'learning_rate': 4.1749782269060045e-05, 'epoch': 56.0}
>>> 2025-09-08 16:52:51,756 - INFO - >>> {'loss': 0.0068, 'grad_norm': 0.16635389626026154, 'learning_rate': 4.097231746055491e-05, 'epoch': 56.61538461538461}
>>> 2025-09-08 16:52:53,920 - INFO - >>> {'loss': 0.0079, 'grad_norm': 0.3508276343345642, 'learning_rate': 4.01971025398011e-05, 'epoch': 57.0}
>>> 2025-09-08 16:52:57,927 - INFO - >>> {'loss': 0.0071, 'grad_norm': 0.16160057485103607, 'learning_rate': 3.942433070660905e-05, 'epoch': 57.61538461538461}
>>> 2025-09-08 16:52:59,672 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.14515553414821625, 'learning_rate': 3.8654194551920485e-05, 'epoch': 58.0}
>>> 2025-09-08 16:53:03,343 - INFO - >>> {'loss': 0.0068, 'grad_norm': 0.22735469043254852, 'learning_rate': 3.788688600981085e-05, 'epoch': 58.61538461538461}
>>> 2025-09-08 16:53:05,648 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.1311519891023636, 'learning_rate': 3.712259630965518e-05, 'epoch': 59.0}
>>> 2025-09-08 16:53:09,393 - INFO - >>> {'loss': 0.0064, 'grad_norm': 0.20516368746757507, 'learning_rate': 3.636151592846985e-05, 'epoch': 59.61538461538461}
>>> 2025-09-08 16:53:11,476 - INFO - >>> {'loss': 0.0057, 'grad_norm': 0.1764199286699295, 'learning_rate': 3.560383454344168e-05, 'epoch': 60.0}
>>> 2025-09-08 16:53:14,903 - INFO - >>> {'loss': 0.0059, 'grad_norm': 0.14079873263835907, 'learning_rate': 3.484974098465636e-05, 'epoch': 60.61538461538461}
>>> 2025-09-08 16:53:17,658 - INFO - >>> {'loss': 0.0063, 'grad_norm': 0.26978281140327454, 'learning_rate': 3.409942318803809e-05, 'epoch': 61.0}
>>> 2025-09-08 16:53:21,431 - INFO - >>> {'loss': 0.0058, 'grad_norm': 0.17882192134857178, 'learning_rate': 3.335306814851196e-05, 'epoch': 61.61538461538461}
>>> 2025-09-08 16:53:23,628 - INFO - >>> {'loss': 0.0058, 'grad_norm': 0.1993488073348999, 'learning_rate': 3.261086187340088e-05, 'epoch': 62.0}
>>> 2025-09-08 16:53:26,886 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.10056696087121964, 'learning_rate': 3.187298933606878e-05, 'epoch': 62.61538461538461}
>>> 2025-09-08 16:53:29,090 - INFO - >>> {'loss': 0.0061, 'grad_norm': 0.23045220971107483, 'learning_rate': 3.11396344298212e-05, 'epoch': 63.0}
>>> 2025-09-08 16:53:33,235 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.18755857646465302, 'learning_rate': 3.0410979922075343e-05, 'epoch': 63.61538461538461}
>>> 2025-09-08 16:53:35,291 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.18387293815612793, 'learning_rate': 2.9687207408810557e-05, 'epoch': 64.0}
>>> 2025-09-08 16:53:38,790 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.12565717101097107, 'learning_rate': 2.8968497269310803e-05, 'epoch': 64.61538461538461}
>>> 2025-09-08 16:53:41,176 - INFO - >>> {'loss': 0.0061, 'grad_norm': 0.27409628033638, 'learning_rate': 2.8255028621210355e-05, 'epoch': 65.0}
>>> 2025-09-08 16:53:44,810 - INFO - >>> {'loss': 0.0057, 'grad_norm': 0.19104571640491486, 'learning_rate': 2.754697927585399e-05, 'epoch': 65.61538461538461}
>>> 2025-09-08 16:53:47,132 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.1369151771068573, 'learning_rate': 2.6844525693982613e-05, 'epoch': 66.0}
>>> 2025-09-08 16:53:50,563 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.11924781650304794, 'learning_rate': 2.614784294175554e-05, 'epoch': 66.61538461538461}
>>> 2025-09-08 16:53:53,240 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.30878758430480957, 'learning_rate': 2.5457104647120322e-05, 'epoch': 67.0}
>>> 2025-09-08 16:53:57,285 - INFO - >>> {'loss': 0.0045, 'grad_norm': 0.09661923348903656, 'learning_rate': 2.4772482956541132e-05, 'epoch': 67.61538461538461}
>>> 2025-09-08 16:53:59,649 - INFO - >>> {'loss': 0.0066, 'grad_norm': 0.239569753408432, 'learning_rate': 2.4094148492096125e-05, 'epoch': 68.0}
>>> 2025-09-08 16:54:03,899 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.14582005143165588, 'learning_rate': 2.3422270308954934e-05, 'epoch': 68.61538461538461}
>>> 2025-09-08 16:54:06,108 - INFO - >>> {'loss': 0.0061, 'grad_norm': 0.3427254855632782, 'learning_rate': 2.2757015853246493e-05, 'epoch': 69.0}
>>> 2025-09-08 16:54:10,176 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.13604967296123505, 'learning_rate': 2.2098550920327998e-05, 'epoch': 69.61538461538461}
>>> 2025-09-08 16:54:12,627 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.2367958426475525, 'learning_rate': 2.1447039613465265e-05, 'epoch': 70.0}
>>> 2025-09-08 16:54:16,658 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.10502971708774567, 'learning_rate': 2.0802644302934683e-05, 'epoch': 70.61538461538461}
>>> 2025-09-08 16:54:18,739 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.0948633924126625, 'learning_rate': 2.0165525585557204e-05, 'epoch': 71.0}
>>> 2025-09-08 16:54:22,098 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.2242784947156906, 'learning_rate': 1.953584224467418e-05, 'epoch': 71.61538461538461}
>>> 2025-09-08 16:54:24,921 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.1668519824743271, 'learning_rate': 1.8913751210575248e-05, 'epoch': 72.0}
>>> 2025-09-08 16:54:28,483 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.21039630472660065, 'learning_rate': 1.8299407521388067e-05, 'epoch': 72.61538461538461}
>>> 2025-09-08 16:54:31,260 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.1426885724067688, 'learning_rate': 1.7692964284439505e-05, 'epoch': 73.0}
>>> 2025-09-08 16:54:34,752 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.10221134126186371, 'learning_rate': 1.7094572638098123e-05, 'epoch': 73.61538461538461}
>>> 2025-09-08 16:54:37,464 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.3005731403827667, 'learning_rate': 1.6504381714107252e-05, 'epoch': 74.0}
>>> 2025-09-08 16:54:41,531 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.15079697966575623, 'learning_rate': 1.5922538600418318e-05, 'epoch': 74.61538461538461}
>>> 2025-09-08 16:54:43,598 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.180209681391716, 'learning_rate': 1.5349188304533413e-05, 'epoch': 75.0}
>>> 2025-09-08 16:54:47,399 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.09933224320411682, 'learning_rate': 1.4784473717366387e-05, 'epoch': 75.61538461538461}
>>> 2025-09-08 16:54:49,507 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.08268923312425613, 'learning_rate': 1.4228535577631442e-05, 'epoch': 76.0}
>>> 2025-09-08 16:54:53,358 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.12613944709300995, 'learning_rate': 1.3681512436768045e-05, 'epoch': 76.61538461538461}
>>> 2025-09-08 16:54:55,776 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.16861645877361298, 'learning_rate': 1.314354062441106e-05, 'epoch': 77.0}
>>> 2025-09-08 16:54:59,740 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.09099571406841278, 'learning_rate': 1.2614754214414548e-05, 'epoch': 77.61538461538461}
>>> 2025-09-08 16:55:01,830 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.13356418907642365, 'learning_rate': 1.2095284991437733e-05, 'epoch': 78.0}
>>> 2025-09-08 16:55:06,146 - INFO - >>> {'loss': 0.0045, 'grad_norm': 0.16654092073440552, 'learning_rate': 1.1585262418101467e-05, 'epoch': 78.61538461538461}
>>> 2025-09-08 16:55:08,209 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.22683776915073395, 'learning_rate': 1.1084813602723515e-05, 'epoch': 79.0}
>>> 2025-09-08 16:55:12,348 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.11552966386079788, 'learning_rate': 1.0594063267640386e-05, 'epoch': 79.61538461538461}
>>> 2025-09-08 16:55:14,384 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.0964985191822052, 'learning_rate': 1.0113133718124035e-05, 'epoch': 80.0}
>>> 2025-09-08 16:55:18,833 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.13395613431930542, 'learning_rate': 9.642144811900739e-06, 'epoch': 80.61538461538461}
>>> 2025-09-08 16:55:20,715 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.31691357493400574, 'learning_rate': 9.181213929280046e-06, 'epoch': 81.0}
>>> 2025-09-08 16:55:24,641 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.08990135788917542, 'learning_rate': 8.7304559439012e-06, 'epoch': 81.61538461538461}
>>> 2025-09-08 16:55:27,075 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.0925072431564331, 'learning_rate': 8.28998319410413e-06, 'epoch': 82.0}
>>> 2025-09-08 16:55:31,211 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.08137654513120651, 'learning_rate': 7.859905454932471e-06, 'epoch': 82.61538461538461}
>>> 2025-09-08 16:55:33,411 - INFO - >>> {'loss': 0.0063, 'grad_norm': 0.16738460958003998, 'learning_rate': 7.440329910775273e-06, 'epoch': 83.0}
>>> 2025-09-08 16:55:36,909 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.10383620113134384, 'learning_rate': 7.031361128654401e-06, 'epoch': 83.61538461538461}
>>> 2025-09-08 16:55:39,425 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.19542649388313293, 'learning_rate': 6.633101032164274e-06, 'epoch': 84.0}
>>> 2025-09-08 16:55:43,410 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.12522093951702118, 'learning_rate': 6.2456488760703205e-06, 'epoch': 84.61538461538461}
>>> 2025-09-08 16:55:45,815 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.24444667994976044, 'learning_rate': 5.869101221572654e-06, 'epoch': 85.0}
>>> 2025-09-08 16:55:48,899 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.07855995744466782, 'learning_rate': 5.5035519122409895e-06, 'epoch': 85.61538461538461}
>>> 2025-09-08 16:55:51,568 - INFO - >>> {'loss': 0.0058, 'grad_norm': 0.12588073313236237, 'learning_rate': 5.149092050626825e-06, 'epoch': 86.0}
>>> 2025-09-08 16:55:55,490 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.11174304783344269, 'learning_rate': 4.805809975558828e-06, 'epoch': 86.61538461538461}
>>> 2025-09-08 16:55:57,696 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.08210781216621399, 'learning_rate': 4.47379124012689e-06, 'epoch': 87.0}
>>> 2025-09-08 16:56:01,768 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.13377749919891357, 'learning_rate': 4.153118590360561e-06, 'epoch': 87.61538461538461}
>>> 2025-09-08 16:56:03,726 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.23226149380207062, 'learning_rate': 3.843871944606969e-06, 'epoch': 88.0}
>>> 2025-09-08 16:56:07,210 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.13662591576576233, 'learning_rate': 3.5461283736134722e-06, 'epoch': 88.61538461538461}
>>> 2025-09-08 16:56:09,775 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.22874735295772552, 'learning_rate': 3.2599620813200837e-06, 'epoch': 89.0}
>>> 2025-09-08 16:56:13,129 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.10901506245136261, 'learning_rate': 2.9854443863662262e-06, 'epoch': 89.61538461538461}
>>> 2025-09-08 16:56:15,705 - INFO - >>> {'loss': 0.0059, 'grad_norm': 0.29065176844596863, 'learning_rate': 2.722643704316652e-06, 'epoch': 90.0}
>>> 2025-09-08 16:56:19,672 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.15143537521362305, 'learning_rate': 2.4716255306108605e-06, 'epoch': 90.61538461538461}
>>> 2025-09-08 16:56:21,789 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.19093765318393707, 'learning_rate': 2.2324524242402613e-06, 'epoch': 91.0}
>>> 2025-09-08 16:56:25,682 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.09128782153129578, 'learning_rate': 2.0051839921571448e-06, 'epoch': 91.61538461538461}
>>> 2025-09-08 16:56:28,006 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.08292265981435776, 'learning_rate': 1.7898768744194162e-06, 'epoch': 92.0}
>>> 2025-09-08 16:56:31,534 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.08410163968801498, 'learning_rate': 1.5865847300746417e-06, 'epoch': 92.61538461538461}
>>> 2025-09-08 16:56:34,223 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.1184401735663414, 'learning_rate': 1.3953582237871521e-06, 'epoch': 93.0}
>>> 2025-09-08 16:56:37,838 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.07643922418355942, 'learning_rate': 1.2162450132113201e-06, 'epoch': 93.61538461538461}
>>> 2025-09-08 16:56:40,013 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.1482822448015213, 'learning_rate': 1.049289737114273e-06, 'epoch': 94.0}
>>> 2025-09-08 16:56:43,664 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.1485927850008011, 'learning_rate': 8.945340042509797e-07, 'epoch': 94.61538461538461}
>>> 2025-09-08 16:56:46,198 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.1842126101255417, 'learning_rate': 7.520163829944804e-07, 'epoch': 95.0}
>>> 2025-09-08 16:56:49,985 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.09270995855331421, 'learning_rate': 6.217723917238128e-07, 'epoch': 95.61538461538461}
>>> 2025-09-08 16:56:52,191 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.07729492336511612, 'learning_rate': 5.038344899721436e-07, 'epoch': 96.0}
>>> 2025-09-08 16:56:56,038 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.0809326246380806, 'learning_rate': 3.9823207033710676e-07, 'epoch': 96.61538461538461}
>>> 2025-09-08 16:56:58,528 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.08592183142900467, 'learning_rate': 3.0499145115561176e-07, 'epoch': 97.0}
>>> 2025-09-08 16:57:01,938 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.1068616509437561, 'learning_rate': 2.2413586994470825e-07, 'epoch': 97.61538461538461}
>>> 2025-09-08 16:57:04,503 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.08526107668876648, 'learning_rate': 1.5568547761034004e-07, 'epoch': 98.0}
>>> 2025-09-08 16:57:08,160 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.13715577125549316, 'learning_rate': 9.965733342532924e-08, 'epoch': 98.61538461538461}
>>> 2025-09-08 16:57:10,485 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.1631108969449997, 'learning_rate': 5.606540077782163e-08, 'epoch': 99.0}
>>> 2025-09-08 16:57:14,041 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.17347702383995056, 'learning_rate': 2.4920543691309138e-08, 'epoch': 99.61538461538461}
>>> 2025-09-08 16:57:16,321 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.2357671707868576, 'learning_rate': 6.2305241171345395e-09, 'epoch': 100.0}
>>> 2025-09-08 16:57:17,002 - INFO - >>> {'train_runtime': 611.9831, 'train_samples_per_second': 16.34, 'train_steps_per_second': 0.327, 'train_loss': 0.5717970017890912, 'epoch': 100.0}
>>> 2025-09-08 16:57:17,004 - INFO - 训练成功！
>>> 2025-09-08 16:57:17,004 - INFO - 模型存放位置：./output/qwen2-0.5b202509081647
>>> 2025-09-08 16:58:10,149 - INFO - ========__main__  202509081658========
>>> 2025-09-08 16:58:10,149 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-08 16:58:10,150 - INFO - 开始进行模型测试
>>> 2025-09-08 16:58:13,597 - INFO - 已选择模型文件夹: qwen2-0.5b202509081647
>>> 2025-09-08 16:58:13,600 - INFO - 最新的 LoRA checkpoint 路径:output/qwen2-0.5b202509081647/checkpoint-200
>>> 2025-09-10 17:29:55,623 - INFO - 导入包完成
>>> 2025-09-10 17:29:55,623 - INFO - ========train Qwen2ForCausalLM  202509101729========
>>> 2025-09-10 17:29:55,624 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-10 17:29:55,624 - INFO - 开始进行训练
>>> 2025-09-10 17:29:55,629 - INFO - 基础配置文件读取完成
>>> 2025-09-10 17:29:55,637 - INFO - 训练配置读取完成
>>> 2025-09-10 17:29:55,638 - INFO - 已启用验证数据集
>>> 2025-09-10 17:29:55,638 - INFO - 数据集路径：dataset/PsyDTCorpus/PsyDTCorpus_train_single_turn_split.json
>>> 2025-09-10 17:29:55,638 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-10 17:29:56,087 - INFO - tokenizer读取完成
>>> 2025-09-10 17:29:56,405 - INFO - model dtype:torch.float16
>>> 2025-09-10 17:29:56,405 - INFO - 模型导入完成
>>> 2025-09-10 17:29:56,405 - INFO - 数据读取开始
>>> 2025-09-10 17:31:39,281 - INFO - 导入包完成
>>> 2025-09-10 17:31:39,281 - INFO - ========train Qwen2ForCausalLM  202509101731========
>>> 2025-09-10 17:31:39,282 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-10 17:31:39,282 - INFO - 开始进行训练
>>> 2025-09-10 17:31:39,287 - INFO - 基础配置文件读取完成
>>> 2025-09-10 17:31:39,295 - INFO - 训练配置读取完成
>>> 2025-09-10 17:31:39,296 - INFO - 已启用验证数据集
>>> 2025-09-10 17:31:39,296 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-09-10 17:31:39,297 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-10 17:31:39,736 - INFO - tokenizer读取完成
>>> 2025-09-10 17:31:40,051 - INFO - model dtype:torch.float16
>>> 2025-09-10 17:31:40,051 - INFO - 模型导入完成
>>> 2025-09-10 17:31:40,051 - INFO - 数据读取开始
>>> 2025-09-10 17:32:28,861 - INFO - 导入包完成
>>> 2025-09-10 17:32:28,862 - INFO - ========train Qwen2ForCausalLM  202509101732========
>>> 2025-09-10 17:32:28,863 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-10 17:32:28,863 - INFO - 开始进行训练
>>> 2025-09-10 17:32:28,879 - INFO - 基础配置文件读取完成
>>> 2025-09-10 17:32:28,906 - INFO - 训练配置读取完成
>>> 2025-09-10 17:32:28,906 - INFO - 已启用验证数据集
>>> 2025-09-10 17:32:28,907 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-09-10 17:32:28,908 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-10 17:32:29,324 - INFO - tokenizer读取完成
>>> 2025-09-10 17:32:29,821 - INFO - model dtype:torch.float16
>>> 2025-09-10 17:32:29,822 - INFO - 模型导入完成
>>> 2025-09-10 17:32:29,822 - INFO - 数据读取开始
>>> 2025-09-10 17:33:42,493 - INFO - 导入包完成
>>> 2025-09-10 17:33:42,494 - INFO - ========train Qwen2ForCausalLM  202509101733========
>>> 2025-09-10 17:33:42,495 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-10 17:33:42,495 - INFO - 开始进行训练
>>> 2025-09-10 17:33:42,511 - INFO - 基础配置文件读取完成
>>> 2025-09-10 17:33:42,538 - INFO - 训练配置读取完成
>>> 2025-09-10 17:33:42,538 - INFO - 已启用验证数据集
>>> 2025-09-10 17:33:42,539 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-09-10 17:33:42,539 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-10 17:33:42,971 - INFO - tokenizer读取完成
>>> 2025-09-10 17:33:43,404 - INFO - model dtype:torch.float16
>>> 2025-09-10 17:33:43,406 - INFO - 模型导入完成
>>> 2025-09-10 17:33:43,406 - INFO - 数据读取开始
>>> 2025-09-10 17:33:44,249 - INFO - 数据下载完成，训练集大小: 4760
>>> 2025-09-10 17:34:48,515 - INFO - 导入包完成
>>> 2025-09-10 17:34:48,516 - INFO - ========train Qwen2ForCausalLM  202509101734========
>>> 2025-09-10 17:34:48,516 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-10 17:34:48,517 - INFO - 开始进行训练
>>> 2025-09-10 17:34:48,532 - INFO - 基础配置文件读取完成
>>> 2025-09-10 17:34:48,559 - INFO - 训练配置读取完成
>>> 2025-09-10 17:34:48,560 - INFO - 已启用验证数据集
>>> 2025-09-10 17:34:48,560 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-09-10 17:34:48,561 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-10 17:34:48,981 - INFO - tokenizer读取完成
>>> 2025-09-10 17:34:49,375 - INFO - model dtype:torch.float16
>>> 2025-09-10 17:34:49,377 - INFO - 模型导入完成
>>> 2025-09-10 17:34:49,377 - INFO - 数据读取开始
>>> 2025-09-10 17:34:50,233 - INFO - 数据下载完成，训练集大小: 2380
>>> 2025-09-10 17:42:22,256 - INFO - 数据映射完成
>>> 2025-09-10 17:42:23,017 - INFO - 数据下载完成，训练集大小: 4760
>>> 2025-09-10 17:48:12,337 - INFO - 导入包完成
>>> 2025-09-10 17:48:12,337 - INFO - ========train Qwen2ForCausalLM  202509101748========
>>> 2025-09-10 17:48:12,338 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-10 17:48:12,338 - INFO - 开始进行训练
>>> 2025-09-10 17:48:12,343 - INFO - 基础配置文件读取完成
>>> 2025-09-10 17:48:12,351 - INFO - 训练配置读取完成
>>> 2025-09-10 17:48:12,352 - INFO - 已启用验证数据集
>>> 2025-09-10 17:48:12,352 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-09-10 17:48:12,353 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-10 17:48:12,800 - INFO - tokenizer读取完成
>>> 2025-09-10 17:48:13,105 - INFO - model dtype:torch.float16
>>> 2025-09-10 17:48:13,105 - INFO - 模型导入完成
>>> 2025-09-10 17:48:13,106 - INFO - 数据读取开始
>>> 2025-09-10 17:48:13,866 - INFO - 数据下载完成，训练集大小: 2380
>>> 2025-09-10 17:48:17,959 - INFO - 数据映射完成
>>> 2025-09-10 17:48:18,220 - INFO - 数据下载完成，训练集大小: 2380
>>> 2025-09-10 17:48:22,361 - INFO - 数据映射完成
>>> 2025-09-10 17:48:22,362 - INFO - 验证数据集处理完成
>>> 2025-09-10 17:48:22,362 - INFO - 打印训练参数如下
>>> 2025-09-10 17:48:22,362 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-10 17:48:22,363 - INFO -   dtype >>> torch.float16
>>> 2025-09-10 17:48:22,363 - INFO -   load_in_4bit >>> True
>>> 2025-09-10 17:48:22,363 - INFO -   batch_size >>> 8
>>> 2025-09-10 17:48:22,364 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-09-10 17:48:22,364 - INFO -   warmup_steps >>> 1
>>> 2025-09-10 17:48:22,364 - INFO -   epoch >>> 70
>>> 2025-09-10 17:48:22,365 - INFO -   eval_steps >>> 10
>>> 2025-09-10 17:48:22,365 - INFO -   learning_rate >>> 0.0001
>>> 2025-09-10 17:48:22,365 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-10 17:48:22,366 - INFO -   max_seq_length >>> 4096
>>> 2025-09-10 17:48:22,366 - INFO -   use_history >>> False
>>> 2025-09-10 17:48:22,366 - INFO -   r >>> 8
>>> 2025-09-10 17:48:22,367 - INFO -   interface_mode >>> False
>>> 2025-09-10 17:48:22,367 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
>>> 2025-09-10 17:48:22,367 - INFO -   lora_alpha >>> 16
>>> 2025-09-10 17:48:22,368 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-10 17:48:22,368 - INFO -   bias >>> none
>>> 2025-09-10 17:48:22,368 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-10 17:48:22,369 - INFO -   random_state >>> 3407
>>> 2025-09-10 17:48:22,369 - INFO -   use_rslora >>> True
>>> 2025-09-10 17:48:22,369 - INFO -   loftq_config >>> None
>>> 2025-09-10 17:48:23,845 - INFO - 开始训练！
>>> 2025-09-10 17:48:31,639 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 11.16 GiB. GPU 0 has a total capacity of 31.73 GiB of which 9.75 GiB is free. Including non-PyTorch memory, this process has 21.97 GiB memory in use. Of the allocated memory 18.65 GiB is allocated by PyTorch, and 2.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-09-10 17:49:19,418 - INFO - 导入包完成
>>> 2025-09-10 17:49:19,419 - INFO - ========train Qwen2ForCausalLM  202509101749========
>>> 2025-09-10 17:49:19,419 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-10 17:49:19,420 - INFO - 开始进行训练
>>> 2025-09-10 17:49:19,425 - INFO - 基础配置文件读取完成
>>> 2025-09-10 17:49:19,433 - INFO - 训练配置读取完成
>>> 2025-09-10 17:49:19,433 - INFO - 已启用验证数据集
>>> 2025-09-10 17:49:19,434 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-09-10 17:49:19,434 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-10 17:49:19,874 - INFO - tokenizer读取完成
>>> 2025-09-10 17:49:20,161 - INFO - model dtype:torch.float16
>>> 2025-09-10 17:49:20,161 - INFO - 模型导入完成
>>> 2025-09-10 17:49:20,162 - INFO - 数据读取开始
>>> 2025-09-10 17:49:21,787 - INFO - 数据下载完成，训练集大小: 952
>>> 2025-09-10 17:50:35,540 - INFO - 数据映射完成
>>> 2025-09-10 17:50:36,307 - INFO - 数据下载完成，训练集大小: 952
>>> 2025-09-10 17:50:40,494 - INFO - 数据映射完成
>>> 2025-09-10 17:50:40,495 - INFO - 验证数据集处理完成
>>> 2025-09-10 17:50:40,495 - INFO - 打印训练参数如下
>>> 2025-09-10 17:50:40,495 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-10 17:50:40,496 - INFO -   dtype >>> torch.float16
>>> 2025-09-10 17:50:40,496 - INFO -   load_in_4bit >>> True
>>> 2025-09-10 17:50:40,497 - INFO -   batch_size >>> 8
>>> 2025-09-10 17:50:40,497 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-09-10 17:50:40,497 - INFO -   warmup_steps >>> 1
>>> 2025-09-10 17:50:40,498 - INFO -   epoch >>> 70
>>> 2025-09-10 17:50:40,498 - INFO -   eval_steps >>> 10
>>> 2025-09-10 17:50:40,498 - INFO -   learning_rate >>> 0.0001
>>> 2025-09-10 17:50:40,499 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-10 17:50:40,499 - INFO -   max_seq_length >>> 4096
>>> 2025-09-10 17:50:40,499 - INFO -   use_history >>> False
>>> 2025-09-10 17:50:40,500 - INFO -   r >>> 8
>>> 2025-09-10 17:50:40,500 - INFO -   interface_mode >>> False
>>> 2025-09-10 17:50:40,500 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
>>> 2025-09-10 17:50:40,501 - INFO -   lora_alpha >>> 16
>>> 2025-09-10 17:50:40,501 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-10 17:50:40,502 - INFO -   bias >>> none
>>> 2025-09-10 17:50:40,502 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-10 17:50:40,502 - INFO -   random_state >>> 3407
>>> 2025-09-10 17:50:40,503 - INFO -   use_rslora >>> True
>>> 2025-09-10 17:50:40,503 - INFO -   loftq_config >>> None
>>> 2025-09-10 17:50:42,036 - INFO - 开始训练！
>>> 2025-09-10 17:51:01,872 - INFO - >>> {'loss': 2.4603, 'grad_norm': 2.2736101150512695, 'learning_rate': 0.0, 'epoch': 0.06722689075630252}
>>> 2025-09-10 17:51:20,565 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 9.97 GiB. GPU 0 has a total capacity of 31.73 GiB of which 682.19 MiB is free. Including non-PyTorch memory, this process has 31.06 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 8.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-09-10 17:51:48,277 - INFO - 导入包完成
>>> 2025-09-10 17:51:48,277 - INFO - ========train Qwen2ForCausalLM  202509101751========
>>> 2025-09-10 17:51:48,278 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-10 17:51:48,278 - INFO - 开始进行训练
>>> 2025-09-10 17:51:48,283 - INFO - 基础配置文件读取完成
>>> 2025-09-10 17:51:48,291 - INFO - 训练配置读取完成
>>> 2025-09-10 17:51:48,292 - INFO - 已启用验证数据集
>>> 2025-09-10 17:51:48,292 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-09-10 17:51:48,293 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-10 17:51:48,743 - INFO - tokenizer读取完成
>>> 2025-09-10 17:51:48,882 - INFO - model dtype:torch.bfloat16
>>> 2025-09-10 17:51:48,883 - INFO - 模型导入完成
>>> 2025-09-10 17:51:48,883 - INFO - 数据读取开始
>>> 2025-09-10 17:51:49,699 - INFO - 数据下载完成，训练集大小: 952
>>> 2025-09-10 17:51:53,820 - INFO - 数据映射完成
>>> 2025-09-10 17:51:54,093 - INFO - 数据下载完成，训练集大小: 952
>>> 2025-09-10 17:51:58,281 - INFO - 数据映射完成
>>> 2025-09-10 17:51:58,282 - INFO - 验证数据集处理完成
>>> 2025-09-10 17:51:58,282 - INFO - 打印训练参数如下
>>> 2025-09-10 17:51:58,283 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-10 17:51:58,283 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-10 17:51:58,283 - INFO -   load_in_4bit >>> True
>>> 2025-09-10 17:51:58,284 - INFO -   batch_size >>> 8
>>> 2025-09-10 17:51:58,284 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-09-10 17:51:58,285 - INFO -   warmup_steps >>> 1
>>> 2025-09-10 17:51:58,285 - INFO -   epoch >>> 70
>>> 2025-09-10 17:51:58,285 - INFO -   eval_steps >>> 10
>>> 2025-09-10 17:51:58,285 - INFO -   learning_rate >>> 0.0001
>>> 2025-09-10 17:51:58,286 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-10 17:51:58,286 - INFO -   max_seq_length >>> 4096
>>> 2025-09-10 17:51:58,287 - INFO -   use_history >>> False
>>> 2025-09-10 17:51:58,287 - INFO -   r >>> 8
>>> 2025-09-10 17:51:58,287 - INFO -   interface_mode >>> False
>>> 2025-09-10 17:51:58,287 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
>>> 2025-09-10 17:51:58,288 - INFO -   lora_alpha >>> 16
>>> 2025-09-10 17:51:58,288 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-10 17:51:58,289 - INFO -   bias >>> none
>>> 2025-09-10 17:51:58,289 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-10 17:51:58,289 - INFO -   random_state >>> 3407
>>> 2025-09-10 17:51:58,290 - INFO -   use_rslora >>> True
>>> 2025-09-10 17:51:58,290 - INFO -   loftq_config >>> None
>>> 2025-09-10 17:51:59,404 - INFO - 开始训练！
>>> 2025-09-10 17:52:09,846 - INFO - 导入包完成
>>> 2025-09-10 17:52:09,846 - INFO - ========train Qwen2ForCausalLM  202509101752========
>>> 2025-09-10 17:52:09,847 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-10 17:52:09,847 - INFO - 开始进行训练
>>> 2025-09-10 17:52:09,852 - INFO - 基础配置文件读取完成
>>> 2025-09-10 17:52:09,860 - INFO - 训练配置读取完成
>>> 2025-09-10 17:52:09,860 - INFO - 已启用验证数据集
>>> 2025-09-10 17:52:09,861 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-09-10 17:52:09,861 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-10 17:52:10,305 - INFO - tokenizer读取完成
>>> 2025-09-10 17:52:10,444 - INFO - model dtype:torch.bfloat16
>>> 2025-09-10 17:52:10,444 - INFO - 模型导入完成
>>> 2025-09-10 17:52:10,444 - INFO - 数据读取开始
>>> 2025-09-10 17:52:11,250 - INFO - 数据下载完成，训练集大小: 952
>>> 2025-09-10 17:52:15,386 - INFO - 数据映射完成
>>> 2025-09-10 17:52:15,643 - INFO - 数据下载完成，训练集大小: 952
>>> 2025-09-10 17:52:19,785 - INFO - 数据映射完成
>>> 2025-09-10 17:52:19,785 - INFO - 验证数据集处理完成
>>> 2025-09-10 17:52:19,786 - INFO - 打印训练参数如下
>>> 2025-09-10 17:52:19,786 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-10 17:52:19,787 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-10 17:52:19,787 - INFO -   load_in_4bit >>> True
>>> 2025-09-10 17:52:19,787 - INFO -   batch_size >>> 8
>>> 2025-09-10 17:52:19,788 - INFO -   gradient_accumulator_steps >>> 8
>>> 2025-09-10 17:52:19,788 - INFO -   warmup_steps >>> 1
>>> 2025-09-10 17:52:19,788 - INFO -   epoch >>> 70
>>> 2025-09-10 17:52:19,789 - INFO -   eval_steps >>> 10
>>> 2025-09-10 17:52:19,789 - INFO -   learning_rate >>> 0.0001
>>> 2025-09-10 17:52:19,790 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-10 17:52:19,790 - INFO -   max_seq_length >>> 4096
>>> 2025-09-10 17:52:19,790 - INFO -   use_history >>> False
>>> 2025-09-10 17:52:19,791 - INFO -   r >>> 8
>>> 2025-09-10 17:52:19,791 - INFO -   interface_mode >>> False
>>> 2025-09-10 17:52:19,791 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-09-10 17:52:19,792 - INFO -   lora_alpha >>> 16
>>> 2025-09-10 17:52:19,792 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-10 17:52:19,792 - INFO -   bias >>> none
>>> 2025-09-10 17:52:19,793 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-10 17:52:19,793 - INFO -   random_state >>> 3407
>>> 2025-09-10 17:52:19,793 - INFO -   use_rslora >>> True
>>> 2025-09-10 17:52:19,794 - INFO -   loftq_config >>> None
>>> 2025-09-10 17:52:20,544 - INFO - 开始训练！
>>> 2025-09-10 17:53:17,804 - INFO - >>> {'loss': 2.461, 'grad_norm': 0.6031935811042786, 'learning_rate': 0.0, 'epoch': 0.06722689075630252}
>>> 2025-09-10 17:54:27,470 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 9.97 GiB. GPU 0 has a total capacity of 31.73 GiB of which 612.19 MiB is free. Including non-PyTorch memory, this process has 31.13 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 9.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-09-10 21:24:21,817 - INFO - 导入包完成
>>> 2025-09-10 21:24:21,818 - INFO - ========train Qwen2ForCausalLM  202509102124========
>>> 2025-09-10 21:24:21,818 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-10 21:24:21,819 - INFO - 开始进行训练
>>> 2025-09-10 21:24:21,824 - INFO - 基础配置文件读取完成
>>> 2025-09-10 21:24:21,832 - INFO - 训练配置读取完成
>>> 2025-09-10 21:24:21,832 - INFO - 已启用验证数据集
>>> 2025-09-10 21:24:21,832 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-09-10 21:24:21,833 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-10 21:24:22,277 - INFO - tokenizer读取完成
>>> 2025-09-10 21:24:22,416 - INFO - model dtype:torch.bfloat16
>>> 2025-09-10 21:24:22,416 - INFO - 模型导入完成
>>> 2025-09-10 21:24:22,417 - INFO - 数据读取开始
>>> 2025-09-10 21:24:23,494 - INFO - 数据下载完成，训练集大小: 952
>>> 2025-09-10 21:25:36,178 - INFO - 数据映射完成
>>> 2025-09-10 21:25:36,958 - INFO - 数据下载完成，训练集大小: 952
>>> 2025-09-10 21:25:41,164 - INFO - 数据映射完成
>>> 2025-09-10 21:25:41,165 - INFO - 验证数据集处理完成
>>> 2025-09-10 21:25:41,166 - INFO - 打印训练参数如下
>>> 2025-09-10 21:25:41,166 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-10 21:25:41,166 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-10 21:25:41,167 - INFO -   load_in_4bit >>> True
>>> 2025-09-10 21:25:41,167 - INFO -   batch_size >>> 4
>>> 2025-09-10 21:25:41,167 - INFO -   gradient_accumulator_steps >>> 1
>>> 2025-09-10 21:25:41,168 - INFO -   warmup_steps >>> 1
>>> 2025-09-10 21:25:41,168 - INFO -   epoch >>> 50
>>> 2025-09-10 21:25:41,168 - INFO -   eval_steps >>> 10
>>> 2025-09-10 21:25:41,169 - INFO -   learning_rate >>> 0.0001
>>> 2025-09-10 21:25:41,169 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-10 21:25:41,169 - INFO -   max_seq_length >>> 2048
>>> 2025-09-10 21:25:41,170 - INFO -   use_history >>> False
>>> 2025-09-10 21:25:41,170 - INFO -   r >>> 8
>>> 2025-09-10 21:25:41,170 - INFO -   interface_mode >>> False
>>> 2025-09-10 21:25:41,171 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-09-10 21:25:41,171 - INFO -   lora_alpha >>> 16
>>> 2025-09-10 21:25:41,172 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-10 21:25:41,172 - INFO -   bias >>> none
>>> 2025-09-10 21:25:41,172 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-10 21:25:41,173 - INFO -   random_state >>> 3407
>>> 2025-09-10 21:25:41,173 - INFO -   use_rslora >>> True
>>> 2025-09-10 21:25:41,173 - INFO -   loftq_config >>> None
>>> 2025-09-10 21:25:41,971 - INFO - 开始训练！
>>> 2025-09-10 21:25:46,484 - INFO - >>> {'loss': 2.4287, 'grad_norm': 0.5875168442726135, 'learning_rate': 0.0, 'epoch': 0.004201680672268907}
>>> 2025-09-10 21:25:50,200 - INFO - >>> {'loss': 2.4574, 'grad_norm': 0.6346808075904846, 'learning_rate': 0.0001, 'epoch': 0.008403361344537815}
>>> 2025-09-10 21:25:53,546 - INFO - >>> {'loss': 2.4789, 'grad_norm': 0.6567534804344177, 'learning_rate': 9.999999825731529e-05, 'epoch': 0.012605042016806723}
>>> 2025-09-10 21:25:56,715 - INFO - >>> {'loss': 2.3668, 'grad_norm': 0.6305710673332214, 'learning_rate': 9.999999302926127e-05, 'epoch': 0.01680672268907563}
>>> 2025-09-10 21:26:00,194 - INFO - >>> {'loss': 2.3463, 'grad_norm': 0.6110877394676208, 'learning_rate': 9.999998431583829e-05, 'epoch': 0.02100840336134454}
>>> 2025-09-10 21:26:03,893 - INFO - >>> {'loss': 2.3966, 'grad_norm': 0.5689691305160522, 'learning_rate': 9.999997211704696e-05, 'epoch': 0.025210084033613446}
>>> 2025-09-10 21:26:06,924 - INFO - >>> {'loss': 2.4722, 'grad_norm': 0.6673633456230164, 'learning_rate': 9.999995643288816e-05, 'epoch': 0.029411764705882353}
>>> 2025-09-10 21:26:10,234 - INFO - >>> {'loss': 2.4689, 'grad_norm': 0.5508779883384705, 'learning_rate': 9.999993726336295e-05, 'epoch': 0.03361344537815126}
>>> 2025-09-10 21:26:13,535 - INFO - >>> {'loss': 2.4641, 'grad_norm': 0.5623907446861267, 'learning_rate': 9.999991460847269e-05, 'epoch': 0.037815126050420166}
>>> 2025-09-10 21:26:16,332 - INFO - >>> {'loss': 2.3615, 'grad_norm': 0.5382528305053711, 'learning_rate': 9.999988846821895e-05, 'epoch': 0.04201680672268908}
>>> 2025-09-10 21:26:19,787 - INFO - >>> {'loss': 2.4394, 'grad_norm': 0.5059790015220642, 'learning_rate': 9.999985884260355e-05, 'epoch': 0.046218487394957986}
>>> 2025-09-10 21:26:23,171 - INFO - >>> {'loss': 2.4329, 'grad_norm': 0.51161789894104, 'learning_rate': 9.999982573162854e-05, 'epoch': 0.05042016806722689}
>>> 2025-09-10 21:26:26,185 - INFO - >>> {'loss': 2.4148, 'grad_norm': 0.5359053015708923, 'learning_rate': 9.999978913529627e-05, 'epoch': 0.0546218487394958}
>>> 2025-09-10 21:26:29,708 - INFO - >>> {'loss': 2.4923, 'grad_norm': 0.526963472366333, 'learning_rate': 9.999974905360925e-05, 'epoch': 0.058823529411764705}
>>> 2025-09-10 21:26:33,098 - INFO - >>> {'loss': 2.4489, 'grad_norm': 0.472150981426239, 'learning_rate': 9.999970548657029e-05, 'epoch': 0.06302521008403361}
>>> 2025-09-10 21:26:36,369 - INFO - >>> {'loss': 2.3518, 'grad_norm': 0.5188753604888916, 'learning_rate': 9.999965843418242e-05, 'epoch': 0.06722689075630252}
>>> 2025-09-10 21:26:39,440 - INFO - >>> {'loss': 2.3858, 'grad_norm': 0.5330212712287903, 'learning_rate': 9.999960789644893e-05, 'epoch': 0.07142857142857142}
>>> 2025-09-10 21:26:43,183 - INFO - >>> {'loss': 2.4251, 'grad_norm': 0.4898853600025177, 'learning_rate': 9.999955387337334e-05, 'epoch': 0.07563025210084033}
>>> 2025-09-10 21:26:47,028 - INFO - >>> {'loss': 2.3657, 'grad_norm': 0.4699532687664032, 'learning_rate': 9.999949636495942e-05, 'epoch': 0.07983193277310924}
>>> 2025-09-10 21:26:50,151 - INFO - >>> {'loss': 2.4405, 'grad_norm': 0.6001156568527222, 'learning_rate': 9.999943537121117e-05, 'epoch': 0.08403361344537816}
>>> 2025-09-10 21:26:53,314 - INFO - >>> {'loss': 2.3374, 'grad_norm': 0.5432220101356506, 'learning_rate': 9.999937089213285e-05, 'epoch': 0.08823529411764706}
>>> 2025-09-10 21:26:57,190 - INFO - >>> {'loss': 2.5151, 'grad_norm': 0.5140359997749329, 'learning_rate': 9.999930292772894e-05, 'epoch': 0.09243697478991597}
>>> 2025-09-10 21:27:01,250 - INFO - >>> {'loss': 2.292, 'grad_norm': 0.5125637054443359, 'learning_rate': 9.999923147800418e-05, 'epoch': 0.09663865546218488}
>>> 2025-09-10 21:27:05,314 - INFO - >>> {'loss': 2.3471, 'grad_norm': 0.5509849786758423, 'learning_rate': 9.999915654296359e-05, 'epoch': 0.10084033613445378}
>>> 2025-09-10 21:27:08,436 - INFO - >>> {'loss': 2.2917, 'grad_norm': 0.5421780943870544, 'learning_rate': 9.999907812261233e-05, 'epoch': 0.10504201680672269}
>>> 2025-09-10 21:27:13,009 - INFO - >>> {'loss': 2.3715, 'grad_norm': 0.48952606320381165, 'learning_rate': 9.999899621695591e-05, 'epoch': 0.1092436974789916}
>>> 2025-09-10 21:27:16,649 - INFO - >>> {'loss': 2.3483, 'grad_norm': 0.5408036112785339, 'learning_rate': 9.999891082600003e-05, 'epoch': 0.1134453781512605}
>>> 2025-09-10 21:27:21,179 - INFO - >>> {'loss': 2.3004, 'grad_norm': 0.5017340779304504, 'learning_rate': 9.999882194975063e-05, 'epoch': 0.11764705882352941}
>>> 2025-09-10 21:27:25,729 - INFO - >>> {'loss': 2.3905, 'grad_norm': 0.5649541020393372, 'learning_rate': 9.999872958821393e-05, 'epoch': 0.12184873949579832}
>>> 2025-09-10 21:27:29,990 - INFO - >>> {'loss': 2.4182, 'grad_norm': 0.5387890934944153, 'learning_rate': 9.999863374139634e-05, 'epoch': 0.12605042016806722}
>>> 2025-09-10 21:27:33,622 - INFO - >>> {'loss': 2.2633, 'grad_norm': 0.54930180311203, 'learning_rate': 9.999853440930456e-05, 'epoch': 0.13025210084033614}
>>> 2025-09-10 21:27:38,164 - INFO - >>> {'loss': 2.3027, 'grad_norm': 0.5531205534934998, 'learning_rate': 9.99984315919455e-05, 'epoch': 0.13445378151260504}
>>> 2025-09-10 21:27:41,594 - INFO - >>> {'loss': 2.2766, 'grad_norm': 0.5810814499855042, 'learning_rate': 9.999832528932636e-05, 'epoch': 0.13865546218487396}
>>> 2025-09-10 21:27:44,853 - INFO - >>> {'loss': 2.31, 'grad_norm': 0.5558860898017883, 'learning_rate': 9.999821550145452e-05, 'epoch': 0.14285714285714285}
>>> 2025-09-10 21:27:48,746 - INFO - >>> {'loss': 2.3679, 'grad_norm': 0.6097549796104431, 'learning_rate': 9.999810222833763e-05, 'epoch': 0.14705882352941177}
>>> 2025-09-10 21:27:52,267 - INFO - >>> {'loss': 2.378, 'grad_norm': 0.6125872135162354, 'learning_rate': 9.999798546998358e-05, 'epoch': 0.15126050420168066}
>>> 2025-09-10 21:27:55,599 - INFO - >>> {'loss': 2.3272, 'grad_norm': 0.5272650122642517, 'learning_rate': 9.999786522640055e-05, 'epoch': 0.15546218487394958}
>>> 2025-09-10 21:27:59,702 - INFO - >>> {'loss': 2.3038, 'grad_norm': 0.5222047567367554, 'learning_rate': 9.999774149759688e-05, 'epoch': 0.15966386554621848}
>>> 2025-09-10 21:28:03,149 - INFO - >>> {'loss': 2.2741, 'grad_norm': 0.6312511563301086, 'learning_rate': 9.999761428358121e-05, 'epoch': 0.1638655462184874}
>>> 2025-09-10 21:28:06,577 - INFO - >>> {'loss': 2.3683, 'grad_norm': 0.6029645204544067, 'learning_rate': 9.999748358436243e-05, 'epoch': 0.16806722689075632}
>>> 2025-09-10 21:28:10,019 - INFO - >>> {'loss': 2.3758, 'grad_norm': 0.5587900280952454, 'learning_rate': 9.99973493999496e-05, 'epoch': 0.1722689075630252}
>>> 2025-09-10 21:28:13,484 - INFO - >>> {'loss': 2.2905, 'grad_norm': 0.5662928819656372, 'learning_rate': 9.999721173035212e-05, 'epoch': 0.17647058823529413}
>>> 2025-09-10 21:28:17,474 - INFO - >>> {'loss': 2.2376, 'grad_norm': 0.6413007974624634, 'learning_rate': 9.999707057557957e-05, 'epoch': 0.18067226890756302}
>>> 2025-09-10 21:28:21,113 - INFO - >>> {'loss': 2.282, 'grad_norm': 0.5449805855751038, 'learning_rate': 9.999692593564178e-05, 'epoch': 0.18487394957983194}
>>> 2025-09-10 21:28:24,399 - INFO - >>> {'loss': 2.2696, 'grad_norm': 0.5168430209159851, 'learning_rate': 9.999677781054885e-05, 'epoch': 0.18907563025210083}
>>> 2025-09-10 21:28:27,797 - INFO - >>> {'loss': 2.3467, 'grad_norm': 0.5888274312019348, 'learning_rate': 9.99966262003111e-05, 'epoch': 0.19327731092436976}
>>> 2025-09-10 21:28:30,974 - INFO - >>> {'loss': 2.2984, 'grad_norm': 0.5566135048866272, 'learning_rate': 9.999647110493909e-05, 'epoch': 0.19747899159663865}
>>> 2025-09-10 21:28:34,407 - INFO - >>> {'loss': 2.3581, 'grad_norm': 0.5969455242156982, 'learning_rate': 9.999631252444363e-05, 'epoch': 0.20168067226890757}
>>> 2025-09-10 21:28:37,910 - INFO - >>> {'loss': 2.3737, 'grad_norm': 0.6043867468833923, 'learning_rate': 9.999615045883578e-05, 'epoch': 0.20588235294117646}
>>> 2025-09-10 21:28:41,269 - INFO - >>> {'loss': 2.3087, 'grad_norm': 0.5498014092445374, 'learning_rate': 9.999598490812685e-05, 'epoch': 0.21008403361344538}
>>> 2025-09-10 21:28:44,575 - INFO - >>> {'loss': 2.3026, 'grad_norm': 0.6263015270233154, 'learning_rate': 9.999581587232836e-05, 'epoch': 0.21428571428571427}
>>> 2025-09-10 21:28:47,980 - INFO - >>> {'loss': 2.1961, 'grad_norm': 0.5926336646080017, 'learning_rate': 9.999564335145209e-05, 'epoch': 0.2184873949579832}
>>> 2025-09-10 21:28:51,316 - INFO - >>> {'loss': 2.3249, 'grad_norm': 0.6269801259040833, 'learning_rate': 9.999546734551009e-05, 'epoch': 0.22268907563025211}
>>> 2025-09-10 21:28:54,422 - INFO - >>> {'loss': 2.2951, 'grad_norm': 0.6592384576797485, 'learning_rate': 9.999528785451462e-05, 'epoch': 0.226890756302521}
>>> 2025-09-10 21:28:57,828 - INFO - >>> {'loss': 2.2435, 'grad_norm': 0.562078058719635, 'learning_rate': 9.999510487847818e-05, 'epoch': 0.23109243697478993}
>>> 2025-09-10 21:29:01,599 - INFO - >>> {'loss': 2.3682, 'grad_norm': 0.5859641432762146, 'learning_rate': 9.999491841741354e-05, 'epoch': 0.23529411764705882}
>>> 2025-09-10 21:29:04,850 - INFO - >>> {'loss': 2.2747, 'grad_norm': 0.6357495784759521, 'learning_rate': 9.999472847133369e-05, 'epoch': 0.23949579831932774}
>>> 2025-09-10 21:29:07,911 - INFO - >>> {'loss': 2.2575, 'grad_norm': 0.5760990381240845, 'learning_rate': 9.999453504025185e-05, 'epoch': 0.24369747899159663}
>>> 2025-09-10 21:29:11,173 - INFO - >>> {'loss': 2.3098, 'grad_norm': 0.6005384922027588, 'learning_rate': 9.999433812418156e-05, 'epoch': 0.24789915966386555}
>>> 2025-09-10 21:29:14,995 - INFO - >>> {'loss': 2.3004, 'grad_norm': 0.5567706823348999, 'learning_rate': 9.999413772313649e-05, 'epoch': 0.25210084033613445}
>>> 2025-09-10 21:29:18,329 - INFO - >>> {'loss': 2.3872, 'grad_norm': 0.6449615359306335, 'learning_rate': 9.999393383713063e-05, 'epoch': 0.25630252100840334}
>>> 2025-09-10 21:30:01,829 - INFO - 导入包完成
>>> 2025-09-10 21:30:01,830 - INFO - ========train Qwen2ForCausalLM  202509102130========
>>> 2025-09-10 21:30:01,830 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-10 21:30:01,831 - INFO - 开始进行训练
>>> 2025-09-10 21:30:01,836 - INFO - 基础配置文件读取完成
>>> 2025-09-10 21:30:01,844 - INFO - 训练配置读取完成
>>> 2025-09-10 21:30:01,844 - INFO - 已启用验证数据集
>>> 2025-09-10 21:30:01,844 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-09-10 21:30:01,845 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-10 21:30:02,287 - INFO - tokenizer读取完成
>>> 2025-09-10 21:30:02,427 - INFO - model dtype:torch.bfloat16
>>> 2025-09-10 21:30:02,427 - INFO - 模型导入完成
>>> 2025-09-10 21:30:02,427 - INFO - 数据读取开始
>>> 2025-09-10 21:30:03,833 - INFO - 数据下载完成，训练集大小: 952
>>> 2025-09-10 21:30:07,954 - INFO - 数据映射完成
>>> 2025-09-10 21:30:08,329 - INFO - 数据下载完成，训练集大小: 952
>>> 2025-09-10 21:30:12,502 - INFO - 数据映射完成
>>> 2025-09-10 21:30:12,503 - INFO - 验证数据集处理完成
>>> 2025-09-10 21:30:12,504 - INFO - 打印训练参数如下
>>> 2025-09-10 21:30:12,504 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-10 21:30:12,505 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-10 21:30:12,505 - INFO -   load_in_4bit >>> False
>>> 2025-09-10 21:30:12,505 - INFO -   batch_size >>> 4
>>> 2025-09-10 21:30:12,506 - INFO -   gradient_accumulator_steps >>> 1
>>> 2025-09-10 21:30:12,506 - INFO -   warmup_steps >>> 1
>>> 2025-09-10 21:30:12,506 - INFO -   epoch >>> 50
>>> 2025-09-10 21:30:12,507 - INFO -   eval_steps >>> 10
>>> 2025-09-10 21:30:12,507 - INFO -   learning_rate >>> 0.0001
>>> 2025-09-10 21:30:12,507 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-10 21:30:12,507 - INFO -   max_seq_length >>> 2048
>>> 2025-09-10 21:30:12,508 - INFO -   use_history >>> False
>>> 2025-09-10 21:30:12,508 - INFO -   r >>> 8
>>> 2025-09-10 21:30:12,508 - INFO -   interface_mode >>> False
>>> 2025-09-10 21:30:12,509 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-09-10 21:30:12,509 - INFO -   lora_alpha >>> 16
>>> 2025-09-10 21:30:12,509 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-10 21:30:12,510 - INFO -   bias >>> none
>>> 2025-09-10 21:30:12,510 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-10 21:30:12,510 - INFO -   random_state >>> 3407
>>> 2025-09-10 21:30:12,511 - INFO -   use_rslora >>> True
>>> 2025-09-10 21:30:12,511 - INFO -   loftq_config >>> None
>>> 2025-09-10 21:30:13,346 - INFO - 开始训练！
>>> 2025-09-10 21:30:17,852 - INFO - >>> {'loss': 2.4287, 'grad_norm': 0.6084955930709839, 'learning_rate': 0.0, 'epoch': 0.004201680672268907}
>>> 2025-09-10 21:30:21,569 - INFO - >>> {'loss': 2.4574, 'grad_norm': 0.6563354134559631, 'learning_rate': 0.0001, 'epoch': 0.008403361344537815}
>>> 2025-09-10 21:30:24,925 - INFO - >>> {'loss': 2.4785, 'grad_norm': 0.6835582256317139, 'learning_rate': 9.999999825731529e-05, 'epoch': 0.012605042016806723}
>>> 2025-09-10 21:30:28,104 - INFO - >>> {'loss': 2.3673, 'grad_norm': 0.6404395699501038, 'learning_rate': 9.999999302926127e-05, 'epoch': 0.01680672268907563}
>>> 2025-09-10 21:30:31,589 - INFO - >>> {'loss': 2.3477, 'grad_norm': 0.636398434638977, 'learning_rate': 9.999998431583829e-05, 'epoch': 0.02100840336134454}
>>> 2025-09-10 21:30:35,294 - INFO - >>> {'loss': 2.3956, 'grad_norm': 0.5811261534690857, 'learning_rate': 9.999997211704696e-05, 'epoch': 0.025210084033613446}
>>> 2025-09-10 21:30:38,331 - INFO - >>> {'loss': 2.47, 'grad_norm': 0.6948734521865845, 'learning_rate': 9.999995643288816e-05, 'epoch': 0.029411764705882353}
>>> 2025-09-10 21:30:41,655 - INFO - >>> {'loss': 2.4689, 'grad_norm': 0.5736225247383118, 'learning_rate': 9.999993726336295e-05, 'epoch': 0.03361344537815126}
>>> 2025-09-10 21:30:44,985 - INFO - >>> {'loss': 2.4626, 'grad_norm': 0.5809930562973022, 'learning_rate': 9.999991460847269e-05, 'epoch': 0.037815126050420166}
>>> 2025-09-10 21:30:47,784 - INFO - >>> {'loss': 2.3585, 'grad_norm': 0.5615923404693604, 'learning_rate': 9.999988846821895e-05, 'epoch': 0.04201680672268908}
>>> 2025-09-10 21:30:51,255 - INFO - >>> {'loss': 2.4385, 'grad_norm': 0.5242776274681091, 'learning_rate': 9.999985884260355e-05, 'epoch': 0.046218487394957986}
>>> 2025-09-10 21:30:54,661 - INFO - >>> {'loss': 2.4348, 'grad_norm': 0.5354211926460266, 'learning_rate': 9.999982573162854e-05, 'epoch': 0.05042016806722689}
>>> 2025-09-10 21:30:57,721 - INFO - >>> {'loss': 2.4147, 'grad_norm': 0.5550392270088196, 'learning_rate': 9.999978913529627e-05, 'epoch': 0.0546218487394958}
>>> 2025-09-10 21:31:01,563 - INFO - >>> {'loss': 2.4901, 'grad_norm': 0.5384076237678528, 'learning_rate': 9.999974905360925e-05, 'epoch': 0.058823529411764705}
>>> 2025-09-10 21:31:05,256 - INFO - >>> {'loss': 2.4492, 'grad_norm': 0.49250075221061707, 'learning_rate': 9.999970548657029e-05, 'epoch': 0.06302521008403361}
>>> 2025-09-10 21:31:08,816 - INFO - >>> {'loss': 2.3492, 'grad_norm': 0.5432614684104919, 'learning_rate': 9.999965843418242e-05, 'epoch': 0.06722689075630252}
>>> 2025-09-10 21:31:12,304 - INFO - >>> {'loss': 2.3832, 'grad_norm': 0.5561001896858215, 'learning_rate': 9.999960789644893e-05, 'epoch': 0.07142857142857142}
>>> 2025-09-10 21:31:27,739 - INFO - 导入包完成
>>> 2025-09-10 21:31:27,740 - INFO - ========train Qwen2ForCausalLM  202509102131========
>>> 2025-09-10 21:31:27,740 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-10 21:31:27,741 - INFO - 开始进行训练
>>> 2025-09-10 21:31:27,746 - INFO - 基础配置文件读取完成
>>> 2025-09-10 21:31:27,754 - INFO - 训练配置读取完成
>>> 2025-09-10 21:31:27,754 - INFO - 已启用验证数据集
>>> 2025-09-10 21:31:27,754 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-09-10 21:31:27,755 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-10 21:31:28,207 - INFO - tokenizer读取完成
>>> 2025-09-10 21:31:28,347 - INFO - model dtype:torch.bfloat16
>>> 2025-09-10 21:31:28,347 - INFO - 模型导入完成
>>> 2025-09-10 21:31:28,348 - INFO - 数据读取开始
>>> 2025-09-10 21:31:29,398 - INFO - 数据下载完成，训练集大小: 952
>>> 2025-09-10 21:31:33,534 - INFO - 数据映射完成
>>> 2025-09-10 21:31:33,818 - INFO - 数据下载完成，训练集大小: 952
>>> 2025-09-10 21:31:38,024 - INFO - 数据映射完成
>>> 2025-09-10 21:31:38,025 - INFO - 验证数据集处理完成
>>> 2025-09-10 21:31:38,025 - INFO - 打印训练参数如下
>>> 2025-09-10 21:31:38,026 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-10 21:31:38,026 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-10 21:31:38,026 - INFO -   load_in_4bit >>> False
>>> 2025-09-10 21:31:38,027 - INFO -   batch_size >>> 4
>>> 2025-09-10 21:31:38,027 - INFO -   gradient_accumulator_steps >>> 1
>>> 2025-09-10 21:31:38,027 - INFO -   warmup_steps >>> 1
>>> 2025-09-10 21:31:38,028 - INFO -   epoch >>> 20
>>> 2025-09-10 21:31:38,028 - INFO -   eval_steps >>> 10
>>> 2025-09-10 21:31:38,028 - INFO -   learning_rate >>> 0.0001
>>> 2025-09-10 21:31:38,029 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-10 21:31:38,029 - INFO -   max_seq_length >>> 2048
>>> 2025-09-10 21:31:38,029 - INFO -   use_history >>> False
>>> 2025-09-10 21:31:38,030 - INFO -   r >>> 8
>>> 2025-09-10 21:31:38,030 - INFO -   interface_mode >>> False
>>> 2025-09-10 21:31:38,030 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-09-10 21:31:38,031 - INFO -   lora_alpha >>> 16
>>> 2025-09-10 21:31:38,031 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-10 21:31:38,031 - INFO -   bias >>> none
>>> 2025-09-10 21:31:38,032 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-10 21:31:38,032 - INFO -   random_state >>> 3407
>>> 2025-09-10 21:31:38,032 - INFO -   use_rslora >>> True
>>> 2025-09-10 21:31:38,033 - INFO -   loftq_config >>> None
>>> 2025-09-10 21:31:38,837 - INFO - 开始训练！
>>> 2025-09-10 21:31:43,403 - INFO - >>> {'loss': 2.4287, 'grad_norm': 0.6216520071029663, 'learning_rate': 0.0, 'epoch': 0.004201680672268907}
>>> 2025-09-10 21:31:47,270 - INFO - >>> {'loss': 2.4574, 'grad_norm': 0.672583818435669, 'learning_rate': 0.0001, 'epoch': 0.008403361344537815}
>>> 2025-09-10 21:31:50,688 - INFO - >>> {'loss': 2.4789, 'grad_norm': 0.6960346698760986, 'learning_rate': 9.999998910547429e-05, 'epoch': 0.012605042016806723}
>>> 2025-09-10 21:31:53,954 - INFO - >>> {'loss': 2.3646, 'grad_norm': 0.6679137945175171, 'learning_rate': 9.999995642190186e-05, 'epoch': 0.01680672268907563}
>>> 2025-09-10 21:31:57,839 - INFO - >>> {'loss': 2.3478, 'grad_norm': 0.6493362784385681, 'learning_rate': 9.999990194929696e-05, 'epoch': 0.02100840336134454}
>>> 2025-09-10 21:32:01,895 - INFO - >>> {'loss': 2.397, 'grad_norm': 0.600318968296051, 'learning_rate': 9.999982568768336e-05, 'epoch': 0.025210084033613446}
>>> 2025-09-10 21:32:05,267 - INFO - >>> {'loss': 2.4713, 'grad_norm': 0.7117699384689331, 'learning_rate': 9.999972763709427e-05, 'epoch': 0.029411764705882353}
>>> 2025-09-10 21:32:09,177 - INFO - >>> {'loss': 2.4689, 'grad_norm': 0.5834893584251404, 'learning_rate': 9.999960779757242e-05, 'epoch': 0.03361344537815126}
>>> 2025-09-10 21:32:13,274 - INFO - >>> {'loss': 2.4651, 'grad_norm': 0.6008449792861938, 'learning_rate': 9.999946616917004e-05, 'epoch': 0.037815126050420166}
>>> 2025-09-10 21:32:16,768 - INFO - >>> {'loss': 2.3595, 'grad_norm': 0.5675035119056702, 'learning_rate': 9.999930275194883e-05, 'epoch': 0.04201680672268908}
>>> 2025-09-10 21:32:21,227 - INFO - >>> {'loss': 2.4391, 'grad_norm': 0.5385668873786926, 'learning_rate': 9.999911754598003e-05, 'epoch': 0.046218487394957986}
>>> 2025-09-10 21:32:25,820 - INFO - >>> {'loss': 2.4333, 'grad_norm': 0.5495668649673462, 'learning_rate': 9.999891055134434e-05, 'epoch': 0.05042016806722689}
>>> 2025-09-10 21:32:30,026 - INFO - >>> {'loss': 2.4136, 'grad_norm': 0.5586966872215271, 'learning_rate': 9.999868176813195e-05, 'epoch': 0.0546218487394958}
>>> 2025-09-10 21:32:34,725 - INFO - >>> {'loss': 2.4896, 'grad_norm': 0.5488194823265076, 'learning_rate': 9.999843119644258e-05, 'epoch': 0.058823529411764705}
>>> 2025-09-10 21:32:39,321 - INFO - >>> {'loss': 2.4493, 'grad_norm': 0.49077093601226807, 'learning_rate': 9.999815883638541e-05, 'epoch': 0.06302521008403361}
>>> 2025-09-10 21:32:43,663 - INFO - >>> {'loss': 2.3476, 'grad_norm': 0.5437124371528625, 'learning_rate': 9.999786468807914e-05, 'epoch': 0.06722689075630252}
>>> 2025-09-10 21:32:47,778 - INFO - >>> {'loss': 2.385, 'grad_norm': 0.5579691529273987, 'learning_rate': 9.999754875165195e-05, 'epoch': 0.07142857142857142}
>>> 2025-09-10 21:32:52,499 - INFO - >>> {'loss': 2.4225, 'grad_norm': 0.5053614377975464, 'learning_rate': 9.999721102724151e-05, 'epoch': 0.07563025210084033}
>>> 2025-09-10 21:32:57,380 - INFO - >>> {'loss': 2.3656, 'grad_norm': 0.4889644980430603, 'learning_rate': 9.9996851514995e-05, 'epoch': 0.07983193277310924}
>>> 2025-09-10 21:33:01,184 - INFO - >>> {'loss': 2.4385, 'grad_norm': 0.6238487958908081, 'learning_rate': 9.999647021506911e-05, 'epoch': 0.08403361344537816}
>>> 2025-09-10 21:33:04,732 - INFO - >>> {'loss': 2.337, 'grad_norm': 0.565025269985199, 'learning_rate': 9.999606712762996e-05, 'epoch': 0.08823529411764706}
>>> 2025-09-10 21:33:08,914 - INFO - >>> {'loss': 2.5122, 'grad_norm': 0.5396057367324829, 'learning_rate': 9.999564225285326e-05, 'epoch': 0.09243697478991597}
>>> 2025-09-10 21:33:13,365 - INFO - >>> {'loss': 2.291, 'grad_norm': 0.5325134992599487, 'learning_rate': 9.999519559092412e-05, 'epoch': 0.09663865546218488}
>>> 2025-09-10 21:33:17,792 - INFO - >>> {'loss': 2.3453, 'grad_norm': 0.5697943568229675, 'learning_rate': 9.99947271420372e-05, 'epoch': 0.10084033613445378}
>>> 2025-09-10 21:33:21,205 - INFO - >>> {'loss': 2.2899, 'grad_norm': 0.5674702525138855, 'learning_rate': 9.999423690639665e-05, 'epoch': 0.10504201680672269}
>>> 2025-09-10 21:33:25,169 - INFO - >>> {'loss': 2.3703, 'grad_norm': 0.510179877281189, 'learning_rate': 9.999372488421608e-05, 'epoch': 0.1092436974789916}
>>> 2025-09-10 21:33:28,461 - INFO - >>> {'loss': 2.3472, 'grad_norm': 0.5645309686660767, 'learning_rate': 9.999319107571865e-05, 'epoch': 0.1134453781512605}
>>> 2025-09-10 21:33:32,482 - INFO - >>> {'loss': 2.3001, 'grad_norm': 0.5121010541915894, 'learning_rate': 9.999263548113698e-05, 'epoch': 0.11764705882352941}
>>> 2025-09-10 21:33:36,085 - INFO - >>> {'loss': 2.3888, 'grad_norm': 0.5782252550125122, 'learning_rate': 9.999205810071315e-05, 'epoch': 0.12184873949579832}
>>> 2025-09-10 21:33:39,633 - INFO - >>> {'loss': 2.4145, 'grad_norm': 0.5438111424446106, 'learning_rate': 9.999145893469882e-05, 'epoch': 0.12605042016806722}
>>> 2025-09-10 21:33:43,141 - INFO - >>> {'loss': 2.2631, 'grad_norm': 0.5731955170631409, 'learning_rate': 9.999083798335507e-05, 'epoch': 0.13025210084033614}
>>> 2025-09-10 21:33:47,630 - INFO - >>> {'loss': 2.3002, 'grad_norm': 0.5658256411552429, 'learning_rate': 9.999019524695251e-05, 'epoch': 0.13445378151260504}
>>> 2025-09-10 21:33:51,102 - INFO - >>> {'loss': 2.2771, 'grad_norm': 0.6009157299995422, 'learning_rate': 9.998953072577121e-05, 'epoch': 0.13865546218487396}
>>> 2025-09-10 21:33:54,542 - INFO - >>> {'loss': 2.3099, 'grad_norm': 0.5669059157371521, 'learning_rate': 9.99888444201008e-05, 'epoch': 0.14285714285714285}
>>> 2025-09-10 21:33:58,229 - INFO - >>> {'loss': 2.3693, 'grad_norm': 0.6264093518257141, 'learning_rate': 9.998813633024032e-05, 'epoch': 0.14705882352941177}
>>> 2025-09-10 21:34:01,705 - INFO - >>> {'loss': 2.3778, 'grad_norm': 0.6375648379325867, 'learning_rate': 9.998740645649833e-05, 'epoch': 0.15126050420168066}
>>> 2025-09-10 21:34:05,237 - INFO - >>> {'loss': 2.326, 'grad_norm': 0.5473340153694153, 'learning_rate': 9.998665479919295e-05, 'epoch': 0.15546218487394958}
>>> 2025-09-10 21:34:09,327 - INFO - >>> {'loss': 2.3042, 'grad_norm': 0.5346829295158386, 'learning_rate': 9.99858813586517e-05, 'epoch': 0.15966386554621848}
>>> 2025-09-10 21:34:12,851 - INFO - >>> {'loss': 2.2722, 'grad_norm': 0.6452155709266663, 'learning_rate': 9.998508613521162e-05, 'epoch': 0.1638655462184874}
>>> 2025-09-10 21:34:16,363 - INFO - >>> {'loss': 2.3688, 'grad_norm': 0.6307805180549622, 'learning_rate': 9.998426912921927e-05, 'epoch': 0.16806722689075632}
>>> 2025-09-10 21:34:19,784 - INFO - >>> {'loss': 2.3745, 'grad_norm': 0.569113552570343, 'learning_rate': 9.998343034103071e-05, 'epoch': 0.1722689075630252}
>>> 2025-09-10 21:34:23,342 - INFO - >>> {'loss': 2.2896, 'grad_norm': 0.5907869338989258, 'learning_rate': 9.998256977101142e-05, 'epoch': 0.17647058823529413}
>>> 2025-09-10 21:34:27,462 - INFO - >>> {'loss': 2.2355, 'grad_norm': 0.6581028699874878, 'learning_rate': 9.998168741953646e-05, 'epoch': 0.18067226890756302}
>>> 2025-09-10 21:34:31,011 - INFO - >>> {'loss': 2.281, 'grad_norm': 0.5560659170150757, 'learning_rate': 9.998078328699032e-05, 'epoch': 0.18487394957983194}
>>> 2025-09-10 21:34:34,355 - INFO - >>> {'loss': 2.2663, 'grad_norm': 0.5344046950340271, 'learning_rate': 9.997985737376701e-05, 'epoch': 0.18907563025210083}
>>> 2025-09-10 21:34:45,465 - INFO - 导入包完成
>>> 2025-09-10 21:34:45,465 - INFO - ========train Qwen2ForCausalLM  202509102134========
>>> 2025-09-10 21:34:45,466 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-10 21:34:45,466 - INFO - 开始进行训练
>>> 2025-09-10 21:34:45,471 - INFO - 基础配置文件读取完成
>>> 2025-09-10 21:34:45,479 - INFO - 训练配置读取完成
>>> 2025-09-10 21:34:45,479 - INFO - 已启用验证数据集
>>> 2025-09-10 21:34:45,479 - INFO - 数据集路径：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/dataset/PsyDTCorpus/PsyDTCorpus_train_mulit_turn_packing.json
>>> 2025-09-10 21:34:45,480 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-10 21:34:45,921 - INFO - tokenizer读取完成
>>> 2025-09-10 21:34:46,059 - INFO - model dtype:torch.bfloat16
>>> 2025-09-10 21:34:46,060 - INFO - 模型导入完成
>>> 2025-09-10 21:34:46,060 - INFO - 数据读取开始
>>> 2025-09-10 21:34:47,873 - INFO - 数据下载完成，训练集大小: 952
>>> 2025-09-10 21:34:52,009 - INFO - 数据映射完成
>>> 2025-09-10 21:34:52,274 - INFO - 数据下载完成，训练集大小: 952
>>> 2025-09-10 21:34:56,433 - INFO - 数据映射完成
>>> 2025-09-10 21:34:56,434 - INFO - 验证数据集处理完成
>>> 2025-09-10 21:34:56,434 - INFO - 打印训练参数如下
>>> 2025-09-10 21:34:56,435 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-10 21:34:56,435 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-10 21:34:56,435 - INFO -   load_in_4bit >>> True
>>> 2025-09-10 21:34:56,436 - INFO -   batch_size >>> 4
>>> 2025-09-10 21:34:56,436 - INFO -   gradient_accumulator_steps >>> 1
>>> 2025-09-10 21:34:56,436 - INFO -   warmup_steps >>> 1
>>> 2025-09-10 21:34:56,437 - INFO -   epoch >>> 15
>>> 2025-09-10 21:34:56,437 - INFO -   eval_steps >>> 5
>>> 2025-09-10 21:34:56,437 - INFO -   learning_rate >>> 2e-05
>>> 2025-09-10 21:34:56,438 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-10 21:34:56,438 - INFO -   max_seq_length >>> 2048
>>> 2025-09-10 21:34:56,438 - INFO -   r >>> 8
>>> 2025-09-10 21:34:56,439 - INFO -   interface_mode >>> False
>>> 2025-09-10 21:34:56,439 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj']
>>> 2025-09-10 21:34:56,439 - INFO -   lora_alpha >>> 16
>>> 2025-09-10 21:34:56,440 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-10 21:34:56,440 - INFO -   bias >>> none
>>> 2025-09-10 21:34:56,440 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-10 21:34:56,441 - INFO -   random_state >>> 3407
>>> 2025-09-10 21:34:56,441 - INFO -   use_rslora >>> True
>>> 2025-09-10 21:34:56,441 - INFO -   loftq_config >>> None
>>> 2025-09-10 21:34:57,255 - INFO - 开始训练！
>>> 2025-09-10 21:35:01,785 - INFO - >>> {'loss': 2.4287, 'grad_norm': 0.6394322514533997, 'learning_rate': 0.0, 'epoch': 0.004201680672268907}
>>> 2025-09-10 21:35:05,508 - INFO - >>> {'loss': 2.4574, 'grad_norm': 0.6877313852310181, 'learning_rate': 2e-05, 'epoch': 0.008403361344537815}
>>> 2025-09-10 21:35:08,866 - INFO - >>> {'loss': 2.4811, 'grad_norm': 0.7291075587272644, 'learning_rate': 1.999999612584827e-05, 'epoch': 0.012605042016806723}
>>> 2025-09-10 21:35:12,045 - INFO - >>> {'loss': 2.3745, 'grad_norm': 0.6898554563522339, 'learning_rate': 1.9999984503396083e-05, 'epoch': 0.01680672268907563}
>>> 2025-09-10 21:35:15,556 - INFO - >>> {'loss': 2.3614, 'grad_norm': 0.7053410410881042, 'learning_rate': 1.999996513265244e-05, 'epoch': 0.02100840336134454}
>>> 2025-09-10 21:35:19,266 - INFO - >>> {'loss': 2.4139, 'grad_norm': 0.691601037979126, 'learning_rate': 1.999993801363235e-05, 'epoch': 0.025210084033613446}
>>> 2025-09-10 21:35:22,315 - INFO - >>> {'loss': 2.5013, 'grad_norm': 0.8613418936729431, 'learning_rate': 1.9999903146356834e-05, 'epoch': 0.029411764705882353}
>>> 2025-09-10 21:35:25,643 - INFO - >>> {'loss': 2.498, 'grad_norm': 0.7185922265052795, 'learning_rate': 1.99998605308529e-05, 'epoch': 0.03361344537815126}
>>> 2025-09-10 21:35:28,963 - INFO - >>> {'loss': 2.5, 'grad_norm': 0.7428873181343079, 'learning_rate': 1.999981016715357e-05, 'epoch': 0.037815126050420166}
>>> 2025-09-10 21:35:31,771 - INFO - >>> {'loss': 2.4, 'grad_norm': 0.7616119980812073, 'learning_rate': 1.9999752055297865e-05, 'epoch': 0.04201680672268908}
>>> 2025-09-10 21:35:35,276 - INFO - >>> {'loss': 2.4793, 'grad_norm': 0.7169798612594604, 'learning_rate': 1.9999686195330815e-05, 'epoch': 0.046218487394957986}
>>> 2025-09-10 21:35:38,731 - INFO - >>> {'loss': 2.4761, 'grad_norm': 0.6594337224960327, 'learning_rate': 1.999961258730345e-05, 'epoch': 0.05042016806722689}
>>> 2025-09-10 21:35:41,882 - INFO - >>> {'loss': 2.4599, 'grad_norm': 0.6808903217315674, 'learning_rate': 1.9999531231272807e-05, 'epoch': 0.0546218487394958}
>>> 2025-09-10 21:35:45,882 - INFO - >>> {'loss': 2.5424, 'grad_norm': 0.708276629447937, 'learning_rate': 1.999944212730192e-05, 'epoch': 0.058823529411764705}
>>> 2025-09-10 21:35:49,761 - INFO - >>> {'loss': 2.4952, 'grad_norm': 0.6223738789558411, 'learning_rate': 1.9999345275459824e-05, 'epoch': 0.06302521008403361}
>>> 2025-09-10 21:35:53,571 - INFO - >>> {'loss': 2.4046, 'grad_norm': 0.6566938757896423, 'learning_rate': 1.999924067582157e-05, 'epoch': 0.06722689075630252}
>>> 2025-09-10 21:35:57,520 - INFO - >>> {'loss': 2.442, 'grad_norm': 0.6598551869392395, 'learning_rate': 1.9999128328468202e-05, 'epoch': 0.07142857142857142}
>>> 2025-09-10 21:36:02,000 - INFO - >>> {'loss': 2.4807, 'grad_norm': 0.6477113962173462, 'learning_rate': 1.999900823348677e-05, 'epoch': 0.07563025210084033}
>>> 2025-09-10 21:36:06,730 - INFO - >>> {'loss': 2.4224, 'grad_norm': 0.600414514541626, 'learning_rate': 1.999888039097033e-05, 'epoch': 0.07983193277310924}
>>> 2025-09-10 21:36:10,909 - INFO - >>> {'loss': 2.511, 'grad_norm': 0.7050040364265442, 'learning_rate': 1.999874480101794e-05, 'epoch': 0.08403361344537816}
>>> 2025-09-10 21:36:14,645 - INFO - >>> {'loss': 2.4019, 'grad_norm': 0.6465978622436523, 'learning_rate': 1.9998601463734646e-05, 'epoch': 0.08823529411764706}
>>> 2025-09-10 21:36:19,165 - INFO - >>> {'loss': 2.5827, 'grad_norm': 0.6639952659606934, 'learning_rate': 1.9998450379231526e-05, 'epoch': 0.09243697478991597}
>>> 2025-09-10 21:36:23,870 - INFO - >>> {'loss': 2.3606, 'grad_norm': 0.6233246922492981, 'learning_rate': 1.999829154762564e-05, 'epoch': 0.09663865546218488}
>>> 2025-09-10 21:36:28,699 - INFO - >>> {'loss': 2.4136, 'grad_norm': 0.6507336497306824, 'learning_rate': 1.9998124969040047e-05, 'epoch': 0.10084033613445378}
>>> 2025-09-10 21:36:32,258 - INFO - >>> {'loss': 2.3712, 'grad_norm': 0.6765439510345459, 'learning_rate': 1.999795064360383e-05, 'epoch': 0.10504201680672269}
>>> 2025-09-10 21:36:36,945 - INFO - >>> {'loss': 2.4409, 'grad_norm': 0.5732343196868896, 'learning_rate': 1.9997768571452055e-05, 'epoch': 0.1092436974789916}
>>> 2025-09-10 21:36:40,279 - INFO - >>> {'loss': 2.4207, 'grad_norm': 0.6270192861557007, 'learning_rate': 1.9997578752725796e-05, 'epoch': 0.1134453781512605}
>>> 2025-09-10 21:36:44,827 - INFO - >>> {'loss': 2.3731, 'grad_norm': 0.5519022345542908, 'learning_rate': 1.9997381187572133e-05, 'epoch': 0.11764705882352941}
>>> 2025-09-10 21:36:48,772 - INFO - >>> {'loss': 2.4746, 'grad_norm': 0.6095325946807861, 'learning_rate': 1.9997175876144145e-05, 'epoch': 0.12184873949579832}
>>> 2025-09-10 21:36:52,910 - INFO - >>> {'loss': 2.4882, 'grad_norm': 0.5696123838424683, 'learning_rate': 1.9996962818600912e-05, 'epoch': 0.12605042016806722}
>>> 2025-09-10 21:36:56,560 - INFO - >>> {'loss': 2.3352, 'grad_norm': 0.5688307285308838, 'learning_rate': 1.999674201510752e-05, 'epoch': 0.13025210084033614}
>>> 2025-09-10 21:37:01,346 - INFO - >>> {'loss': 2.3791, 'grad_norm': 0.588158130645752, 'learning_rate': 1.9996513465835052e-05, 'epoch': 0.13445378151260504}
>>> 2025-09-10 21:37:04,903 - INFO - >>> {'loss': 2.3687, 'grad_norm': 0.6345852017402649, 'learning_rate': 1.9996277170960597e-05, 'epoch': 0.13865546218487396}
>>> 2025-09-10 21:37:08,495 - INFO - >>> {'loss': 2.3929, 'grad_norm': 0.5687671899795532, 'learning_rate': 1.9996033130667243e-05, 'epoch': 0.14285714285714285}
>>> 2025-09-10 21:37:12,364 - INFO - >>> {'loss': 2.4741, 'grad_norm': 0.667876124382019, 'learning_rate': 1.9995781345144077e-05, 'epoch': 0.14705882352941177}
>>> 2025-09-10 21:37:15,916 - INFO - >>> {'loss': 2.4509, 'grad_norm': 0.5717254281044006, 'learning_rate': 1.999552181458619e-05, 'epoch': 0.15126050420168066}
>>> 2025-09-10 21:37:19,587 - INFO - >>> {'loss': 2.4051, 'grad_norm': 0.5799915194511414, 'learning_rate': 1.999525453919468e-05, 'epoch': 0.15546218487394958}
>>> 2025-09-10 21:37:23,996 - INFO - >>> {'loss': 2.3835, 'grad_norm': 0.5337483286857605, 'learning_rate': 1.9994979519176638e-05, 'epoch': 0.15966386554621848}
>>> 2025-09-10 21:37:27,601 - INFO - >>> {'loss': 2.3567, 'grad_norm': 0.5917676091194153, 'learning_rate': 1.999469675474515e-05, 'epoch': 0.1638655462184874}
>>> 2025-09-10 21:37:31,258 - INFO - >>> {'loss': 2.4524, 'grad_norm': 0.6027796864509583, 'learning_rate': 1.999440624611932e-05, 'epoch': 0.16806722689075632}
>>> 2025-09-10 21:37:34,962 - INFO - >>> {'loss': 2.4627, 'grad_norm': 0.5482191443443298, 'learning_rate': 1.999410799352424e-05, 'epoch': 0.1722689075630252}
>>> 2025-09-10 21:37:38,683 - INFO - >>> {'loss': 2.3797, 'grad_norm': 0.5750756859779358, 'learning_rate': 1.9993801997191005e-05, 'epoch': 0.17647058823529413}
>>> 2025-09-10 21:37:43,064 - INFO - >>> {'loss': 2.3353, 'grad_norm': 0.6280002593994141, 'learning_rate': 1.999348825735671e-05, 'epoch': 0.18067226890756302}
>>> 2025-09-10 21:37:46,682 - INFO - >>> {'loss': 2.3685, 'grad_norm': 0.5681867003440857, 'learning_rate': 1.999316677426445e-05, 'epoch': 0.18487394957983194}
>>> 2025-09-10 21:37:50,324 - INFO - >>> {'loss': 2.3574, 'grad_norm': 0.5197552442550659, 'learning_rate': 1.9992837548163315e-05, 'epoch': 0.18907563025210083}
>>> 2025-09-10 21:37:53,790 - INFO - >>> {'loss': 2.437, 'grad_norm': 0.5820505023002625, 'learning_rate': 1.999250057930841e-05, 'epoch': 0.19327731092436976}
>>> 2025-09-10 21:37:57,204 - INFO - >>> {'loss': 2.3938, 'grad_norm': 0.5567046999931335, 'learning_rate': 1.9992155867960822e-05, 'epoch': 0.19747899159663865}
>>> 2025-09-10 21:38:00,687 - INFO - >>> {'loss': 2.4572, 'grad_norm': 0.574350893497467, 'learning_rate': 1.9991803414387643e-05, 'epoch': 0.20168067226890757}
>>> 2025-09-10 21:38:04,232 - INFO - >>> {'loss': 2.479, 'grad_norm': 0.591079592704773, 'learning_rate': 1.9991443218861967e-05, 'epoch': 0.20588235294117646}
>>> 2025-09-10 21:38:07,669 - INFO - >>> {'loss': 2.4077, 'grad_norm': 0.556147038936615, 'learning_rate': 1.9991075281662887e-05, 'epoch': 0.21008403361344538}
>>> 2025-09-10 21:38:11,074 - INFO - >>> {'loss': 2.3914, 'grad_norm': 0.5710482597351074, 'learning_rate': 1.9990699603075484e-05, 'epoch': 0.21428571428571427}
>>> 2025-09-10 21:38:14,635 - INFO - >>> {'loss': 2.293, 'grad_norm': 0.549002468585968, 'learning_rate': 1.9990316183390854e-05, 'epoch': 0.2184873949579832}
>>> 2025-09-10 21:38:18,174 - INFO - >>> {'loss': 2.4225, 'grad_norm': 0.6021741032600403, 'learning_rate': 1.9989925022906076e-05, 'epoch': 0.22268907563025211}
>>> 2025-09-10 21:38:21,384 - INFO - >>> {'loss': 2.3941, 'grad_norm': 0.6282451748847961, 'learning_rate': 1.9989526121924237e-05, 'epoch': 0.226890756302521}
>>> 2025-09-10 21:38:24,822 - INFO - >>> {'loss': 2.3351, 'grad_norm': 0.5208063721656799, 'learning_rate': 1.9989119480754417e-05, 'epoch': 0.23109243697478993}
>>> 2025-09-10 21:38:28,694 - INFO - >>> {'loss': 2.4572, 'grad_norm': 0.5375000834465027, 'learning_rate': 1.9988705099711686e-05, 'epoch': 0.23529411764705882}
>>> 2025-09-10 21:38:31,981 - INFO - >>> {'loss': 2.3703, 'grad_norm': 0.6007147431373596, 'learning_rate': 1.9988282979117134e-05, 'epoch': 0.23949579831932774}
>>> 2025-09-10 21:38:35,067 - INFO - >>> {'loss': 2.3583, 'grad_norm': 0.5606264472007751, 'learning_rate': 1.998785311929782e-05, 'epoch': 0.24369747899159663}
>>> 2025-09-10 21:38:38,384 - INFO - >>> {'loss': 2.407, 'grad_norm': 0.5621200203895569, 'learning_rate': 1.9987415520586817e-05, 'epoch': 0.24789915966386555}
>>> 2025-09-10 21:38:42,370 - INFO - >>> {'loss': 2.3938, 'grad_norm': 0.5324040651321411, 'learning_rate': 1.998697018332319e-05, 'epoch': 0.25210084033613445}
>>> 2025-09-10 21:38:45,789 - INFO - >>> {'loss': 2.4843, 'grad_norm': 0.6149929165840149, 'learning_rate': 1.9986517107852003e-05, 'epoch': 0.25630252100840334}
>>> 2025-09-10 21:38:49,260 - INFO - >>> {'loss': 2.439, 'grad_norm': 0.5809705853462219, 'learning_rate': 1.9986056294524307e-05, 'epoch': 0.2605042016806723}
>>> 2025-09-10 21:38:53,043 - INFO - >>> {'loss': 2.4495, 'grad_norm': 0.5058859586715698, 'learning_rate': 1.9985587743697155e-05, 'epoch': 0.2647058823529412}
>>> 2025-09-10 21:38:56,024 - INFO - >>> {'loss': 2.5174, 'grad_norm': 0.6512894034385681, 'learning_rate': 1.99851114557336e-05, 'epoch': 0.2689075630252101}
>>> 2025-09-10 21:39:00,082 - INFO - >>> {'loss': 2.4483, 'grad_norm': 0.5584078431129456, 'learning_rate': 1.9984627431002675e-05, 'epoch': 0.27310924369747897}
>>> 2025-09-10 21:39:03,822 - INFO - >>> {'loss': 2.3792, 'grad_norm': 0.537478506565094, 'learning_rate': 1.9984135669879425e-05, 'epoch': 0.2773109243697479}
>>> 2025-09-10 21:39:06,897 - INFO - >>> {'loss': 2.4299, 'grad_norm': 0.5851690173149109, 'learning_rate': 1.9983636172744876e-05, 'epoch': 0.2815126050420168}
>>> 2025-09-10 21:39:10,020 - INFO - >>> {'loss': 2.4527, 'grad_norm': 0.5620008707046509, 'learning_rate': 1.998312893998606e-05, 'epoch': 0.2857142857142857}
>>> 2025-09-10 21:39:14,024 - INFO - >>> {'loss': 2.3617, 'grad_norm': 0.5271346569061279, 'learning_rate': 1.998261397199599e-05, 'epoch': 0.28991596638655465}
>>> 2025-09-10 21:39:17,825 - INFO - >>> {'loss': 2.4959, 'grad_norm': 0.5534709692001343, 'learning_rate': 1.9982091269173686e-05, 'epoch': 0.29411764705882354}
>>> 2025-09-10 21:39:20,841 - INFO - >>> {'loss': 2.442, 'grad_norm': 0.6008668541908264, 'learning_rate': 1.9981560831924142e-05, 'epoch': 0.29831932773109243}
>>> 2025-09-10 21:39:24,612 - INFO - >>> {'loss': 2.5218, 'grad_norm': 0.5615116953849792, 'learning_rate': 1.9981022660658368e-05, 'epoch': 0.3025210084033613}
>>> 2025-09-10 21:39:28,304 - INFO - >>> {'loss': 2.304, 'grad_norm': 0.5007149577140808, 'learning_rate': 1.998047675579335e-05, 'epoch': 0.3067226890756303}
>>> 2025-09-10 21:39:32,026 - INFO - >>> {'loss': 2.4645, 'grad_norm': 0.5595803260803223, 'learning_rate': 1.9979923117752075e-05, 'epoch': 0.31092436974789917}
>>> 2025-09-10 21:39:35,037 - INFO - >>> {'loss': 2.3493, 'grad_norm': 0.6061548590660095, 'learning_rate': 1.997936174696352e-05, 'epoch': 0.31512605042016806}
>>> 2025-09-10 21:39:38,817 - INFO - >>> {'loss': 2.3507, 'grad_norm': 0.5707679390907288, 'learning_rate': 1.9978792643862646e-05, 'epoch': 0.31932773109243695}
>>> 2025-09-10 21:39:42,421 - INFO - >>> {'loss': 2.3423, 'grad_norm': 0.52519291639328, 'learning_rate': 1.9978215808890412e-05, 'epoch': 0.3235294117647059}
>>> 2025-09-10 21:39:46,002 - INFO - >>> {'loss': 2.4392, 'grad_norm': 0.541126012802124, 'learning_rate': 1.9977631242493774e-05, 'epoch': 0.3277310924369748}
>>> 2025-09-10 21:39:49,356 - INFO - >>> {'loss': 2.367, 'grad_norm': 0.6082859039306641, 'learning_rate': 1.9977038945125664e-05, 'epoch': 0.3319327731092437}
>>> 2025-09-10 21:39:52,618 - INFO - >>> {'loss': 2.3396, 'grad_norm': 0.6032472848892212, 'learning_rate': 1.9976438917245017e-05, 'epoch': 0.33613445378151263}
>>> 2025-09-10 21:39:56,616 - INFO - >>> {'loss': 2.3874, 'grad_norm': 0.5567066073417664, 'learning_rate': 1.997583115931675e-05, 'epoch': 0.3403361344537815}
>>> 2025-09-10 21:40:00,592 - INFO - >>> {'loss': 2.3603, 'grad_norm': 0.552821934223175, 'learning_rate': 1.9975215671811777e-05, 'epoch': 0.3445378151260504}
>>> 2025-09-10 21:40:03,936 - INFO - >>> {'loss': 2.3655, 'grad_norm': 0.5859478116035461, 'learning_rate': 1.9974592455206986e-05, 'epoch': 0.3487394957983193}
>>> 2025-09-10 21:40:07,171 - INFO - >>> {'loss': 2.4078, 'grad_norm': 0.5592656135559082, 'learning_rate': 1.997396150998528e-05, 'epoch': 0.35294117647058826}
>>> 2025-09-10 21:40:10,822 - INFO - >>> {'loss': 2.5056, 'grad_norm': 0.5812514424324036, 'learning_rate': 1.9973322836635517e-05, 'epoch': 0.35714285714285715}
>>> 2025-09-10 21:40:14,793 - INFO - >>> {'loss': 2.3632, 'grad_norm': 0.5675286054611206, 'learning_rate': 1.9972676435652572e-05, 'epoch': 0.36134453781512604}
>>> 2025-09-10 21:40:18,779 - INFO - >>> {'loss': 2.427, 'grad_norm': 0.6378394961357117, 'learning_rate': 1.9972022307537295e-05, 'epoch': 0.36554621848739494}
>>> 2025-09-10 21:40:22,176 - INFO - >>> {'loss': 2.4153, 'grad_norm': 0.6052626371383667, 'learning_rate': 1.9971360452796523e-05, 'epoch': 0.3697478991596639}
>>> 2025-09-10 21:40:25,597 - INFO - >>> {'loss': 2.3031, 'grad_norm': 0.5796220898628235, 'learning_rate': 1.9970690871943076e-05, 'epoch': 0.3739495798319328}
>>> 2025-09-10 21:40:29,563 - INFO - >>> {'loss': 2.3884, 'grad_norm': 0.5886850357055664, 'learning_rate': 1.9970013565495776e-05, 'epoch': 0.37815126050420167}
>>> 2025-09-10 21:40:33,293 - INFO - >>> {'loss': 2.3081, 'grad_norm': 0.5954414010047913, 'learning_rate': 1.996932853397941e-05, 'epoch': 0.38235294117647056}
>>> 2025-09-10 21:40:36,121 - INFO - >>> {'loss': 2.3077, 'grad_norm': 0.5837258100509644, 'learning_rate': 1.996863577792477e-05, 'epoch': 0.3865546218487395}
>>> 2025-09-10 21:40:39,139 - INFO - >>> {'loss': 2.3544, 'grad_norm': 0.6119072437286377, 'learning_rate': 1.9967935297868614e-05, 'epoch': 0.3907563025210084}
>>> 2025-09-10 21:40:42,577 - INFO - >>> {'loss': 2.4351, 'grad_norm': 0.6090976595878601, 'learning_rate': 1.996722709435371e-05, 'epoch': 0.3949579831932773}
>>> 2025-09-10 21:40:46,402 - INFO - >>> {'loss': 2.386, 'grad_norm': 0.5302468538284302, 'learning_rate': 1.996651116792878e-05, 'epoch': 0.39915966386554624}
>>> 2025-09-10 21:40:49,791 - INFO - >>> {'loss': 2.3285, 'grad_norm': 0.5451772212982178, 'learning_rate': 1.9965787519148552e-05, 'epoch': 0.40336134453781514}
>>> 2025-09-10 21:40:53,907 - INFO - >>> {'loss': 2.3884, 'grad_norm': 0.5832557082176208, 'learning_rate': 1.9965056148573735e-05, 'epoch': 0.40756302521008403}
>>> 2025-09-10 21:40:57,155 - INFO - >>> {'loss': 2.3168, 'grad_norm': 0.60252845287323, 'learning_rate': 1.9964317056771016e-05, 'epoch': 0.4117647058823529}
>>> 2025-09-10 21:41:00,713 - INFO - >>> {'loss': 2.2805, 'grad_norm': 0.5534393787384033, 'learning_rate': 1.9963570244313058e-05, 'epoch': 0.41596638655462187}
>>> 2025-09-10 21:41:04,548 - INFO - >>> {'loss': 2.3346, 'grad_norm': 0.5389023423194885, 'learning_rate': 1.9962815711778525e-05, 'epoch': 0.42016806722689076}
>>> 2025-09-10 21:41:08,632 - INFO - >>> {'loss': 2.3892, 'grad_norm': 0.537551760673523, 'learning_rate': 1.9962053459752043e-05, 'epoch': 0.42436974789915966}
>>> 2025-09-10 21:41:11,890 - INFO - >>> {'loss': 2.3534, 'grad_norm': 0.6179515719413757, 'learning_rate': 1.9961283488824233e-05, 'epoch': 0.42857142857142855}
>>> 2025-09-10 21:41:15,983 - INFO - >>> {'loss': 2.4129, 'grad_norm': 0.5357309579849243, 'learning_rate': 1.9960505799591686e-05, 'epoch': 0.4327731092436975}
>>> 2025-09-10 21:41:19,436 - INFO - >>> {'loss': 2.2735, 'grad_norm': 0.5501484870910645, 'learning_rate': 1.995972039265699e-05, 'epoch': 0.4369747899159664}
>>> 2025-09-10 21:41:22,648 - INFO - >>> {'loss': 2.4676, 'grad_norm': 0.6285449266433716, 'learning_rate': 1.995892726862869e-05, 'epoch': 0.4411764705882353}
>>> 2025-09-10 21:41:26,614 - INFO - >>> {'loss': 2.4109, 'grad_norm': 0.5991584062576294, 'learning_rate': 1.995812642812133e-05, 'epoch': 0.44537815126050423}
>>> 2025-09-10 21:41:29,959 - INFO - >>> {'loss': 2.3646, 'grad_norm': 0.5313077569007874, 'learning_rate': 1.9957317871755423e-05, 'epoch': 0.4495798319327731}
>>> 2025-09-10 21:41:33,888 - INFO - >>> {'loss': 2.3298, 'grad_norm': 0.5759447813034058, 'learning_rate': 1.995650160015746e-05, 'epoch': 0.453781512605042}
>>> 2025-09-10 21:41:37,144 - INFO - >>> {'loss': 2.4479, 'grad_norm': 0.5160660147666931, 'learning_rate': 1.9955677613959922e-05, 'epoch': 0.4579831932773109}
>>> 2025-09-10 21:41:40,386 - INFO - >>> {'loss': 2.3968, 'grad_norm': 0.5720430612564087, 'learning_rate': 1.995484591380125e-05, 'epoch': 0.46218487394957986}
>>> 2025-09-10 21:41:43,415 - INFO - >>> {'loss': 2.2803, 'grad_norm': 0.6272074580192566, 'learning_rate': 1.9954006500325874e-05, 'epoch': 0.46638655462184875}
>>> 2025-09-10 21:41:47,342 - INFO - >>> {'loss': 2.3652, 'grad_norm': 0.50071120262146, 'learning_rate': 1.9953159374184196e-05, 'epoch': 0.47058823529411764}
>>> 2025-09-10 21:41:50,268 - INFO - >>> {'loss': 2.3158, 'grad_norm': 0.6232668161392212, 'learning_rate': 1.99523045360326e-05, 'epoch': 0.47478991596638653}
>>> 2025-09-10 21:41:54,142 - INFO - >>> {'loss': 2.3718, 'grad_norm': 0.6233944892883301, 'learning_rate': 1.995144198653343e-05, 'epoch': 0.4789915966386555}
>>> 2025-09-10 21:41:58,150 - INFO - >>> {'loss': 2.336, 'grad_norm': 0.5351839661598206, 'learning_rate': 1.9950571726355027e-05, 'epoch': 0.4831932773109244}
>>> 2025-09-10 21:42:01,898 - INFO - >>> {'loss': 2.4407, 'grad_norm': 0.5433918237686157, 'learning_rate': 1.9949693756171683e-05, 'epoch': 0.48739495798319327}
>>> 2025-09-10 21:42:04,778 - INFO - >>> {'loss': 2.3963, 'grad_norm': 0.6335973143577576, 'learning_rate': 1.9948808076663688e-05, 'epoch': 0.49159663865546216}
>>> 2025-09-10 21:42:08,544 - INFO - >>> {'loss': 2.3759, 'grad_norm': 0.5496375560760498, 'learning_rate': 1.9947914688517288e-05, 'epoch': 0.4957983193277311}
>>> 2025-09-10 21:42:11,426 - INFO - >>> {'loss': 2.3162, 'grad_norm': 0.6780466437339783, 'learning_rate': 1.9947013592424706e-05, 'epoch': 0.5}
>>> 2025-09-10 21:42:14,421 - INFO - >>> {'loss': 2.2785, 'grad_norm': 0.6214895844459534, 'learning_rate': 1.9946104789084136e-05, 'epoch': 0.5042016806722689}
>>> 2025-09-10 21:42:17,310 - INFO - >>> {'loss': 2.3806, 'grad_norm': 0.598643958568573, 'learning_rate': 1.9945188279199756e-05, 'epoch': 0.5084033613445378}
>>> 2025-09-10 21:42:21,315 - INFO - >>> {'loss': 2.3609, 'grad_norm': 0.5760445594787598, 'learning_rate': 1.9944264063481697e-05, 'epoch': 0.5126050420168067}
>>> 2025-09-10 21:42:24,332 - INFO - >>> {'loss': 2.2581, 'grad_norm': 0.5762882828712463, 'learning_rate': 1.994333214264607e-05, 'epoch': 0.5168067226890757}
>>> 2025-09-10 21:42:28,055 - INFO - >>> {'loss': 2.3134, 'grad_norm': 0.5716032385826111, 'learning_rate': 1.9942392517414964e-05, 'epoch': 0.5210084033613446}
>>> 2025-09-10 21:42:32,025 - INFO - >>> {'loss': 2.3478, 'grad_norm': 0.5588255524635315, 'learning_rate': 1.994144518851642e-05, 'epoch': 0.5252100840336135}
>>> 2025-09-10 21:42:35,455 - INFO - >>> {'loss': 2.3096, 'grad_norm': 0.5678391456604004, 'learning_rate': 1.994049015668446e-05, 'epoch': 0.5294117647058824}
>>> 2025-09-10 21:42:38,663 - INFO - >>> {'loss': 2.3523, 'grad_norm': 0.590183436870575, 'learning_rate': 1.993952742265907e-05, 'epoch': 0.5336134453781513}
>>> 2025-09-10 21:42:42,262 - INFO - >>> {'loss': 2.3504, 'grad_norm': 0.6466556191444397, 'learning_rate': 1.993855698718621e-05, 'epoch': 0.5378151260504201}
>>> 2025-09-10 21:42:45,602 - INFO - >>> {'loss': 2.3159, 'grad_norm': 0.6065594553947449, 'learning_rate': 1.9937578851017797e-05, 'epoch': 0.542016806722689}
>>> 2025-09-10 21:42:49,013 - INFO - >>> {'loss': 2.4408, 'grad_norm': 0.6085406541824341, 'learning_rate': 1.9936593014911726e-05, 'epoch': 0.5462184873949579}
>>> 2025-09-10 21:42:52,985 - INFO - >>> {'loss': 2.4476, 'grad_norm': 0.5758613348007202, 'learning_rate': 1.993559947963185e-05, 'epoch': 0.5504201680672269}
>>> 2025-09-10 21:42:56,569 - INFO - >>> {'loss': 2.3436, 'grad_norm': 0.595117449760437, 'learning_rate': 1.993459824594799e-05, 'epoch': 0.5546218487394958}
>>> 2025-09-10 21:43:00,402 - INFO - >>> {'loss': 2.3595, 'grad_norm': 0.5514702796936035, 'learning_rate': 1.9933589314635936e-05, 'epoch': 0.5588235294117647}
>>> 2025-09-10 21:43:03,650 - INFO - >>> {'loss': 2.3972, 'grad_norm': 0.6396974325180054, 'learning_rate': 1.993257268647743e-05, 'epoch': 0.5630252100840336}
>>> 2025-09-10 21:43:07,614 - INFO - >>> {'loss': 2.3444, 'grad_norm': 0.5674092769622803, 'learning_rate': 1.9931548362260198e-05, 'epoch': 0.5672268907563025}
>>> 2025-09-10 21:43:10,553 - INFO - >>> {'loss': 2.3873, 'grad_norm': 0.6622957587242126, 'learning_rate': 1.993051634277791e-05, 'epoch': 0.5714285714285714}
>>> 2025-09-10 21:43:13,868 - INFO - >>> {'loss': 2.2887, 'grad_norm': 0.5736569762229919, 'learning_rate': 1.9929476628830206e-05, 'epoch': 0.5756302521008403}
>>> 2025-09-10 21:43:16,901 - INFO - >>> {'loss': 2.2612, 'grad_norm': 0.6238332986831665, 'learning_rate': 1.9928429221222688e-05, 'epoch': 0.5798319327731093}
>>> 2025-09-10 21:43:20,303 - INFO - >>> {'loss': 2.3773, 'grad_norm': 0.5582574009895325, 'learning_rate': 1.9927374120766926e-05, 'epoch': 0.5840336134453782}
>>> 2025-09-10 21:43:23,650 - INFO - >>> {'loss': 2.3695, 'grad_norm': 0.5647401213645935, 'learning_rate': 1.9926311328280432e-05, 'epoch': 0.5882352941176471}
>>> 2025-09-10 21:43:27,348 - INFO - >>> {'loss': 2.4402, 'grad_norm': 0.6077022552490234, 'learning_rate': 1.99252408445867e-05, 'epoch': 0.592436974789916}
>>> 2025-09-10 21:43:30,334 - INFO - >>> {'loss': 2.2853, 'grad_norm': 0.5846941471099854, 'learning_rate': 1.9924162670515165e-05, 'epoch': 0.5966386554621849}
>>> 2025-09-10 21:43:33,355 - INFO - >>> {'loss': 2.3877, 'grad_norm': 0.6236441731452942, 'learning_rate': 1.9923076806901238e-05, 'epoch': 0.6008403361344538}
>>> 2025-09-10 21:43:36,861 - INFO - >>> {'loss': 2.2472, 'grad_norm': 0.5812708139419556, 'learning_rate': 1.9921983254586276e-05, 'epoch': 0.6050420168067226}
>>> 2025-09-10 21:43:40,132 - INFO - >>> {'loss': 2.3285, 'grad_norm': 0.5460807681083679, 'learning_rate': 1.9920882014417594e-05, 'epoch': 0.6092436974789915}
>>> 2025-09-10 21:43:44,077 - INFO - >>> {'loss': 2.3118, 'grad_norm': 0.5603464841842651, 'learning_rate': 1.9919773087248464e-05, 'epoch': 0.6134453781512605}
>>> 2025-09-10 21:43:47,269 - INFO - >>> {'loss': 2.3554, 'grad_norm': 0.6279476284980774, 'learning_rate': 1.991865647393812e-05, 'epoch': 0.6176470588235294}
>>> 2025-09-10 21:43:50,384 - INFO - >>> {'loss': 2.3626, 'grad_norm': 0.6205427646636963, 'learning_rate': 1.991753217535175e-05, 'epoch': 0.6218487394957983}
>>> 2025-09-10 21:43:53,583 - INFO - >>> {'loss': 2.2875, 'grad_norm': 0.6053813695907593, 'learning_rate': 1.9916400192360492e-05, 'epoch': 0.6260504201680672}
>>> 2025-09-10 21:43:56,838 - INFO - >>> {'loss': 2.4236, 'grad_norm': 0.5786722302436829, 'learning_rate': 1.991526052584144e-05, 'epoch': 0.6302521008403361}
>>> 2025-09-10 21:43:59,823 - INFO - >>> {'loss': 2.3441, 'grad_norm': 0.6446066498756409, 'learning_rate': 1.9914113176677648e-05, 'epoch': 0.634453781512605}
>>> 2025-09-10 21:44:03,476 - INFO - >>> {'loss': 2.2681, 'grad_norm': 0.5738996863365173, 'learning_rate': 1.991295814575811e-05, 'epoch': 0.6386554621848739}
>>> 2025-09-10 21:44:06,827 - INFO - >>> {'loss': 2.3307, 'grad_norm': 0.5643746256828308, 'learning_rate': 1.991179543397778e-05, 'epoch': 0.6428571428571429}
>>> 2025-09-10 21:44:10,599 - INFO - >>> {'loss': 2.239, 'grad_norm': 0.6346470713615417, 'learning_rate': 1.9910625042237563e-05, 'epoch': 0.6470588235294118}
>>> 2025-09-10 21:44:13,579 - INFO - >>> {'loss': 2.2919, 'grad_norm': 0.5957013964653015, 'learning_rate': 1.9909446971444315e-05, 'epoch': 0.6512605042016807}
>>> 2025-09-10 21:44:16,126 - INFO - >>> {'loss': 2.3824, 'grad_norm': 0.6577438116073608, 'learning_rate': 1.9908261222510843e-05, 'epoch': 0.6554621848739496}
>>> 2025-09-10 21:44:19,649 - INFO - >>> {'loss': 2.3621, 'grad_norm': 0.5716233849525452, 'learning_rate': 1.9907067796355895e-05, 'epoch': 0.6596638655462185}
>>> 2025-09-10 21:44:23,505 - INFO - >>> {'loss': 2.3148, 'grad_norm': 0.5539534091949463, 'learning_rate': 1.9905866693904182e-05, 'epoch': 0.6638655462184874}
>>> 2025-09-10 21:44:27,496 - INFO - >>> {'loss': 2.3543, 'grad_norm': 0.6246611475944519, 'learning_rate': 1.9904657916086348e-05, 'epoch': 0.6680672268907563}
>>> 2025-09-10 21:44:30,833 - INFO - >>> {'loss': 2.3639, 'grad_norm': 0.6415057182312012, 'learning_rate': 1.990344146383899e-05, 'epoch': 0.6722689075630253}
>>> 2025-09-10 21:44:34,034 - INFO - >>> {'loss': 2.263, 'grad_norm': 0.5754451155662537, 'learning_rate': 1.9902217338104663e-05, 'epoch': 0.6764705882352942}
>>> 2025-09-10 21:44:37,323 - INFO - >>> {'loss': 2.3198, 'grad_norm': 0.572446882724762, 'learning_rate': 1.9900985539831845e-05, 'epoch': 0.680672268907563}
>>> 2025-09-10 21:44:40,384 - INFO - >>> {'loss': 2.3048, 'grad_norm': 0.6708996295928955, 'learning_rate': 1.9899746069974975e-05, 'epoch': 0.6848739495798319}
>>> 2025-09-10 21:44:43,486 - INFO - >>> {'loss': 2.4449, 'grad_norm': 0.654991865158081, 'learning_rate': 1.989849892949443e-05, 'epoch': 0.6890756302521008}
>>> 2025-09-10 21:44:47,281 - INFO - >>> {'loss': 2.3736, 'grad_norm': 0.5520665645599365, 'learning_rate': 1.9897244119356534e-05, 'epoch': 0.6932773109243697}
>>> 2025-09-10 21:44:50,498 - INFO - >>> {'loss': 2.362, 'grad_norm': 0.6125948429107666, 'learning_rate': 1.9895981640533554e-05, 'epoch': 0.6974789915966386}
>>> 2025-09-10 21:44:54,276 - INFO - >>> {'loss': 2.3148, 'grad_norm': 0.6329863667488098, 'learning_rate': 1.9894711494003697e-05, 'epoch': 0.7016806722689075}
>>> 2025-09-10 21:44:57,560 - INFO - >>> {'loss': 2.3721, 'grad_norm': 0.6125743985176086, 'learning_rate': 1.9893433680751105e-05, 'epoch': 0.7058823529411765}
>>> 2025-09-10 21:45:01,081 - INFO - >>> {'loss': 2.3473, 'grad_norm': 0.7375991344451904, 'learning_rate': 1.9892148201765867e-05, 'epoch': 0.7100840336134454}
>>> 2025-09-10 21:45:05,096 - INFO - >>> {'loss': 2.4216, 'grad_norm': 0.5830665826797485, 'learning_rate': 1.989085505804402e-05, 'epoch': 0.7142857142857143}
>>> 2025-09-10 21:45:08,992 - INFO - >>> {'loss': 2.2801, 'grad_norm': 0.5331107974052429, 'learning_rate': 1.9889554250587526e-05, 'epoch': 0.7184873949579832}
>>> 2025-09-10 21:45:12,595 - INFO - >>> {'loss': 2.2363, 'grad_norm': 0.595312774181366, 'learning_rate': 1.988824578040429e-05, 'epoch': 0.7226890756302521}
>>> 2025-09-10 21:45:16,208 - INFO - >>> {'loss': 2.4241, 'grad_norm': 0.6215507388114929, 'learning_rate': 1.9886929648508147e-05, 'epoch': 0.726890756302521}
>>> 2025-09-10 21:45:19,412 - INFO - >>> {'loss': 2.2824, 'grad_norm': 0.5874890089035034, 'learning_rate': 1.9885605855918887e-05, 'epoch': 0.7310924369747899}
>>> 2025-09-10 21:45:22,627 - INFO - >>> {'loss': 2.3574, 'grad_norm': 0.6017706394195557, 'learning_rate': 1.988427440366222e-05, 'epoch': 0.7352941176470589}
>>> 2025-09-10 21:45:25,507 - INFO - >>> {'loss': 2.2938, 'grad_norm': 0.6369587182998657, 'learning_rate': 1.9882935292769797e-05, 'epoch': 0.7394957983193278}
>>> 2025-09-10 21:45:28,673 - INFO - >>> {'loss': 2.3415, 'grad_norm': 0.6942248940467834, 'learning_rate': 1.98815885242792e-05, 'epoch': 0.7436974789915967}
>>> 2025-09-10 21:45:32,311 - INFO - >>> {'loss': 2.3963, 'grad_norm': 0.5567525029182434, 'learning_rate': 1.9880234099233948e-05, 'epoch': 0.7478991596638656}
>>> 2025-09-10 21:45:35,312 - INFO - >>> {'loss': 2.259, 'grad_norm': 0.6515668034553528, 'learning_rate': 1.987887201868349e-05, 'epoch': 0.7521008403361344}
>>> 2025-09-10 21:45:38,537 - INFO - >>> {'loss': 2.3466, 'grad_norm': 0.5809314846992493, 'learning_rate': 1.9877502283683206e-05, 'epoch': 0.7563025210084033}
>>> 2025-09-10 21:45:42,284 - INFO - >>> {'loss': 2.4783, 'grad_norm': 0.5494795441627502, 'learning_rate': 1.987612489529441e-05, 'epoch': 0.7605042016806722}
>>> 2025-09-10 21:45:45,727 - INFO - >>> {'loss': 2.3351, 'grad_norm': 0.628356397151947, 'learning_rate': 1.9874739854584342e-05, 'epoch': 0.7647058823529411}
>>> 2025-09-10 21:45:49,640 - INFO - >>> {'loss': 2.3874, 'grad_norm': 0.5655295252799988, 'learning_rate': 1.9873347162626176e-05, 'epoch': 0.7689075630252101}
>>> 2025-09-10 21:45:53,170 - INFO - >>> {'loss': 2.1616, 'grad_norm': 0.6393876075744629, 'learning_rate': 1.987194682049901e-05, 'epoch': 0.773109243697479}
>>> 2025-09-10 21:45:57,240 - INFO - >>> {'loss': 2.223, 'grad_norm': 0.6231682300567627, 'learning_rate': 1.9870538829287874e-05, 'epoch': 0.7773109243697479}
>>> 2025-09-10 21:46:00,781 - INFO - >>> {'loss': 2.3832, 'grad_norm': 0.585738480091095, 'learning_rate': 1.986912319008372e-05, 'epoch': 0.7815126050420168}
>>> 2025-09-10 21:46:04,425 - INFO - >>> {'loss': 2.3825, 'grad_norm': 0.6470069289207458, 'learning_rate': 1.986769990398343e-05, 'epoch': 0.7857142857142857}
>>> 2025-09-10 21:46:07,280 - INFO - >>> {'loss': 2.2986, 'grad_norm': 0.6471642255783081, 'learning_rate': 1.986626897208981e-05, 'epoch': 0.7899159663865546}
>>> 2025-09-10 21:46:11,201 - INFO - >>> {'loss': 2.3189, 'grad_norm': 0.6135426759719849, 'learning_rate': 1.9864830395511588e-05, 'epoch': 0.7941176470588235}
>>> 2025-09-10 21:46:14,419 - INFO - >>> {'loss': 2.3858, 'grad_norm': 0.5931560397148132, 'learning_rate': 1.9863384175363413e-05, 'epoch': 0.7983193277310925}
>>> 2025-09-10 21:46:17,730 - INFO - >>> {'loss': 2.3449, 'grad_norm': 0.6263277530670166, 'learning_rate': 1.9861930312765865e-05, 'epoch': 0.8025210084033614}
>>> 2025-09-10 21:46:21,376 - INFO - >>> {'loss': 2.347, 'grad_norm': 0.6255541443824768, 'learning_rate': 1.9860468808845442e-05, 'epoch': 0.8067226890756303}
>>> 2025-09-10 21:46:25,392 - INFO - >>> {'loss': 2.3268, 'grad_norm': 0.612591564655304, 'learning_rate': 1.985899966473456e-05, 'epoch': 0.8109243697478992}
>>> 2025-09-10 21:46:28,676 - INFO - >>> {'loss': 2.4292, 'grad_norm': 0.6645586490631104, 'learning_rate': 1.9857522881571554e-05, 'epoch': 0.8151260504201681}
>>> 2025-09-10 21:46:32,442 - INFO - >>> {'loss': 2.4223, 'grad_norm': 0.653092622756958, 'learning_rate': 1.985603846050068e-05, 'epoch': 0.819327731092437}
>>> 2025-09-10 21:46:36,168 - INFO - >>> {'loss': 2.4652, 'grad_norm': 0.6206228137016296, 'learning_rate': 1.9854546402672118e-05, 'epoch': 0.8235294117647058}
>>> 2025-09-10 21:46:39,588 - INFO - >>> {'loss': 2.4185, 'grad_norm': 0.6426772475242615, 'learning_rate': 1.9853046709241958e-05, 'epoch': 0.8277310924369747}
>>> 2025-09-10 21:46:43,197 - INFO - >>> {'loss': 2.2269, 'grad_norm': 0.6103796362876892, 'learning_rate': 1.98515393813722e-05, 'epoch': 0.8319327731092437}
>>> 2025-09-10 21:46:47,006 - INFO - >>> {'loss': 2.2714, 'grad_norm': 0.5695846676826477, 'learning_rate': 1.985002442023078e-05, 'epoch': 0.8361344537815126}
>>> 2025-09-10 21:46:50,497 - INFO - >>> {'loss': 2.4328, 'grad_norm': 0.5588126182556152, 'learning_rate': 1.9848501826991525e-05, 'epoch': 0.8403361344537815}
>>> 2025-09-10 21:46:54,309 - INFO - >>> {'loss': 2.3443, 'grad_norm': 0.6369994878768921, 'learning_rate': 1.9846971602834194e-05, 'epoch': 0.8445378151260504}
>>> 2025-09-10 21:46:57,665 - INFO - >>> {'loss': 2.4109, 'grad_norm': 0.6488515138626099, 'learning_rate': 1.984543374894445e-05, 'epoch': 0.8487394957983193}
>>> 2025-09-10 21:47:01,355 - INFO - >>> {'loss': 2.3056, 'grad_norm': 0.6341816186904907, 'learning_rate': 1.984388826651386e-05, 'epoch': 0.8529411764705882}
>>> 2025-09-10 21:47:04,383 - INFO - >>> {'loss': 2.2408, 'grad_norm': 0.7076416611671448, 'learning_rate': 1.9842335156739924e-05, 'epoch': 0.8571428571428571}
>>> 2025-09-10 21:47:08,067 - INFO - >>> {'loss': 2.3182, 'grad_norm': 0.6248112916946411, 'learning_rate': 1.9840774420826033e-05, 'epoch': 0.8613445378151261}
>>> 2025-09-10 21:47:11,292 - INFO - >>> {'loss': 2.1652, 'grad_norm': 0.6115483045578003, 'learning_rate': 1.9839206059981487e-05, 'epoch': 0.865546218487395}
>>> 2025-09-10 21:47:14,759 - INFO - >>> {'loss': 2.2434, 'grad_norm': 0.618973433971405, 'learning_rate': 1.9837630075421508e-05, 'epoch': 0.8697478991596639}
>>> 2025-09-10 21:47:18,707 - INFO - >>> {'loss': 2.3843, 'grad_norm': 0.6595507860183716, 'learning_rate': 1.9836046468367214e-05, 'epoch': 0.8739495798319328}
>>> 2025-09-10 21:47:22,630 - INFO - >>> {'loss': 2.2525, 'grad_norm': 0.6033240556716919, 'learning_rate': 1.9834455240045625e-05, 'epoch': 0.8781512605042017}
>>> 2025-09-10 21:47:26,437 - INFO - >>> {'loss': 2.3889, 'grad_norm': 0.6370041370391846, 'learning_rate': 1.9832856391689686e-05, 'epoch': 0.8823529411764706}
>>> 2025-09-10 21:47:30,182 - INFO - >>> {'loss': 2.2943, 'grad_norm': 0.7145622968673706, 'learning_rate': 1.9831249924538222e-05, 'epoch': 0.8865546218487395}
>>> 2025-09-10 21:47:33,223 - INFO - >>> {'loss': 2.3521, 'grad_norm': 0.6498042941093445, 'learning_rate': 1.982963583983598e-05, 'epoch': 0.8907563025210085}
>>> 2025-09-10 21:47:36,430 - INFO - >>> {'loss': 2.3632, 'grad_norm': 0.6811874508857727, 'learning_rate': 1.9828014138833595e-05, 'epoch': 0.8949579831932774}
>>> 2025-09-10 21:47:39,510 - INFO - >>> {'loss': 2.3466, 'grad_norm': 0.6365419626235962, 'learning_rate': 1.9826384822787615e-05, 'epoch': 0.8991596638655462}
>>> 2025-09-10 21:47:42,797 - INFO - >>> {'loss': 2.4041, 'grad_norm': 0.605036199092865, 'learning_rate': 1.9824747892960482e-05, 'epoch': 0.9033613445378151}
>>> 2025-09-10 21:47:45,856 - INFO - >>> {'loss': 2.3436, 'grad_norm': 0.6222942471504211, 'learning_rate': 1.982310335062054e-05, 'epoch': 0.907563025210084}
>>> 2025-09-10 21:47:48,837 - INFO - >>> {'loss': 2.3455, 'grad_norm': 0.7104374170303345, 'learning_rate': 1.9821451197042028e-05, 'epoch': 0.9117647058823529}
>>> 2025-09-10 21:47:52,659 - INFO - >>> {'loss': 2.3674, 'grad_norm': 0.631704568862915, 'learning_rate': 1.9819791433505088e-05, 'epoch': 0.9159663865546218}
>>> 2025-09-10 21:47:55,661 - INFO - >>> {'loss': 2.3471, 'grad_norm': 0.6737807393074036, 'learning_rate': 1.981812406129575e-05, 'epoch': 0.9201680672268907}
>>> 2025-09-10 21:47:59,589 - INFO - >>> {'loss': 2.2914, 'grad_norm': 0.6239562630653381, 'learning_rate': 1.981644908170595e-05, 'epoch': 0.9243697478991597}
>>> 2025-09-10 21:48:02,864 - INFO - >>> {'loss': 2.3118, 'grad_norm': 0.6435682773590088, 'learning_rate': 1.981476649603351e-05, 'epoch': 0.9285714285714286}
>>> 2025-09-10 21:48:06,770 - INFO - >>> {'loss': 2.3392, 'grad_norm': 0.6917195320129395, 'learning_rate': 1.981307630558215e-05, 'epoch': 0.9327731092436975}
>>> 2025-09-10 21:48:10,082 - INFO - >>> {'loss': 2.3194, 'grad_norm': 0.6177275776863098, 'learning_rate': 1.9811378511661478e-05, 'epoch': 0.9369747899159664}
>>> 2025-09-10 21:48:13,014 - INFO - >>> {'loss': 2.2519, 'grad_norm': 0.7043333053588867, 'learning_rate': 1.9809673115587004e-05, 'epoch': 0.9411764705882353}
>>> 2025-09-10 21:48:15,858 - INFO - >>> {'loss': 2.2745, 'grad_norm': 0.6618670225143433, 'learning_rate': 1.980796011868011e-05, 'epoch': 0.9453781512605042}
>>> 2025-09-10 21:48:19,177 - INFO - >>> {'loss': 2.2741, 'grad_norm': 0.6645126938819885, 'learning_rate': 1.980623952226808e-05, 'epoch': 0.9495798319327731}
>>> 2025-09-10 21:48:22,574 - INFO - >>> {'loss': 2.2718, 'grad_norm': 0.7796646952629089, 'learning_rate': 1.9804511327684094e-05, 'epoch': 0.9537815126050421}
>>> 2025-09-10 21:48:25,680 - INFO - >>> {'loss': 2.2346, 'grad_norm': 0.6449596285820007, 'learning_rate': 1.98027755362672e-05, 'epoch': 0.957983193277311}
>>> 2025-09-10 21:48:28,923 - INFO - >>> {'loss': 2.3818, 'grad_norm': 0.7294785976409912, 'learning_rate': 1.9801032149362344e-05, 'epoch': 0.9621848739495799}
>>> 2025-09-10 21:48:32,531 - INFO - >>> {'loss': 2.3489, 'grad_norm': 0.7629554867744446, 'learning_rate': 1.9799281168320358e-05, 'epoch': 0.9663865546218487}
>>> 2025-09-10 21:48:35,666 - INFO - >>> {'loss': 2.3983, 'grad_norm': 0.6255521774291992, 'learning_rate': 1.9797522594497953e-05, 'epoch': 0.9705882352941176}
>>> 2025-09-10 21:48:38,646 - INFO - >>> {'loss': 2.2627, 'grad_norm': 0.6772466897964478, 'learning_rate': 1.9795756429257725e-05, 'epoch': 0.9747899159663865}
>>> 2025-09-10 21:48:41,872 - INFO - >>> {'loss': 2.2775, 'grad_norm': 0.6742714643478394, 'learning_rate': 1.9793982673968153e-05, 'epoch': 0.9789915966386554}
>>> 2025-09-10 21:48:45,710 - INFO - >>> {'loss': 2.3323, 'grad_norm': 0.6845072507858276, 'learning_rate': 1.9792201330003595e-05, 'epoch': 0.9831932773109243}
>>> 2025-09-10 21:48:49,771 - INFO - >>> {'loss': 2.3536, 'grad_norm': 0.5972249507904053, 'learning_rate': 1.979041239874429e-05, 'epoch': 0.9873949579831933}
>>> 2025-09-10 21:48:53,313 - INFO - >>> {'loss': 2.3424, 'grad_norm': 0.6204319596290588, 'learning_rate': 1.9788615881576364e-05, 'epoch': 0.9915966386554622}
>>> 2025-09-10 21:48:56,665 - INFO - >>> {'loss': 2.3486, 'grad_norm': 0.6212671399116516, 'learning_rate': 1.9786811779891798e-05, 'epoch': 0.9957983193277311}
>>> 2025-09-10 21:48:59,957 - INFO - >>> {'loss': 2.3921, 'grad_norm': 0.6641868948936462, 'learning_rate': 1.978500009508848e-05, 'epoch': 1.0}
>>> 2025-09-10 21:49:03,188 - INFO - >>> {'loss': 2.2974, 'grad_norm': 0.669854998588562, 'learning_rate': 1.9783180828570152e-05, 'epoch': 1.004201680672269}
>>> 2025-09-10 21:49:06,993 - INFO - >>> {'loss': 2.3288, 'grad_norm': 0.6436747312545776, 'learning_rate': 1.9781353981746437e-05, 'epoch': 1.0084033613445378}
>>> 2025-09-10 21:49:10,207 - INFO - >>> {'loss': 2.2161, 'grad_norm': 0.7254784107208252, 'learning_rate': 1.9779519556032828e-05, 'epoch': 1.0126050420168067}
>>> 2025-09-10 21:49:13,574 - INFO - >>> {'loss': 2.3464, 'grad_norm': 0.5973689556121826, 'learning_rate': 1.97776775528507e-05, 'epoch': 1.0168067226890756}
>>> 2025-09-10 21:49:16,505 - INFO - >>> {'loss': 2.3257, 'grad_norm': 0.5930591225624084, 'learning_rate': 1.9775827973627288e-05, 'epoch': 1.0210084033613445}
>>> 2025-09-10 21:49:19,857 - INFO - >>> {'loss': 2.3168, 'grad_norm': 0.7281045317649841, 'learning_rate': 1.9773970819795704e-05, 'epoch': 1.0252100840336134}
>>> 2025-09-10 21:49:23,689 - INFO - >>> {'loss': 2.2969, 'grad_norm': 0.6684903502464294, 'learning_rate': 1.977210609279493e-05, 'epoch': 1.0294117647058822}
>>> 2025-09-10 21:49:27,463 - INFO - >>> {'loss': 2.3863, 'grad_norm': 0.7589330077171326, 'learning_rate': 1.9770233794069806e-05, 'epoch': 1.0336134453781514}
>>> 2025-09-10 21:49:31,496 - INFO - >>> {'loss': 2.3498, 'grad_norm': 0.6692590713500977, 'learning_rate': 1.976835392507105e-05, 'epoch': 1.0378151260504203}
>>> 2025-09-10 21:49:34,911 - INFO - >>> {'loss': 2.3071, 'grad_norm': 0.7261987328529358, 'learning_rate': 1.9766466487255247e-05, 'epoch': 1.0420168067226891}
>>> 2025-09-10 21:49:38,734 - INFO - >>> {'loss': 2.4015, 'grad_norm': 0.6688446402549744, 'learning_rate': 1.9764571482084832e-05, 'epoch': 1.046218487394958}
>>> 2025-09-10 21:49:41,946 - INFO - >>> {'loss': 2.2292, 'grad_norm': 0.7192071676254272, 'learning_rate': 1.976266891102812e-05, 'epoch': 1.050420168067227}
>>> 2025-09-10 21:49:45,399 - INFO - >>> {'loss': 2.2874, 'grad_norm': 0.6381813287734985, 'learning_rate': 1.9760758775559275e-05, 'epoch': 1.0546218487394958}
>>> 2025-09-10 21:49:49,353 - INFO - >>> {'loss': 2.2418, 'grad_norm': 0.6382160782814026, 'learning_rate': 1.9758841077158325e-05, 'epoch': 1.0588235294117647}
>>> 2025-09-10 21:49:52,672 - INFO - >>> {'loss': 2.2996, 'grad_norm': 0.6366922855377197, 'learning_rate': 1.9756915817311173e-05, 'epoch': 1.0630252100840336}
>>> 2025-09-10 21:49:55,984 - INFO - >>> {'loss': 2.3607, 'grad_norm': 0.6835470199584961, 'learning_rate': 1.9754982997509557e-05, 'epoch': 1.0672268907563025}
>>> 2025-09-10 21:49:59,281 - INFO - >>> {'loss': 2.2995, 'grad_norm': 0.7219323515892029, 'learning_rate': 1.9753042619251094e-05, 'epoch': 1.0714285714285714}
>>> 2025-09-10 21:50:02,330 - INFO - >>> {'loss': 2.2148, 'grad_norm': 0.6700946688652039, 'learning_rate': 1.975109468403924e-05, 'epoch': 1.0756302521008403}
>>> 2025-09-10 21:50:05,720 - INFO - >>> {'loss': 2.293, 'grad_norm': 0.639032781124115, 'learning_rate': 1.9749139193383318e-05, 'epoch': 1.0798319327731092}
>>> 2025-09-10 21:50:09,372 - INFO - >>> {'loss': 2.2303, 'grad_norm': 0.6531884074211121, 'learning_rate': 1.97471761487985e-05, 'epoch': 1.084033613445378}
>>> 2025-09-10 21:50:12,699 - INFO - >>> {'loss': 2.3694, 'grad_norm': 0.6240989565849304, 'learning_rate': 1.9745205551805813e-05, 'epoch': 1.088235294117647}
>>> 2025-09-10 21:50:15,977 - INFO - >>> {'loss': 2.2972, 'grad_norm': 0.6633146405220032, 'learning_rate': 1.9743227403932135e-05, 'epoch': 1.092436974789916}
>>> 2025-09-10 21:50:20,031 - INFO - >>> {'loss': 2.4184, 'grad_norm': 0.7003659009933472, 'learning_rate': 1.97412417067102e-05, 'epoch': 1.096638655462185}
>>> 2025-09-10 21:50:23,657 - INFO - >>> {'loss': 2.3019, 'grad_norm': 0.6462277173995972, 'learning_rate': 1.9739248461678578e-05, 'epoch': 1.1008403361344539}
>>> 2025-09-10 21:50:26,553 - INFO - >>> {'loss': 2.2508, 'grad_norm': 0.7791843414306641, 'learning_rate': 1.9737247670381702e-05, 'epoch': 1.1050420168067228}
>>> 2025-09-10 21:50:30,115 - INFO - >>> {'loss': 2.3685, 'grad_norm': 0.6299949288368225, 'learning_rate': 1.973523933436984e-05, 'epoch': 1.1092436974789917}
>>> 2025-09-10 21:50:33,702 - INFO - >>> {'loss': 2.3378, 'grad_norm': 0.7079420685768127, 'learning_rate': 1.973322345519912e-05, 'epoch': 1.1134453781512605}
>>> 2025-09-10 21:50:37,206 - INFO - >>> {'loss': 2.3627, 'grad_norm': 0.73394376039505, 'learning_rate': 1.97312000344315e-05, 'epoch': 1.1176470588235294}
>>> 2025-09-10 21:50:40,928 - INFO - >>> {'loss': 2.3197, 'grad_norm': 0.7139759659767151, 'learning_rate': 1.972916907363479e-05, 'epoch': 1.1218487394957983}
>>> 2025-09-10 21:50:44,333 - INFO - >>> {'loss': 2.3322, 'grad_norm': 0.7065051794052124, 'learning_rate': 1.9727130574382642e-05, 'epoch': 1.1260504201680672}
>>> 2025-09-10 21:50:47,440 - INFO - >>> {'loss': 2.2354, 'grad_norm': 0.6761510372161865, 'learning_rate': 1.9725084538254542e-05, 'epoch': 1.1302521008403361}
>>> 2025-09-10 21:50:50,640 - INFO - >>> {'loss': 2.2175, 'grad_norm': 0.7554724812507629, 'learning_rate': 1.9723030966835823e-05, 'epoch': 1.134453781512605}
>>> 2025-09-10 21:50:54,351 - INFO - >>> {'loss': 2.3377, 'grad_norm': 0.6227965950965881, 'learning_rate': 1.9720969861717658e-05, 'epoch': 1.138655462184874}
>>> 2025-09-10 21:50:57,452 - INFO - >>> {'loss': 2.3233, 'grad_norm': 0.76085364818573, 'learning_rate': 1.9718901224497048e-05, 'epoch': 1.1428571428571428}
>>> 2025-09-10 21:51:01,386 - INFO - >>> {'loss': 2.2504, 'grad_norm': 0.6348181962966919, 'learning_rate': 1.9716825056776838e-05, 'epoch': 1.1470588235294117}
>>> 2025-09-10 21:51:05,549 - INFO - >>> {'loss': 2.3917, 'grad_norm': 0.5958492159843445, 'learning_rate': 1.971474136016571e-05, 'epoch': 1.1512605042016806}
>>> 2025-09-10 21:51:09,098 - INFO - >>> {'loss': 2.3723, 'grad_norm': 0.6712456345558167, 'learning_rate': 1.971265013627817e-05, 'epoch': 1.1554621848739495}
>>> 2025-09-10 21:51:12,123 - INFO - >>> {'loss': 2.2584, 'grad_norm': 0.8080620765686035, 'learning_rate': 1.9710551386734562e-05, 'epoch': 1.1596638655462184}
>>> 2025-09-10 21:51:15,876 - INFO - >>> {'loss': 2.3636, 'grad_norm': 0.656005859375, 'learning_rate': 1.9708445113161066e-05, 'epoch': 1.1638655462184875}
>>> 2025-09-10 21:51:19,445 - INFO - >>> {'loss': 2.2657, 'grad_norm': 0.6515986323356628, 'learning_rate': 1.970633131718968e-05, 'epoch': 1.1680672268907564}
>>> 2025-09-10 21:51:23,064 - INFO - >>> {'loss': 2.3246, 'grad_norm': 0.7555423974990845, 'learning_rate': 1.9704210000458242e-05, 'epoch': 1.1722689075630253}
>>> 2025-09-10 21:51:27,030 - INFO - >>> {'loss': 2.2686, 'grad_norm': 0.7052974700927734, 'learning_rate': 1.9702081164610415e-05, 'epoch': 1.1764705882352942}
>>> 2025-09-10 21:51:30,331 - INFO - >>> {'loss': 2.3711, 'grad_norm': 0.6587238311767578, 'learning_rate': 1.9699944811295676e-05, 'epoch': 1.180672268907563}
>>> 2025-09-10 21:51:33,870 - INFO - >>> {'loss': 2.3365, 'grad_norm': 0.6455852389335632, 'learning_rate': 1.9697800942169343e-05, 'epoch': 1.184873949579832}
>>> 2025-09-10 21:51:37,175 - INFO - >>> {'loss': 2.2849, 'grad_norm': 0.740665078163147, 'learning_rate': 1.969564955889255e-05, 'epoch': 1.1890756302521008}
>>> 2025-09-10 21:51:40,721 - INFO - >>> {'loss': 2.2496, 'grad_norm': 0.6409319639205933, 'learning_rate': 1.9693490663132255e-05, 'epoch': 1.1932773109243697}
>>> 2025-09-10 21:51:43,841 - INFO - >>> {'loss': 2.3023, 'grad_norm': 0.757697582244873, 'learning_rate': 1.9691324256561233e-05, 'epoch': 1.1974789915966386}
>>> 2025-09-10 21:51:47,882 - INFO - >>> {'loss': 2.2992, 'grad_norm': 0.7363500595092773, 'learning_rate': 1.9689150340858087e-05, 'epoch': 1.2016806722689075}
>>> 2025-09-10 21:51:51,169 - INFO - >>> {'loss': 2.3321, 'grad_norm': 0.7266390919685364, 'learning_rate': 1.9686968917707224e-05, 'epoch': 1.2058823529411764}
>>> 2025-09-10 21:51:54,936 - INFO - >>> {'loss': 2.4463, 'grad_norm': 0.683219850063324, 'learning_rate': 1.9684779988798884e-05, 'epoch': 1.2100840336134453}
>>> 2025-09-10 21:51:58,680 - INFO - >>> {'loss': 2.2612, 'grad_norm': 0.7358818650245667, 'learning_rate': 1.9682583555829116e-05, 'epoch': 1.2142857142857142}
>>> 2025-09-10 21:52:01,964 - INFO - >>> {'loss': 2.3081, 'grad_norm': 0.7148830890655518, 'learning_rate': 1.9680379620499777e-05, 'epoch': 1.2184873949579833}
>>> 2025-09-10 21:52:05,433 - INFO - >>> {'loss': 2.2956, 'grad_norm': 0.644312858581543, 'learning_rate': 1.9678168184518548e-05, 'epoch': 1.2226890756302522}
>>> 2025-09-10 21:52:08,862 - INFO - >>> {'loss': 2.2004, 'grad_norm': 0.7057283520698547, 'learning_rate': 1.967594924959892e-05, 'epoch': 1.226890756302521}
>>> 2025-09-10 21:52:11,980 - INFO - >>> {'loss': 2.3925, 'grad_norm': 0.651771605014801, 'learning_rate': 1.967372281746018e-05, 'epoch': 1.23109243697479}
>>> 2025-09-10 21:52:15,310 - INFO - >>> {'loss': 2.4316, 'grad_norm': 0.7247549295425415, 'learning_rate': 1.9671488889827444e-05, 'epoch': 1.2352941176470589}
>>> 2025-09-10 21:52:19,094 - INFO - >>> {'loss': 2.2, 'grad_norm': 0.6570184230804443, 'learning_rate': 1.9669247468431622e-05, 'epoch': 1.2394957983193278}
>>> 2025-09-10 21:52:22,055 - INFO - >>> {'loss': 2.2721, 'grad_norm': 0.6909899115562439, 'learning_rate': 1.9666998555009443e-05, 'epoch': 1.2436974789915967}
>>> 2025-09-10 21:52:25,745 - INFO - >>> {'loss': 2.3478, 'grad_norm': 0.6539210677146912, 'learning_rate': 1.9664742151303425e-05, 'epoch': 1.2478991596638656}
>>> 2025-09-10 21:52:29,241 - INFO - >>> {'loss': 2.3648, 'grad_norm': 0.7274339199066162, 'learning_rate': 1.96624782590619e-05, 'epoch': 1.2521008403361344}
>>> 2025-09-10 21:52:33,192 - INFO - >>> {'loss': 2.4447, 'grad_norm': 0.7039383053779602, 'learning_rate': 1.9660206880039004e-05, 'epoch': 1.2563025210084033}
>>> 2025-09-10 21:52:36,977 - INFO - >>> {'loss': 2.2262, 'grad_norm': 0.6900342702865601, 'learning_rate': 1.9657928015994666e-05, 'epoch': 1.2605042016806722}
>>> 2025-09-10 21:52:40,684 - INFO - >>> {'loss': 2.2648, 'grad_norm': 0.7809689044952393, 'learning_rate': 1.965564166869462e-05, 'epoch': 1.2647058823529411}
>>> 2025-09-10 21:52:43,720 - INFO - >>> {'loss': 2.3848, 'grad_norm': 0.7867737412452698, 'learning_rate': 1.9653347839910402e-05, 'epoch': 1.26890756302521}
>>> 2025-09-10 21:52:46,992 - INFO - >>> {'loss': 2.3653, 'grad_norm': 0.6961444616317749, 'learning_rate': 1.9651046531419335e-05, 'epoch': 1.273109243697479}
>>> 2025-09-10 21:52:51,001 - INFO - >>> {'loss': 2.291, 'grad_norm': 0.6508405804634094, 'learning_rate': 1.9648737745004544e-05, 'epoch': 1.2773109243697478}
>>> 2025-09-10 21:52:54,677 - INFO - >>> {'loss': 2.2874, 'grad_norm': 0.7041557431221008, 'learning_rate': 1.9646421482454947e-05, 'epoch': 1.2815126050420167}
>>> 2025-09-10 21:52:57,972 - INFO - >>> {'loss': 2.1825, 'grad_norm': 0.8049773573875427, 'learning_rate': 1.9644097745565253e-05, 'epoch': 1.2857142857142856}
>>> 2025-09-10 21:53:02,035 - INFO - >>> {'loss': 2.3013, 'grad_norm': 0.6745384335517883, 'learning_rate': 1.9641766536135968e-05, 'epoch': 1.2899159663865547}
>>> 2025-09-10 21:53:05,475 - INFO - >>> {'loss': 2.4097, 'grad_norm': 0.7149138450622559, 'learning_rate': 1.9639427855973378e-05, 'epoch': 1.2941176470588236}
>>> 2025-09-10 21:53:08,706 - INFO - >>> {'loss': 2.2438, 'grad_norm': 0.6729497909545898, 'learning_rate': 1.963708170688957e-05, 'epoch': 1.2983193277310925}
>>> 2025-09-10 21:53:11,896 - INFO - >>> {'loss': 2.3507, 'grad_norm': 0.7038363814353943, 'learning_rate': 1.9634728090702402e-05, 'epoch': 1.3025210084033614}
>>> 2025-09-10 21:53:15,766 - INFO - >>> {'loss': 2.2535, 'grad_norm': 0.6487658619880676, 'learning_rate': 1.9632367009235537e-05, 'epoch': 1.3067226890756303}
>>> 2025-09-10 21:53:19,665 - INFO - >>> {'loss': 2.2785, 'grad_norm': 0.7394013404846191, 'learning_rate': 1.9629998464318408e-05, 'epoch': 1.3109243697478992}
>>> 2025-09-10 21:53:22,969 - INFO - >>> {'loss': 2.358, 'grad_norm': 0.7104865908622742, 'learning_rate': 1.962762245778624e-05, 'epoch': 1.315126050420168}
>>> 2025-09-10 21:53:26,307 - INFO - >>> {'loss': 2.2515, 'grad_norm': 0.7353197932243347, 'learning_rate': 1.9625238991480027e-05, 'epoch': 1.319327731092437}
>>> 2025-09-10 21:53:30,195 - INFO - >>> {'loss': 2.2336, 'grad_norm': 0.718752920627594, 'learning_rate': 1.9622848067246555e-05, 'epoch': 1.3235294117647058}
>>> 2025-09-10 21:53:33,295 - INFO - >>> {'loss': 2.2949, 'grad_norm': 0.7087387442588806, 'learning_rate': 1.9620449686938387e-05, 'epoch': 1.3277310924369747}
>>> 2025-09-10 21:53:36,942 - INFO - >>> {'loss': 2.3625, 'grad_norm': 0.801281750202179, 'learning_rate': 1.9618043852413858e-05, 'epoch': 1.3319327731092436}
>>> 2025-09-10 21:53:40,365 - INFO - >>> {'loss': 2.2323, 'grad_norm': 0.6566835045814514, 'learning_rate': 1.961563056553708e-05, 'epoch': 1.3361344537815127}
>>> 2025-09-10 21:53:44,210 - INFO - >>> {'loss': 2.4285, 'grad_norm': 0.66480553150177, 'learning_rate': 1.9613209828177945e-05, 'epoch': 1.3403361344537816}
>>> 2025-09-10 21:53:47,828 - INFO - >>> {'loss': 2.1833, 'grad_norm': 0.7526676654815674, 'learning_rate': 1.9610781642212114e-05, 'epoch': 1.3445378151260505}
>>> 2025-09-10 21:53:51,923 - INFO - >>> {'loss': 2.3265, 'grad_norm': 0.7345483303070068, 'learning_rate': 1.9608346009521015e-05, 'epoch': 1.3487394957983194}
>>> 2025-09-10 21:53:55,051 - INFO - >>> {'loss': 2.276, 'grad_norm': 0.7079396843910217, 'learning_rate': 1.9605902931991853e-05, 'epoch': 1.3529411764705883}
>>> 2025-09-10 21:53:58,246 - INFO - >>> {'loss': 2.2706, 'grad_norm': 0.7249614596366882, 'learning_rate': 1.96034524115176e-05, 'epoch': 1.3571428571428572}
>>> 2025-09-10 21:54:01,445 - INFO - >>> {'loss': 2.356, 'grad_norm': 0.7213780879974365, 'learning_rate': 1.9600994449996985e-05, 'epoch': 1.361344537815126}
>>> 2025-09-10 21:54:05,270 - INFO - >>> {'loss': 2.322, 'grad_norm': 0.709425687789917, 'learning_rate': 1.9598529049334524e-05, 'epoch': 1.365546218487395}
>>> 2025-09-10 21:54:09,300 - INFO - >>> {'loss': 2.3608, 'grad_norm': 0.6630476713180542, 'learning_rate': 1.9596056211440474e-05, 'epoch': 1.3697478991596639}
>>> 2025-09-10 21:54:12,540 - INFO - >>> {'loss': 2.2504, 'grad_norm': 0.717650830745697, 'learning_rate': 1.959357593823087e-05, 'epoch': 1.3739495798319328}
>>> 2025-09-10 21:54:16,608 - INFO - >>> {'loss': 2.2747, 'grad_norm': 0.6789302825927734, 'learning_rate': 1.9591088231627502e-05, 'epoch': 1.3781512605042017}
>>> 2025-09-10 21:54:19,919 - INFO - >>> {'loss': 2.1812, 'grad_norm': 0.7102088928222656, 'learning_rate': 1.958859309355792e-05, 'epoch': 1.3823529411764706}
>>> 2025-09-10 21:54:23,018 - INFO - >>> {'loss': 2.366, 'grad_norm': 0.7451513409614563, 'learning_rate': 1.9586090525955433e-05, 'epoch': 1.3865546218487395}
>>> 2025-09-10 21:54:27,207 - INFO - >>> {'loss': 2.366, 'grad_norm': 0.7077986598014832, 'learning_rate': 1.9583580530759107e-05, 'epoch': 1.3907563025210083}
>>> 2025-09-10 21:54:30,696 - INFO - >>> {'loss': 2.393, 'grad_norm': 0.6907768249511719, 'learning_rate': 1.9581063109913763e-05, 'epoch': 1.3949579831932772}
>>> 2025-09-10 21:54:34,178 - INFO - >>> {'loss': 2.1924, 'grad_norm': 0.6602088212966919, 'learning_rate': 1.957853826536997e-05, 'epoch': 1.3991596638655461}
>>> 2025-09-10 21:54:36,793 - INFO - >>> {'loss': 2.252, 'grad_norm': 0.8318003416061401, 'learning_rate': 1.957600599908406e-05, 'epoch': 1.403361344537815}
>>> 2025-09-10 21:54:40,806 - INFO - >>> {'loss': 2.1808, 'grad_norm': 0.740102231502533, 'learning_rate': 1.9573466313018107e-05, 'epoch': 1.407563025210084}
>>> 2025-09-10 21:54:44,081 - INFO - >>> {'loss': 2.3336, 'grad_norm': 0.6890445351600647, 'learning_rate': 1.957091920913994e-05, 'epoch': 1.4117647058823528}
>>> 2025-09-10 21:54:47,288 - INFO - >>> {'loss': 2.2937, 'grad_norm': 0.6954479217529297, 'learning_rate': 1.9568364689423127e-05, 'epoch': 1.415966386554622}
>>> 2025-09-10 21:54:50,741 - INFO - >>> {'loss': 2.3201, 'grad_norm': 0.822136640548706, 'learning_rate': 1.9565802755846995e-05, 'epoch': 1.4201680672268908}
>>> 2025-09-10 21:54:54,675 - INFO - >>> {'loss': 2.3527, 'grad_norm': 0.7315612435340881, 'learning_rate': 1.9563233410396597e-05, 'epoch': 1.4243697478991597}
>>> 2025-09-10 21:54:57,878 - INFO - >>> {'loss': 2.3209, 'grad_norm': 0.8150179982185364, 'learning_rate': 1.9560656655062752e-05, 'epoch': 1.4285714285714286}
>>> 2025-09-10 21:55:00,746 - INFO - >>> {'loss': 2.2482, 'grad_norm': 0.7649897336959839, 'learning_rate': 1.9558072491842e-05, 'epoch': 1.4327731092436975}
>>> 2025-09-10 21:55:04,009 - INFO - >>> {'loss': 2.3307, 'grad_norm': 0.7489320635795593, 'learning_rate': 1.955548092273663e-05, 'epoch': 1.4369747899159664}
>>> 2025-09-10 21:55:07,141 - INFO - >>> {'loss': 2.3239, 'grad_norm': 0.772057056427002, 'learning_rate': 1.955288194975467e-05, 'epoch': 1.4411764705882353}
>>> 2025-09-10 21:55:11,058 - INFO - >>> {'loss': 2.3027, 'grad_norm': 0.6823354363441467, 'learning_rate': 1.955027557490988e-05, 'epoch': 1.4453781512605042}
>>> 2025-09-10 21:55:14,175 - INFO - >>> {'loss': 2.2356, 'grad_norm': 0.7341320514678955, 'learning_rate': 1.9547661800221765e-05, 'epoch': 1.449579831932773}
>>> 2025-09-10 21:55:18,243 - INFO - >>> {'loss': 2.3567, 'grad_norm': 0.7165125012397766, 'learning_rate': 1.9545040627715554e-05, 'epoch': 1.453781512605042}
>>> 2025-09-10 21:55:21,560 - INFO - >>> {'loss': 2.3013, 'grad_norm': 0.7806471586227417, 'learning_rate': 1.9542412059422208e-05, 'epoch': 1.4579831932773109}
>>> 2025-09-10 21:55:24,878 - INFO - >>> {'loss': 2.2904, 'grad_norm': 0.750087559223175, 'learning_rate': 1.9539776097378424e-05, 'epoch': 1.46218487394958}
>>> 2025-09-10 21:55:28,628 - INFO - >>> {'loss': 2.2415, 'grad_norm': 0.7292935252189636, 'learning_rate': 1.9537132743626627e-05, 'epoch': 1.4663865546218489}
>>> 2025-09-10 21:55:31,717 - INFO - >>> {'loss': 2.3168, 'grad_norm': 0.7103049159049988, 'learning_rate': 1.953448200021496e-05, 'epoch': 1.4705882352941178}
>>> 2025-09-10 21:55:35,219 - INFO - >>> {'loss': 2.2522, 'grad_norm': 0.7235203981399536, 'learning_rate': 1.9531823869197314e-05, 'epoch': 1.4747899159663866}
>>> 2025-09-10 21:55:38,829 - INFO - >>> {'loss': 2.3784, 'grad_norm': 0.6940334439277649, 'learning_rate': 1.9529158352633275e-05, 'epoch': 1.4789915966386555}
>>> 2025-09-10 21:55:42,209 - INFO - >>> {'loss': 2.308, 'grad_norm': 0.7668916583061218, 'learning_rate': 1.952648545258817e-05, 'epoch': 1.4831932773109244}
>>> 2025-09-10 21:55:45,876 - INFO - >>> {'loss': 2.3356, 'grad_norm': 0.7586330771446228, 'learning_rate': 1.952380517113305e-05, 'epoch': 1.4873949579831933}
>>> 2025-09-10 21:55:49,139 - INFO - >>> {'loss': 2.2916, 'grad_norm': 0.6906353235244751, 'learning_rate': 1.952111751034467e-05, 'epoch': 1.4915966386554622}
>>> 2025-09-10 21:55:52,403 - INFO - >>> {'loss': 2.3032, 'grad_norm': 0.6893821358680725, 'learning_rate': 1.9518422472305517e-05, 'epoch': 1.495798319327731}
>>> 2025-09-10 21:55:55,673 - INFO - >>> {'loss': 2.2753, 'grad_norm': 0.7341980338096619, 'learning_rate': 1.9515720059103785e-05, 'epoch': 1.5}
>>> 2025-09-10 21:55:58,850 - INFO - >>> {'loss': 2.3704, 'grad_norm': 0.9015942811965942, 'learning_rate': 1.9513010272833383e-05, 'epoch': 1.504201680672269}
>>> 2025-09-10 21:56:02,391 - INFO - >>> {'loss': 2.2852, 'grad_norm': 0.7428170442581177, 'learning_rate': 1.9510293115593942e-05, 'epoch': 1.5084033613445378}
>>> 2025-09-10 21:56:05,654 - INFO - >>> {'loss': 2.2411, 'grad_norm': 0.7565418481826782, 'learning_rate': 1.9507568589490796e-05, 'epoch': 1.5126050420168067}
>>> 2025-09-10 21:56:09,796 - INFO - >>> {'loss': 2.273, 'grad_norm': 0.683623194694519, 'learning_rate': 1.950483669663499e-05, 'epoch': 1.5168067226890756}
>>> 2025-09-10 21:56:12,964 - INFO - >>> {'loss': 2.2241, 'grad_norm': 0.7747807502746582, 'learning_rate': 1.9502097439143272e-05, 'epoch': 1.5210084033613445}
>>> 2025-09-10 21:56:16,726 - INFO - >>> {'loss': 2.3244, 'grad_norm': 0.6772392392158508, 'learning_rate': 1.949935081913811e-05, 'epoch': 1.5252100840336134}
>>> 2025-09-10 21:56:20,098 - INFO - >>> {'loss': 2.2879, 'grad_norm': 0.7105038166046143, 'learning_rate': 1.949659683874767e-05, 'epoch': 1.5294117647058822}
>>> 2025-09-10 21:56:24,008 - INFO - >>> {'loss': 2.3816, 'grad_norm': 0.7170499563217163, 'learning_rate': 1.9493835500105808e-05, 'epoch': 1.5336134453781511}
>>> 2025-09-10 21:56:27,858 - INFO - >>> {'loss': 2.3362, 'grad_norm': 0.739711582660675, 'learning_rate': 1.94910668053521e-05, 'epoch': 1.53781512605042}
>>> 2025-09-10 21:56:31,543 - INFO - >>> {'loss': 2.2531, 'grad_norm': 0.70505291223526, 'learning_rate': 1.948829075663182e-05, 'epoch': 1.542016806722689}
>>> 2025-09-10 21:56:35,220 - INFO - >>> {'loss': 2.2312, 'grad_norm': 0.6742658615112305, 'learning_rate': 1.9485507356095925e-05, 'epoch': 1.5462184873949578}
>>> 2025-09-10 21:56:39,174 - INFO - >>> {'loss': 2.245, 'grad_norm': 0.7217268943786621, 'learning_rate': 1.9482716605901088e-05, 'epoch': 1.550420168067227}
>>> 2025-09-10 21:56:43,204 - INFO - >>> {'loss': 2.1956, 'grad_norm': 0.6621860265731812, 'learning_rate': 1.9479918508209654e-05, 'epoch': 1.5546218487394958}
>>> 2025-09-10 21:56:46,753 - INFO - >>> {'loss': 2.318, 'grad_norm': 0.7217153906822205, 'learning_rate': 1.9477113065189684e-05, 'epoch': 1.5588235294117647}
>>> 2025-09-10 21:56:50,800 - INFO - >>> {'loss': 2.2808, 'grad_norm': 0.8089632391929626, 'learning_rate': 1.947430027901492e-05, 'epoch': 1.5630252100840336}
>>> 2025-09-10 21:56:53,756 - INFO - >>> {'loss': 2.3254, 'grad_norm': 0.8189285397529602, 'learning_rate': 1.947148015186479e-05, 'epoch': 1.5672268907563025}
>>> 2025-09-10 21:56:57,238 - INFO - >>> {'loss': 2.3436, 'grad_norm': 0.7123722434043884, 'learning_rate': 1.946865268592442e-05, 'epoch': 1.5714285714285714}
>>> 2025-09-10 21:57:00,170 - INFO - >>> {'loss': 2.2739, 'grad_norm': 0.7844818234443665, 'learning_rate': 1.946581788338461e-05, 'epoch': 1.5756302521008403}
>>> 2025-09-10 21:57:03,695 - INFO - >>> {'loss': 2.2312, 'grad_norm': 0.8418989777565002, 'learning_rate': 1.9462975746441854e-05, 'epoch': 1.5798319327731094}
>>> 2025-09-10 21:57:06,816 - INFO - >>> {'loss': 2.2998, 'grad_norm': 0.7648465633392334, 'learning_rate': 1.9460126277298324e-05, 'epoch': 1.5840336134453783}
>>> 2025-09-10 21:57:10,873 - INFO - >>> {'loss': 2.3031, 'grad_norm': 0.674714207649231, 'learning_rate': 1.9457269478161875e-05, 'epoch': 1.5882352941176472}
>>> 2025-09-10 21:57:13,946 - INFO - >>> {'loss': 2.3028, 'grad_norm': 0.7082140445709229, 'learning_rate': 1.9454405351246045e-05, 'epoch': 1.592436974789916}
>>> 2025-09-10 21:57:16,778 - INFO - >>> {'loss': 2.2725, 'grad_norm': 0.7777122855186462, 'learning_rate': 1.9451533898770047e-05, 'epoch': 1.596638655462185}
>>> 2025-09-10 21:57:20,057 - INFO - >>> {'loss': 2.2198, 'grad_norm': 0.8229149580001831, 'learning_rate': 1.9448655122958765e-05, 'epoch': 1.6008403361344539}
>>> 2025-09-10 21:57:23,422 - INFO - >>> {'loss': 2.2413, 'grad_norm': 0.7490678429603577, 'learning_rate': 1.9445769026042764e-05, 'epoch': 1.6050420168067228}
>>> 2025-09-10 21:57:27,250 - INFO - >>> {'loss': 2.3425, 'grad_norm': 0.6548004746437073, 'learning_rate': 1.944287561025828e-05, 'epoch': 1.6092436974789917}
>>> 2025-09-10 21:57:30,222 - INFO - >>> {'loss': 2.1959, 'grad_norm': 0.8058242797851562, 'learning_rate': 1.9439974877847222e-05, 'epoch': 1.6134453781512605}
>>> 2025-09-10 21:57:33,623 - INFO - >>> {'loss': 2.2786, 'grad_norm': 0.780864417552948, 'learning_rate': 1.9437066831057158e-05, 'epoch': 1.6176470588235294}
>>> 2025-09-10 21:57:37,212 - INFO - >>> {'loss': 2.2903, 'grad_norm': 0.702570915222168, 'learning_rate': 1.943415147214134e-05, 'epoch': 1.6218487394957983}
>>> 2025-09-10 21:57:40,462 - INFO - >>> {'loss': 2.3272, 'grad_norm': 0.7037235498428345, 'learning_rate': 1.943122880335867e-05, 'epoch': 1.6260504201680672}
>>> 2025-09-10 21:57:44,127 - INFO - >>> {'loss': 2.2814, 'grad_norm': 0.7813713550567627, 'learning_rate': 1.942829882697372e-05, 'epoch': 1.6302521008403361}
>>> 2025-09-10 21:57:47,877 - INFO - >>> {'loss': 2.253, 'grad_norm': 0.6994063854217529, 'learning_rate': 1.942536154525673e-05, 'epoch': 1.634453781512605}
>>> 2025-09-10 21:57:50,900 - INFO - >>> {'loss': 2.1806, 'grad_norm': 0.886008620262146, 'learning_rate': 1.942241696048359e-05, 'epoch': 1.638655462184874}
>>> 2025-09-10 21:57:54,831 - INFO - >>> {'loss': 2.1282, 'grad_norm': 0.7016956806182861, 'learning_rate': 1.9419465074935857e-05, 'epoch': 1.6428571428571428}
>>> 2025-09-10 21:57:58,185 - INFO - >>> {'loss': 2.4108, 'grad_norm': 0.7701191902160645, 'learning_rate': 1.941650589090074e-05, 'epoch': 1.6470588235294117}
>>> 2025-09-10 21:58:01,035 - INFO - >>> {'loss': 2.3414, 'grad_norm': 0.7834477424621582, 'learning_rate': 1.9413539410671104e-05, 'epoch': 1.6512605042016806}
>>> 2025-09-10 21:58:04,700 - INFO - >>> {'loss': 2.2536, 'grad_norm': 0.7882601022720337, 'learning_rate': 1.9410565636545468e-05, 'epoch': 1.6554621848739495}
>>> 2025-09-10 21:58:08,442 - INFO - >>> {'loss': 2.2879, 'grad_norm': 0.716392457485199, 'learning_rate': 1.9407584570828005e-05, 'epoch': 1.6596638655462184}
>>> 2025-09-10 21:58:12,280 - INFO - >>> {'loss': 2.3427, 'grad_norm': 0.7522398829460144, 'learning_rate': 1.940459621582853e-05, 'epoch': 1.6638655462184873}
>>> 2025-09-10 21:58:16,017 - INFO - >>> {'loss': 2.3064, 'grad_norm': 0.7879883050918579, 'learning_rate': 1.940160057386252e-05, 'epoch': 1.6680672268907561}
>>> 2025-09-10 21:58:19,252 - INFO - >>> {'loss': 2.3773, 'grad_norm': 0.7631681561470032, 'learning_rate': 1.939859764725108e-05, 'epoch': 1.6722689075630253}
>>> 2025-09-10 21:58:22,507 - INFO - >>> {'loss': 2.2178, 'grad_norm': 0.7814970016479492, 'learning_rate': 1.9395587438320972e-05, 'epoch': 1.6764705882352942}
>>> 2025-09-10 21:58:26,484 - INFO - >>> {'loss': 2.2979, 'grad_norm': 0.7050052285194397, 'learning_rate': 1.9392569949404597e-05, 'epoch': 1.680672268907563}
>>> 2025-09-10 21:58:30,700 - INFO - >>> {'loss': 2.2562, 'grad_norm': 0.696103036403656, 'learning_rate': 1.938954518284e-05, 'epoch': 1.684873949579832}
>>> 2025-09-10 21:58:34,123 - INFO - >>> {'loss': 2.2531, 'grad_norm': 0.708125650882721, 'learning_rate': 1.938651314097086e-05, 'epoch': 1.6890756302521008}
>>> 2025-09-10 21:58:37,657 - INFO - >>> {'loss': 2.2911, 'grad_norm': 0.7665582299232483, 'learning_rate': 1.938347382614649e-05, 'epoch': 1.6932773109243697}
>>> 2025-09-10 21:58:41,386 - INFO - >>> {'loss': 2.2978, 'grad_norm': 0.7793912291526794, 'learning_rate': 1.9380427240721853e-05, 'epoch': 1.6974789915966386}
>>> 2025-09-10 21:58:44,687 - INFO - >>> {'loss': 2.2173, 'grad_norm': 0.7896984219551086, 'learning_rate': 1.937737338705753e-05, 'epoch': 1.7016806722689075}
>>> 2025-09-10 21:58:48,557 - INFO - >>> {'loss': 2.3072, 'grad_norm': 0.7399922609329224, 'learning_rate': 1.937431226751974e-05, 'epoch': 1.7058823529411766}
>>> 2025-09-10 21:58:51,826 - INFO - >>> {'loss': 2.2657, 'grad_norm': 0.769964337348938, 'learning_rate': 1.937124388448033e-05, 'epoch': 1.7100840336134455}
>>> 2025-09-10 21:58:55,482 - INFO - >>> {'loss': 2.2794, 'grad_norm': 0.766678512096405, 'learning_rate': 1.936816824031678e-05, 'epoch': 1.7142857142857144}
>>> 2025-09-10 21:58:59,004 - INFO - >>> {'loss': 2.1882, 'grad_norm': 0.7776327729225159, 'learning_rate': 1.9365085337412187e-05, 'epoch': 1.7184873949579833}
>>> 2025-09-10 21:59:02,197 - INFO - >>> {'loss': 2.2984, 'grad_norm': 0.8053687214851379, 'learning_rate': 1.9361995178155283e-05, 'epoch': 1.7226890756302522}
>>> 2025-09-10 21:59:06,127 - INFO - >>> {'loss': 2.2589, 'grad_norm': 0.7503902912139893, 'learning_rate': 1.9358897764940415e-05, 'epoch': 1.726890756302521}
>>> 2025-09-10 21:59:10,096 - INFO - >>> {'loss': 2.2445, 'grad_norm': 0.6809141635894775, 'learning_rate': 1.9355793100167554e-05, 'epoch': 1.73109243697479}
>>> 2025-09-10 21:59:13,914 - INFO - >>> {'loss': 2.2054, 'grad_norm': 0.7466416954994202, 'learning_rate': 1.9352681186242287e-05, 'epoch': 1.7352941176470589}
>>> 2025-09-10 21:59:17,198 - INFO - >>> {'loss': 2.2635, 'grad_norm': 0.760273814201355, 'learning_rate': 1.934956202557582e-05, 'epoch': 1.7394957983193278}
>>> 2025-09-10 21:59:20,688 - INFO - >>> {'loss': 2.323, 'grad_norm': 0.7169307470321655, 'learning_rate': 1.934643562058497e-05, 'epoch': 1.7436974789915967}
>>> 2025-09-10 21:59:23,923 - INFO - >>> {'loss': 2.2104, 'grad_norm': 0.7552562952041626, 'learning_rate': 1.9343301973692177e-05, 'epoch': 1.7478991596638656}
>>> 2025-09-10 21:59:27,099 - INFO - >>> {'loss': 2.1592, 'grad_norm': 0.7272364497184753, 'learning_rate': 1.9340161087325483e-05, 'epoch': 1.7521008403361344}
>>> 2025-09-10 21:59:30,673 - INFO - >>> {'loss': 2.3607, 'grad_norm': 0.8282106518745422, 'learning_rate': 1.9337012963918538e-05, 'epoch': 1.7563025210084033}
>>> 2025-09-10 21:59:34,070 - INFO - >>> {'loss': 2.3083, 'grad_norm': 0.7155178785324097, 'learning_rate': 1.933385760591061e-05, 'epoch': 1.7605042016806722}
>>> 2025-09-10 21:59:37,374 - INFO - >>> {'loss': 2.285, 'grad_norm': 0.7596738934516907, 'learning_rate': 1.933069501574656e-05, 'epoch': 1.7647058823529411}
>>> 2025-09-10 21:59:40,920 - INFO - >>> {'loss': 2.2387, 'grad_norm': 0.7033698558807373, 'learning_rate': 1.9327525195876863e-05, 'epoch': 1.76890756302521}
>>> 2025-09-10 21:59:44,911 - INFO - >>> {'loss': 2.2779, 'grad_norm': 0.7593777775764465, 'learning_rate': 1.9324348148757592e-05, 'epoch': 1.773109243697479}
>>> 2025-09-10 21:59:47,840 - INFO - >>> {'loss': 2.2602, 'grad_norm': 0.7667009830474854, 'learning_rate': 1.9321163876850417e-05, 'epoch': 1.7773109243697478}
>>> 2025-09-10 21:59:51,150 - INFO - >>> {'loss': 2.3322, 'grad_norm': 0.7269104719161987, 'learning_rate': 1.9317972382622608e-05, 'epoch': 1.7815126050420167}
>>> 2025-09-10 21:59:54,140 - INFO - >>> {'loss': 2.321, 'grad_norm': 0.7889808416366577, 'learning_rate': 1.9314773668547035e-05, 'epoch': 1.7857142857142856}
>>> 2025-09-10 21:59:57,458 - INFO - >>> {'loss': 2.2411, 'grad_norm': 0.7320980429649353, 'learning_rate': 1.9311567737102158e-05, 'epoch': 1.7899159663865545}
>>> 2025-09-10 22:00:00,574 - INFO - >>> {'loss': 2.3153, 'grad_norm': 0.7417038679122925, 'learning_rate': 1.9308354590772023e-05, 'epoch': 1.7941176470588234}
>>> 2025-09-10 22:00:03,663 - INFO - >>> {'loss': 2.2163, 'grad_norm': 0.7787134647369385, 'learning_rate': 1.9305134232046284e-05, 'epoch': 1.7983193277310925}
>>> 2025-09-10 22:00:07,484 - INFO - >>> {'loss': 2.1852, 'grad_norm': 0.7377128005027771, 'learning_rate': 1.9301906663420167e-05, 'epoch': 1.8025210084033614}
>>> 2025-09-10 22:00:10,481 - INFO - >>> {'loss': 2.2848, 'grad_norm': 0.7853397130966187, 'learning_rate': 1.929867188739449e-05, 'epoch': 1.8067226890756303}
>>> 2025-09-10 22:00:14,377 - INFO - >>> {'loss': 2.2843, 'grad_norm': 0.7615327835083008, 'learning_rate': 1.9295429906475657e-05, 'epoch': 1.8109243697478992}
>>> 2025-09-10 22:00:18,093 - INFO - >>> {'loss': 2.2447, 'grad_norm': 0.7505935430526733, 'learning_rate': 1.9292180723175656e-05, 'epoch': 1.815126050420168}
>>> 2025-09-10 22:00:21,746 - INFO - >>> {'loss': 2.3945, 'grad_norm': 0.7339171767234802, 'learning_rate': 1.9288924340012042e-05, 'epoch': 1.819327731092437}
>>> 2025-09-10 22:00:25,771 - INFO - >>> {'loss': 2.3439, 'grad_norm': 0.6793441772460938, 'learning_rate': 1.9285660759507974e-05, 'epoch': 1.8235294117647058}
>>> 2025-09-10 22:00:28,627 - INFO - >>> {'loss': 2.3567, 'grad_norm': 0.7596607208251953, 'learning_rate': 1.9282389984192162e-05, 'epoch': 1.8277310924369747}
>>> 2025-09-10 22:00:31,915 - INFO - >>> {'loss': 2.2814, 'grad_norm': 0.7422841191291809, 'learning_rate': 1.9279112016598908e-05, 'epoch': 1.8319327731092439}
>>> 2025-09-10 22:00:35,981 - INFO - >>> {'loss': 2.3516, 'grad_norm': 0.7448376417160034, 'learning_rate': 1.9275826859268078e-05, 'epoch': 1.8361344537815127}
>>> 2025-09-10 22:00:39,884 - INFO - >>> {'loss': 2.2314, 'grad_norm': 0.7465810179710388, 'learning_rate': 1.9272534514745112e-05, 'epoch': 1.8403361344537816}
>>> 2025-09-10 22:00:43,610 - INFO - >>> {'loss': 2.2738, 'grad_norm': 0.7554600834846497, 'learning_rate': 1.9269234985581023e-05, 'epoch': 1.8445378151260505}
>>> 2025-09-10 22:00:46,802 - INFO - >>> {'loss': 2.2662, 'grad_norm': 0.7870391011238098, 'learning_rate': 1.926592827433238e-05, 'epoch': 1.8487394957983194}
>>> 2025-09-10 22:00:49,868 - INFO - >>> {'loss': 2.1945, 'grad_norm': 0.8507332801818848, 'learning_rate': 1.9262614383561322e-05, 'epoch': 1.8529411764705883}
>>> 2025-09-10 22:00:53,118 - INFO - >>> {'loss': 2.3018, 'grad_norm': 0.7611860036849976, 'learning_rate': 1.9259293315835558e-05, 'epoch': 1.8571428571428572}
>>> 2025-09-10 22:00:56,797 - INFO - >>> {'loss': 2.359, 'grad_norm': 0.779958963394165, 'learning_rate': 1.925596507372835e-05, 'epoch': 1.861344537815126}
>>> 2025-09-10 22:01:00,047 - INFO - >>> {'loss': 2.1772, 'grad_norm': 0.7730816602706909, 'learning_rate': 1.9252629659818522e-05, 'epoch': 1.865546218487395}
>>> 2025-09-10 22:01:03,321 - INFO - >>> {'loss': 2.2605, 'grad_norm': 0.746192455291748, 'learning_rate': 1.924928707669045e-05, 'epoch': 1.8697478991596639}
>>> 2025-09-10 22:01:06,697 - INFO - >>> {'loss': 2.2331, 'grad_norm': 0.7349842190742493, 'learning_rate': 1.9245937326934077e-05, 'epoch': 1.8739495798319328}
>>> 2025-09-10 22:01:10,019 - INFO - >>> {'loss': 2.3299, 'grad_norm': 0.750635027885437, 'learning_rate': 1.924258041314488e-05, 'epoch': 1.8781512605042017}
>>> 2025-09-10 22:01:13,786 - INFO - >>> {'loss': 2.3985, 'grad_norm': 0.7588256597518921, 'learning_rate': 1.9239216337923904e-05, 'epoch': 1.8823529411764706}
>>> 2025-09-10 22:01:16,834 - INFO - >>> {'loss': 2.2112, 'grad_norm': 0.8274116516113281, 'learning_rate': 1.9235845103877735e-05, 'epoch': 1.8865546218487395}
>>> 2025-09-10 22:01:20,678 - INFO - >>> {'loss': 2.3254, 'grad_norm': 0.7665609121322632, 'learning_rate': 1.923246671361851e-05, 'epoch': 1.8907563025210083}
>>> 2025-09-10 22:01:23,971 - INFO - >>> {'loss': 2.2466, 'grad_norm': 0.8497161269187927, 'learning_rate': 1.9229081169763902e-05, 'epoch': 1.8949579831932772}
>>> 2025-09-10 22:01:27,922 - INFO - >>> {'loss': 2.3518, 'grad_norm': 0.8105790615081787, 'learning_rate': 1.9225688474937142e-05, 'epoch': 1.8991596638655461}
>>> 2025-09-10 22:01:31,317 - INFO - >>> {'loss': 2.2061, 'grad_norm': 0.9511879682540894, 'learning_rate': 1.9222288631766982e-05, 'epoch': 1.903361344537815}
>>> 2025-09-10 22:01:35,104 - INFO - >>> {'loss': 2.2751, 'grad_norm': 0.736616313457489, 'learning_rate': 1.9218881642887735e-05, 'epoch': 1.907563025210084}
>>> 2025-09-10 22:01:38,351 - INFO - >>> {'loss': 2.292, 'grad_norm': 0.8114677667617798, 'learning_rate': 1.9215467510939234e-05, 'epoch': 1.9117647058823528}
>>> 2025-09-10 22:01:41,218 - INFO - >>> {'loss': 2.2362, 'grad_norm': 0.8388770818710327, 'learning_rate': 1.9212046238566854e-05, 'epoch': 1.9159663865546217}
>>> 2025-09-10 22:01:44,240 - INFO - >>> {'loss': 2.2937, 'grad_norm': 0.8353500962257385, 'learning_rate': 1.9208617828421493e-05, 'epoch': 1.9201680672268906}
>>> 2025-09-10 22:01:47,205 - INFO - >>> {'loss': 2.1686, 'grad_norm': 0.8561001420021057, 'learning_rate': 1.92051822831596e-05, 'epoch': 1.9243697478991597}
>>> 2025-09-10 22:01:50,681 - INFO - >>> {'loss': 2.2478, 'grad_norm': 0.6990328431129456, 'learning_rate': 1.9201739605443128e-05, 'epoch': 1.9285714285714286}
>>> 2025-09-10 22:01:53,878 - INFO - >>> {'loss': 2.2041, 'grad_norm': 0.7682883739471436, 'learning_rate': 1.9198289797939577e-05, 'epoch': 1.9327731092436975}
>>> 2025-09-10 22:01:57,367 - INFO - >>> {'loss': 2.2345, 'grad_norm': 0.7319589257240295, 'learning_rate': 1.9194832863321955e-05, 'epoch': 1.9369747899159664}
>>> 2025-09-10 22:02:01,229 - INFO - >>> {'loss': 2.285, 'grad_norm': 0.7686344981193542, 'learning_rate': 1.9191368804268805e-05, 'epoch': 1.9411764705882353}
>>> 2025-09-10 22:02:04,945 - INFO - >>> {'loss': 2.2583, 'grad_norm': 0.7716132402420044, 'learning_rate': 1.918789762346418e-05, 'epoch': 1.9453781512605042}
>>> 2025-09-10 22:02:08,561 - INFO - >>> {'loss': 2.1415, 'grad_norm': 0.8103162050247192, 'learning_rate': 1.9184419323597666e-05, 'epoch': 1.949579831932773}
>>> 2025-09-10 22:02:11,586 - INFO - >>> {'loss': 2.2976, 'grad_norm': 0.7915427088737488, 'learning_rate': 1.9180933907364344e-05, 'epoch': 1.9537815126050422}
>>> 2025-09-10 22:02:14,859 - INFO - >>> {'loss': 2.242, 'grad_norm': 0.752429723739624, 'learning_rate': 1.9177441377464826e-05, 'epoch': 1.957983193277311}
>>> 2025-09-10 22:02:18,590 - INFO - >>> {'loss': 2.202, 'grad_norm': 0.7418950200080872, 'learning_rate': 1.9173941736605233e-05, 'epoch': 1.96218487394958}
>>> 2025-09-10 22:02:21,894 - INFO - >>> {'loss': 2.273, 'grad_norm': 0.76429682970047, 'learning_rate': 1.9170434987497185e-05, 'epoch': 1.9663865546218489}
>>> 2025-09-10 22:02:25,125 - INFO - >>> {'loss': 2.3014, 'grad_norm': 0.7197808623313904, 'learning_rate': 1.9166921132857824e-05, 'epoch': 1.9705882352941178}
>>> 2025-09-10 22:02:28,251 - INFO - >>> {'loss': 2.453, 'grad_norm': 0.8074314594268799, 'learning_rate': 1.916340017540979e-05, 'epoch': 1.9747899159663866}
>>> 2025-09-10 22:02:32,320 - INFO - >>> {'loss': 2.2431, 'grad_norm': 0.8030474185943604, 'learning_rate': 1.9159872117881226e-05, 'epoch': 1.9789915966386555}
>>> 2025-09-10 22:02:35,981 - INFO - >>> {'loss': 2.3138, 'grad_norm': 0.7361499071121216, 'learning_rate': 1.9156336963005778e-05, 'epoch': 1.9831932773109244}
>>> 2025-09-10 22:02:39,583 - INFO - >>> {'loss': 2.2759, 'grad_norm': 0.7115439772605896, 'learning_rate': 1.9152794713522592e-05, 'epoch': 1.9873949579831933}
>>> 2025-09-10 22:02:43,614 - INFO - >>> {'loss': 2.3812, 'grad_norm': 0.7130894064903259, 'learning_rate': 1.9149245372176312e-05, 'epoch': 1.9915966386554622}
>>> 2025-09-10 22:02:47,103 - INFO - >>> {'loss': 2.2632, 'grad_norm': 0.8364686369895935, 'learning_rate': 1.9145688941717074e-05, 'epoch': 1.995798319327731}
>>> 2025-09-10 22:02:50,608 - INFO - >>> {'loss': 2.2505, 'grad_norm': 0.8073416948318481, 'learning_rate': 1.9142125424900508e-05, 'epoch': 2.0}
>>> 2025-09-10 22:02:54,569 - INFO - >>> {'loss': 2.2398, 'grad_norm': 0.74481201171875, 'learning_rate': 1.9138554824487733e-05, 'epoch': 2.004201680672269}
>>> 2025-09-10 22:02:58,054 - INFO - >>> {'loss': 2.3274, 'grad_norm': 0.7703312635421753, 'learning_rate': 1.9134977143245366e-05, 'epoch': 2.008403361344538}
>>> 2025-09-10 22:03:01,305 - INFO - >>> {'loss': 2.3496, 'grad_norm': 0.8287364840507507, 'learning_rate': 1.9131392383945492e-05, 'epoch': 2.0126050420168067}
>>> 2025-09-10 22:03:04,664 - INFO - >>> {'loss': 2.3165, 'grad_norm': 0.8665860295295715, 'learning_rate': 1.91278005493657e-05, 'epoch': 2.0168067226890756}
>>> 2025-09-10 22:03:07,998 - INFO - >>> {'loss': 2.2855, 'grad_norm': 0.7460871934890747, 'learning_rate': 1.912420164228905e-05, 'epoch': 2.0210084033613445}
>>> 2025-09-10 22:03:11,764 - INFO - >>> {'loss': 2.2433, 'grad_norm': 0.747010350227356, 'learning_rate': 1.9120595665504084e-05, 'epoch': 2.0252100840336134}
>>> 2025-09-10 22:03:15,160 - INFO - >>> {'loss': 2.2215, 'grad_norm': 0.8108947277069092, 'learning_rate': 1.911698262180482e-05, 'epoch': 2.0294117647058822}
>>> 2025-09-10 22:03:18,828 - INFO - >>> {'loss': 2.2908, 'grad_norm': 0.8245753645896912, 'learning_rate': 1.9113362513990758e-05, 'epoch': 2.033613445378151}
>>> 2025-09-10 22:03:22,017 - INFO - >>> {'loss': 2.2594, 'grad_norm': 0.8124682903289795, 'learning_rate': 1.9109735344866866e-05, 'epoch': 2.03781512605042}
>>> 2025-09-10 22:03:25,067 - INFO - >>> {'loss': 2.1375, 'grad_norm': 0.873754620552063, 'learning_rate': 1.910610111724358e-05, 'epoch': 2.042016806722689}
>>> 2025-09-10 22:03:28,899 - INFO - >>> {'loss': 2.186, 'grad_norm': 0.7473469376564026, 'learning_rate': 1.910245983393682e-05, 'epoch': 2.046218487394958}
>>> 2025-09-10 22:03:32,449 - INFO - >>> {'loss': 2.2552, 'grad_norm': 0.81402188539505, 'learning_rate': 1.9098811497767952e-05, 'epoch': 2.0504201680672267}
>>> 2025-09-10 22:03:35,728 - INFO - >>> {'loss': 2.2157, 'grad_norm': 0.8636999726295471, 'learning_rate': 1.909515611156382e-05, 'epoch': 2.0546218487394956}
>>> 2025-09-10 22:03:39,521 - INFO - >>> {'loss': 2.2206, 'grad_norm': 0.7700485587120056, 'learning_rate': 1.9091493678156735e-05, 'epoch': 2.0588235294117645}
>>> 2025-09-10 22:03:43,050 - INFO - >>> {'loss': 2.341, 'grad_norm': 0.7374715209007263, 'learning_rate': 1.9087824200384457e-05, 'epoch': 2.0630252100840334}
>>> 2025-09-10 22:03:46,370 - INFO - >>> {'loss': 2.273, 'grad_norm': 0.941992461681366, 'learning_rate': 1.9084147681090205e-05, 'epoch': 2.0672268907563027}
>>> 2025-09-10 22:03:49,395 - INFO - >>> {'loss': 2.269, 'grad_norm': 0.8115736842155457, 'learning_rate': 1.908046412312267e-05, 'epoch': 2.0714285714285716}
>>> 2025-09-10 22:03:52,624 - INFO - >>> {'loss': 2.2587, 'grad_norm': 0.8369731903076172, 'learning_rate': 1.9076773529335967e-05, 'epoch': 2.0756302521008405}
>>> 2025-09-10 22:03:56,481 - INFO - >>> {'loss': 2.1877, 'grad_norm': 0.8253766298294067, 'learning_rate': 1.9073075902589693e-05, 'epoch': 2.0798319327731094}
>>> 2025-09-10 22:03:59,503 - INFO - >>> {'loss': 2.3246, 'grad_norm': 0.7893931269645691, 'learning_rate': 1.9069371245748874e-05, 'epoch': 2.0840336134453783}
>>> 2025-09-10 22:04:03,500 - INFO - >>> {'loss': 2.2121, 'grad_norm': 0.7972169518470764, 'learning_rate': 1.9065659561684e-05, 'epoch': 2.088235294117647}
>>> 2025-09-10 22:04:06,993 - INFO - >>> {'loss': 2.3898, 'grad_norm': 0.7339212894439697, 'learning_rate': 1.9061940853270986e-05, 'epoch': 2.092436974789916}
>>> 2025-09-10 22:04:10,450 - INFO - >>> {'loss': 2.3368, 'grad_norm': 0.8144427537918091, 'learning_rate': 1.9058215123391207e-05, 'epoch': 2.096638655462185}
>>> 2025-09-10 22:04:14,015 - INFO - >>> {'loss': 2.2401, 'grad_norm': 0.8086562752723694, 'learning_rate': 1.905448237493147e-05, 'epoch': 2.100840336134454}
>>> 2025-09-10 22:04:18,128 - INFO - >>> {'loss': 2.214, 'grad_norm': 0.8372650146484375, 'learning_rate': 1.9050742610784015e-05, 'epoch': 2.1050420168067228}
>>> 2025-09-10 22:04:21,945 - INFO - >>> {'loss': 2.2393, 'grad_norm': 0.8631358742713928, 'learning_rate': 1.9046995833846537e-05, 'epoch': 2.1092436974789917}
>>> 2025-09-10 22:04:25,101 - INFO - >>> {'loss': 2.2804, 'grad_norm': 0.8777221441268921, 'learning_rate': 1.9043242047022142e-05, 'epoch': 2.1134453781512605}
>>> 2025-09-10 22:04:28,460 - INFO - >>> {'loss': 2.1566, 'grad_norm': 0.8207266926765442, 'learning_rate': 1.9039481253219386e-05, 'epoch': 2.1176470588235294}
>>> 2025-09-10 22:04:31,439 - INFO - >>> {'loss': 2.3234, 'grad_norm': 0.7476335167884827, 'learning_rate': 1.903571345535224e-05, 'epoch': 2.1218487394957983}
>>> 2025-09-10 22:04:34,873 - INFO - >>> {'loss': 2.292, 'grad_norm': 0.7431336045265198, 'learning_rate': 1.903193865634011e-05, 'epoch': 2.1260504201680672}
>>> 2025-09-10 22:04:38,399 - INFO - >>> {'loss': 2.255, 'grad_norm': 0.8168439269065857, 'learning_rate': 1.9028156859107824e-05, 'epoch': 2.130252100840336}
>>> 2025-09-10 22:04:42,141 - INFO - >>> {'loss': 2.2584, 'grad_norm': 0.8087865114212036, 'learning_rate': 1.902436806658564e-05, 'epoch': 2.134453781512605}
>>> 2025-09-10 22:04:45,052 - INFO - >>> {'loss': 2.2558, 'grad_norm': 0.8890758156776428, 'learning_rate': 1.902057228170922e-05, 'epoch': 2.138655462184874}
>>> 2025-09-10 22:04:48,622 - INFO - >>> {'loss': 2.2281, 'grad_norm': 0.7677997350692749, 'learning_rate': 1.9016769507419653e-05, 'epoch': 2.142857142857143}
>>> 2025-09-10 22:04:52,475 - INFO - >>> {'loss': 2.2862, 'grad_norm': 0.8465014100074768, 'learning_rate': 1.9012959746663453e-05, 'epoch': 2.1470588235294117}
>>> 2025-09-10 22:04:56,194 - INFO - >>> {'loss': 2.2497, 'grad_norm': 0.7970752120018005, 'learning_rate': 1.9009143002392534e-05, 'epoch': 2.1512605042016806}
>>> 2025-09-10 22:04:59,566 - INFO - >>> {'loss': 2.3112, 'grad_norm': 0.8649385571479797, 'learning_rate': 1.9005319277564224e-05, 'epoch': 2.1554621848739495}
>>> 2025-09-10 22:05:02,638 - INFO - >>> {'loss': 2.2965, 'grad_norm': 0.833223283290863, 'learning_rate': 1.900148857514126e-05, 'epoch': 2.1596638655462184}
>>> 2025-09-10 22:05:06,252 - INFO - >>> {'loss': 2.2635, 'grad_norm': 0.8146710395812988, 'learning_rate': 1.8997650898091788e-05, 'epoch': 2.1638655462184873}
>>> 2025-09-10 22:05:09,477 - INFO - >>> {'loss': 2.1624, 'grad_norm': 0.9108803272247314, 'learning_rate': 1.899380624938936e-05, 'epoch': 2.168067226890756}
>>> 2025-09-10 22:05:13,171 - INFO - >>> {'loss': 2.1832, 'grad_norm': 0.8161070942878723, 'learning_rate': 1.8989954632012922e-05, 'epoch': 2.172268907563025}
>>> 2025-09-10 22:05:16,743 - INFO - >>> {'loss': 2.1909, 'grad_norm': 0.8123688697814941, 'learning_rate': 1.8986096048946826e-05, 'epoch': 2.176470588235294}
>>> 2025-09-10 22:05:20,469 - INFO - >>> {'loss': 2.458, 'grad_norm': 0.8091484308242798, 'learning_rate': 1.8982230503180815e-05, 'epoch': 2.180672268907563}
>>> 2025-09-10 22:05:24,121 - INFO - >>> {'loss': 2.3419, 'grad_norm': 0.783117949962616, 'learning_rate': 1.8978357997710036e-05, 'epoch': 2.184873949579832}
>>> 2025-09-10 22:05:27,355 - INFO - >>> {'loss': 2.3374, 'grad_norm': 0.7464082837104797, 'learning_rate': 1.8974478535535025e-05, 'epoch': 2.189075630252101}
>>> 2025-09-10 22:05:30,320 - INFO - >>> {'loss': 2.4083, 'grad_norm': 1.0237818956375122, 'learning_rate': 1.8970592119661702e-05, 'epoch': 2.19327731092437}
>>> 2025-09-10 22:05:34,097 - INFO - >>> {'loss': 2.2465, 'grad_norm': 0.7906341552734375, 'learning_rate': 1.896669875310138e-05, 'epoch': 2.197478991596639}
>>> 2025-09-10 22:05:37,421 - INFO - >>> {'loss': 2.3377, 'grad_norm': 0.8308597803115845, 'learning_rate': 1.8962798438870767e-05, 'epoch': 2.2016806722689077}
>>> 2025-09-10 22:05:41,411 - INFO - >>> {'loss': 2.3578, 'grad_norm': 0.9020781517028809, 'learning_rate': 1.895889117999193e-05, 'epoch': 2.2058823529411766}
>>> 2025-09-10 22:05:45,417 - INFO - >>> {'loss': 2.2697, 'grad_norm': 0.8115485906600952, 'learning_rate': 1.8954976979492347e-05, 'epoch': 2.2100840336134455}
>>> 2025-09-10 22:05:48,280 - INFO - >>> {'loss': 2.2482, 'grad_norm': 0.8883583545684814, 'learning_rate': 1.895105584040485e-05, 'epoch': 2.2142857142857144}
>>> 2025-09-10 22:05:51,714 - INFO - >>> {'loss': 2.172, 'grad_norm': 0.8365580439567566, 'learning_rate': 1.8947127765767655e-05, 'epoch': 2.2184873949579833}
>>> 2025-09-10 22:05:54,767 - INFO - >>> {'loss': 2.2562, 'grad_norm': 0.9008135795593262, 'learning_rate': 1.894319275862436e-05, 'epoch': 2.222689075630252}
>>> 2025-09-10 22:05:58,136 - INFO - >>> {'loss': 2.311, 'grad_norm': 0.8929284811019897, 'learning_rate': 1.8939250822023923e-05, 'epoch': 2.226890756302521}
>>> 2025-09-10 22:06:02,174 - INFO - >>> {'loss': 2.3935, 'grad_norm': 0.7742114067077637, 'learning_rate': 1.893530195902068e-05, 'epoch': 2.23109243697479}
>>> 2025-09-10 22:06:05,452 - INFO - >>> {'loss': 2.1622, 'grad_norm': 0.8885066509246826, 'learning_rate': 1.8931346172674326e-05, 'epoch': 2.235294117647059}
>>> 2025-09-10 22:06:09,136 - INFO - >>> {'loss': 2.2372, 'grad_norm': 0.8112291097640991, 'learning_rate': 1.8927383466049926e-05, 'epoch': 2.2394957983193278}
>>> 2025-09-10 22:06:12,541 - INFO - >>> {'loss': 2.2674, 'grad_norm': 0.8405029773712158, 'learning_rate': 1.8923413842217905e-05, 'epoch': 2.2436974789915967}
>>> 2025-09-10 22:06:16,017 - INFO - >>> {'loss': 2.3255, 'grad_norm': 0.8852116465568542, 'learning_rate': 1.891943730425405e-05, 'epoch': 2.2478991596638656}
>>> 2025-09-10 22:06:19,767 - INFO - >>> {'loss': 2.1565, 'grad_norm': 0.797741711139679, 'learning_rate': 1.8915453855239504e-05, 'epoch': 2.2521008403361344}
>>> 2025-09-10 22:06:23,860 - INFO - >>> {'loss': 2.2453, 'grad_norm': 0.8187928199768066, 'learning_rate': 1.8911463498260757e-05, 'epoch': 2.2563025210084033}
>>> 2025-09-10 22:06:27,065 - INFO - >>> {'loss': 2.2401, 'grad_norm': 0.8427135944366455, 'learning_rate': 1.8907466236409666e-05, 'epoch': 2.2605042016806722}
>>> 2025-09-10 22:06:30,918 - INFO - >>> {'loss': 2.2607, 'grad_norm': 0.8797745108604431, 'learning_rate': 1.890346207278343e-05, 'epoch': 2.264705882352941}
>>> 2025-09-10 22:06:34,851 - INFO - >>> {'loss': 2.314, 'grad_norm': 0.7475911378860474, 'learning_rate': 1.8899451010484594e-05, 'epoch': 2.26890756302521}
>>> 2025-09-10 22:06:38,571 - INFO - >>> {'loss': 2.2665, 'grad_norm': 0.7498636245727539, 'learning_rate': 1.8895433052621047e-05, 'epoch': 2.273109243697479}
>>> 2025-09-10 22:06:41,612 - INFO - >>> {'loss': 2.2555, 'grad_norm': 0.8853561282157898, 'learning_rate': 1.8891408202306036e-05, 'epoch': 2.277310924369748}
>>> 2025-09-10 22:06:45,042 - INFO - >>> {'loss': 2.2206, 'grad_norm': 0.8015971183776855, 'learning_rate': 1.888737646265813e-05, 'epoch': 2.2815126050420167}
>>> 2025-09-10 22:06:48,740 - INFO - >>> {'loss': 2.2994, 'grad_norm': 0.7935510277748108, 'learning_rate': 1.888333783680124e-05, 'epoch': 2.2857142857142856}
>>> 2025-09-10 22:06:52,581 - INFO - >>> {'loss': 2.3597, 'grad_norm': 0.7924713492393494, 'learning_rate': 1.887929232786462e-05, 'epoch': 2.2899159663865545}
>>> 2025-09-10 22:06:56,649 - INFO - >>> {'loss': 2.2576, 'grad_norm': 0.8395788669586182, 'learning_rate': 1.8875239938982855e-05, 'epoch': 2.2941176470588234}
>>> 2025-09-10 22:06:59,689 - INFO - >>> {'loss': 2.3294, 'grad_norm': 0.8761265873908997, 'learning_rate': 1.8871180673295856e-05, 'epoch': 2.2983193277310923}
>>> 2025-09-10 22:07:03,058 - INFO - >>> {'loss': 2.272, 'grad_norm': 0.8725638389587402, 'learning_rate': 1.8867114533948864e-05, 'epoch': 2.302521008403361}
>>> 2025-09-10 22:07:06,376 - INFO - >>> {'loss': 2.3347, 'grad_norm': 0.8113510012626648, 'learning_rate': 1.8863041524092452e-05, 'epoch': 2.30672268907563}
>>> 2025-09-10 22:07:09,724 - INFO - >>> {'loss': 2.2317, 'grad_norm': 0.7558261752128601, 'learning_rate': 1.8858961646882504e-05, 'epoch': 2.310924369747899}
>>> 2025-09-10 22:07:13,754 - INFO - >>> {'loss': 2.2924, 'grad_norm': 0.7895153164863586, 'learning_rate': 1.8854874905480244e-05, 'epoch': 2.315126050420168}
>>> 2025-09-10 22:07:17,520 - INFO - >>> {'loss': 2.3374, 'grad_norm': 0.7800594568252563, 'learning_rate': 1.8850781303052194e-05, 'epoch': 2.3193277310924367}
>>> 2025-09-10 22:07:21,372 - INFO - >>> {'loss': 2.2108, 'grad_norm': 0.8512746095657349, 'learning_rate': 1.8846680842770202e-05, 'epoch': 2.323529411764706}
>>> 2025-09-10 22:07:25,294 - INFO - >>> {'loss': 2.3393, 'grad_norm': 0.7428902983665466, 'learning_rate': 1.884257352781143e-05, 'epoch': 2.327731092436975}
>>> 2025-09-10 22:07:28,948 - INFO - >>> {'loss': 2.2337, 'grad_norm': 0.835326611995697, 'learning_rate': 1.8838459361358354e-05, 'epoch': 2.331932773109244}
>>> 2025-09-10 22:07:32,565 - INFO - >>> {'loss': 2.2623, 'grad_norm': 0.8411202430725098, 'learning_rate': 1.8834338346598754e-05, 'epoch': 2.3361344537815127}
>>> 2025-09-10 22:07:35,901 - INFO - >>> {'loss': 2.3289, 'grad_norm': 0.9445221424102783, 'learning_rate': 1.883021048672571e-05, 'epoch': 2.3403361344537816}
>>> 2025-09-10 22:07:39,273 - INFO - >>> {'loss': 2.2466, 'grad_norm': 0.9663377404212952, 'learning_rate': 1.8826075784937623e-05, 'epoch': 2.3445378151260505}
>>> 2025-09-10 22:07:42,992 - INFO - >>> {'loss': 2.1702, 'grad_norm': 0.8412076830863953, 'learning_rate': 1.882193424443818e-05, 'epoch': 2.3487394957983194}
>>> 2025-09-10 22:07:45,844 - INFO - >>> {'loss': 2.2691, 'grad_norm': 0.9281711578369141, 'learning_rate': 1.8817785868436372e-05, 'epoch': 2.3529411764705883}
>>> 2025-09-10 22:07:49,040 - INFO - >>> {'loss': 2.274, 'grad_norm': 0.8323931694030762, 'learning_rate': 1.881363066014649e-05, 'epoch': 2.357142857142857}
>>> 2025-09-10 22:07:53,024 - INFO - >>> {'loss': 2.3028, 'grad_norm': 0.7915641665458679, 'learning_rate': 1.8809468622788112e-05, 'epoch': 2.361344537815126}
>>> 2025-09-10 22:07:56,946 - INFO - >>> {'loss': 2.2436, 'grad_norm': 0.7877037525177002, 'learning_rate': 1.8805299759586114e-05, 'epoch': 2.365546218487395}
>>> 2025-09-10 22:08:00,302 - INFO - >>> {'loss': 2.2662, 'grad_norm': 0.7812945246696472, 'learning_rate': 1.8801124073770655e-05, 'epoch': 2.369747899159664}
>>> 2025-09-10 22:08:03,926 - INFO - >>> {'loss': 2.3261, 'grad_norm': 0.7970548272132874, 'learning_rate': 1.8796941568577183e-05, 'epoch': 2.3739495798319328}
>>> 2025-09-10 22:08:07,492 - INFO - >>> {'loss': 2.3581, 'grad_norm': 0.8505333065986633, 'learning_rate': 1.8792752247246433e-05, 'epoch': 2.3781512605042017}
>>> 2025-09-10 22:08:10,556 - INFO - >>> {'loss': 2.2383, 'grad_norm': 0.985518217086792, 'learning_rate': 1.8788556113024415e-05, 'epoch': 2.3823529411764706}
>>> 2025-09-10 22:08:13,849 - INFO - >>> {'loss': 2.3183, 'grad_norm': 0.8688968420028687, 'learning_rate': 1.8784353169162424e-05, 'epoch': 2.3865546218487395}
>>> 2025-09-10 22:08:17,156 - INFO - >>> {'loss': 2.1677, 'grad_norm': 0.8392160534858704, 'learning_rate': 1.8780143418917025e-05, 'epoch': 2.3907563025210083}
>>> 2025-09-10 22:08:20,926 - INFO - >>> {'loss': 2.3602, 'grad_norm': 0.7366485595703125, 'learning_rate': 1.8775926865550062e-05, 'epoch': 2.3949579831932772}
>>> 2025-09-10 22:08:24,202 - INFO - >>> {'loss': 2.2236, 'grad_norm': 0.8204934000968933, 'learning_rate': 1.877170351232865e-05, 'epoch': 2.399159663865546}
>>> 2025-09-10 22:08:27,501 - INFO - >>> {'loss': 2.2406, 'grad_norm': 0.8775877952575684, 'learning_rate': 1.876747336252517e-05, 'epoch': 2.403361344537815}
>>> 2025-09-10 22:08:31,245 - INFO - >>> {'loss': 2.2299, 'grad_norm': 0.8240558505058289, 'learning_rate': 1.876323641941727e-05, 'epoch': 2.407563025210084}
>>> 2025-09-10 22:08:35,317 - INFO - >>> {'loss': 2.2896, 'grad_norm': 0.8067054152488708, 'learning_rate': 1.8758992686287862e-05, 'epoch': 2.411764705882353}
>>> 2025-09-10 22:08:39,011 - INFO - >>> {'loss': 2.2977, 'grad_norm': 0.8055606484413147, 'learning_rate': 1.8754742166425118e-05, 'epoch': 2.4159663865546217}
>>> 2025-09-10 22:08:41,689 - INFO - >>> {'loss': 2.2326, 'grad_norm': 0.9207316040992737, 'learning_rate': 1.8750484863122476e-05, 'epoch': 2.4201680672268906}
>>> 2025-09-10 22:08:45,334 - INFO - >>> {'loss': 2.2982, 'grad_norm': 0.9143565893173218, 'learning_rate': 1.8746220779678617e-05, 'epoch': 2.4243697478991595}
>>> 2025-09-10 22:08:48,744 - INFO - >>> {'loss': 2.1584, 'grad_norm': 0.8223146796226501, 'learning_rate': 1.8741949919397484e-05, 'epoch': 2.4285714285714284}
>>> 2025-09-10 22:08:52,739 - INFO - >>> {'loss': 2.3547, 'grad_norm': 0.8297324776649475, 'learning_rate': 1.873767228558827e-05, 'epoch': 2.4327731092436977}
>>> 2025-09-10 22:08:56,747 - INFO - >>> {'loss': 2.3476, 'grad_norm': 0.8346736431121826, 'learning_rate': 1.8733387881565414e-05, 'epoch': 2.4369747899159666}
>>> 2025-09-10 22:09:00,571 - INFO - >>> {'loss': 2.2415, 'grad_norm': 0.8315669298171997, 'learning_rate': 1.8729096710648604e-05, 'epoch': 2.4411764705882355}
>>> 2025-09-10 22:09:04,513 - INFO - >>> {'loss': 2.2288, 'grad_norm': 0.7777879238128662, 'learning_rate': 1.872479877616277e-05, 'epoch': 2.4453781512605044}
>>> 2025-09-10 22:09:07,940 - INFO - >>> {'loss': 2.2708, 'grad_norm': 0.796112060546875, 'learning_rate': 1.872049408143808e-05, 'epoch': 2.4495798319327733}
>>> 2025-09-10 22:09:11,169 - INFO - >>> {'loss': 2.2625, 'grad_norm': 0.8396050930023193, 'learning_rate': 1.871618262980994e-05, 'epoch': 2.453781512605042}
>>> 2025-09-10 22:09:14,997 - INFO - >>> {'loss': 2.3451, 'grad_norm': 0.7719279527664185, 'learning_rate': 1.8711864424619e-05, 'epoch': 2.457983193277311}
>>> 2025-09-10 22:09:18,243 - INFO - >>> {'loss': 2.2097, 'grad_norm': 0.9315716624259949, 'learning_rate': 1.870753946921113e-05, 'epoch': 2.46218487394958}
>>> 2025-09-10 22:09:21,880 - INFO - >>> {'loss': 2.2445, 'grad_norm': 0.7887502908706665, 'learning_rate': 1.8703207766937444e-05, 'epoch': 2.466386554621849}
>>> 2025-09-10 22:09:25,605 - INFO - >>> {'loss': 2.3964, 'grad_norm': 0.8178530931472778, 'learning_rate': 1.869886932115427e-05, 'epoch': 2.4705882352941178}
>>> 2025-09-10 22:09:29,434 - INFO - >>> {'loss': 2.1422, 'grad_norm': 0.8482569456100464, 'learning_rate': 1.869452413522317e-05, 'epoch': 2.4747899159663866}
>>> 2025-09-10 22:09:33,111 - INFO - >>> {'loss': 2.326, 'grad_norm': 0.8020867705345154, 'learning_rate': 1.8690172212510923e-05, 'epoch': 2.4789915966386555}
>>> 2025-09-10 22:09:36,632 - INFO - >>> {'loss': 2.3097, 'grad_norm': 0.7995378375053406, 'learning_rate': 1.8685813556389535e-05, 'epoch': 2.4831932773109244}
>>> 2025-09-10 22:09:39,961 - INFO - >>> {'loss': 2.2102, 'grad_norm': 0.801772952079773, 'learning_rate': 1.8681448170236223e-05, 'epoch': 2.4873949579831933}
>>> 2025-09-10 22:09:43,356 - INFO - >>> {'loss': 2.4298, 'grad_norm': 0.8236325979232788, 'learning_rate': 1.867707605743342e-05, 'epoch': 2.491596638655462}
>>> 2025-09-10 22:09:47,323 - INFO - >>> {'loss': 2.2461, 'grad_norm': 0.8194761276245117, 'learning_rate': 1.8672697221368774e-05, 'epoch': 2.495798319327731}
>>> 2025-09-10 22:09:50,950 - INFO - >>> {'loss': 2.1649, 'grad_norm': 0.795356810092926, 'learning_rate': 1.8668311665435137e-05, 'epoch': 2.5}
>>> 2025-09-10 22:09:53,992 - INFO - >>> {'loss': 2.2908, 'grad_norm': 0.8088753819465637, 'learning_rate': 1.8663919393030573e-05, 'epoch': 2.504201680672269}
>>> 2025-09-10 22:09:57,515 - INFO - >>> {'loss': 2.2564, 'grad_norm': 0.7871472835540771, 'learning_rate': 1.865952040755835e-05, 'epoch': 2.508403361344538}
>>> 2025-09-10 22:10:00,584 - INFO - >>> {'loss': 2.2737, 'grad_norm': 0.9066575765609741, 'learning_rate': 1.865511471242693e-05, 'epoch': 2.5126050420168067}
>>> 2025-09-10 22:10:04,325 - INFO - >>> {'loss': 2.2621, 'grad_norm': 0.8081035017967224, 'learning_rate': 1.8650702311049983e-05, 'epoch': 2.5168067226890756}
>>> 2025-09-10 22:10:08,141 - INFO - >>> {'loss': 2.2331, 'grad_norm': 0.9318521618843079, 'learning_rate': 1.864628320684637e-05, 'epoch': 2.5210084033613445}
>>> 2025-09-10 22:10:12,370 - INFO - >>> {'loss': 2.2632, 'grad_norm': 0.745440661907196, 'learning_rate': 1.8641857403240146e-05, 'epoch': 2.5252100840336134}
>>> 2025-09-10 22:10:16,176 - INFO - >>> {'loss': 2.2217, 'grad_norm': 0.8094995617866516, 'learning_rate': 1.863742490366056e-05, 'epoch': 2.5294117647058822}
>>> 2025-09-10 22:10:19,614 - INFO - >>> {'loss': 2.1945, 'grad_norm': 0.7964927554130554, 'learning_rate': 1.863298571154205e-05, 'epoch': 2.533613445378151}
>>> 2025-09-10 22:10:23,645 - INFO - >>> {'loss': 2.3423, 'grad_norm': 0.7669112682342529, 'learning_rate': 1.862853983032423e-05, 'epoch': 2.53781512605042}
>>> 2025-09-10 22:10:27,487 - INFO - >>> {'loss': 2.3375, 'grad_norm': 0.7701174020767212, 'learning_rate': 1.862408726345191e-05, 'epoch': 2.542016806722689}
>>> 2025-09-10 22:10:30,999 - INFO - >>> {'loss': 2.3025, 'grad_norm': 0.8056226968765259, 'learning_rate': 1.8619628014375068e-05, 'epoch': 2.546218487394958}
>>> 2025-09-10 22:10:34,900 - INFO - >>> {'loss': 2.2401, 'grad_norm': 0.7856170535087585, 'learning_rate': 1.861516208654887e-05, 'epoch': 2.5504201680672267}
>>> 2025-09-10 22:10:38,578 - INFO - >>> {'loss': 2.2169, 'grad_norm': 0.8543348908424377, 'learning_rate': 1.8610689483433654e-05, 'epoch': 2.5546218487394956}
>>> 2025-09-10 22:10:41,951 - INFO - >>> {'loss': 2.2676, 'grad_norm': 0.8440356850624084, 'learning_rate': 1.8606210208494923e-05, 'epoch': 2.5588235294117645}
>>> 2025-09-10 22:10:45,130 - INFO - >>> {'loss': 2.1582, 'grad_norm': 0.9117280840873718, 'learning_rate': 1.8601724265203357e-05, 'epoch': 2.5630252100840334}
>>> 2025-09-10 22:10:49,100 - INFO - >>> {'loss': 2.3355, 'grad_norm': 0.8540722727775574, 'learning_rate': 1.85972316570348e-05, 'epoch': 2.5672268907563023}
>>> 2025-09-10 22:10:52,925 - INFO - >>> {'loss': 2.161, 'grad_norm': 0.8249107599258423, 'learning_rate': 1.8592732387470268e-05, 'epoch': 2.571428571428571}
>>> 2025-09-10 22:10:56,598 - INFO - >>> {'loss': 2.2893, 'grad_norm': 0.7154554724693298, 'learning_rate': 1.8588226459995923e-05, 'epoch': 2.57563025210084}
>>> 2025-09-10 22:10:59,878 - INFO - >>> {'loss': 2.2523, 'grad_norm': 0.8765535354614258, 'learning_rate': 1.8583713878103097e-05, 'epoch': 2.5798319327731094}
>>> 2025-09-10 22:11:03,285 - INFO - >>> {'loss': 2.2492, 'grad_norm': 0.8625472187995911, 'learning_rate': 1.857919464528828e-05, 'epoch': 2.5840336134453783}
>>> 2025-09-10 22:11:06,984 - INFO - >>> {'loss': 2.2792, 'grad_norm': 0.7520395517349243, 'learning_rate': 1.8574668765053103e-05, 'epoch': 2.588235294117647}
>>> 2025-09-10 22:11:10,486 - INFO - >>> {'loss': 2.2003, 'grad_norm': 0.9322078227996826, 'learning_rate': 1.857013624090436e-05, 'epoch': 2.592436974789916}
>>> 2025-09-10 22:11:14,269 - INFO - >>> {'loss': 2.1861, 'grad_norm': 0.7817859053611755, 'learning_rate': 1.8565597076353992e-05, 'epoch': 2.596638655462185}
>>> 2025-09-10 22:11:18,194 - INFO - >>> {'loss': 2.1893, 'grad_norm': 0.8008379340171814, 'learning_rate': 1.856105127491907e-05, 'epoch': 2.600840336134454}
>>> 2025-09-10 22:11:22,204 - INFO - >>> {'loss': 2.2769, 'grad_norm': 0.8647089600563049, 'learning_rate': 1.855649884012183e-05, 'epoch': 2.6050420168067228}
>>> 2025-09-10 22:11:25,602 - INFO - >>> {'loss': 2.3331, 'grad_norm': 0.777221143245697, 'learning_rate': 1.8551939775489635e-05, 'epoch': 2.6092436974789917}
>>> 2025-09-10 22:11:28,987 - INFO - >>> {'loss': 2.2371, 'grad_norm': 0.8672909736633301, 'learning_rate': 1.854737408455498e-05, 'epoch': 2.6134453781512605}
>>> 2025-09-10 22:11:32,293 - INFO - >>> {'loss': 2.248, 'grad_norm': 0.8715713620185852, 'learning_rate': 1.854280177085551e-05, 'epoch': 2.6176470588235294}
>>> 2025-09-10 22:11:35,945 - INFO - >>> {'loss': 2.217, 'grad_norm': 0.8326819539070129, 'learning_rate': 1.8538222837933982e-05, 'epoch': 2.6218487394957983}
>>> 2025-09-10 22:11:39,902 - INFO - >>> {'loss': 2.231, 'grad_norm': 0.7913375496864319, 'learning_rate': 1.85336372893383e-05, 'epoch': 2.6260504201680672}
>>> 2025-09-10 22:11:43,448 - INFO - >>> {'loss': 2.2864, 'grad_norm': 0.7757076025009155, 'learning_rate': 1.852904512862148e-05, 'epoch': 2.630252100840336}
>>> 2025-09-10 22:11:46,790 - INFO - >>> {'loss': 2.2358, 'grad_norm': 0.8167536854743958, 'learning_rate': 1.852444635934168e-05, 'epoch': 2.634453781512605}
>>> 2025-09-10 22:11:50,173 - INFO - >>> {'loss': 2.242, 'grad_norm': 0.7844709157943726, 'learning_rate': 1.851984098506215e-05, 'epoch': 2.638655462184874}
>>> 2025-09-10 22:11:53,510 - INFO - >>> {'loss': 2.1345, 'grad_norm': 0.856521487236023, 'learning_rate': 1.8515229009351282e-05, 'epoch': 2.642857142857143}
>>> 2025-09-10 22:11:56,772 - INFO - >>> {'loss': 2.1862, 'grad_norm': 0.9185473918914795, 'learning_rate': 1.8510610435782577e-05, 'epoch': 2.6470588235294117}
>>> 2025-09-10 22:11:59,875 - INFO - >>> {'loss': 2.2312, 'grad_norm': 0.852989673614502, 'learning_rate': 1.850598526793464e-05, 'epoch': 2.6512605042016806}
>>> 2025-09-10 22:12:03,645 - INFO - >>> {'loss': 2.318, 'grad_norm': 0.8778272867202759, 'learning_rate': 1.8501353509391195e-05, 'epoch': 2.6554621848739495}
>>> 2025-09-10 22:12:07,127 - INFO - >>> {'loss': 2.2737, 'grad_norm': 0.9219567179679871, 'learning_rate': 1.8496715163741067e-05, 'epoch': 2.6596638655462184}
>>> 2025-09-10 22:12:10,221 - INFO - >>> {'loss': 2.3343, 'grad_norm': 0.8482128381729126, 'learning_rate': 1.849207023457819e-05, 'epoch': 2.6638655462184873}
>>> 2025-09-10 22:12:13,706 - INFO - >>> {'loss': 2.2929, 'grad_norm': 0.8902922868728638, 'learning_rate': 1.8487418725501593e-05, 'epoch': 2.668067226890756}
>>> 2025-09-10 22:12:17,680 - INFO - >>> {'loss': 2.3013, 'grad_norm': 0.8564308881759644, 'learning_rate': 1.8482760640115408e-05, 'epoch': 2.6722689075630255}
>>> 2025-09-10 22:12:20,957 - INFO - >>> {'loss': 2.2365, 'grad_norm': 0.8961617350578308, 'learning_rate': 1.8478095982028862e-05, 'epoch': 2.6764705882352944}
>>> 2025-09-10 22:12:24,489 - INFO - >>> {'loss': 2.2507, 'grad_norm': 1.0078715085983276, 'learning_rate': 1.847342475485627e-05, 'epoch': 2.6806722689075633}
>>> 2025-09-10 22:12:27,862 - INFO - >>> {'loss': 2.2957, 'grad_norm': 0.834484875202179, 'learning_rate': 1.8468746962217045e-05, 'epoch': 2.684873949579832}
>>> 2025-09-10 22:12:31,238 - INFO - >>> {'loss': 2.3381, 'grad_norm': 0.8396396040916443, 'learning_rate': 1.8464062607735678e-05, 'epoch': 2.689075630252101}
>>> 2025-09-10 22:12:34,934 - INFO - >>> {'loss': 2.3696, 'grad_norm': 0.8531140089035034, 'learning_rate': 1.8459371695041754e-05, 'epoch': 2.69327731092437}
>>> 2025-09-10 22:12:38,299 - INFO - >>> {'loss': 2.3872, 'grad_norm': 0.8630563616752625, 'learning_rate': 1.8454674227769928e-05, 'epoch': 2.697478991596639}
>>> 2025-09-10 22:12:41,244 - INFO - >>> {'loss': 2.3438, 'grad_norm': 0.9064790606498718, 'learning_rate': 1.8449970209559948e-05, 'epoch': 2.7016806722689077}
>>> 2025-09-10 22:12:44,427 - INFO - >>> {'loss': 2.165, 'grad_norm': 0.8603702187538147, 'learning_rate': 1.8445259644056625e-05, 'epoch': 2.7058823529411766}
>>> 2025-09-10 22:12:47,739 - INFO - >>> {'loss': 2.2455, 'grad_norm': 0.9312475919723511, 'learning_rate': 1.844054253490985e-05, 'epoch': 2.7100840336134455}
>>> 2025-09-10 22:12:50,866 - INFO - >>> {'loss': 2.3519, 'grad_norm': 0.8587968349456787, 'learning_rate': 1.843581888577458e-05, 'epoch': 2.7142857142857144}
>>> 2025-09-10 22:12:54,082 - INFO - >>> {'loss': 2.1609, 'grad_norm': 0.8686569929122925, 'learning_rate': 1.8431088700310846e-05, 'epoch': 2.7184873949579833}
>>> 2025-09-10 22:12:57,995 - INFO - >>> {'loss': 2.2616, 'grad_norm': 0.8312291502952576, 'learning_rate': 1.8426351982183735e-05, 'epoch': 2.722689075630252}
>>> 2025-09-10 22:13:01,491 - INFO - >>> {'loss': 2.282, 'grad_norm': 0.8193633556365967, 'learning_rate': 1.84216087350634e-05, 'epoch': 2.726890756302521}
>>> 2025-09-10 22:13:05,542 - INFO - >>> {'loss': 2.1827, 'grad_norm': 0.75818932056427, 'learning_rate': 1.841685896262506e-05, 'epoch': 2.73109243697479}
>>> 2025-09-10 22:13:09,441 - INFO - >>> {'loss': 2.2424, 'grad_norm': 0.821102499961853, 'learning_rate': 1.841210266854897e-05, 'epoch': 2.735294117647059}
>>> 2025-09-10 22:13:13,622 - INFO - >>> {'loss': 2.2836, 'grad_norm': 0.8508961796760559, 'learning_rate': 1.840733985652046e-05, 'epoch': 2.7394957983193278}
>>> 2025-09-10 22:13:17,007 - INFO - >>> {'loss': 2.2259, 'grad_norm': 0.8387213945388794, 'learning_rate': 1.8402570530229903e-05, 'epoch': 2.7436974789915967}
>>> 2025-09-10 22:13:20,985 - INFO - >>> {'loss': 2.1771, 'grad_norm': 0.8354451060295105, 'learning_rate': 1.839779469337271e-05, 'epoch': 2.7478991596638656}
>>> 2025-09-10 22:13:24,526 - INFO - >>> {'loss': 2.268, 'grad_norm': 0.8464263677597046, 'learning_rate': 1.839301234964935e-05, 'epoch': 2.7521008403361344}
>>> 2025-09-10 22:13:28,076 - INFO - >>> {'loss': 2.2374, 'grad_norm': 0.9100134372711182, 'learning_rate': 1.8388223502765328e-05, 'epoch': 2.7563025210084033}
>>> 2025-09-10 22:13:31,146 - INFO - >>> {'loss': 2.3566, 'grad_norm': 0.8881435990333557, 'learning_rate': 1.8383428156431184e-05, 'epoch': 2.7605042016806722}
>>> 2025-09-10 22:13:34,593 - INFO - >>> {'loss': 2.2214, 'grad_norm': 0.83697909116745, 'learning_rate': 1.83786263143625e-05, 'epoch': 2.764705882352941}
>>> 2025-09-10 22:13:38,056 - INFO - >>> {'loss': 2.1661, 'grad_norm': 0.8417865633964539, 'learning_rate': 1.8373817980279893e-05, 'epoch': 2.76890756302521}
>>> 2025-09-10 22:13:41,147 - INFO - >>> {'loss': 2.2249, 'grad_norm': 0.9319252967834473, 'learning_rate': 1.8369003157909e-05, 'epoch': 2.773109243697479}
>>> 2025-09-10 22:13:45,159 - INFO - >>> {'loss': 2.2394, 'grad_norm': 0.8356283903121948, 'learning_rate': 1.836418185098049e-05, 'epoch': 2.777310924369748}
>>> 2025-09-10 22:13:49,124 - INFO - >>> {'loss': 2.2676, 'grad_norm': 0.8270955085754395, 'learning_rate': 1.8359354063230068e-05, 'epoch': 2.7815126050420167}
>>> 2025-09-10 22:13:52,856 - INFO - >>> {'loss': 2.3103, 'grad_norm': 0.7604230046272278, 'learning_rate': 1.8354519798398444e-05, 'epoch': 2.7857142857142856}
>>> 2025-09-10 22:13:56,574 - INFO - >>> {'loss': 2.3395, 'grad_norm': 0.8160949349403381, 'learning_rate': 1.834967906023135e-05, 'epoch': 2.7899159663865545}
>>> 2025-09-10 22:14:00,323 - INFO - >>> {'loss': 2.1666, 'grad_norm': 0.8696394562721252, 'learning_rate': 1.834483185247954e-05, 'epoch': 2.7941176470588234}
>>> 2025-09-10 22:14:04,122 - INFO - >>> {'loss': 2.2385, 'grad_norm': 0.9150638580322266, 'learning_rate': 1.833997817889878e-05, 'epoch': 2.7983193277310923}
>>> 2025-09-10 22:14:07,912 - INFO - >>> {'loss': 2.3189, 'grad_norm': 0.9706003069877625, 'learning_rate': 1.8335118043249844e-05, 'epoch': 2.802521008403361}
>>> 2025-09-10 22:14:11,331 - INFO - >>> {'loss': 2.2148, 'grad_norm': 0.8098156452178955, 'learning_rate': 1.8330251449298505e-05, 'epoch': 2.80672268907563}
>>> 2025-09-10 22:14:15,381 - INFO - >>> {'loss': 2.2517, 'grad_norm': 0.7928622961044312, 'learning_rate': 1.8325378400815556e-05, 'epoch': 2.810924369747899}
>>> 2025-09-10 22:14:18,817 - INFO - >>> {'loss': 2.3128, 'grad_norm': 0.8138745427131653, 'learning_rate': 1.8320498901576776e-05, 'epoch': 2.815126050420168}
>>> 2025-09-10 22:14:22,156 - INFO - >>> {'loss': 2.2675, 'grad_norm': 0.8998333811759949, 'learning_rate': 1.8315612955362953e-05, 'epoch': 2.8193277310924367}
>>> 2025-09-10 22:14:25,231 - INFO - >>> {'loss': 2.2366, 'grad_norm': 0.914055347442627, 'learning_rate': 1.831072056595987e-05, 'epoch': 2.8235294117647056}
>>> 2025-09-10 22:14:28,916 - INFO - >>> {'loss': 2.2856, 'grad_norm': 0.8867859840393066, 'learning_rate': 1.830582173715829e-05, 'epoch': 2.8277310924369745}
>>> 2025-09-10 22:14:32,790 - INFO - >>> {'loss': 2.318, 'grad_norm': 0.7759544253349304, 'learning_rate': 1.8300916472753984e-05, 'epoch': 2.831932773109244}
>>> 2025-09-10 22:14:36,158 - INFO - >>> {'loss': 2.163, 'grad_norm': 0.8653221130371094, 'learning_rate': 1.829600477654769e-05, 'epoch': 2.8361344537815127}
>>> 2025-09-10 22:14:39,593 - INFO - >>> {'loss': 2.2871, 'grad_norm': 0.9107792377471924, 'learning_rate': 1.8291086652345145e-05, 'epoch': 2.8403361344537816}
>>> 2025-09-10 22:14:43,572 - INFO - >>> {'loss': 2.2202, 'grad_norm': 0.9127411246299744, 'learning_rate': 1.8286162103957062e-05, 'epoch': 2.8445378151260505}
>>> 2025-09-10 22:14:47,676 - INFO - >>> {'loss': 2.2675, 'grad_norm': 0.8356267213821411, 'learning_rate': 1.8281231135199125e-05, 'epoch': 2.8487394957983194}
>>> 2025-09-10 22:14:51,740 - INFO - >>> {'loss': 2.3294, 'grad_norm': 0.8702985048294067, 'learning_rate': 1.8276293749892008e-05, 'epoch': 2.8529411764705883}
>>> 2025-09-10 22:14:55,555 - INFO - >>> {'loss': 2.3395, 'grad_norm': 0.8286640644073486, 'learning_rate': 1.827134995186133e-05, 'epoch': 2.857142857142857}
>>> 2025-09-10 22:14:59,613 - INFO - >>> {'loss': 2.2141, 'grad_norm': 0.8146951794624329, 'learning_rate': 1.8266399744937717e-05, 'epoch': 2.861344537815126}
>>> 2025-09-10 22:15:02,459 - INFO - >>> {'loss': 2.2557, 'grad_norm': 0.8993731141090393, 'learning_rate': 1.8261443132956724e-05, 'epoch': 2.865546218487395}
>>> 2025-09-10 22:15:05,342 - INFO - >>> {'loss': 2.2512, 'grad_norm': 0.9945814609527588, 'learning_rate': 1.825648011975889e-05, 'epoch': 2.869747899159664}
>>> 2025-09-10 22:15:09,137 - INFO - >>> {'loss': 2.314, 'grad_norm': 0.8054792284965515, 'learning_rate': 1.8251510709189706e-05, 'epoch': 2.8739495798319328}
>>> 2025-09-10 22:15:12,491 - INFO - >>> {'loss': 2.2738, 'grad_norm': 0.8931210041046143, 'learning_rate': 1.8246534905099626e-05, 'epoch': 2.8781512605042017}
>>> 2025-09-10 22:15:15,818 - INFO - >>> {'loss': 2.2104, 'grad_norm': 0.9034867286682129, 'learning_rate': 1.824155271134405e-05, 'epoch': 2.8823529411764706}
>>> 2025-09-10 22:15:19,653 - INFO - >>> {'loss': 2.2322, 'grad_norm': 0.9129059314727783, 'learning_rate': 1.8236564131783333e-05, 'epoch': 2.8865546218487395}
>>> 2025-09-10 22:15:22,901 - INFO - >>> {'loss': 2.1753, 'grad_norm': 0.7843219041824341, 'learning_rate': 1.8231569170282783e-05, 'epoch': 2.8907563025210083}
>>> 2025-09-10 22:15:26,595 - INFO - >>> {'loss': 2.3044, 'grad_norm': 0.8095802068710327, 'learning_rate': 1.8226567830712644e-05, 'epoch': 2.8949579831932772}
>>> 2025-09-10 22:15:29,913 - INFO - >>> {'loss': 2.2149, 'grad_norm': 0.9031873345375061, 'learning_rate': 1.8221560116948103e-05, 'epoch': 2.899159663865546}
>>> 2025-09-10 22:15:33,873 - INFO - >>> {'loss': 2.1878, 'grad_norm': 0.8218493461608887, 'learning_rate': 1.821654603286929e-05, 'epoch': 2.903361344537815}
>>> 2025-09-10 22:15:37,134 - INFO - >>> {'loss': 2.2954, 'grad_norm': 0.978966474533081, 'learning_rate': 1.8211525582361272e-05, 'epoch': 2.907563025210084}
>>> 2025-09-10 22:15:40,435 - INFO - >>> {'loss': 2.2635, 'grad_norm': 0.9231210350990295, 'learning_rate': 1.820649876931405e-05, 'epoch': 2.911764705882353}
>>> 2025-09-10 22:15:44,204 - INFO - >>> {'loss': 2.1894, 'grad_norm': 0.8936318159103394, 'learning_rate': 1.8201465597622544e-05, 'epoch': 2.9159663865546217}
>>> 2025-09-10 22:15:47,865 - INFO - >>> {'loss': 2.2587, 'grad_norm': 0.8534274697303772, 'learning_rate': 1.819642607118661e-05, 'epoch': 2.9201680672268906}
>>> 2025-09-10 22:15:51,113 - INFO - >>> {'loss': 2.3129, 'grad_norm': 0.8195251822471619, 'learning_rate': 1.8191380193911024e-05, 'epoch': 2.92436974789916}
>>> 2025-09-10 22:15:54,534 - INFO - >>> {'loss': 2.2664, 'grad_norm': 0.9594703316688538, 'learning_rate': 1.818632796970549e-05, 'epoch': 2.928571428571429}
>>> 2025-09-10 22:15:57,871 - INFO - >>> {'loss': 2.302, 'grad_norm': 0.8131330013275146, 'learning_rate': 1.8181269402484622e-05, 'epoch': 2.9327731092436977}
>>> 2025-09-10 22:16:00,731 - INFO - >>> {'loss': 2.2106, 'grad_norm': 0.8986170887947083, 'learning_rate': 1.8176204496167948e-05, 'epoch': 2.9369747899159666}
>>> 2025-09-10 22:16:03,796 - INFO - >>> {'loss': 2.3112, 'grad_norm': 0.9317590594291687, 'learning_rate': 1.817113325467992e-05, 'epoch': 2.9411764705882355}
>>> 2025-09-10 22:16:07,603 - INFO - >>> {'loss': 2.2221, 'grad_norm': 0.942327082157135, 'learning_rate': 1.8166055681949884e-05, 'epoch': 2.9453781512605044}
>>> 2025-09-10 22:16:10,717 - INFO - >>> {'loss': 2.2975, 'grad_norm': 0.8528614044189453, 'learning_rate': 1.8160971781912097e-05, 'epoch': 2.9495798319327733}
>>> 2025-09-10 22:16:14,409 - INFO - >>> {'loss': 2.2459, 'grad_norm': 0.8988120555877686, 'learning_rate': 1.8155881558505717e-05, 'epoch': 2.953781512605042}
>>> 2025-09-10 22:16:17,881 - INFO - >>> {'loss': 2.1547, 'grad_norm': 0.8682712912559509, 'learning_rate': 1.8150785015674814e-05, 'epoch': 2.957983193277311}
>>> 2025-09-10 22:16:20,933 - INFO - >>> {'loss': 2.1239, 'grad_norm': 0.8836120367050171, 'learning_rate': 1.814568215736833e-05, 'epoch': 2.96218487394958}
>>> 2025-09-10 22:16:23,984 - INFO - >>> {'loss': 2.1967, 'grad_norm': 0.9138118624687195, 'learning_rate': 1.8140572987540118e-05, 'epoch': 2.966386554621849}
>>> 2025-09-10 22:16:27,335 - INFO - >>> {'loss': 2.2947, 'grad_norm': 0.8543798327445984, 'learning_rate': 1.813545751014893e-05, 'epoch': 2.9705882352941178}
>>> 2025-09-10 22:16:31,428 - INFO - >>> {'loss': 2.2804, 'grad_norm': 0.8484563231468201, 'learning_rate': 1.8130335729158376e-05, 'epoch': 2.9747899159663866}
>>> 2025-09-10 22:16:34,682 - INFO - >>> {'loss': 2.2473, 'grad_norm': 0.8304604887962341, 'learning_rate': 1.812520764853698e-05, 'epoch': 2.9789915966386555}
>>> 2025-09-10 22:16:38,387 - INFO - >>> {'loss': 2.2053, 'grad_norm': 0.9514666795730591, 'learning_rate': 1.8120073272258126e-05, 'epoch': 2.9831932773109244}
>>> 2025-09-10 22:16:41,919 - INFO - >>> {'loss': 2.1683, 'grad_norm': 0.9051789045333862, 'learning_rate': 1.8114932604300094e-05, 'epoch': 2.9873949579831933}
>>> 2025-09-10 22:16:45,517 - INFO - >>> {'loss': 2.2459, 'grad_norm': 0.8418107628822327, 'learning_rate': 1.810978564864602e-05, 'epoch': 2.991596638655462}
>>> 2025-09-10 22:16:49,390 - INFO - >>> {'loss': 2.297, 'grad_norm': 0.88270103931427, 'learning_rate': 1.8104632409283925e-05, 'epoch': 2.995798319327731}
>>> 2025-09-10 22:16:52,846 - INFO - >>> {'loss': 2.1634, 'grad_norm': 0.8901097774505615, 'learning_rate': 1.8099472890206696e-05, 'epoch': 3.0}
>>> 2025-09-10 22:16:56,699 - INFO - >>> {'loss': 2.2988, 'grad_norm': 0.8198055624961853, 'learning_rate': 1.8094307095412088e-05, 'epoch': 3.004201680672269}
>>> 2025-09-10 22:17:00,515 - INFO - >>> {'loss': 2.2545, 'grad_norm': 0.8412492275238037, 'learning_rate': 1.8089135028902707e-05, 'epoch': 3.008403361344538}
>>> 2025-09-10 22:17:04,050 - INFO - >>> {'loss': 2.322, 'grad_norm': 0.845768928527832, 'learning_rate': 1.8083956694686033e-05, 'epoch': 3.0126050420168067}
>>> 2025-09-10 22:17:08,084 - INFO - >>> {'loss': 2.2698, 'grad_norm': 0.7955915331840515, 'learning_rate': 1.80787720967744e-05, 'epoch': 3.0168067226890756}
>>> 2025-09-10 22:17:11,445 - INFO - >>> {'loss': 2.2081, 'grad_norm': 0.8793937563896179, 'learning_rate': 1.807358123918498e-05, 'epoch': 3.0210084033613445}
>>> 2025-09-10 22:17:15,280 - INFO - >>> {'loss': 2.2819, 'grad_norm': 0.8341402411460876, 'learning_rate': 1.806838412593982e-05, 'epoch': 3.0252100840336134}
>>> 2025-09-10 22:17:18,646 - INFO - >>> {'loss': 2.0829, 'grad_norm': 1.0372644662857056, 'learning_rate': 1.8063180761065793e-05, 'epoch': 3.0294117647058822}
>>> 2025-09-10 22:17:21,924 - INFO - >>> {'loss': 2.2319, 'grad_norm': 0.8810106515884399, 'learning_rate': 1.805797114859463e-05, 'epoch': 3.033613445378151}
>>> 2025-09-10 22:17:25,432 - INFO - >>> {'loss': 2.2162, 'grad_norm': 0.8052067160606384, 'learning_rate': 1.805275529256289e-05, 'epoch': 3.03781512605042}
>>> 2025-09-10 22:17:28,687 - INFO - >>> {'loss': 2.3286, 'grad_norm': 0.9925247430801392, 'learning_rate': 1.804753319701198e-05, 'epoch': 3.042016806722689}
>>> 2025-09-10 22:17:32,198 - INFO - >>> {'loss': 2.1495, 'grad_norm': 0.8294845819473267, 'learning_rate': 1.8042304865988138e-05, 'epoch': 3.046218487394958}
>>> 2025-09-10 22:17:35,328 - INFO - >>> {'loss': 2.2932, 'grad_norm': 0.8879892826080322, 'learning_rate': 1.8037070303542434e-05, 'epoch': 3.0504201680672267}
>>> 2025-09-10 22:17:38,590 - INFO - >>> {'loss': 2.2331, 'grad_norm': 0.9848431944847107, 'learning_rate': 1.8031829513730766e-05, 'epoch': 3.0546218487394956}
>>> 2025-09-10 22:17:42,413 - INFO - >>> {'loss': 2.2532, 'grad_norm': 0.9112218618392944, 'learning_rate': 1.8026582500613855e-05, 'epoch': 3.0588235294117645}
>>> 2025-09-10 22:17:45,889 - INFO - >>> {'loss': 2.2663, 'grad_norm': 0.852496862411499, 'learning_rate': 1.802132926825725e-05, 'epoch': 3.0630252100840334}
>>> 2025-09-10 22:17:48,931 - INFO - >>> {'loss': 2.1936, 'grad_norm': 0.928224503993988, 'learning_rate': 1.801606982073131e-05, 'epoch': 3.0672268907563027}
>>> 2025-09-10 22:17:51,965 - INFO - >>> {'loss': 2.2413, 'grad_norm': 0.9048638343811035, 'learning_rate': 1.8010804162111215e-05, 'epoch': 3.0714285714285716}
>>> 2025-09-10 22:17:55,755 - INFO - >>> {'loss': 2.2638, 'grad_norm': 0.9100524187088013, 'learning_rate': 1.800553229647696e-05, 'epoch': 3.0756302521008405}
>>> 2025-09-10 22:17:59,744 - INFO - >>> {'loss': 2.2929, 'grad_norm': 0.8729097247123718, 'learning_rate': 1.8000254227913346e-05, 'epoch': 3.0798319327731094}
>>> 2025-09-10 22:18:02,872 - INFO - >>> {'loss': 2.3282, 'grad_norm': 0.8599436283111572, 'learning_rate': 1.799496996050998e-05, 'epoch': 3.0840336134453783}
>>> 2025-09-10 22:18:06,119 - INFO - >>> {'loss': 2.2375, 'grad_norm': 1.0092921257019043, 'learning_rate': 1.798967949836127e-05, 'epoch': 3.088235294117647}
>>> 2025-09-10 22:18:09,885 - INFO - >>> {'loss': 2.1694, 'grad_norm': 0.8793180584907532, 'learning_rate': 1.7984382845566433e-05, 'epoch': 3.092436974789916}
>>> 2025-09-10 22:18:13,968 - INFO - >>> {'loss': 2.2225, 'grad_norm': 0.8075695037841797, 'learning_rate': 1.7979080006229474e-05, 'epoch': 3.096638655462185}
>>> 2025-09-10 22:18:17,895 - INFO - >>> {'loss': 2.2547, 'grad_norm': 0.9180944561958313, 'learning_rate': 1.7973770984459186e-05, 'epoch': 3.100840336134454}
>>> 2025-09-10 22:18:21,381 - INFO - >>> {'loss': 2.2099, 'grad_norm': 0.8651089072227478, 'learning_rate': 1.796845578436917e-05, 'epoch': 3.1050420168067228}
>>> 2025-09-10 22:18:24,920 - INFO - >>> {'loss': 2.1818, 'grad_norm': 0.8723177909851074, 'learning_rate': 1.7963134410077802e-05, 'epoch': 3.1092436974789917}
>>> 2025-09-10 22:18:28,541 - INFO - >>> {'loss': 2.194, 'grad_norm': 0.9709435701370239, 'learning_rate': 1.7957806865708244e-05, 'epoch': 3.1134453781512605}
>>> 2025-09-10 22:18:32,373 - INFO - >>> {'loss': 2.1593, 'grad_norm': 0.819343626499176, 'learning_rate': 1.7952473155388436e-05, 'epoch': 3.1176470588235294}
>>> 2025-09-10 22:18:35,885 - INFO - >>> {'loss': 2.1989, 'grad_norm': 0.9229530692100525, 'learning_rate': 1.79471332832511e-05, 'epoch': 3.1218487394957983}
>>> 2025-09-10 22:18:40,007 - INFO - >>> {'loss': 2.2679, 'grad_norm': 0.7383795976638794, 'learning_rate': 1.7941787253433734e-05, 'epoch': 3.1260504201680672}
>>> 2025-09-10 22:18:43,704 - INFO - >>> {'loss': 2.2795, 'grad_norm': 0.9071753621101379, 'learning_rate': 1.7936435070078602e-05, 'epoch': 3.130252100840336}
>>> 2025-09-10 22:18:46,667 - INFO - >>> {'loss': 2.3251, 'grad_norm': 0.9219383597373962, 'learning_rate': 1.7931076737332734e-05, 'epoch': 3.134453781512605}
>>> 2025-09-10 22:18:50,053 - INFO - >>> {'loss': 2.176, 'grad_norm': 0.9312608242034912, 'learning_rate': 1.792571225934794e-05, 'epoch': 3.138655462184874}
>>> 2025-09-10 22:18:53,678 - INFO - >>> {'loss': 2.1521, 'grad_norm': 0.9362261891365051, 'learning_rate': 1.7920341640280767e-05, 'epoch': 3.142857142857143}
>>> 2025-09-10 22:18:57,653 - INFO - >>> {'loss': 2.2224, 'grad_norm': 0.918171763420105, 'learning_rate': 1.7914964884292543e-05, 'epoch': 3.1470588235294117}
>>> 2025-09-10 22:19:01,442 - INFO - >>> {'loss': 2.2173, 'grad_norm': 0.8404167294502258, 'learning_rate': 1.7909581995549336e-05, 'epoch': 3.1512605042016806}
>>> 2025-09-10 22:19:04,765 - INFO - >>> {'loss': 2.2657, 'grad_norm': 0.9025554656982422, 'learning_rate': 1.7904192978221978e-05, 'epoch': 3.1554621848739495}
>>> 2025-09-10 22:19:08,413 - INFO - >>> {'loss': 2.216, 'grad_norm': 0.9867837429046631, 'learning_rate': 1.7898797836486036e-05, 'epoch': 3.1596638655462184}
>>> 2025-09-10 22:19:12,166 - INFO - >>> {'loss': 2.3267, 'grad_norm': 0.9648093581199646, 'learning_rate': 1.7893396574521834e-05, 'epoch': 3.1638655462184873}
>>> 2025-09-10 22:19:15,518 - INFO - >>> {'loss': 2.2253, 'grad_norm': 0.8875378370285034, 'learning_rate': 1.788798919651443e-05, 'epoch': 3.168067226890756}
>>> 2025-09-10 22:19:18,480 - INFO - >>> {'loss': 2.2738, 'grad_norm': 0.9685620069503784, 'learning_rate': 1.7882575706653634e-05, 'epoch': 3.172268907563025}
>>> 2025-09-10 22:19:22,211 - INFO - >>> {'loss': 2.1762, 'grad_norm': 0.7806559801101685, 'learning_rate': 1.787715610913397e-05, 'epoch': 3.176470588235294}
>>> 2025-09-10 22:19:25,454 - INFO - >>> {'loss': 2.2747, 'grad_norm': 0.8030394315719604, 'learning_rate': 1.7871730408154714e-05, 'epoch': 3.180672268907563}
>>> 2025-09-10 22:19:29,247 - INFO - >>> {'loss': 2.3342, 'grad_norm': 0.8793873190879822, 'learning_rate': 1.786629860791986e-05, 'epoch': 3.184873949579832}
>>> 2025-09-10 22:19:32,069 - INFO - >>> {'loss': 2.2056, 'grad_norm': 1.0298256874084473, 'learning_rate': 1.7860860712638134e-05, 'epoch': 3.189075630252101}
>>> 2025-09-10 22:19:35,661 - INFO - >>> {'loss': 2.2134, 'grad_norm': 0.8664652705192566, 'learning_rate': 1.7855416726522977e-05, 'epoch': 3.19327731092437}
>>> 2025-09-10 22:19:39,187 - INFO - >>> {'loss': 2.2946, 'grad_norm': 0.8530965447425842, 'learning_rate': 1.7849966653792566e-05, 'epoch': 3.197478991596639}
>>> 2025-09-10 22:19:42,283 - INFO - >>> {'loss': 2.2345, 'grad_norm': 0.9841359257698059, 'learning_rate': 1.7844510498669772e-05, 'epoch': 3.2016806722689077}
>>> 2025-09-10 22:19:45,689 - INFO - >>> {'loss': 2.2457, 'grad_norm': 0.8807244896888733, 'learning_rate': 1.7839048265382196e-05, 'epoch': 3.2058823529411766}
>>> 2025-09-10 22:19:49,668 - INFO - >>> {'loss': 2.2651, 'grad_norm': 0.8234082460403442, 'learning_rate': 1.7833579958162136e-05, 'epoch': 3.2100840336134455}
>>> 2025-09-10 22:19:53,228 - INFO - >>> {'loss': 2.1888, 'grad_norm': 0.9303844571113586, 'learning_rate': 1.7828105581246612e-05, 'epoch': 3.2142857142857144}
>>> 2025-09-10 22:19:56,349 - INFO - >>> {'loss': 2.2249, 'grad_norm': 0.935225248336792, 'learning_rate': 1.7822625138877326e-05, 'epoch': 3.2184873949579833}
>>> 2025-09-10 22:20:00,111 - INFO - >>> {'loss': 2.2043, 'grad_norm': 0.8309416174888611, 'learning_rate': 1.78171386353007e-05, 'epoch': 3.222689075630252}
>>> 2025-09-10 22:20:03,894 - INFO - >>> {'loss': 2.2706, 'grad_norm': 0.9964028596878052, 'learning_rate': 1.7811646074767835e-05, 'epoch': 3.226890756302521}
>>> 2025-09-10 22:20:07,801 - INFO - >>> {'loss': 2.238, 'grad_norm': 0.8485813140869141, 'learning_rate': 1.7806147461534545e-05, 'epoch': 3.23109243697479}
>>> 2025-09-10 22:20:11,575 - INFO - >>> {'loss': 2.2022, 'grad_norm': 0.8668590784072876, 'learning_rate': 1.7800642799861315e-05, 'epoch': 3.235294117647059}
>>> 2025-09-10 22:20:15,399 - INFO - >>> {'loss': 2.2553, 'grad_norm': 0.8302441835403442, 'learning_rate': 1.779513209401332e-05, 'epoch': 3.2394957983193278}
>>> 2025-09-10 22:20:18,548 - INFO - >>> {'loss': 2.312, 'grad_norm': 0.9664711356163025, 'learning_rate': 1.778961534826043e-05, 'epoch': 3.2436974789915967}
>>> 2025-09-10 22:20:21,614 - INFO - >>> {'loss': 2.2566, 'grad_norm': 1.0084148645401, 'learning_rate': 1.7784092566877186e-05, 'epoch': 3.2478991596638656}
>>> 2025-09-10 22:20:24,848 - INFO - >>> {'loss': 2.208, 'grad_norm': 0.9365705847740173, 'learning_rate': 1.7778563754142803e-05, 'epoch': 3.2521008403361344}
>>> 2025-09-10 22:20:28,529 - INFO - >>> {'loss': 2.2245, 'grad_norm': 0.9768350720405579, 'learning_rate': 1.777302891434117e-05, 'epoch': 3.2563025210084033}
>>> 2025-09-10 22:20:31,955 - INFO - >>> {'loss': 2.2351, 'grad_norm': 0.9286491870880127, 'learning_rate': 1.7767488051760858e-05, 'epoch': 3.2605042016806722}
>>> 2025-09-10 22:20:35,302 - INFO - >>> {'loss': 2.3766, 'grad_norm': 0.9725916981697083, 'learning_rate': 1.7761941170695088e-05, 'epoch': 3.264705882352941}
>>> 2025-09-10 22:20:38,379 - INFO - >>> {'loss': 2.2578, 'grad_norm': 0.8948935866355896, 'learning_rate': 1.7756388275441757e-05, 'epoch': 3.26890756302521}
>>> 2025-09-10 22:20:41,519 - INFO - >>> {'loss': 2.3047, 'grad_norm': 0.9728753566741943, 'learning_rate': 1.7750829370303413e-05, 'epoch': 3.273109243697479}
>>> 2025-09-10 22:20:45,173 - INFO - >>> {'loss': 2.1601, 'grad_norm': 1.0073331594467163, 'learning_rate': 1.7745264459587266e-05, 'epoch': 3.277310924369748}
>>> 2025-09-10 22:20:48,169 - INFO - >>> {'loss': 2.2406, 'grad_norm': 0.9698658585548401, 'learning_rate': 1.7739693547605177e-05, 'epoch': 3.2815126050420167}
>>> 2025-09-10 22:20:51,933 - INFO - >>> {'loss': 2.2452, 'grad_norm': 0.8711909651756287, 'learning_rate': 1.773411663867366e-05, 'epoch': 3.2857142857142856}
>>> 2025-09-10 22:20:55,945 - INFO - >>> {'loss': 2.2307, 'grad_norm': 0.8695172667503357, 'learning_rate': 1.772853373711387e-05, 'epoch': 3.2899159663865545}
>>> 2025-09-10 22:20:59,655 - INFO - >>> {'loss': 2.2853, 'grad_norm': 0.8607990145683289, 'learning_rate': 1.7722944847251607e-05, 'epoch': 3.2941176470588234}
>>> 2025-09-10 22:21:02,673 - INFO - >>> {'loss': 2.2627, 'grad_norm': 0.9440174102783203, 'learning_rate': 1.771734997341732e-05, 'epoch': 3.2983193277310923}
>>> 2025-09-10 22:21:06,741 - INFO - >>> {'loss': 2.2189, 'grad_norm': 0.7992054224014282, 'learning_rate': 1.771174911994608e-05, 'epoch': 3.302521008403361}
>>> 2025-09-10 22:21:10,014 - INFO - >>> {'loss': 2.2129, 'grad_norm': 0.8899982571601868, 'learning_rate': 1.77061422911776e-05, 'epoch': 3.30672268907563}
>>> 2025-09-10 22:21:13,300 - INFO - >>> {'loss': 2.2202, 'grad_norm': 1.0188703536987305, 'learning_rate': 1.770052949145622e-05, 'epoch': 3.310924369747899}
>>> 2025-09-10 22:21:16,979 - INFO - >>> {'loss': 2.2138, 'grad_norm': 0.8136571049690247, 'learning_rate': 1.7694910725130913e-05, 'epoch': 3.315126050420168}
>>> 2025-09-10 22:21:20,009 - INFO - >>> {'loss': 2.1789, 'grad_norm': 0.9370080232620239, 'learning_rate': 1.7689285996555265e-05, 'epoch': 3.3193277310924367}
>>> 2025-09-10 22:21:23,060 - INFO - >>> {'loss': 2.2771, 'grad_norm': 1.0199147462844849, 'learning_rate': 1.7683655310087485e-05, 'epoch': 3.323529411764706}
>>> 2025-09-10 22:21:26,587 - INFO - >>> {'loss': 2.2297, 'grad_norm': 0.933860182762146, 'learning_rate': 1.7678018670090406e-05, 'epoch': 3.327731092436975}
>>> 2025-09-10 22:21:30,000 - INFO - >>> {'loss': 2.2818, 'grad_norm': 0.8889676928520203, 'learning_rate': 1.767237608093146e-05, 'epoch': 3.331932773109244}
>>> 2025-09-10 22:21:33,658 - INFO - >>> {'loss': 2.1218, 'grad_norm': 0.939860463142395, 'learning_rate': 1.7666727546982702e-05, 'epoch': 3.3361344537815127}
>>> 2025-09-10 22:21:36,687 - INFO - >>> {'loss': 2.1496, 'grad_norm': 0.9608103632926941, 'learning_rate': 1.7661073072620785e-05, 'epoch': 3.3403361344537816}
>>> 2025-09-10 22:21:40,353 - INFO - >>> {'loss': 2.278, 'grad_norm': 0.8113869428634644, 'learning_rate': 1.7655412662226967e-05, 'epoch': 3.3445378151260505}
>>> 2025-09-10 22:21:44,412 - INFO - >>> {'loss': 2.2865, 'grad_norm': 0.8784053325653076, 'learning_rate': 1.7649746320187105e-05, 'epoch': 3.3487394957983194}
>>> 2025-09-10 22:21:47,715 - INFO - >>> {'loss': 2.221, 'grad_norm': 0.9203938841819763, 'learning_rate': 1.764407405089166e-05, 'epoch': 3.3529411764705883}
>>> 2025-09-10 22:21:51,006 - INFO - >>> {'loss': 2.2769, 'grad_norm': 0.9569215774536133, 'learning_rate': 1.7638395858735666e-05, 'epoch': 3.357142857142857}
>>> 2025-09-10 22:21:54,543 - INFO - >>> {'loss': 2.2815, 'grad_norm': 0.8618686199188232, 'learning_rate': 1.7632711748118772e-05, 'epoch': 3.361344537815126}
>>> 2025-09-10 22:21:57,908 - INFO - >>> {'loss': 2.2502, 'grad_norm': 1.041520595550537, 'learning_rate': 1.762702172344519e-05, 'epoch': 3.365546218487395}
>>> 2025-09-10 22:22:01,591 - INFO - >>> {'loss': 2.3156, 'grad_norm': 0.8217893838882446, 'learning_rate': 1.7621325789123722e-05, 'epoch': 3.369747899159664}
>>> 2025-09-10 22:22:04,626 - INFO - >>> {'loss': 2.2295, 'grad_norm': 0.9806041121482849, 'learning_rate': 1.761562394956776e-05, 'epoch': 3.3739495798319328}
>>> 2025-09-10 22:22:08,396 - INFO - >>> {'loss': 2.3231, 'grad_norm': 0.8121023774147034, 'learning_rate': 1.7609916209195257e-05, 'epoch': 3.3781512605042017}
>>> 2025-09-10 22:22:11,987 - INFO - >>> {'loss': 2.1775, 'grad_norm': 0.9218750596046448, 'learning_rate': 1.7604202572428744e-05, 'epoch': 3.3823529411764706}
>>> 2025-09-10 22:22:15,777 - INFO - >>> {'loss': 2.2313, 'grad_norm': 0.845866858959198, 'learning_rate': 1.759848304369532e-05, 'epoch': 3.3865546218487395}
>>> 2025-09-10 22:22:18,716 - INFO - >>> {'loss': 2.235, 'grad_norm': 1.0116491317749023, 'learning_rate': 1.759275762742665e-05, 'epoch': 3.3907563025210083}
>>> 2025-09-10 22:22:22,419 - INFO - >>> {'loss': 2.1084, 'grad_norm': 0.8153939247131348, 'learning_rate': 1.7587026328058958e-05, 'epoch': 3.3949579831932772}
>>> 2025-09-10 22:22:25,284 - INFO - >>> {'loss': 2.1479, 'grad_norm': 0.999864399433136, 'learning_rate': 1.7581289150033036e-05, 'epoch': 3.399159663865546}
>>> 2025-09-10 22:22:29,168 - INFO - >>> {'loss': 2.2758, 'grad_norm': 0.834204375743866, 'learning_rate': 1.7575546097794213e-05, 'epoch': 3.403361344537815}
>>> 2025-09-10 22:22:32,682 - INFO - >>> {'loss': 2.2745, 'grad_norm': 0.9559968113899231, 'learning_rate': 1.7569797175792385e-05, 'epoch': 3.407563025210084}
>>> 2025-09-10 22:22:35,567 - INFO - >>> {'loss': 2.2441, 'grad_norm': 0.9933448433876038, 'learning_rate': 1.7564042388481993e-05, 'epoch': 3.411764705882353}
>>> 2025-09-10 22:22:38,785 - INFO - >>> {'loss': 2.2417, 'grad_norm': 0.8438660502433777, 'learning_rate': 1.7558281740322024e-05, 'epoch': 3.4159663865546217}
>>> 2025-09-10 22:22:42,923 - INFO - >>> {'loss': 2.3192, 'grad_norm': 0.9288619756698608, 'learning_rate': 1.755251523577599e-05, 'epoch': 3.4201680672268906}
>>> 2025-09-10 22:22:46,520 - INFO - >>> {'loss': 2.2034, 'grad_norm': 0.9231699109077454, 'learning_rate': 1.7546742879311968e-05, 'epoch': 3.4243697478991595}
>>> 2025-09-10 22:22:49,548 - INFO - >>> {'loss': 2.2647, 'grad_norm': 1.0013625621795654, 'learning_rate': 1.754096467540255e-05, 'epoch': 3.4285714285714284}
>>> 2025-09-10 22:22:53,392 - INFO - >>> {'loss': 2.2749, 'grad_norm': 0.9047514796257019, 'learning_rate': 1.7535180628524857e-05, 'epoch': 3.4327731092436977}
>>> 2025-09-10 22:22:56,693 - INFO - >>> {'loss': 2.2173, 'grad_norm': 0.9470909833908081, 'learning_rate': 1.7529390743160553e-05, 'epoch': 3.4369747899159666}
>>> 2025-09-10 22:22:59,945 - INFO - >>> {'loss': 2.2734, 'grad_norm': 0.9506648182868958, 'learning_rate': 1.7523595023795814e-05, 'epoch': 3.4411764705882355}
>>> 2025-09-10 22:23:02,985 - INFO - >>> {'loss': 2.1985, 'grad_norm': 0.94139564037323, 'learning_rate': 1.751779347492134e-05, 'epoch': 3.4453781512605044}
>>> 2025-09-10 22:23:06,300 - INFO - >>> {'loss': 2.2219, 'grad_norm': 0.9709213972091675, 'learning_rate': 1.7511986101032344e-05, 'epoch': 3.4495798319327733}
>>> 2025-09-10 22:23:09,843 - INFO - >>> {'loss': 2.2715, 'grad_norm': 0.8283590078353882, 'learning_rate': 1.7506172906628558e-05, 'epoch': 3.453781512605042}
>>> 2025-09-10 22:23:12,899 - INFO - >>> {'loss': 2.2346, 'grad_norm': 0.9500868320465088, 'learning_rate': 1.7500353896214225e-05, 'epoch': 3.457983193277311}
>>> 2025-09-10 22:23:16,103 - INFO - >>> {'loss': 2.1397, 'grad_norm': 0.9963483214378357, 'learning_rate': 1.749452907429808e-05, 'epoch': 3.46218487394958}
>>> 2025-09-10 22:23:19,177 - INFO - >>> {'loss': 2.3489, 'grad_norm': 0.9702417850494385, 'learning_rate': 1.7488698445393385e-05, 'epoch': 3.466386554621849}
>>> 2025-09-10 22:23:22,884 - INFO - >>> {'loss': 2.3725, 'grad_norm': 0.9132190346717834, 'learning_rate': 1.7482862014017878e-05, 'epoch': 3.4705882352941178}
>>> 2025-09-10 22:23:26,891 - INFO - >>> {'loss': 2.3412, 'grad_norm': 0.9723485112190247, 'learning_rate': 1.747701978469381e-05, 'epoch': 3.4747899159663866}
>>> 2025-09-10 22:23:30,157 - INFO - >>> {'loss': 2.1669, 'grad_norm': 0.857082724571228, 'learning_rate': 1.7471171761947912e-05, 'epoch': 3.4789915966386555}
>>> 2025-09-10 22:23:34,170 - INFO - >>> {'loss': 2.1695, 'grad_norm': 0.8836025595664978, 'learning_rate': 1.7465317950311414e-05, 'epoch': 3.4831932773109244}
>>> 2025-09-10 22:23:37,191 - INFO - >>> {'loss': 2.214, 'grad_norm': 0.9102168083190918, 'learning_rate': 1.7459458354320024e-05, 'epoch': 3.4873949579831933}
>>> 2025-09-10 22:23:40,263 - INFO - >>> {'loss': 2.2759, 'grad_norm': 0.9099526405334473, 'learning_rate': 1.7453592978513934e-05, 'epoch': 3.491596638655462}
>>> 2025-09-10 22:23:44,081 - INFO - >>> {'loss': 2.2437, 'grad_norm': 0.9297972917556763, 'learning_rate': 1.744772182743782e-05, 'epoch': 3.495798319327731}
>>> 2025-09-10 22:23:47,144 - INFO - >>> {'loss': 2.2584, 'grad_norm': 0.9761933088302612, 'learning_rate': 1.7441844905640827e-05, 'epoch': 3.5}
>>> 2025-09-10 22:23:50,344 - INFO - >>> {'loss': 2.1989, 'grad_norm': 0.8770825266838074, 'learning_rate': 1.7435962217676564e-05, 'epoch': 3.504201680672269}
>>> 2025-09-10 22:23:54,276 - INFO - >>> {'loss': 2.3214, 'grad_norm': 0.9490856528282166, 'learning_rate': 1.7430073768103128e-05, 'epoch': 3.508403361344538}
>>> 2025-09-10 22:23:57,690 - INFO - >>> {'loss': 2.2078, 'grad_norm': 0.9164750576019287, 'learning_rate': 1.7424179561483058e-05, 'epoch': 3.5126050420168067}
>>> 2025-09-10 22:24:01,056 - INFO - >>> {'loss': 2.1727, 'grad_norm': 0.8545618057250977, 'learning_rate': 1.7418279602383372e-05, 'epoch': 3.5168067226890756}
>>> 2025-09-10 22:24:05,073 - INFO - >>> {'loss': 2.3283, 'grad_norm': 0.8600975871086121, 'learning_rate': 1.7412373895375532e-05, 'epoch': 3.5210084033613445}
>>> 2025-09-10 22:24:08,512 - INFO - >>> {'loss': 2.1708, 'grad_norm': 0.8893265128135681, 'learning_rate': 1.7406462445035467e-05, 'epoch': 3.5252100840336134}
>>> 2025-09-10 22:24:11,841 - INFO - >>> {'loss': 2.2013, 'grad_norm': 0.8212019205093384, 'learning_rate': 1.7400545255943538e-05, 'epoch': 3.5294117647058822}
>>> 2025-09-10 22:24:14,935 - INFO - >>> {'loss': 2.3916, 'grad_norm': 0.9636291265487671, 'learning_rate': 1.7394622332684568e-05, 'epoch': 3.533613445378151}
>>> 2025-09-10 22:24:18,775 - INFO - >>> {'loss': 2.2514, 'grad_norm': 0.8412872552871704, 'learning_rate': 1.7388693679847816e-05, 'epoch': 3.53781512605042}
>>> 2025-09-10 22:24:21,752 - INFO - >>> {'loss': 2.2034, 'grad_norm': 0.9394099116325378, 'learning_rate': 1.738275930202699e-05, 'epoch': 3.542016806722689}
>>> 2025-09-10 22:24:24,807 - INFO - >>> {'loss': 2.2302, 'grad_norm': 0.9716130495071411, 'learning_rate': 1.737681920382021e-05, 'epoch': 3.546218487394958}
>>> 2025-09-10 22:24:28,643 - INFO - >>> {'loss': 2.3252, 'grad_norm': 0.8863876461982727, 'learning_rate': 1.7370873389830057e-05, 'epoch': 3.5504201680672267}
>>> 2025-09-10 22:24:32,307 - INFO - >>> {'loss': 2.306, 'grad_norm': 0.8746685981750488, 'learning_rate': 1.7364921864663526e-05, 'epoch': 3.5546218487394956}
>>> 2025-09-10 22:24:35,723 - INFO - >>> {'loss': 2.1372, 'grad_norm': 0.9433948397636414, 'learning_rate': 1.7358964632932033e-05, 'epoch': 3.5588235294117645}
>>> 2025-09-10 22:24:39,756 - INFO - >>> {'loss': 2.3312, 'grad_norm': 0.9082313179969788, 'learning_rate': 1.735300169925143e-05, 'epoch': 3.5630252100840334}
>>> 2025-09-10 22:24:43,358 - INFO - >>> {'loss': 2.205, 'grad_norm': 0.9282753467559814, 'learning_rate': 1.7347033068241977e-05, 'epoch': 3.5672268907563023}
>>> 2025-09-10 22:24:46,964 - INFO - >>> {'loss': 2.2748, 'grad_norm': 0.9936347603797913, 'learning_rate': 1.7341058744528348e-05, 'epoch': 3.571428571428571}
>>> 2025-09-10 22:24:50,247 - INFO - >>> {'loss': 2.2867, 'grad_norm': 0.9238707423210144, 'learning_rate': 1.733507873273963e-05, 'epoch': 3.57563025210084}
>>> 2025-09-10 22:24:53,487 - INFO - >>> {'loss': 2.2758, 'grad_norm': 0.9320566654205322, 'learning_rate': 1.732909303750932e-05, 'epoch': 3.5798319327731094}
>>> 2025-09-10 22:24:56,877 - INFO - >>> {'loss': 2.2082, 'grad_norm': 0.9305030703544617, 'learning_rate': 1.7323101663475313e-05, 'epoch': 3.5840336134453783}
>>> 2025-09-10 22:25:00,274 - INFO - >>> {'loss': 2.1823, 'grad_norm': 0.8609389662742615, 'learning_rate': 1.7317104615279917e-05, 'epoch': 3.588235294117647}
>>> 2025-09-10 22:25:03,998 - INFO - >>> {'loss': 2.2827, 'grad_norm': 0.9528653025627136, 'learning_rate': 1.7311101897569813e-05, 'epoch': 3.592436974789916}
>>> 2025-09-10 22:25:07,061 - INFO - >>> {'loss': 2.103, 'grad_norm': 0.9422031044960022, 'learning_rate': 1.7305093514996095e-05, 'epoch': 3.596638655462185}
>>> 2025-09-10 22:25:10,988 - INFO - >>> {'loss': 2.2654, 'grad_norm': 0.8993072509765625, 'learning_rate': 1.7299079472214245e-05, 'epoch': 3.600840336134454}
>>> 2025-09-10 22:25:14,056 - INFO - >>> {'loss': 2.2975, 'grad_norm': 0.8936375975608826, 'learning_rate': 1.7293059773884124e-05, 'epoch': 3.6050420168067228}
>>> 2025-09-10 22:25:17,404 - INFO - >>> {'loss': 2.2596, 'grad_norm': 1.0942052602767944, 'learning_rate': 1.728703442466997e-05, 'epoch': 3.6092436974789917}
>>> 2025-09-10 22:25:20,757 - INFO - >>> {'loss': 2.3542, 'grad_norm': 0.9243943691253662, 'learning_rate': 1.7281003429240408e-05, 'epoch': 3.6134453781512605}
>>> 2025-09-10 22:25:24,007 - INFO - >>> {'loss': 2.2305, 'grad_norm': 0.9215699434280396, 'learning_rate': 1.7274966792268442e-05, 'epoch': 3.6176470588235294}
>>> 2025-09-10 22:25:27,213 - INFO - >>> {'loss': 2.2341, 'grad_norm': 0.7949696183204651, 'learning_rate': 1.7268924518431437e-05, 'epoch': 3.6218487394957983}
>>> 2025-09-10 22:25:30,899 - INFO - >>> {'loss': 2.1926, 'grad_norm': 0.9419025182723999, 'learning_rate': 1.7262876612411138e-05, 'epoch': 3.6260504201680672}
>>> 2025-09-10 22:25:34,750 - INFO - >>> {'loss': 2.2885, 'grad_norm': 0.8759612441062927, 'learning_rate': 1.7256823078893635e-05, 'epoch': 3.630252100840336}
>>> 2025-09-10 22:25:37,809 - INFO - >>> {'loss': 2.2476, 'grad_norm': 0.9950739741325378, 'learning_rate': 1.7250763922569394e-05, 'epoch': 3.634453781512605}
>>> 2025-09-10 22:25:41,601 - INFO - >>> {'loss': 2.2986, 'grad_norm': 0.8668268322944641, 'learning_rate': 1.7244699148133233e-05, 'epoch': 3.638655462184874}
>>> 2025-09-10 22:25:45,344 - INFO - >>> {'loss': 2.2469, 'grad_norm': 0.9591310620307922, 'learning_rate': 1.7238628760284327e-05, 'epoch': 3.642857142857143}
>>> 2025-09-10 22:25:49,208 - INFO - >>> {'loss': 2.3314, 'grad_norm': 0.8773513436317444, 'learning_rate': 1.723255276372619e-05, 'epoch': 3.6470588235294117}
>>> 2025-09-10 22:25:52,543 - INFO - >>> {'loss': 2.2641, 'grad_norm': 0.9335417151451111, 'learning_rate': 1.7226471163166697e-05, 'epoch': 3.6512605042016806}
>>> 2025-09-10 22:25:55,938 - INFO - >>> {'loss': 2.1519, 'grad_norm': 0.9411596655845642, 'learning_rate': 1.7220383963318047e-05, 'epoch': 3.6554621848739495}
>>> 2025-09-10 22:25:59,282 - INFO - >>> {'loss': 2.3631, 'grad_norm': 0.9513255953788757, 'learning_rate': 1.7214291168896795e-05, 'epoch': 3.6596638655462184}
>>> 2025-09-10 22:26:02,386 - INFO - >>> {'loss': 2.235, 'grad_norm': 0.9179592728614807, 'learning_rate': 1.7208192784623818e-05, 'epoch': 3.6638655462184873}
>>> 2025-09-10 22:26:05,550 - INFO - >>> {'loss': 2.2278, 'grad_norm': 0.9766784310340881, 'learning_rate': 1.7202088815224332e-05, 'epoch': 3.668067226890756}
>>> 2025-09-10 22:26:08,633 - INFO - >>> {'loss': 2.2381, 'grad_norm': 0.8951795697212219, 'learning_rate': 1.719597926542788e-05, 'epoch': 3.6722689075630255}
>>> 2025-09-10 22:26:11,486 - INFO - >>> {'loss': 2.2545, 'grad_norm': 0.9830906391143799, 'learning_rate': 1.7189864139968316e-05, 'epoch': 3.6764705882352944}
>>> 2025-09-10 22:26:15,205 - INFO - >>> {'loss': 2.3338, 'grad_norm': 0.8790514469146729, 'learning_rate': 1.7183743443583837e-05, 'epoch': 3.6806722689075633}
>>> 2025-09-10 22:26:19,217 - INFO - >>> {'loss': 2.1929, 'grad_norm': 0.8105255365371704, 'learning_rate': 1.7177617181016937e-05, 'epoch': 3.684873949579832}
>>> 2025-09-10 22:26:23,211 - INFO - >>> {'loss': 2.3945, 'grad_norm': 0.9474080801010132, 'learning_rate': 1.717148535701444e-05, 'epoch': 3.689075630252101}
>>> 2025-09-10 22:26:26,216 - INFO - >>> {'loss': 2.261, 'grad_norm': 0.9560482501983643, 'learning_rate': 1.7165347976327453e-05, 'epoch': 3.69327731092437}
>>> 2025-09-10 22:26:29,050 - INFO - >>> {'loss': 2.2059, 'grad_norm': 1.0163495540618896, 'learning_rate': 1.7159205043711417e-05, 'epoch': 3.697478991596639}
>>> 2025-09-10 22:26:32,337 - INFO - >>> {'loss': 2.331, 'grad_norm': 0.9857044816017151, 'learning_rate': 1.715305656392606e-05, 'epoch': 3.7016806722689077}
>>> 2025-09-10 22:26:36,211 - INFO - >>> {'loss': 2.2538, 'grad_norm': 1.029115080833435, 'learning_rate': 1.714690254173541e-05, 'epoch': 3.7058823529411766}
>>> 2025-09-10 22:26:39,546 - INFO - >>> {'loss': 2.243, 'grad_norm': 0.9500064253807068, 'learning_rate': 1.714074298190779e-05, 'epoch': 3.7100840336134455}
>>> 2025-09-10 22:26:42,879 - INFO - >>> {'loss': 2.2007, 'grad_norm': 0.9959481954574585, 'learning_rate': 1.7134577889215815e-05, 'epoch': 3.7142857142857144}
>>> 2025-09-10 22:26:45,907 - INFO - >>> {'loss': 2.2535, 'grad_norm': 1.028149962425232, 'learning_rate': 1.7128407268436382e-05, 'epoch': 3.7184873949579833}
>>> 2025-09-10 22:26:49,569 - INFO - >>> {'loss': 2.1949, 'grad_norm': 0.8969436287879944, 'learning_rate': 1.7122231124350683e-05, 'epoch': 3.722689075630252}
>>> 2025-09-10 22:26:52,957 - INFO - >>> {'loss': 2.1586, 'grad_norm': 1.0108131170272827, 'learning_rate': 1.7116049461744176e-05, 'epoch': 3.726890756302521}
>>> 2025-09-10 22:26:56,295 - INFO - >>> {'loss': 2.2209, 'grad_norm': 0.8953768014907837, 'learning_rate': 1.71098622854066e-05, 'epoch': 3.73109243697479}
>>> 2025-09-10 22:26:59,506 - INFO - >>> {'loss': 2.2022, 'grad_norm': 0.9106987118721008, 'learning_rate': 1.7103669600131966e-05, 'epoch': 3.735294117647059}
>>> 2025-09-10 22:27:03,285 - INFO - >>> {'loss': 2.3161, 'grad_norm': 0.9025691747665405, 'learning_rate': 1.7097471410718562e-05, 'epoch': 3.7394957983193278}
>>> 2025-09-10 22:27:06,683 - INFO - >>> {'loss': 2.3239, 'grad_norm': 0.9214723110198975, 'learning_rate': 1.7091267721968926e-05, 'epoch': 3.7436974789915967}
>>> 2025-09-10 22:27:10,078 - INFO - >>> {'loss': 2.2251, 'grad_norm': 0.9157845973968506, 'learning_rate': 1.708505853868987e-05, 'epoch': 3.7478991596638656}
>>> 2025-09-10 22:27:13,066 - INFO - >>> {'loss': 2.256, 'grad_norm': 0.9996840953826904, 'learning_rate': 1.707884386569245e-05, 'epoch': 3.7521008403361344}
>>> 2025-09-10 22:27:16,989 - INFO - >>> {'loss': 2.3567, 'grad_norm': 0.8697893619537354, 'learning_rate': 1.7072623707791992e-05, 'epoch': 3.7563025210084033}
>>> 2025-09-10 22:27:20,202 - INFO - >>> {'loss': 2.2052, 'grad_norm': 0.9574936628341675, 'learning_rate': 1.706639806980806e-05, 'epoch': 3.7605042016806722}
>>> 2025-09-10 22:27:23,692 - INFO - >>> {'loss': 2.3033, 'grad_norm': 0.9008945226669312, 'learning_rate': 1.706016695656446e-05, 'epoch': 3.764705882352941}
>>> 2025-09-10 22:27:27,092 - INFO - >>> {'loss': 2.1894, 'grad_norm': 0.9739304184913635, 'learning_rate': 1.705393037288926e-05, 'epoch': 3.76890756302521}
>>> 2025-09-10 22:27:30,383 - INFO - >>> {'loss': 2.2486, 'grad_norm': 0.8970420956611633, 'learning_rate': 1.7047688323614745e-05, 'epoch': 3.773109243697479}
>>> 2025-09-10 22:27:34,304 - INFO - >>> {'loss': 2.0833, 'grad_norm': 0.8949309587478638, 'learning_rate': 1.7041440813577445e-05, 'epoch': 3.777310924369748}
>>> 2025-09-10 22:27:37,235 - INFO - >>> {'loss': 2.3168, 'grad_norm': 0.9765955209732056, 'learning_rate': 1.7035187847618125e-05, 'epoch': 3.7815126050420167}
>>> 2025-09-10 22:27:41,159 - INFO - >>> {'loss': 2.1887, 'grad_norm': 0.911926805973053, 'learning_rate': 1.702892943058177e-05, 'epoch': 3.7857142857142856}
>>> 2025-09-10 22:27:45,086 - INFO - >>> {'loss': 2.2652, 'grad_norm': 0.8894686102867126, 'learning_rate': 1.7022665567317592e-05, 'epoch': 3.7899159663865545}
>>> 2025-09-10 22:27:48,074 - INFO - >>> {'loss': 2.3016, 'grad_norm': 0.9692448973655701, 'learning_rate': 1.7016396262679023e-05, 'epoch': 3.7941176470588234}
>>> 2025-09-10 22:27:51,095 - INFO - >>> {'loss': 2.133, 'grad_norm': 1.0005033016204834, 'learning_rate': 1.7010121521523707e-05, 'epoch': 3.7983193277310923}
>>> 2025-09-10 22:27:54,492 - INFO - >>> {'loss': 2.2988, 'grad_norm': 1.0184718370437622, 'learning_rate': 1.700384134871351e-05, 'epoch': 3.802521008403361}
>>> 2025-09-10 22:27:57,873 - INFO - >>> {'loss': 2.204, 'grad_norm': 0.9592058658599854, 'learning_rate': 1.6997555749114493e-05, 'epoch': 3.80672268907563}
>>> 2025-09-10 22:28:01,468 - INFO - >>> {'loss': 2.2013, 'grad_norm': 0.9567479491233826, 'learning_rate': 1.6991264727596937e-05, 'epoch': 3.810924369747899}
>>> 2025-09-10 22:28:05,422 - INFO - >>> {'loss': 2.2019, 'grad_norm': 0.9743782877922058, 'learning_rate': 1.698496828903531e-05, 'epoch': 3.815126050420168}
>>> 2025-09-10 22:28:08,777 - INFO - >>> {'loss': 2.2953, 'grad_norm': 0.9865188598632812, 'learning_rate': 1.697866643830829e-05, 'epoch': 3.8193277310924367}
>>> 2025-09-10 22:28:12,052 - INFO - >>> {'loss': 2.2927, 'grad_norm': 0.906758725643158, 'learning_rate': 1.6972359180298733e-05, 'epoch': 3.8235294117647056}
>>> 2025-09-10 22:28:15,400 - INFO - >>> {'loss': 2.2277, 'grad_norm': 0.9215683937072754, 'learning_rate': 1.6966046519893704e-05, 'epoch': 3.8277310924369745}
>>> 2025-09-10 22:28:18,421 - INFO - >>> {'loss': 2.2139, 'grad_norm': 0.9349493384361267, 'learning_rate': 1.6959728461984437e-05, 'epoch': 3.831932773109244}
>>> 2025-09-10 22:28:22,109 - INFO - >>> {'loss': 2.2373, 'grad_norm': 0.9089315533638, 'learning_rate': 1.6953405011466356e-05, 'epoch': 3.8361344537815127}
>>> 2025-09-10 22:28:25,326 - INFO - >>> {'loss': 2.2845, 'grad_norm': 0.9133599400520325, 'learning_rate': 1.6947076173239068e-05, 'epoch': 3.8403361344537816}
>>> 2025-09-10 22:28:28,685 - INFO - >>> {'loss': 2.228, 'grad_norm': 1.004308819770813, 'learning_rate': 1.6940741952206342e-05, 'epoch': 3.8445378151260505}
>>> 2025-09-10 22:28:32,024 - INFO - >>> {'loss': 2.3275, 'grad_norm': 1.0267183780670166, 'learning_rate': 1.6934402353276125e-05, 'epoch': 3.8487394957983194}
>>> 2025-09-10 22:28:35,732 - INFO - >>> {'loss': 2.2001, 'grad_norm': 0.8807220458984375, 'learning_rate': 1.6928057381360535e-05, 'epoch': 3.8529411764705883}
>>> 2025-09-10 22:28:39,678 - INFO - >>> {'loss': 2.3896, 'grad_norm': 1.0042999982833862, 'learning_rate': 1.692170704137585e-05, 'epoch': 3.857142857142857}
>>> 2025-09-10 22:28:43,448 - INFO - >>> {'loss': 2.2147, 'grad_norm': 0.9025874137878418, 'learning_rate': 1.6915351338242496e-05, 'epoch': 3.861344537815126}
>>> 2025-09-10 22:28:46,802 - INFO - >>> {'loss': 2.2757, 'grad_norm': 0.8713467121124268, 'learning_rate': 1.6908990276885075e-05, 'epoch': 3.865546218487395}
>>> 2025-09-10 22:28:49,829 - INFO - >>> {'loss': 2.06, 'grad_norm': 0.9109483361244202, 'learning_rate': 1.6902623862232323e-05, 'epoch': 3.869747899159664}
>>> 2025-09-10 22:28:53,780 - INFO - >>> {'loss': 2.2171, 'grad_norm': 0.8970353007316589, 'learning_rate': 1.6896252099217138e-05, 'epoch': 3.8739495798319328}
>>> 2025-09-10 22:28:57,528 - INFO - >>> {'loss': 2.1989, 'grad_norm': 0.9693824052810669, 'learning_rate': 1.6889874992776554e-05, 'epoch': 3.8781512605042017}
>>> 2025-09-10 22:29:00,583 - INFO - >>> {'loss': 2.1952, 'grad_norm': 1.1394696235656738, 'learning_rate': 1.6883492547851745e-05, 'epoch': 3.8823529411764706}
>>> 2025-09-10 22:29:03,645 - INFO - >>> {'loss': 2.1872, 'grad_norm': 0.9138185381889343, 'learning_rate': 1.687710476938802e-05, 'epoch': 3.8865546218487395}
>>> 2025-09-10 22:29:06,813 - INFO - >>> {'loss': 2.3336, 'grad_norm': 1.0084363222122192, 'learning_rate': 1.687071166233483e-05, 'epoch': 3.8907563025210083}
>>> 2025-09-10 22:29:10,691 - INFO - >>> {'loss': 2.1488, 'grad_norm': 0.9400103092193604, 'learning_rate': 1.6864313231645743e-05, 'epoch': 3.8949579831932772}
>>> 2025-09-10 22:29:14,157 - INFO - >>> {'loss': 2.2167, 'grad_norm': 0.894184410572052, 'learning_rate': 1.6857909482278457e-05, 'epoch': 3.899159663865546}
>>> 2025-09-10 22:29:17,049 - INFO - >>> {'loss': 2.2177, 'grad_norm': 0.8967370390892029, 'learning_rate': 1.6851500419194796e-05, 'epoch': 3.903361344537815}
>>> 2025-09-10 22:29:20,306 - INFO - >>> {'loss': 2.2633, 'grad_norm': 0.9429660439491272, 'learning_rate': 1.6845086047360696e-05, 'epoch': 3.907563025210084}
>>> 2025-09-10 22:29:23,947 - INFO - >>> {'loss': 2.1604, 'grad_norm': 0.9182410836219788, 'learning_rate': 1.6838666371746198e-05, 'epoch': 3.911764705882353}
>>> 2025-09-10 22:29:27,196 - INFO - >>> {'loss': 2.2409, 'grad_norm': 0.9993258118629456, 'learning_rate': 1.6832241397325476e-05, 'epoch': 3.9159663865546217}
>>> 2025-09-10 22:29:30,793 - INFO - >>> {'loss': 2.2134, 'grad_norm': 0.9142584800720215, 'learning_rate': 1.6825811129076783e-05, 'epoch': 3.9201680672268906}
>>> 2025-09-10 22:29:34,304 - INFO - >>> {'loss': 2.2922, 'grad_norm': 0.8379700183868408, 'learning_rate': 1.6819375571982496e-05, 'epoch': 3.92436974789916}
>>> 2025-09-10 22:29:38,328 - INFO - >>> {'loss': 2.2892, 'grad_norm': 0.8764767646789551, 'learning_rate': 1.681293473102907e-05, 'epoch': 3.928571428571429}
>>> 2025-09-10 22:29:41,645 - INFO - >>> {'loss': 2.1335, 'grad_norm': 0.9622217416763306, 'learning_rate': 1.6806488611207072e-05, 'epoch': 3.9327731092436977}
>>> 2025-09-10 22:29:45,091 - INFO - >>> {'loss': 2.2891, 'grad_norm': 0.9709415435791016, 'learning_rate': 1.6800037217511142e-05, 'epoch': 3.9369747899159666}
>>> 2025-09-10 22:29:48,646 - INFO - >>> {'loss': 2.1846, 'grad_norm': 0.9222338795661926, 'learning_rate': 1.6793580554940025e-05, 'epoch': 3.9411764705882355}
>>> 2025-09-10 22:29:51,748 - INFO - >>> {'loss': 2.2648, 'grad_norm': 0.9672924280166626, 'learning_rate': 1.6787118628496536e-05, 'epoch': 3.9453781512605044}
>>> 2025-09-10 22:29:54,805 - INFO - >>> {'loss': 2.2022, 'grad_norm': 0.9683222770690918, 'learning_rate': 1.678065144318757e-05, 'epoch': 3.9495798319327733}
>>> 2025-09-10 22:29:58,739 - INFO - >>> {'loss': 2.2763, 'grad_norm': 0.8791508078575134, 'learning_rate': 1.6774179004024103e-05, 'epoch': 3.953781512605042}
>>> 2025-09-10 22:30:02,458 - INFO - >>> {'loss': 2.1412, 'grad_norm': 1.0091081857681274, 'learning_rate': 1.6767701316021167e-05, 'epoch': 3.957983193277311}
>>> 2025-09-10 22:30:06,171 - INFO - >>> {'loss': 2.2562, 'grad_norm': 0.8942965269088745, 'learning_rate': 1.6761218384197886e-05, 'epoch': 3.96218487394958}
>>> 2025-09-10 22:30:09,125 - INFO - >>> {'loss': 2.2377, 'grad_norm': 1.0499924421310425, 'learning_rate': 1.675473021357742e-05, 'epoch': 3.966386554621849}
>>> 2025-09-10 22:30:12,149 - INFO - >>> {'loss': 2.2228, 'grad_norm': 1.112760305404663, 'learning_rate': 1.6748236809187005e-05, 'epoch': 3.9705882352941178}
>>> 2025-09-10 22:30:15,681 - INFO - >>> {'loss': 2.2595, 'grad_norm': 0.8330934047698975, 'learning_rate': 1.674173817605793e-05, 'epoch': 3.9747899159663866}
>>> 2025-09-10 22:30:18,913 - INFO - >>> {'loss': 2.2932, 'grad_norm': 0.9590380191802979, 'learning_rate': 1.673523431922553e-05, 'epoch': 3.9789915966386555}
>>> 2025-09-10 22:30:22,423 - INFO - >>> {'loss': 2.2229, 'grad_norm': 0.8948655128479004, 'learning_rate': 1.672872524372919e-05, 'epoch': 3.9831932773109244}
>>> 2025-09-10 22:30:26,363 - INFO - >>> {'loss': 2.2534, 'grad_norm': 0.9680988192558289, 'learning_rate': 1.6722210954612343e-05, 'epoch': 3.9873949579831933}
>>> 2025-09-10 22:30:29,526 - INFO - >>> {'loss': 2.3324, 'grad_norm': 0.972869873046875, 'learning_rate': 1.6715691456922452e-05, 'epoch': 3.991596638655462}
>>> 2025-09-10 22:30:32,783 - INFO - >>> {'loss': 2.2566, 'grad_norm': 0.929251492023468, 'learning_rate': 1.670916675571103e-05, 'epoch': 3.995798319327731}
>>> 2025-09-10 22:30:36,165 - INFO - >>> {'loss': 2.3424, 'grad_norm': 0.9425768852233887, 'learning_rate': 1.6702636856033608e-05, 'epoch': 4.0}
>>> 2025-09-10 22:30:40,150 - INFO - >>> {'loss': 2.1708, 'grad_norm': 0.907782793045044, 'learning_rate': 1.669610176294975e-05, 'epoch': 4.004201680672269}
>>> 2025-09-10 22:30:43,252 - INFO - >>> {'loss': 2.2065, 'grad_norm': 0.9595661759376526, 'learning_rate': 1.6689561481523045e-05, 'epoch': 4.008403361344538}
>>> 2025-09-10 22:30:46,527 - INFO - >>> {'loss': 2.1653, 'grad_norm': 0.9678639769554138, 'learning_rate': 1.6683016016821102e-05, 'epoch': 4.012605042016807}
>>> 2025-09-10 22:30:50,224 - INFO - >>> {'loss': 2.2162, 'grad_norm': 0.9252249002456665, 'learning_rate': 1.667646537391555e-05, 'epoch': 4.016806722689076}
>>> 2025-09-10 22:30:53,544 - INFO - >>> {'loss': 2.2588, 'grad_norm': 0.9223973155021667, 'learning_rate': 1.6669909557882017e-05, 'epoch': 4.0210084033613445}
>>> 2025-09-10 22:30:57,551 - INFO - >>> {'loss': 2.2881, 'grad_norm': 0.8453827500343323, 'learning_rate': 1.6663348573800154e-05, 'epoch': 4.025210084033613}
>>> 2025-09-10 22:31:00,969 - INFO - >>> {'loss': 2.2104, 'grad_norm': 1.0817086696624756, 'learning_rate': 1.665678242675361e-05, 'epoch': 4.029411764705882}
>>> 2025-09-10 22:31:04,962 - INFO - >>> {'loss': 2.2332, 'grad_norm': 0.939873218536377, 'learning_rate': 1.6650211121830035e-05, 'epoch': 4.033613445378151}
>>> 2025-09-10 22:31:08,434 - INFO - >>> {'loss': 2.1878, 'grad_norm': 0.9135322570800781, 'learning_rate': 1.6643634664121075e-05, 'epoch': 4.03781512605042}
>>> 2025-09-10 22:31:12,580 - INFO - >>> {'loss': 2.2186, 'grad_norm': 0.8858047127723694, 'learning_rate': 1.6637053058722367e-05, 'epoch': 4.042016806722689}
>>> 2025-09-10 22:31:15,991 - INFO - >>> {'loss': 2.2406, 'grad_norm': 0.9636262655258179, 'learning_rate': 1.6630466310733545e-05, 'epoch': 4.046218487394958}
>>> 2025-09-10 22:31:18,940 - INFO - >>> {'loss': 2.2753, 'grad_norm': 0.9258819818496704, 'learning_rate': 1.662387442525821e-05, 'epoch': 4.050420168067227}
>>> 2025-09-10 22:31:22,849 - INFO - >>> {'loss': 2.2451, 'grad_norm': 0.9700101613998413, 'learning_rate': 1.661727740740397e-05, 'epoch': 4.054621848739496}
>>> 2025-09-10 22:31:25,939 - INFO - >>> {'loss': 2.2317, 'grad_norm': 0.9938113689422607, 'learning_rate': 1.661067526228238e-05, 'epoch': 4.0588235294117645}
>>> 2025-09-10 22:31:29,212 - INFO - >>> {'loss': 2.1877, 'grad_norm': 0.9839586615562439, 'learning_rate': 1.6604067995008996e-05, 'epoch': 4.063025210084033}
>>> 2025-09-10 22:31:32,567 - INFO - >>> {'loss': 2.1934, 'grad_norm': 0.9621774554252625, 'learning_rate': 1.6597455610703313e-05, 'epoch': 4.067226890756302}
>>> 2025-09-10 22:31:36,473 - INFO - >>> {'loss': 2.2565, 'grad_norm': 0.95526123046875, 'learning_rate': 1.659083811448882e-05, 'epoch': 4.071428571428571}
>>> 2025-09-10 22:31:39,546 - INFO - >>> {'loss': 2.2118, 'grad_norm': 0.9363901615142822, 'learning_rate': 1.6584215511492948e-05, 'epoch': 4.07563025210084}
>>> 2025-09-10 22:31:43,581 - INFO - >>> {'loss': 2.2452, 'grad_norm': 0.983756959438324, 'learning_rate': 1.65775878068471e-05, 'epoch': 4.079831932773109}
>>> 2025-09-10 22:31:46,874 - INFO - >>> {'loss': 2.3228, 'grad_norm': 0.9127413630485535, 'learning_rate': 1.6570955005686608e-05, 'epoch': 4.084033613445378}
>>> 2025-09-10 22:31:50,178 - INFO - >>> {'loss': 2.2183, 'grad_norm': 0.9723289608955383, 'learning_rate': 1.6564317113150777e-05, 'epoch': 4.088235294117647}
>>> 2025-09-10 22:31:54,160 - INFO - >>> {'loss': 2.2438, 'grad_norm': 0.8235729336738586, 'learning_rate': 1.6557674134382846e-05, 'epoch': 4.092436974789916}
>>> 2025-09-10 22:31:57,876 - INFO - >>> {'loss': 2.3143, 'grad_norm': 0.9870149493217468, 'learning_rate': 1.6551026074529994e-05, 'epoch': 4.0966386554621845}
>>> 2025-09-10 22:32:01,184 - INFO - >>> {'loss': 2.2287, 'grad_norm': 0.9233588576316833, 'learning_rate': 1.6544372938743343e-05, 'epoch': 4.100840336134453}
>>> 2025-09-10 22:32:04,740 - INFO - >>> {'loss': 2.2927, 'grad_norm': 0.9137598872184753, 'learning_rate': 1.6537714732177944e-05, 'epoch': 4.105042016806722}
>>> 2025-09-10 22:32:08,045 - INFO - >>> {'loss': 2.2848, 'grad_norm': 0.9391646385192871, 'learning_rate': 1.6531051459992773e-05, 'epoch': 4.109243697478991}
>>> 2025-09-10 22:32:11,885 - INFO - >>> {'loss': 2.2917, 'grad_norm': 0.9403067827224731, 'learning_rate': 1.652438312735074e-05, 'epoch': 4.11344537815126}
>>> 2025-09-10 22:32:15,326 - INFO - >>> {'loss': 2.256, 'grad_norm': 0.9015050530433655, 'learning_rate': 1.6517709739418672e-05, 'epoch': 4.117647058823529}
>>> 2025-09-10 22:32:19,382 - INFO - >>> {'loss': 2.2643, 'grad_norm': 0.9472804665565491, 'learning_rate': 1.6511031301367312e-05, 'epoch': 4.121848739495798}
>>> 2025-09-10 22:32:23,485 - INFO - >>> {'loss': 2.1084, 'grad_norm': 0.9735137224197388, 'learning_rate': 1.6504347818371313e-05, 'epoch': 4.126050420168067}
>>> 2025-09-10 22:32:27,395 - INFO - >>> {'loss': 2.2195, 'grad_norm': 1.0306493043899536, 'learning_rate': 1.6497659295609248e-05, 'epoch': 4.130252100840337}
>>> 2025-09-10 22:32:31,020 - INFO - >>> {'loss': 2.3157, 'grad_norm': 0.9742120504379272, 'learning_rate': 1.6490965738263576e-05, 'epoch': 4.1344537815126055}
>>> 2025-09-10 22:32:34,541 - INFO - >>> {'loss': 2.2229, 'grad_norm': 0.9331338405609131, 'learning_rate': 1.6484267151520676e-05, 'epoch': 4.138655462184874}
>>> 2025-09-10 22:32:37,898 - INFO - >>> {'loss': 2.3129, 'grad_norm': 0.9482915997505188, 'learning_rate': 1.6477563540570818e-05, 'epoch': 4.142857142857143}
>>> 2025-09-10 22:32:41,767 - INFO - >>> {'loss': 2.3251, 'grad_norm': 0.8155330419540405, 'learning_rate': 1.647085491060816e-05, 'epoch': 4.147058823529412}
>>> 2025-09-10 22:32:44,977 - INFO - >>> {'loss': 2.215, 'grad_norm': 0.9605169296264648, 'learning_rate': 1.646414126683075e-05, 'epoch': 4.151260504201681}
>>> 2025-09-10 22:32:49,134 - INFO - >>> {'loss': 2.2656, 'grad_norm': 0.9268060326576233, 'learning_rate': 1.6457422614440525e-05, 'epoch': 4.15546218487395}
>>> 2025-09-10 22:32:52,180 - INFO - >>> {'loss': 2.2771, 'grad_norm': 1.0090843439102173, 'learning_rate': 1.6450698958643303e-05, 'epoch': 4.159663865546219}
>>> 2025-09-10 22:32:55,769 - INFO - >>> {'loss': 2.2525, 'grad_norm': 1.0164960622787476, 'learning_rate': 1.644397030464877e-05, 'epoch': 4.163865546218488}
>>> 2025-09-10 22:32:59,455 - INFO - >>> {'loss': 2.1664, 'grad_norm': 0.8558473587036133, 'learning_rate': 1.64372366576705e-05, 'epoch': 4.168067226890757}
>>> 2025-09-10 22:33:02,534 - INFO - >>> {'loss': 2.192, 'grad_norm': 0.9358248114585876, 'learning_rate': 1.643049802292592e-05, 'epoch': 4.1722689075630255}
>>> 2025-09-10 22:33:06,243 - INFO - >>> {'loss': 2.238, 'grad_norm': 0.9116067290306091, 'learning_rate': 1.6423754405636338e-05, 'epoch': 4.176470588235294}
>>> 2025-09-10 22:33:10,133 - INFO - >>> {'loss': 2.2369, 'grad_norm': 0.9347884654998779, 'learning_rate': 1.6417005811026897e-05, 'epoch': 4.180672268907563}
>>> 2025-09-10 22:33:13,514 - INFO - >>> {'loss': 2.1816, 'grad_norm': 0.9342244267463684, 'learning_rate': 1.6410252244326633e-05, 'epoch': 4.184873949579832}
>>> 2025-09-10 22:33:17,276 - INFO - >>> {'loss': 2.178, 'grad_norm': 0.9785662889480591, 'learning_rate': 1.6403493710768396e-05, 'epoch': 4.189075630252101}
>>> 2025-09-10 22:33:21,092 - INFO - >>> {'loss': 2.2281, 'grad_norm': 0.9995097517967224, 'learning_rate': 1.6396730215588913e-05, 'epoch': 4.19327731092437}
>>> 2025-09-10 22:33:25,071 - INFO - >>> {'loss': 2.262, 'grad_norm': 0.8772557377815247, 'learning_rate': 1.6389961764028747e-05, 'epoch': 4.197478991596639}
>>> 2025-09-10 22:33:28,084 - INFO - >>> {'loss': 2.2634, 'grad_norm': 0.995364785194397, 'learning_rate': 1.638318836133229e-05, 'epoch': 4.201680672268908}
>>> 2025-09-10 22:33:32,028 - INFO - >>> {'loss': 2.2666, 'grad_norm': 1.0038429498672485, 'learning_rate': 1.6376410012747794e-05, 'epoch': 4.205882352941177}
>>> 2025-09-10 22:33:35,277 - INFO - >>> {'loss': 2.2182, 'grad_norm': 1.0522093772888184, 'learning_rate': 1.636962672352731e-05, 'epoch': 4.2100840336134455}
>>> 2025-09-10 22:33:38,316 - INFO - >>> {'loss': 2.3519, 'grad_norm': 0.9845213890075684, 'learning_rate': 1.6362838498926756e-05, 'epoch': 4.214285714285714}
>>> 2025-09-10 22:33:41,718 - INFO - >>> {'loss': 2.1798, 'grad_norm': 0.9444113969802856, 'learning_rate': 1.6356045344205845e-05, 'epoch': 4.218487394957983}
>>> 2025-09-10 22:33:44,778 - INFO - >>> {'loss': 2.2095, 'grad_norm': 0.9535048604011536, 'learning_rate': 1.634924726462812e-05, 'epoch': 4.222689075630252}
>>> 2025-09-10 22:33:48,706 - INFO - >>> {'loss': 2.1965, 'grad_norm': 0.8858835101127625, 'learning_rate': 1.6342444265460934e-05, 'epoch': 4.226890756302521}
>>> 2025-09-10 22:33:52,562 - INFO - >>> {'loss': 2.2475, 'grad_norm': 0.9591279029846191, 'learning_rate': 1.633563635197547e-05, 'epoch': 4.23109243697479}
>>> 2025-09-10 22:33:56,396 - INFO - >>> {'loss': 2.2055, 'grad_norm': 0.8582325577735901, 'learning_rate': 1.6328823529446697e-05, 'epoch': 4.235294117647059}
>>> 2025-09-10 22:34:00,135 - INFO - >>> {'loss': 2.1126, 'grad_norm': 1.0104581117630005, 'learning_rate': 1.6322005803153396e-05, 'epoch': 4.239495798319328}
>>> 2025-09-10 22:34:03,602 - INFO - >>> {'loss': 2.1842, 'grad_norm': 0.9863191843032837, 'learning_rate': 1.6315183178378152e-05, 'epoch': 4.243697478991597}
>>> 2025-09-10 22:34:07,089 - INFO - >>> {'loss': 2.2838, 'grad_norm': 1.0056602954864502, 'learning_rate': 1.630835566040734e-05, 'epoch': 4.2478991596638656}
>>> 2025-09-10 22:34:10,056 - INFO - >>> {'loss': 2.1701, 'grad_norm': 1.14906907081604, 'learning_rate': 1.6301523254531133e-05, 'epoch': 4.2521008403361344}
>>> 2025-09-10 22:34:13,234 - INFO - >>> {'loss': 2.2227, 'grad_norm': 1.0284936428070068, 'learning_rate': 1.6294685966043476e-05, 'epoch': 4.256302521008403}
>>> 2025-09-10 22:34:16,735 - INFO - >>> {'loss': 2.2233, 'grad_norm': 0.8917887806892395, 'learning_rate': 1.6287843800242117e-05, 'epoch': 4.260504201680672}
>>> 2025-09-10 22:34:20,675 - INFO - >>> {'loss': 2.2329, 'grad_norm': 0.9238805174827576, 'learning_rate': 1.6280996762428567e-05, 'epoch': 4.264705882352941}
>>> 2025-09-10 22:34:24,219 - INFO - >>> {'loss': 2.2328, 'grad_norm': 0.9438989162445068, 'learning_rate': 1.6274144857908126e-05, 'epoch': 4.26890756302521}
>>> 2025-09-10 22:34:27,579 - INFO - >>> {'loss': 2.1872, 'grad_norm': 0.8836238980293274, 'learning_rate': 1.6267288091989848e-05, 'epoch': 4.273109243697479}
>>> 2025-09-10 22:34:30,743 - INFO - >>> {'loss': 2.1405, 'grad_norm': 0.956303596496582, 'learning_rate': 1.6260426469986574e-05, 'epoch': 4.277310924369748}
>>> 2025-09-10 22:34:33,889 - INFO - >>> {'loss': 2.2751, 'grad_norm': 0.9676316976547241, 'learning_rate': 1.6253559997214888e-05, 'epoch': 4.281512605042017}
>>> 2025-09-10 22:34:37,079 - INFO - >>> {'loss': 2.1999, 'grad_norm': 1.0065491199493408, 'learning_rate': 1.6246688678995148e-05, 'epoch': 4.285714285714286}
>>> 2025-09-10 22:34:40,626 - INFO - >>> {'loss': 2.3824, 'grad_norm': 0.9824561476707458, 'learning_rate': 1.6239812520651453e-05, 'epoch': 4.2899159663865545}
>>> 2025-09-10 22:34:43,680 - INFO - >>> {'loss': 2.1835, 'grad_norm': 0.9727126359939575, 'learning_rate': 1.623293152751167e-05, 'epoch': 4.294117647058823}
>>> 2025-09-10 22:34:46,795 - INFO - >>> {'loss': 2.2145, 'grad_norm': 1.0836724042892456, 'learning_rate': 1.6226045704907388e-05, 'epoch': 4.298319327731092}
>>> 2025-09-10 22:34:50,098 - INFO - >>> {'loss': 2.2, 'grad_norm': 1.0796328783035278, 'learning_rate': 1.621915505817396e-05, 'epoch': 4.302521008403361}
>>> 2025-09-10 22:34:53,143 - INFO - >>> {'loss': 2.3332, 'grad_norm': 1.059875726699829, 'learning_rate': 1.6212259592650466e-05, 'epoch': 4.30672268907563}
>>> 2025-09-10 22:34:57,188 - INFO - >>> {'loss': 2.1548, 'grad_norm': 0.9584982991218567, 'learning_rate': 1.6205359313679723e-05, 'epoch': 4.310924369747899}
>>> 2025-09-10 22:35:00,939 - INFO - >>> {'loss': 2.3322, 'grad_norm': 0.925072193145752, 'learning_rate': 1.6198454226608274e-05, 'epoch': 4.315126050420168}
>>> 2025-09-10 22:35:04,477 - INFO - >>> {'loss': 2.1907, 'grad_norm': 0.9753975868225098, 'learning_rate': 1.6191544336786394e-05, 'epoch': 4.319327731092437}
>>> 2025-09-10 22:35:07,998 - INFO - >>> {'loss': 2.2448, 'grad_norm': 0.9985554218292236, 'learning_rate': 1.618462964956807e-05, 'epoch': 4.323529411764706}
>>> 2025-09-10 22:35:11,979 - INFO - >>> {'loss': 2.2537, 'grad_norm': 0.9395522475242615, 'learning_rate': 1.6177710170311017e-05, 'epoch': 4.3277310924369745}
>>> 2025-09-10 22:35:15,527 - INFO - >>> {'loss': 2.1606, 'grad_norm': 0.9212247729301453, 'learning_rate': 1.617078590437665e-05, 'epoch': 4.331932773109243}
>>> 2025-09-10 22:35:19,349 - INFO - >>> {'loss': 2.217, 'grad_norm': 0.9642640948295593, 'learning_rate': 1.616385685713011e-05, 'epoch': 4.336134453781512}
>>> 2025-09-10 22:35:23,148 - INFO - >>> {'loss': 2.262, 'grad_norm': 0.93910813331604, 'learning_rate': 1.6156923033940225e-05, 'epoch': 4.340336134453781}
>>> 2025-09-10 22:35:26,295 - INFO - >>> {'loss': 2.1359, 'grad_norm': 0.9357398748397827, 'learning_rate': 1.614998444017954e-05, 'epoch': 4.34453781512605}
>>> 2025-09-10 22:35:29,580 - INFO - >>> {'loss': 2.2846, 'grad_norm': 0.9130706787109375, 'learning_rate': 1.6143041081224276e-05, 'epoch': 4.348739495798319}
>>> 2025-09-10 22:35:32,608 - INFO - >>> {'loss': 2.2056, 'grad_norm': 1.0841612815856934, 'learning_rate': 1.613609296245437e-05, 'epoch': 4.352941176470588}
>>> 2025-09-10 22:35:35,973 - INFO - >>> {'loss': 2.2905, 'grad_norm': 0.9381730556488037, 'learning_rate': 1.6129140089253428e-05, 'epoch': 4.357142857142857}
>>> 2025-09-10 22:35:39,421 - INFO - >>> {'loss': 2.3201, 'grad_norm': 0.9845321774482727, 'learning_rate': 1.612218246700875e-05, 'epoch': 4.361344537815126}
>>> 2025-09-10 22:35:42,468 - INFO - >>> {'loss': 2.1015, 'grad_norm': 1.075725793838501, 'learning_rate': 1.611522010111131e-05, 'epoch': 4.3655462184873945}
>>> 2025-09-10 22:35:46,477 - INFO - >>> {'loss': 2.1957, 'grad_norm': 0.9091364741325378, 'learning_rate': 1.6108252996955768e-05, 'epoch': 4.369747899159664}
>>> 2025-09-10 22:35:50,090 - INFO - >>> {'loss': 2.1834, 'grad_norm': 0.996025025844574, 'learning_rate': 1.610128115994044e-05, 'epoch': 4.373949579831933}
>>> 2025-09-10 22:35:53,814 - INFO - >>> {'loss': 2.2029, 'grad_norm': 0.958588182926178, 'learning_rate': 1.6094304595467317e-05, 'epoch': 4.378151260504202}
>>> 2025-09-10 22:35:57,192 - INFO - >>> {'loss': 2.2707, 'grad_norm': 0.9450662136077881, 'learning_rate': 1.608732330894206e-05, 'epoch': 4.382352941176471}
>>> 2025-09-10 22:36:00,443 - INFO - >>> {'loss': 2.2168, 'grad_norm': 1.0300432443618774, 'learning_rate': 1.608033730577397e-05, 'epoch': 4.38655462184874}
>>> 2025-09-10 22:36:03,686 - INFO - >>> {'loss': 2.1763, 'grad_norm': 1.0018763542175293, 'learning_rate': 1.6073346591376032e-05, 'epoch': 4.390756302521009}
>>> 2025-09-10 22:36:07,737 - INFO - >>> {'loss': 2.2429, 'grad_norm': 0.8975003361701965, 'learning_rate': 1.6066351171164842e-05, 'epoch': 4.394957983193278}
>>> 2025-09-10 22:36:11,086 - INFO - >>> {'loss': 2.2535, 'grad_norm': 0.9509738087654114, 'learning_rate': 1.605935105056068e-05, 'epoch': 4.399159663865547}
>>> 2025-09-10 22:36:14,856 - INFO - >>> {'loss': 2.242, 'grad_norm': 0.8997370600700378, 'learning_rate': 1.6052346234987446e-05, 'epoch': 4.4033613445378155}
>>> 2025-09-10 22:36:18,406 - INFO - >>> {'loss': 2.2323, 'grad_norm': 0.9013583660125732, 'learning_rate': 1.6045336729872684e-05, 'epoch': 4.407563025210084}
>>> 2025-09-10 22:36:21,811 - INFO - >>> {'loss': 2.2285, 'grad_norm': 0.9597215056419373, 'learning_rate': 1.603832254064757e-05, 'epoch': 4.411764705882353}
>>> 2025-09-10 22:36:25,618 - INFO - >>> {'loss': 2.1649, 'grad_norm': 1.00423264503479, 'learning_rate': 1.6031303672746914e-05, 'epoch': 4.415966386554622}
>>> 2025-09-10 22:36:28,630 - INFO - >>> {'loss': 2.3163, 'grad_norm': 1.0445748567581177, 'learning_rate': 1.6024280131609144e-05, 'epoch': 4.420168067226891}
>>> 2025-09-10 22:36:32,004 - INFO - >>> {'loss': 2.2064, 'grad_norm': 0.9236282110214233, 'learning_rate': 1.6017251922676316e-05, 'epoch': 4.42436974789916}
>>> 2025-09-10 22:36:36,185 - INFO - >>> {'loss': 2.2297, 'grad_norm': 0.8833604454994202, 'learning_rate': 1.60102190513941e-05, 'epoch': 4.428571428571429}
>>> 2025-09-10 22:36:39,560 - INFO - >>> {'loss': 2.2344, 'grad_norm': 1.0160531997680664, 'learning_rate': 1.600318152321177e-05, 'epoch': 4.432773109243698}
>>> 2025-09-10 22:36:42,895 - INFO - >>> {'loss': 2.2418, 'grad_norm': 0.9260271787643433, 'learning_rate': 1.599613934358223e-05, 'epoch': 4.436974789915967}
>>> 2025-09-10 22:36:46,180 - INFO - >>> {'loss': 2.3123, 'grad_norm': 1.071617841720581, 'learning_rate': 1.5989092517961962e-05, 'epoch': 4.4411764705882355}
>>> 2025-09-10 22:36:50,184 - INFO - >>> {'loss': 2.2609, 'grad_norm': 0.9090000987052917, 'learning_rate': 1.5982041051811073e-05, 'epoch': 4.445378151260504}
>>> 2025-09-10 22:36:54,084 - INFO - >>> {'loss': 2.2901, 'grad_norm': 0.8669224381446838, 'learning_rate': 1.5974984950593243e-05, 'epoch': 4.449579831932773}
>>> 2025-09-10 22:36:57,829 - INFO - >>> {'loss': 2.2693, 'grad_norm': 0.9291934967041016, 'learning_rate': 1.596792421977575e-05, 'epoch': 4.453781512605042}
>>> 2025-09-10 22:37:01,821 - INFO - >>> {'loss': 2.3651, 'grad_norm': 0.9293401837348938, 'learning_rate': 1.5960858864829475e-05, 'epoch': 4.457983193277311}
>>> 2025-09-10 22:37:04,781 - INFO - >>> {'loss': 2.2057, 'grad_norm': 1.081725001335144, 'learning_rate': 1.595378889122886e-05, 'epoch': 4.46218487394958}
>>> 2025-09-10 22:37:07,799 - INFO - >>> {'loss': 2.2061, 'grad_norm': 1.0531342029571533, 'learning_rate': 1.5946714304451938e-05, 'epoch': 4.466386554621849}
>>> 2025-09-10 22:37:11,518 - INFO - >>> {'loss': 2.1379, 'grad_norm': 0.964824378490448, 'learning_rate': 1.593963510998031e-05, 'epoch': 4.470588235294118}
>>> 2025-09-10 22:37:14,683 - INFO - >>> {'loss': 2.1663, 'grad_norm': 1.1171766519546509, 'learning_rate': 1.593255131329916e-05, 'epoch': 4.474789915966387}
>>> 2025-09-10 22:37:17,977 - INFO - >>> {'loss': 2.1598, 'grad_norm': 0.9652256369590759, 'learning_rate': 1.5925462919897223e-05, 'epoch': 4.4789915966386555}
>>> 2025-09-10 22:37:21,549 - INFO - >>> {'loss': 2.2096, 'grad_norm': 0.9013519883155823, 'learning_rate': 1.59183699352668e-05, 'epoch': 4.483193277310924}
>>> 2025-09-10 22:37:25,077 - INFO - >>> {'loss': 2.1254, 'grad_norm': 0.9600539207458496, 'learning_rate': 1.591127236490375e-05, 'epoch': 4.487394957983193}
>>> 2025-09-10 22:37:28,149 - INFO - >>> {'loss': 2.1262, 'grad_norm': 1.1243433952331543, 'learning_rate': 1.5904170214307488e-05, 'epoch': 4.491596638655462}
>>> 2025-09-10 22:37:31,482 - INFO - >>> {'loss': 2.2934, 'grad_norm': 0.943840742111206, 'learning_rate': 1.5897063488980976e-05, 'epoch': 4.495798319327731}
>>> 2025-09-10 22:37:34,409 - INFO - >>> {'loss': 2.2279, 'grad_norm': 1.0149704217910767, 'learning_rate': 1.588995219443072e-05, 'epoch': 4.5}
>>> 2025-09-10 22:37:37,622 - INFO - >>> {'loss': 2.1855, 'grad_norm': 1.0666176080703735, 'learning_rate': 1.588283633616677e-05, 'epoch': 4.504201680672269}
>>> 2025-09-10 22:37:40,678 - INFO - >>> {'loss': 2.1904, 'grad_norm': 1.0058443546295166, 'learning_rate': 1.58757159197027e-05, 'epoch': 4.508403361344538}
>>> 2025-09-10 22:37:44,169 - INFO - >>> {'loss': 2.2946, 'grad_norm': 0.9513444900512695, 'learning_rate': 1.5868590950555635e-05, 'epoch': 4.512605042016807}
>>> 2025-09-10 22:37:47,220 - INFO - >>> {'loss': 2.2094, 'grad_norm': 1.0916930437088013, 'learning_rate': 1.586146143424621e-05, 'epoch': 4.516806722689076}
>>> 2025-09-10 22:37:50,341 - INFO - >>> {'loss': 2.1573, 'grad_norm': 1.0777816772460938, 'learning_rate': 1.5854327376298594e-05, 'epoch': 4.5210084033613445}
>>> 2025-09-10 22:37:53,337 - INFO - >>> {'loss': 2.1749, 'grad_norm': 1.0074383020401, 'learning_rate': 1.5847188782240473e-05, 'epoch': 4.525210084033613}
>>> 2025-09-10 22:37:56,574 - INFO - >>> {'loss': 2.1848, 'grad_norm': 1.033217430114746, 'learning_rate': 1.584004565760304e-05, 'epoch': 4.529411764705882}
>>> 2025-09-10 22:38:00,527 - INFO - >>> {'loss': 2.255, 'grad_norm': 0.9212995171546936, 'learning_rate': 1.5832898007921015e-05, 'epoch': 4.533613445378151}
>>> 2025-09-10 22:38:04,271 - INFO - >>> {'loss': 2.3224, 'grad_norm': 0.983319103717804, 'learning_rate': 1.5825745838732603e-05, 'epoch': 4.53781512605042}
>>> 2025-09-10 22:38:07,328 - INFO - >>> {'loss': 2.147, 'grad_norm': 1.1712161302566528, 'learning_rate': 1.581858915557953e-05, 'epoch': 4.542016806722689}
>>> 2025-09-10 22:38:10,238 - INFO - >>> {'loss': 2.2118, 'grad_norm': 0.9837235808372498, 'learning_rate': 1.5811427964007004e-05, 'epoch': 4.546218487394958}
>>> 2025-09-10 22:38:13,635 - INFO - >>> {'loss': 2.3159, 'grad_norm': 1.0681179761886597, 'learning_rate': 1.5804262269563743e-05, 'epoch': 4.550420168067227}
>>> 2025-09-10 22:38:16,963 - INFO - >>> {'loss': 2.1821, 'grad_norm': 1.0049675703048706, 'learning_rate': 1.5797092077801934e-05, 'epoch': 4.554621848739496}
>>> 2025-09-10 22:38:20,294 - INFO - >>> {'loss': 2.2666, 'grad_norm': 0.9897975325584412, 'learning_rate': 1.5789917394277268e-05, 'epoch': 4.5588235294117645}
>>> 2025-09-10 22:38:23,583 - INFO - >>> {'loss': 2.2475, 'grad_norm': 1.0086700916290283, 'learning_rate': 1.57827382245489e-05, 'epoch': 4.563025210084033}
>>> 2025-09-10 22:38:27,351 - INFO - >>> {'loss': 2.1594, 'grad_norm': 0.9620401263237, 'learning_rate': 1.5775554574179474e-05, 'epoch': 4.567226890756302}
>>> 2025-09-10 22:38:31,368 - INFO - >>> {'loss': 2.1806, 'grad_norm': 0.9816635251045227, 'learning_rate': 1.57683664487351e-05, 'epoch': 4.571428571428571}
>>> 2025-09-10 22:38:35,158 - INFO - >>> {'loss': 2.2407, 'grad_norm': 0.9972255229949951, 'learning_rate': 1.5761173853785352e-05, 'epoch': 4.57563025210084}
>>> 2025-09-10 22:38:38,602 - INFO - >>> {'loss': 2.1907, 'grad_norm': 0.9053831100463867, 'learning_rate': 1.5753976794903273e-05, 'epoch': 4.579831932773109}
>>> 2025-09-10 22:38:42,069 - INFO - >>> {'loss': 2.2425, 'grad_norm': 0.9156021475791931, 'learning_rate': 1.5746775277665364e-05, 'epoch': 4.584033613445378}
>>> 2025-09-10 22:38:45,916 - INFO - >>> {'loss': 2.2681, 'grad_norm': 0.8796291351318359, 'learning_rate': 1.5739569307651578e-05, 'epoch': 4.588235294117647}
>>> 2025-09-10 22:38:49,289 - INFO - >>> {'loss': 2.2802, 'grad_norm': 0.9853112101554871, 'learning_rate': 1.5732358890445317e-05, 'epoch': 4.592436974789916}
>>> 2025-09-10 22:38:53,325 - INFO - >>> {'loss': 2.3276, 'grad_norm': 1.0015952587127686, 'learning_rate': 1.5725144031633438e-05, 'epoch': 4.5966386554621845}
>>> 2025-09-10 22:38:56,964 - INFO - >>> {'loss': 2.3264, 'grad_norm': 1.0008018016815186, 'learning_rate': 1.5717924736806222e-05, 'epoch': 4.600840336134453}
>>> 2025-09-10 22:39:00,224 - INFO - >>> {'loss': 2.1877, 'grad_norm': 1.0103623867034912, 'learning_rate': 1.5710701011557407e-05, 'epoch': 4.605042016806722}
>>> 2025-09-10 22:39:04,238 - INFO - >>> {'loss': 2.2636, 'grad_norm': 0.9353006482124329, 'learning_rate': 1.570347286148415e-05, 'epoch': 4.609243697478991}
>>> 2025-09-10 22:39:08,235 - INFO - >>> {'loss': 2.1336, 'grad_norm': 0.995715320110321, 'learning_rate': 1.5696240292187037e-05, 'epoch': 4.61344537815126}
>>> 2025-09-10 22:39:11,739 - INFO - >>> {'loss': 2.1472, 'grad_norm': 0.9315681457519531, 'learning_rate': 1.5689003309270096e-05, 'epoch': 4.617647058823529}
>>> 2025-09-10 22:39:14,860 - INFO - >>> {'loss': 2.3678, 'grad_norm': 1.1165450811386108, 'learning_rate': 1.5681761918340744e-05, 'epoch': 4.621848739495798}
>>> 2025-09-10 22:39:18,825 - INFO - >>> {'loss': 2.3218, 'grad_norm': 0.9818713068962097, 'learning_rate': 1.5674516125009843e-05, 'epoch': 4.626050420168067}
>>> 2025-09-10 22:39:22,678 - INFO - >>> {'loss': 2.2586, 'grad_norm': 1.0364729166030884, 'learning_rate': 1.5667265934891646e-05, 'epoch': 4.630252100840336}
>>> 2025-09-10 22:39:26,394 - INFO - >>> {'loss': 2.1714, 'grad_norm': 1.0692036151885986, 'learning_rate': 1.5660011353603827e-05, 'epoch': 4.634453781512605}
>>> 2025-09-10 22:39:30,159 - INFO - >>> {'loss': 2.2836, 'grad_norm': 0.9518516659736633, 'learning_rate': 1.565275238676745e-05, 'epoch': 4.6386554621848735}
>>> 2025-09-10 22:39:33,535 - INFO - >>> {'loss': 2.2765, 'grad_norm': 0.980858564376831, 'learning_rate': 1.5645489040006987e-05, 'epoch': 4.642857142857143}
>>> 2025-09-10 22:39:37,021 - INFO - >>> {'loss': 2.3057, 'grad_norm': 0.9466207027435303, 'learning_rate': 1.5638221318950297e-05, 'epoch': 4.647058823529412}
>>> 2025-09-10 22:39:40,213 - INFO - >>> {'loss': 2.2193, 'grad_norm': 1.027638554573059, 'learning_rate': 1.563094922922863e-05, 'epoch': 4.651260504201681}
>>> 2025-09-10 22:39:43,540 - INFO - >>> {'loss': 2.1453, 'grad_norm': 1.0100396871566772, 'learning_rate': 1.5623672776476625e-05, 'epoch': 4.65546218487395}
>>> 2025-09-10 22:39:47,241 - INFO - >>> {'loss': 2.3661, 'grad_norm': 0.9595017433166504, 'learning_rate': 1.561639196633229e-05, 'epoch': 4.659663865546219}
>>> 2025-09-10 22:39:50,278 - INFO - >>> {'loss': 2.1916, 'grad_norm': 1.0377957820892334, 'learning_rate': 1.560910680443703e-05, 'epoch': 4.663865546218488}
>>> 2025-09-10 22:39:53,912 - INFO - >>> {'loss': 2.2626, 'grad_norm': 1.0168159008026123, 'learning_rate': 1.5601817296435602e-05, 'epoch': 4.668067226890757}
>>> 2025-09-10 22:39:57,179 - INFO - >>> {'loss': 2.1465, 'grad_norm': 0.9906303882598877, 'learning_rate': 1.559452344797614e-05, 'epoch': 4.6722689075630255}
>>> 2025-09-10 22:40:00,806 - INFO - >>> {'loss': 2.2261, 'grad_norm': 0.9659812450408936, 'learning_rate': 1.5587225264710136e-05, 'epoch': 4.676470588235294}
>>> 2025-09-10 22:40:04,832 - INFO - >>> {'loss': 2.2508, 'grad_norm': 0.8948331475257874, 'learning_rate': 1.557992275229245e-05, 'epoch': 4.680672268907563}
>>> 2025-09-10 22:40:07,956 - INFO - >>> {'loss': 2.2761, 'grad_norm': 1.1201039552688599, 'learning_rate': 1.5572615916381285e-05, 'epoch': 4.684873949579832}
>>> 2025-09-10 22:40:11,834 - INFO - >>> {'loss': 2.3371, 'grad_norm': 0.9366739392280579, 'learning_rate': 1.5565304762638202e-05, 'epoch': 4.689075630252101}
>>> 2025-09-10 22:40:15,401 - INFO - >>> {'loss': 2.241, 'grad_norm': 0.8698704838752747, 'learning_rate': 1.5557989296728105e-05, 'epoch': 4.69327731092437}
>>> 2025-09-10 22:40:18,590 - INFO - >>> {'loss': 2.2379, 'grad_norm': 0.9706054329872131, 'learning_rate': 1.5550669524319234e-05, 'epoch': 4.697478991596639}
>>> 2025-09-10 22:40:22,539 - INFO - >>> {'loss': 2.225, 'grad_norm': 0.9490045309066772, 'learning_rate': 1.5543345451083182e-05, 'epoch': 4.701680672268908}
>>> 2025-09-10 22:40:25,983 - INFO - >>> {'loss': 2.139, 'grad_norm': 0.9007669687271118, 'learning_rate': 1.5536017082694846e-05, 'epoch': 4.705882352941177}
>>> 2025-09-10 22:40:29,546 - INFO - >>> {'loss': 2.2611, 'grad_norm': 0.9671441316604614, 'learning_rate': 1.5528684424832484e-05, 'epoch': 4.7100840336134455}
>>> 2025-09-10 22:40:32,974 - INFO - >>> {'loss': 2.2327, 'grad_norm': 1.0816808938980103, 'learning_rate': 1.5521347483177654e-05, 'epoch': 4.714285714285714}
>>> 2025-09-10 22:40:36,309 - INFO - >>> {'loss': 2.2251, 'grad_norm': 1.1028884649276733, 'learning_rate': 1.5514006263415243e-05, 'epoch': 4.718487394957983}
>>> 2025-09-10 22:40:39,414 - INFO - >>> {'loss': 2.2263, 'grad_norm': 0.9985587000846863, 'learning_rate': 1.550666077123345e-05, 'epoch': 4.722689075630252}
>>> 2025-09-10 22:40:42,480 - INFO - >>> {'loss': 2.2332, 'grad_norm': 1.0478515625, 'learning_rate': 1.5499311012323787e-05, 'epoch': 4.726890756302521}
>>> 2025-09-10 22:40:46,130 - INFO - >>> {'loss': 2.2458, 'grad_norm': 0.9937717914581299, 'learning_rate': 1.5491956992381066e-05, 'epoch': 4.73109243697479}
>>> 2025-09-10 22:40:49,824 - INFO - >>> {'loss': 2.1673, 'grad_norm': 0.9792698621749878, 'learning_rate': 1.548459871710341e-05, 'epoch': 4.735294117647059}
>>> 2025-09-10 22:40:53,277 - INFO - >>> {'loss': 2.296, 'grad_norm': 1.1761811971664429, 'learning_rate': 1.5477236192192236e-05, 'epoch': 4.739495798319328}
>>> 2025-09-10 22:40:57,023 - INFO - >>> {'loss': 2.2555, 'grad_norm': 1.0023127794265747, 'learning_rate': 1.5469869423352245e-05, 'epoch': 4.743697478991597}
>>> 2025-09-10 22:41:01,095 - INFO - >>> {'loss': 2.1837, 'grad_norm': 0.9646337032318115, 'learning_rate': 1.5462498416291433e-05, 'epoch': 4.7478991596638656}
>>> 2025-09-10 22:41:04,164 - INFO - >>> {'loss': 2.1799, 'grad_norm': 0.99297034740448, 'learning_rate': 1.545512317672109e-05, 'epoch': 4.7521008403361344}
>>> 2025-09-10 22:41:07,376 - INFO - >>> {'loss': 2.3872, 'grad_norm': 1.0721218585968018, 'learning_rate': 1.544774371035576e-05, 'epoch': 4.756302521008403}
>>> 2025-09-10 22:41:11,239 - INFO - >>> {'loss': 2.2777, 'grad_norm': 0.9566870331764221, 'learning_rate': 1.5440360022913287e-05, 'epoch': 4.760504201680672}
>>> 2025-09-10 22:41:14,455 - INFO - >>> {'loss': 2.1826, 'grad_norm': 0.9981492161750793, 'learning_rate': 1.5432972120114777e-05, 'epoch': 4.764705882352941}
>>> 2025-09-10 22:41:17,652 - INFO - >>> {'loss': 2.2484, 'grad_norm': 1.039652705192566, 'learning_rate': 1.5425580007684597e-05, 'epoch': 4.76890756302521}
>>> 2025-09-10 22:41:21,618 - INFO - >>> {'loss': 2.2856, 'grad_norm': 1.0350233316421509, 'learning_rate': 1.5418183691350383e-05, 'epoch': 4.773109243697479}
>>> 2025-09-10 22:41:25,253 - INFO - >>> {'loss': 2.1858, 'grad_norm': 1.003071665763855, 'learning_rate': 1.5410783176843025e-05, 'epoch': 4.777310924369748}
>>> 2025-09-10 22:41:28,848 - INFO - >>> {'loss': 2.2588, 'grad_norm': 1.127785086631775, 'learning_rate': 1.5403378469896666e-05, 'epoch': 4.781512605042017}
>>> 2025-09-10 22:41:32,219 - INFO - >>> {'loss': 2.2483, 'grad_norm': 1.0430617332458496, 'learning_rate': 1.5395969576248696e-05, 'epoch': 4.785714285714286}
>>> 2025-09-10 22:41:35,273 - INFO - >>> {'loss': 2.1641, 'grad_norm': 0.9685050249099731, 'learning_rate': 1.5388556501639752e-05, 'epoch': 4.7899159663865545}
>>> 2025-09-10 22:41:38,550 - INFO - >>> {'loss': 2.1308, 'grad_norm': 1.0244663953781128, 'learning_rate': 1.5381139251813707e-05, 'epoch': 4.794117647058823}
>>> 2025-09-10 22:41:41,871 - INFO - >>> {'loss': 2.2761, 'grad_norm': 1.0328480005264282, 'learning_rate': 1.5373717832517677e-05, 'epoch': 4.798319327731092}
>>> 2025-09-10 22:41:45,660 - INFO - >>> {'loss': 2.1951, 'grad_norm': 0.9859650135040283, 'learning_rate': 1.5366292249501998e-05, 'epoch': 4.802521008403361}
>>> 2025-09-10 22:41:49,457 - INFO - >>> {'loss': 2.1569, 'grad_norm': 0.9508160948753357, 'learning_rate': 1.5358862508520237e-05, 'epoch': 4.80672268907563}
>>> 2025-09-10 22:41:52,821 - INFO - >>> {'loss': 2.2899, 'grad_norm': 1.0032734870910645, 'learning_rate': 1.535142861532919e-05, 'epoch': 4.810924369747899}
>>> 2025-09-10 22:41:56,177 - INFO - >>> {'loss': 2.1509, 'grad_norm': 0.998791515827179, 'learning_rate': 1.5343990575688846e-05, 'epoch': 4.815126050420168}
>>> 2025-09-10 22:41:59,553 - INFO - >>> {'loss': 2.3554, 'grad_norm': 0.9904676079750061, 'learning_rate': 1.533654839536244e-05, 'epoch': 4.819327731092437}
>>> 2025-09-10 22:42:02,791 - INFO - >>> {'loss': 2.225, 'grad_norm': 1.0404871702194214, 'learning_rate': 1.5329102080116395e-05, 'epoch': 4.823529411764706}
>>> 2025-09-10 22:42:06,458 - INFO - >>> {'loss': 2.1645, 'grad_norm': 0.9982835054397583, 'learning_rate': 1.5321651635720343e-05, 'epoch': 4.8277310924369745}
>>> 2025-09-10 22:42:09,490 - INFO - >>> {'loss': 2.2213, 'grad_norm': 1.2036265134811401, 'learning_rate': 1.531419706794711e-05, 'epoch': 4.831932773109243}
>>> 2025-09-10 22:42:13,175 - INFO - >>> {'loss': 2.2549, 'grad_norm': 0.9453580975532532, 'learning_rate': 1.530673838257272e-05, 'epoch': 4.836134453781512}
>>> 2025-09-10 22:42:16,799 - INFO - >>> {'loss': 2.1835, 'grad_norm': 0.982326865196228, 'learning_rate': 1.5299275585376402e-05, 'epoch': 4.840336134453781}
>>> 2025-09-10 22:42:20,033 - INFO - >>> {'loss': 2.2417, 'grad_norm': 0.9510723352432251, 'learning_rate': 1.529180868214054e-05, 'epoch': 4.84453781512605}
>>> 2025-09-10 22:42:23,811 - INFO - >>> {'loss': 2.2645, 'grad_norm': 1.0245463848114014, 'learning_rate': 1.5284337678650725e-05, 'epoch': 4.848739495798319}
>>> 2025-09-10 22:42:27,169 - INFO - >>> {'loss': 2.2206, 'grad_norm': 0.9602105617523193, 'learning_rate': 1.527686258069572e-05, 'epoch': 4.852941176470588}
>>> 2025-09-10 22:42:30,372 - INFO - >>> {'loss': 2.2622, 'grad_norm': 1.08991277217865, 'learning_rate': 1.526938339406746e-05, 'epoch': 4.857142857142857}
>>> 2025-09-10 22:42:34,234 - INFO - >>> {'loss': 2.2267, 'grad_norm': 0.9456642270088196, 'learning_rate': 1.5261900124561037e-05, 'epoch': 4.8613445378151265}
>>> 2025-09-10 22:42:37,507 - INFO - >>> {'loss': 2.2222, 'grad_norm': 1.118633508682251, 'learning_rate': 1.5254412777974719e-05, 'epoch': 4.865546218487395}
>>> 2025-09-10 22:42:41,296 - INFO - >>> {'loss': 2.2117, 'grad_norm': 0.934397280216217, 'learning_rate': 1.524692136010993e-05, 'epoch': 4.869747899159664}
>>> 2025-09-10 22:42:44,590 - INFO - >>> {'loss': 2.1624, 'grad_norm': 1.02867591381073, 'learning_rate': 1.5239425876771248e-05, 'epoch': 4.873949579831933}
>>> 2025-09-10 22:42:48,572 - INFO - >>> {'loss': 2.2213, 'grad_norm': 0.9143986701965332, 'learning_rate': 1.5231926333766403e-05, 'epoch': 4.878151260504202}
>>> 2025-09-10 22:42:51,988 - INFO - >>> {'loss': 2.2514, 'grad_norm': 0.8849033713340759, 'learning_rate': 1.5224422736906263e-05, 'epoch': 4.882352941176471}
>>> 2025-09-10 22:42:55,179 - INFO - >>> {'loss': 2.2947, 'grad_norm': 0.9746070504188538, 'learning_rate': 1.5216915092004847e-05, 'epoch': 4.88655462184874}
>>> 2025-09-10 22:42:58,271 - INFO - >>> {'loss': 2.1929, 'grad_norm': 1.0067143440246582, 'learning_rate': 1.5209403404879305e-05, 'epoch': 4.890756302521009}
>>> 2025-09-10 22:43:01,608 - INFO - >>> {'loss': 2.2172, 'grad_norm': 0.895146369934082, 'learning_rate': 1.5201887681349918e-05, 'epoch': 4.894957983193278}
>>> 2025-09-10 22:43:05,322 - INFO - >>> {'loss': 2.2593, 'grad_norm': 1.010697603225708, 'learning_rate': 1.5194367927240104e-05, 'epoch': 4.899159663865547}
>>> 2025-09-10 22:43:09,456 - INFO - >>> {'loss': 2.3158, 'grad_norm': 1.005529522895813, 'learning_rate': 1.5186844148376385e-05, 'epoch': 4.9033613445378155}
>>> 2025-09-10 22:43:13,180 - INFO - >>> {'loss': 2.2226, 'grad_norm': 1.1331149339675903, 'learning_rate': 1.5179316350588422e-05, 'epoch': 4.907563025210084}
>>> 2025-09-10 22:43:16,673 - INFO - >>> {'loss': 2.2143, 'grad_norm': 0.949951708316803, 'learning_rate': 1.5171784539708978e-05, 'epoch': 4.911764705882353}
>>> 2025-09-10 22:43:19,991 - INFO - >>> {'loss': 2.1387, 'grad_norm': 1.0246686935424805, 'learning_rate': 1.5164248721573931e-05, 'epoch': 4.915966386554622}
>>> 2025-09-10 22:43:23,342 - INFO - >>> {'loss': 2.1525, 'grad_norm': 0.9363744258880615, 'learning_rate': 1.5156708902022259e-05, 'epoch': 4.920168067226891}
>>> 2025-09-10 22:43:26,721 - INFO - >>> {'loss': 2.1778, 'grad_norm': 0.9854848384857178, 'learning_rate': 1.5149165086896043e-05, 'epoch': 4.92436974789916}
>>> 2025-09-10 22:43:29,631 - INFO - >>> {'loss': 2.2547, 'grad_norm': 1.072590708732605, 'learning_rate': 1.5141617282040461e-05, 'epoch': 4.928571428571429}
>>> 2025-09-10 22:43:32,706 - INFO - >>> {'loss': 2.2036, 'grad_norm': 1.112957239151001, 'learning_rate': 1.5134065493303783e-05, 'epoch': 4.932773109243698}
>>> 2025-09-10 22:43:36,182 - INFO - >>> {'loss': 2.2757, 'grad_norm': 0.9065937995910645, 'learning_rate': 1.512650972653736e-05, 'epoch': 4.936974789915967}
>>> 2025-09-10 22:43:39,672 - INFO - >>> {'loss': 2.1629, 'grad_norm': 1.0195529460906982, 'learning_rate': 1.511894998759563e-05, 'epoch': 4.9411764705882355}
>>> 2025-09-10 22:43:43,665 - INFO - >>> {'loss': 2.2916, 'grad_norm': 0.9803791046142578, 'learning_rate': 1.5111386282336113e-05, 'epoch': 4.945378151260504}
>>> 2025-09-10 22:43:47,661 - INFO - >>> {'loss': 2.2769, 'grad_norm': 0.9959064722061157, 'learning_rate': 1.5103818616619394e-05, 'epoch': 4.949579831932773}
>>> 2025-09-10 22:43:51,647 - INFO - >>> {'loss': 2.2485, 'grad_norm': 1.0379424095153809, 'learning_rate': 1.5096246996309127e-05, 'epoch': 4.953781512605042}
>>> 2025-09-10 22:43:55,460 - INFO - >>> {'loss': 2.2111, 'grad_norm': 1.0429142713546753, 'learning_rate': 1.5088671427272039e-05, 'epoch': 4.957983193277311}
>>> 2025-09-10 22:43:58,548 - INFO - >>> {'loss': 2.2277, 'grad_norm': 1.0652536153793335, 'learning_rate': 1.5081091915377905e-05, 'epoch': 4.96218487394958}
>>> 2025-09-10 22:44:01,764 - INFO - >>> {'loss': 2.2112, 'grad_norm': 0.9690779447555542, 'learning_rate': 1.5073508466499566e-05, 'epoch': 4.966386554621849}
>>> 2025-09-10 22:44:05,464 - INFO - >>> {'loss': 2.1985, 'grad_norm': 0.9920217990875244, 'learning_rate': 1.5065921086512906e-05, 'epoch': 4.970588235294118}
>>> 2025-09-10 22:44:08,805 - INFO - >>> {'loss': 2.1453, 'grad_norm': 1.0126063823699951, 'learning_rate': 1.5058329781296857e-05, 'epoch': 4.974789915966387}
>>> 2025-09-10 22:44:12,521 - INFO - >>> {'loss': 2.183, 'grad_norm': 1.0604592561721802, 'learning_rate': 1.5050734556733393e-05, 'epoch': 4.9789915966386555}
>>> 2025-09-10 22:44:15,604 - INFO - >>> {'loss': 2.2565, 'grad_norm': 1.0235917568206787, 'learning_rate': 1.5043135418707526e-05, 'epoch': 4.983193277310924}
>>> 2025-09-10 22:44:19,209 - INFO - >>> {'loss': 2.254, 'grad_norm': 0.9493706822395325, 'learning_rate': 1.5035532373107293e-05, 'epoch': 4.987394957983193}
>>> 2025-09-10 22:44:22,437 - INFO - >>> {'loss': 2.1767, 'grad_norm': 1.0546659231185913, 'learning_rate': 1.5027925425823773e-05, 'epoch': 4.991596638655462}
>>> 2025-09-10 22:44:25,872 - INFO - >>> {'loss': 2.3322, 'grad_norm': 1.051666498184204, 'learning_rate': 1.5020314582751052e-05, 'epoch': 4.995798319327731}
>>> 2025-09-10 22:44:29,110 - INFO - >>> {'loss': 2.1627, 'grad_norm': 1.1623133420944214, 'learning_rate': 1.5012699849786248e-05, 'epoch': 5.0}
>>> 2025-09-10 22:44:32,968 - INFO - >>> {'loss': 2.2836, 'grad_norm': 1.1183258295059204, 'learning_rate': 1.5005081232829484e-05, 'epoch': 5.004201680672269}
>>> 2025-09-10 22:44:36,328 - INFO - >>> {'loss': 2.187, 'grad_norm': 0.8869118690490723, 'learning_rate': 1.4997458737783897e-05, 'epoch': 5.008403361344538}
>>> 2025-09-10 22:44:40,070 - INFO - >>> {'loss': 2.2027, 'grad_norm': 1.0566977262496948, 'learning_rate': 1.4989832370555624e-05, 'epoch': 5.012605042016807}
>>> 2025-09-10 22:44:44,108 - INFO - >>> {'loss': 2.3293, 'grad_norm': 1.064979910850525, 'learning_rate': 1.498220213705381e-05, 'epoch': 5.016806722689076}
>>> 2025-09-10 22:44:47,814 - INFO - >>> {'loss': 2.2578, 'grad_norm': 0.9158453345298767, 'learning_rate': 1.4974568043190591e-05, 'epoch': 5.0210084033613445}
>>> 2025-09-10 22:44:50,950 - INFO - >>> {'loss': 2.1355, 'grad_norm': 1.018467903137207, 'learning_rate': 1.4966930094881091e-05, 'epoch': 5.025210084033613}
>>> 2025-09-10 22:44:53,874 - INFO - >>> {'loss': 2.3444, 'grad_norm': 1.0938289165496826, 'learning_rate': 1.4959288298043431e-05, 'epoch': 5.029411764705882}
>>> 2025-09-10 22:44:57,778 - INFO - >>> {'loss': 2.2797, 'grad_norm': 1.0177955627441406, 'learning_rate': 1.4951642658598697e-05, 'epoch': 5.033613445378151}
>>> 2025-09-10 22:45:01,437 - INFO - >>> {'loss': 2.2316, 'grad_norm': 0.9921842217445374, 'learning_rate': 1.4943993182470972e-05, 'epoch': 5.03781512605042}
>>> 2025-09-10 22:45:05,256 - INFO - >>> {'loss': 2.2593, 'grad_norm': 0.9878306984901428, 'learning_rate': 1.4936339875587298e-05, 'epoch': 5.042016806722689}
>>> 2025-09-10 22:45:09,472 - INFO - >>> {'loss': 2.252, 'grad_norm': 1.008972406387329, 'learning_rate': 1.492868274387769e-05, 'epoch': 5.046218487394958}
>>> 2025-09-10 22:45:13,535 - INFO - >>> {'loss': 2.3127, 'grad_norm': 0.9550853371620178, 'learning_rate': 1.4921021793275126e-05, 'epoch': 5.050420168067227}
>>> 2025-09-10 22:45:17,663 - INFO - >>> {'loss': 2.2556, 'grad_norm': 0.84853595495224, 'learning_rate': 1.4913357029715543e-05, 'epoch': 5.054621848739496}
>>> 2025-09-10 22:45:20,648 - INFO - >>> {'loss': 2.1465, 'grad_norm': 1.1286994218826294, 'learning_rate': 1.4905688459137828e-05, 'epoch': 5.0588235294117645}
>>> 2025-09-10 22:45:24,148 - INFO - >>> {'loss': 2.2061, 'grad_norm': 1.1606354713439941, 'learning_rate': 1.4898016087483834e-05, 'epoch': 5.063025210084033}
>>> 2025-09-10 22:45:27,897 - INFO - >>> {'loss': 2.2591, 'grad_norm': 0.9588056206703186, 'learning_rate': 1.4890339920698334e-05, 'epoch': 5.067226890756302}
>>> 2025-09-10 22:45:31,527 - INFO - >>> {'loss': 2.2132, 'grad_norm': 0.9968182444572449, 'learning_rate': 1.4882659964729063e-05, 'epoch': 5.071428571428571}
>>> 2025-09-10 22:45:34,633 - INFO - >>> {'loss': 2.1874, 'grad_norm': 1.1950466632843018, 'learning_rate': 1.4874976225526683e-05, 'epoch': 5.07563025210084}
>>> 2025-09-10 22:45:38,077 - INFO - >>> {'loss': 2.2627, 'grad_norm': 0.9929765462875366, 'learning_rate': 1.4867288709044783e-05, 'epoch': 5.079831932773109}
>>> 2025-09-10 22:45:42,014 - INFO - >>> {'loss': 2.2793, 'grad_norm': 0.9786597490310669, 'learning_rate': 1.4859597421239892e-05, 'epoch': 5.084033613445378}
>>> 2025-09-10 22:45:45,596 - INFO - >>> {'loss': 2.275, 'grad_norm': 0.9366963505744934, 'learning_rate': 1.485190236807145e-05, 'epoch': 5.088235294117647}
>>> 2025-09-10 22:45:48,794 - INFO - >>> {'loss': 2.1724, 'grad_norm': 0.9821820855140686, 'learning_rate': 1.4844203555501812e-05, 'epoch': 5.092436974789916}
>>> 2025-09-10 22:45:52,775 - INFO - >>> {'loss': 2.2133, 'grad_norm': 0.9617718458175659, 'learning_rate': 1.4836500989496262e-05, 'epoch': 5.0966386554621845}
>>> 2025-09-10 22:45:55,864 - INFO - >>> {'loss': 2.2346, 'grad_norm': 1.0765571594238281, 'learning_rate': 1.4828794676022975e-05, 'epoch': 5.100840336134453}
>>> 2025-09-10 22:45:59,170 - INFO - >>> {'loss': 2.283, 'grad_norm': 1.0947155952453613, 'learning_rate': 1.4821084621053036e-05, 'epoch': 5.105042016806722}
>>> 2025-09-10 22:46:02,909 - INFO - >>> {'loss': 2.1614, 'grad_norm': 1.025386929512024, 'learning_rate': 1.4813370830560434e-05, 'epoch': 5.109243697478991}
>>> 2025-09-10 22:46:06,466 - INFO - >>> {'loss': 2.226, 'grad_norm': 0.982812762260437, 'learning_rate': 1.4805653310522044e-05, 'epoch': 5.11344537815126}
>>> 2025-09-10 22:46:09,499 - INFO - >>> {'loss': 2.2845, 'grad_norm': 1.067397117614746, 'learning_rate': 1.4797932066917636e-05, 'epoch': 5.117647058823529}
>>> 2025-09-10 22:46:13,491 - INFO - >>> {'loss': 2.2279, 'grad_norm': 0.9506877064704895, 'learning_rate': 1.4790207105729866e-05, 'epoch': 5.121848739495798}
>>> 2025-09-10 22:46:16,833 - INFO - >>> {'loss': 2.2926, 'grad_norm': 0.9636428952217102, 'learning_rate': 1.4782478432944265e-05, 'epoch': 5.126050420168067}
>>> 2025-09-10 22:46:20,447 - INFO - >>> {'loss': 2.1907, 'grad_norm': 0.9548443555831909, 'learning_rate': 1.4774746054549246e-05, 'epoch': 5.130252100840337}
>>> 2025-09-10 22:46:24,476 - INFO - >>> {'loss': 2.3269, 'grad_norm': 0.9746261835098267, 'learning_rate': 1.4767009976536088e-05, 'epoch': 5.1344537815126055}
>>> 2025-09-10 22:46:27,897 - INFO - >>> {'loss': 2.1759, 'grad_norm': 1.0228477716445923, 'learning_rate': 1.475927020489894e-05, 'epoch': 5.138655462184874}
>>> 2025-09-10 22:46:31,728 - INFO - >>> {'loss': 2.1045, 'grad_norm': 0.9922662973403931, 'learning_rate': 1.4751526745634811e-05, 'epoch': 5.142857142857143}
>>> 2025-09-10 22:46:34,807 - INFO - >>> {'loss': 2.1445, 'grad_norm': 0.9688737392425537, 'learning_rate': 1.4743779604743574e-05, 'epoch': 5.147058823529412}
>>> 2025-09-10 22:46:38,120 - INFO - >>> {'loss': 2.2964, 'grad_norm': 1.1485073566436768, 'learning_rate': 1.4736028788227941e-05, 'epoch': 5.151260504201681}
>>> 2025-09-10 22:46:42,070 - INFO - >>> {'loss': 2.1698, 'grad_norm': 0.8861842155456543, 'learning_rate': 1.4728274302093484e-05, 'epoch': 5.15546218487395}
>>> 2025-09-10 22:46:46,063 - INFO - >>> {'loss': 2.2421, 'grad_norm': 0.9812535643577576, 'learning_rate': 1.472051615234861e-05, 'epoch': 5.159663865546219}
>>> 2025-09-10 22:46:49,975 - INFO - >>> {'loss': 2.2226, 'grad_norm': 1.094197154045105, 'learning_rate': 1.4712754345004575e-05, 'epoch': 5.163865546218488}
>>> 2025-09-10 22:46:53,260 - INFO - >>> {'loss': 2.2112, 'grad_norm': 1.0380868911743164, 'learning_rate': 1.470498888607546e-05, 'epoch': 5.168067226890757}
>>> 2025-09-10 22:46:57,194 - INFO - >>> {'loss': 2.2817, 'grad_norm': 1.1257658004760742, 'learning_rate': 1.4697219781578176e-05, 'epoch': 5.1722689075630255}
>>> 2025-09-10 22:47:01,216 - INFO - >>> {'loss': 2.2673, 'grad_norm': 0.9631420969963074, 'learning_rate': 1.4689447037532464e-05, 'epoch': 5.176470588235294}
>>> 2025-09-10 22:47:04,900 - INFO - >>> {'loss': 2.0744, 'grad_norm': 0.9924840331077576, 'learning_rate': 1.4681670659960883e-05, 'epoch': 5.180672268907563}
>>> 2025-09-10 22:47:08,300 - INFO - >>> {'loss': 2.2092, 'grad_norm': 0.9962190985679626, 'learning_rate': 1.4673890654888803e-05, 'epoch': 5.184873949579832}
>>> 2025-09-10 22:47:11,430 - INFO - >>> {'loss': 2.174, 'grad_norm': 1.1159206628799438, 'learning_rate': 1.466610702834441e-05, 'epoch': 5.189075630252101}
>>> 2025-09-10 22:47:15,465 - INFO - >>> {'loss': 2.208, 'grad_norm': 1.006351351737976, 'learning_rate': 1.4658319786358692e-05, 'epoch': 5.19327731092437}
>>> 2025-09-10 22:47:19,402 - INFO - >>> {'loss': 2.2337, 'grad_norm': 0.9618241190910339, 'learning_rate': 1.4650528934965443e-05, 'epoch': 5.197478991596639}
>>> 2025-09-10 22:47:23,255 - INFO - >>> {'loss': 2.2333, 'grad_norm': 0.9696429371833801, 'learning_rate': 1.4642734480201248e-05, 'epoch': 5.201680672268908}
>>> 2025-09-10 22:47:26,916 - INFO - >>> {'loss': 2.2016, 'grad_norm': 0.9780815839767456, 'learning_rate': 1.4634936428105491e-05, 'epoch': 5.205882352941177}
>>> 2025-09-10 22:47:29,857 - INFO - >>> {'loss': 2.2727, 'grad_norm': 1.0463343858718872, 'learning_rate': 1.4627134784720336e-05, 'epoch': 5.2100840336134455}
>>> 2025-09-10 22:47:33,679 - INFO - >>> {'loss': 2.2447, 'grad_norm': 1.0522297620773315, 'learning_rate': 1.4619329556090735e-05, 'epoch': 5.214285714285714}
>>> 2025-09-10 22:47:36,963 - INFO - >>> {'loss': 2.2185, 'grad_norm': 0.9993864893913269, 'learning_rate': 1.4611520748264413e-05, 'epoch': 5.218487394957983}
>>> 2025-09-10 22:47:40,289 - INFO - >>> {'loss': 2.208, 'grad_norm': 0.9723591804504395, 'learning_rate': 1.460370836729188e-05, 'epoch': 5.222689075630252}
>>> 2025-09-10 22:47:44,183 - INFO - >>> {'loss': 2.2315, 'grad_norm': 0.9521310925483704, 'learning_rate': 1.4595892419226396e-05, 'epoch': 5.226890756302521}
>>> 2025-09-10 22:47:48,027 - INFO - >>> {'loss': 2.2331, 'grad_norm': 1.0133485794067383, 'learning_rate': 1.4588072910123997e-05, 'epoch': 5.23109243697479}
>>> 2025-09-10 22:47:51,281 - INFO - >>> {'loss': 2.2908, 'grad_norm': 1.034429669380188, 'learning_rate': 1.458024984604348e-05, 'epoch': 5.235294117647059}
>>> 2025-09-10 22:47:55,012 - INFO - >>> {'loss': 2.2237, 'grad_norm': 0.9537288546562195, 'learning_rate': 1.4572423233046386e-05, 'epoch': 5.239495798319328}
>>> 2025-09-10 22:47:58,140 - INFO - >>> {'loss': 2.2068, 'grad_norm': 1.0066076517105103, 'learning_rate': 1.456459307719702e-05, 'epoch': 5.243697478991597}
>>> 2025-09-10 22:48:01,391 - INFO - >>> {'loss': 2.1896, 'grad_norm': 1.0474339723587036, 'learning_rate': 1.4556759384562418e-05, 'epoch': 5.2478991596638656}
>>> 2025-09-10 22:48:04,758 - INFO - >>> {'loss': 2.1604, 'grad_norm': 0.9674246311187744, 'learning_rate': 1.4548922161212365e-05, 'epoch': 5.2521008403361344}
>>> 2025-09-10 22:48:08,622 - INFO - >>> {'loss': 2.2031, 'grad_norm': 1.090461254119873, 'learning_rate': 1.454108141321938e-05, 'epoch': 5.256302521008403}
>>> 2025-09-10 22:48:11,290 - INFO - >>> {'loss': 2.2953, 'grad_norm': 1.0996062755584717, 'learning_rate': 1.4533237146658711e-05, 'epoch': 5.260504201680672}
>>> 2025-09-10 22:48:14,993 - INFO - >>> {'loss': 2.1684, 'grad_norm': 1.0451536178588867, 'learning_rate': 1.4525389367608336e-05, 'epoch': 5.264705882352941}
>>> 2025-09-10 22:48:18,355 - INFO - >>> {'loss': 2.2671, 'grad_norm': 1.0305322408676147, 'learning_rate': 1.4517538082148952e-05, 'epoch': 5.26890756302521}
>>> 2025-09-10 22:48:21,630 - INFO - >>> {'loss': 2.1709, 'grad_norm': 0.9463234543800354, 'learning_rate': 1.4509683296363971e-05, 'epoch': 5.273109243697479}
>>> 2025-09-10 22:48:25,010 - INFO - >>> {'loss': 2.2095, 'grad_norm': 1.0152088403701782, 'learning_rate': 1.4501825016339522e-05, 'epoch': 5.277310924369748}
>>> 2025-09-10 22:48:28,668 - INFO - >>> {'loss': 2.1653, 'grad_norm': 1.0177360773086548, 'learning_rate': 1.4493963248164437e-05, 'epoch': 5.281512605042017}
>>> 2025-09-10 22:48:32,003 - INFO - >>> {'loss': 2.2118, 'grad_norm': 1.0003842115402222, 'learning_rate': 1.4486097997930253e-05, 'epoch': 5.285714285714286}
>>> 2025-09-10 22:48:35,361 - INFO - >>> {'loss': 2.1933, 'grad_norm': 1.0297951698303223, 'learning_rate': 1.4478229271731206e-05, 'epoch': 5.2899159663865545}
>>> 2025-09-10 22:48:38,479 - INFO - >>> {'loss': 2.1625, 'grad_norm': 1.1478303670883179, 'learning_rate': 1.4470357075664218e-05, 'epoch': 5.294117647058823}
>>> 2025-09-10 22:48:41,700 - INFO - >>> {'loss': 2.1301, 'grad_norm': 1.1034141778945923, 'learning_rate': 1.4462481415828915e-05, 'epoch': 5.298319327731092}
>>> 2025-09-10 22:48:45,092 - INFO - >>> {'loss': 2.217, 'grad_norm': 1.1167477369308472, 'learning_rate': 1.4454602298327588e-05, 'epoch': 5.302521008403361}
>>> 2025-09-10 22:48:48,075 - INFO - >>> {'loss': 2.1182, 'grad_norm': 1.0710020065307617, 'learning_rate': 1.4446719729265225e-05, 'epoch': 5.30672268907563}
>>> 2025-09-10 22:48:51,158 - INFO - >>> {'loss': 2.2153, 'grad_norm': 1.1046185493469238, 'learning_rate': 1.4438833714749474e-05, 'epoch': 5.310924369747899}
>>> 2025-09-10 22:48:54,462 - INFO - >>> {'loss': 2.2042, 'grad_norm': 0.9549710154533386, 'learning_rate': 1.443094426089066e-05, 'epoch': 5.315126050420168}
>>> 2025-09-10 22:48:58,520 - INFO - >>> {'loss': 2.1961, 'grad_norm': 0.9036561250686646, 'learning_rate': 1.442305137380177e-05, 'epoch': 5.319327731092437}
>>> 2025-09-10 22:49:01,579 - INFO - >>> {'loss': 2.1078, 'grad_norm': 1.1216319799423218, 'learning_rate': 1.4415155059598452e-05, 'epoch': 5.323529411764706}
>>> 2025-09-10 22:49:04,587 - INFO - >>> {'loss': 2.2747, 'grad_norm': 0.9266380071640015, 'learning_rate': 1.4407255324399015e-05, 'epoch': 5.3277310924369745}
>>> 2025-09-10 22:49:07,974 - INFO - >>> {'loss': 2.141, 'grad_norm': 0.9442558884620667, 'learning_rate': 1.4399352174324409e-05, 'epoch': 5.331932773109243}
>>> 2025-09-10 22:49:11,274 - INFO - >>> {'loss': 2.1123, 'grad_norm': 1.1879339218139648, 'learning_rate': 1.4391445615498232e-05, 'epoch': 5.336134453781512}
>>> 2025-09-10 22:49:15,001 - INFO - >>> {'loss': 2.2655, 'grad_norm': 1.019624948501587, 'learning_rate': 1.438353565404673e-05, 'epoch': 5.340336134453781}
>>> 2025-09-10 22:49:18,643 - INFO - >>> {'loss': 2.1471, 'grad_norm': 1.1006866693496704, 'learning_rate': 1.437562229609878e-05, 'epoch': 5.34453781512605}
>>> 2025-09-10 22:49:22,177 - INFO - >>> {'loss': 2.1744, 'grad_norm': 0.9943732023239136, 'learning_rate': 1.4367705547785894e-05, 'epoch': 5.348739495798319}
>>> 2025-09-10 22:49:26,184 - INFO - >>> {'loss': 2.192, 'grad_norm': 0.9624606966972351, 'learning_rate': 1.4359785415242207e-05, 'epoch': 5.352941176470588}
>>> 2025-09-10 22:49:29,038 - INFO - >>> {'loss': 2.3116, 'grad_norm': 1.0636733770370483, 'learning_rate': 1.4351861904604475e-05, 'epoch': 5.357142857142857}
>>> 2025-09-10 22:49:33,116 - INFO - >>> {'loss': 2.273, 'grad_norm': 1.0259302854537964, 'learning_rate': 1.4343935022012078e-05, 'epoch': 5.361344537815126}
>>> 2025-09-10 22:49:36,719 - INFO - >>> {'loss': 2.1863, 'grad_norm': 1.1759858131408691, 'learning_rate': 1.4336004773607009e-05, 'epoch': 5.3655462184873945}
>>> 2025-09-10 22:49:39,716 - INFO - >>> {'loss': 2.1435, 'grad_norm': 1.1353040933609009, 'learning_rate': 1.4328071165533851e-05, 'epoch': 5.369747899159664}
>>> 2025-09-10 22:49:43,104 - INFO - >>> {'loss': 2.2095, 'grad_norm': 1.0950909852981567, 'learning_rate': 1.4320134203939825e-05, 'epoch': 5.373949579831933}
>>> 2025-09-10 22:49:46,326 - INFO - >>> {'loss': 2.1621, 'grad_norm': 1.1481266021728516, 'learning_rate': 1.4312193894974713e-05, 'epoch': 5.378151260504202}
>>> 2025-09-10 22:49:49,562 - INFO - >>> {'loss': 2.2752, 'grad_norm': 1.0948139429092407, 'learning_rate': 1.4304250244790914e-05, 'epoch': 5.382352941176471}
>>> 2025-09-10 22:49:52,533 - INFO - >>> {'loss': 2.1483, 'grad_norm': 1.0942747592926025, 'learning_rate': 1.4296303259543406e-05, 'epoch': 5.38655462184874}
>>> 2025-09-10 22:49:55,598 - INFO - >>> {'loss': 2.1323, 'grad_norm': 1.1236683130264282, 'learning_rate': 1.4288352945389762e-05, 'epoch': 5.390756302521009}
>>> 2025-09-10 22:49:59,005 - INFO - >>> {'loss': 2.142, 'grad_norm': 0.9563867449760437, 'learning_rate': 1.4280399308490118e-05, 'epoch': 5.394957983193278}
>>> 2025-09-10 22:50:02,655 - INFO - >>> {'loss': 2.2134, 'grad_norm': 1.0119768381118774, 'learning_rate': 1.4272442355007196e-05, 'epoch': 5.399159663865547}
>>> 2025-09-10 22:50:05,950 - INFO - >>> {'loss': 2.2587, 'grad_norm': 0.9781339168548584, 'learning_rate': 1.4264482091106287e-05, 'epoch': 5.4033613445378155}
>>> 2025-09-10 22:50:09,597 - INFO - >>> {'loss': 2.1976, 'grad_norm': 0.9968785643577576, 'learning_rate': 1.425651852295524e-05, 'epoch': 5.407563025210084}
>>> 2025-09-10 22:50:13,310 - INFO - >>> {'loss': 2.1989, 'grad_norm': 1.0501571893692017, 'learning_rate': 1.4248551656724479e-05, 'epoch': 5.411764705882353}
>>> 2025-09-10 22:50:16,590 - INFO - >>> {'loss': 2.2445, 'grad_norm': 1.1258209943771362, 'learning_rate': 1.4240581498586961e-05, 'epoch': 5.415966386554622}
>>> 2025-09-10 22:50:19,943 - INFO - >>> {'loss': 2.2372, 'grad_norm': 1.051240086555481, 'learning_rate': 1.4232608054718218e-05, 'epoch': 5.420168067226891}
>>> 2025-09-10 22:50:23,494 - INFO - >>> {'loss': 2.2447, 'grad_norm': 1.009881854057312, 'learning_rate': 1.4224631331296306e-05, 'epoch': 5.42436974789916}
>>> 2025-09-10 22:50:26,685 - INFO - >>> {'loss': 2.2219, 'grad_norm': 1.0617493391036987, 'learning_rate': 1.421665133450184e-05, 'epoch': 5.428571428571429}
>>> 2025-09-10 22:50:29,867 - INFO - >>> {'loss': 2.2979, 'grad_norm': 0.9859188199043274, 'learning_rate': 1.4208668070517964e-05, 'epoch': 5.432773109243698}
>>> 2025-09-10 22:50:33,484 - INFO - >>> {'loss': 2.2391, 'grad_norm': 0.9082360863685608, 'learning_rate': 1.4200681545530346e-05, 'epoch': 5.436974789915967}
>>> 2025-09-10 22:50:37,136 - INFO - >>> {'loss': 2.1735, 'grad_norm': 1.1187275648117065, 'learning_rate': 1.4192691765727196e-05, 'epoch': 5.4411764705882355}
>>> 2025-09-10 22:50:40,679 - INFO - >>> {'loss': 2.1561, 'grad_norm': 1.0514750480651855, 'learning_rate': 1.4184698737299232e-05, 'epoch': 5.445378151260504}
>>> 2025-09-10 22:50:44,328 - INFO - >>> {'loss': 2.2186, 'grad_norm': 1.0246236324310303, 'learning_rate': 1.4176702466439699e-05, 'epoch': 5.449579831932773}
>>> 2025-09-10 22:50:48,141 - INFO - >>> {'loss': 2.2001, 'grad_norm': 1.0933427810668945, 'learning_rate': 1.4168702959344347e-05, 'epoch': 5.453781512605042}
>>> 2025-09-10 22:50:52,086 - INFO - >>> {'loss': 2.1707, 'grad_norm': 0.9441274404525757, 'learning_rate': 1.4160700222211438e-05, 'epoch': 5.457983193277311}
>>> 2025-09-10 22:50:55,618 - INFO - >>> {'loss': 2.11, 'grad_norm': 0.9474889636039734, 'learning_rate': 1.4152694261241735e-05, 'epoch': 5.46218487394958}
>>> 2025-09-10 22:50:59,732 - INFO - >>> {'loss': 2.244, 'grad_norm': 1.025194525718689, 'learning_rate': 1.4144685082638505e-05, 'epoch': 5.466386554621849}
>>> 2025-09-10 22:51:03,065 - INFO - >>> {'loss': 2.2355, 'grad_norm': 1.0145759582519531, 'learning_rate': 1.4136672692607494e-05, 'epoch': 5.470588235294118}
>>> 2025-09-10 22:51:06,882 - INFO - >>> {'loss': 2.2611, 'grad_norm': 0.9745910167694092, 'learning_rate': 1.4128657097356949e-05, 'epoch': 5.474789915966387}
>>> 2025-09-10 22:51:10,161 - INFO - >>> {'loss': 2.2175, 'grad_norm': 1.0101733207702637, 'learning_rate': 1.41206383030976e-05, 'epoch': 5.4789915966386555}
>>> 2025-09-10 22:51:13,498 - INFO - >>> {'loss': 2.2823, 'grad_norm': 0.977693498134613, 'learning_rate': 1.4112616316042644e-05, 'epoch': 5.483193277310924}
>>> 2025-09-10 22:51:16,903 - INFO - >>> {'loss': 2.2011, 'grad_norm': 1.0223298072814941, 'learning_rate': 1.4104591142407769e-05, 'epoch': 5.487394957983193}
>>> 2025-09-10 22:51:20,597 - INFO - >>> {'loss': 2.2555, 'grad_norm': 1.116280436515808, 'learning_rate': 1.4096562788411114e-05, 'epoch': 5.491596638655462}
>>> 2025-09-10 22:51:24,002 - INFO - >>> {'loss': 2.1961, 'grad_norm': 1.05609929561615, 'learning_rate': 1.4088531260273298e-05, 'epoch': 5.495798319327731}
>>> 2025-09-10 22:51:27,645 - INFO - >>> {'loss': 2.2053, 'grad_norm': 1.1180658340454102, 'learning_rate': 1.408049656421739e-05, 'epoch': 5.5}
>>> 2025-09-10 22:51:30,649 - INFO - >>> {'loss': 2.2689, 'grad_norm': 1.0477879047393799, 'learning_rate': 1.407245870646892e-05, 'epoch': 5.504201680672269}
>>> 2025-09-10 22:51:34,157 - INFO - >>> {'loss': 2.2316, 'grad_norm': 1.024137258529663, 'learning_rate': 1.4064417693255856e-05, 'epoch': 5.508403361344538}
>>> 2025-09-10 22:51:38,122 - INFO - >>> {'loss': 2.357, 'grad_norm': 1.1093740463256836, 'learning_rate': 1.4056373530808629e-05, 'epoch': 5.512605042016807}
>>> 2025-09-10 22:51:42,049 - INFO - >>> {'loss': 2.2488, 'grad_norm': 0.9221462607383728, 'learning_rate': 1.4048326225360094e-05, 'epoch': 5.516806722689076}
>>> 2025-09-10 22:51:45,042 - INFO - >>> {'loss': 2.1939, 'grad_norm': 1.0841290950775146, 'learning_rate': 1.4040275783145547e-05, 'epoch': 5.5210084033613445}
>>> 2025-09-10 22:51:48,317 - INFO - >>> {'loss': 2.2022, 'grad_norm': 0.9638248682022095, 'learning_rate': 1.403222221040272e-05, 'epoch': 5.525210084033613}
>>> 2025-09-10 22:51:51,739 - INFO - >>> {'loss': 2.195, 'grad_norm': 1.0158376693725586, 'learning_rate': 1.4024165513371761e-05, 'epoch': 5.529411764705882}
>>> 2025-09-10 22:51:55,430 - INFO - >>> {'loss': 2.1142, 'grad_norm': 0.9844100475311279, 'learning_rate': 1.4016105698295244e-05, 'epoch': 5.533613445378151}
>>> 2025-09-10 22:51:58,772 - INFO - >>> {'loss': 2.1965, 'grad_norm': 1.03715980052948, 'learning_rate': 1.4008042771418157e-05, 'epoch': 5.53781512605042}
>>> 2025-09-10 22:52:02,045 - INFO - >>> {'loss': 2.134, 'grad_norm': 0.9530277848243713, 'learning_rate': 1.3999976738987904e-05, 'epoch': 5.542016806722689}
>>> 2025-09-10 22:52:05,218 - INFO - >>> {'loss': 2.2608, 'grad_norm': 0.9916041493415833, 'learning_rate': 1.399190760725429e-05, 'epoch': 5.546218487394958}
>>> 2025-09-10 22:52:08,377 - INFO - >>> {'loss': 2.2532, 'grad_norm': 1.0819498300552368, 'learning_rate': 1.3983835382469524e-05, 'epoch': 5.550420168067227}
>>> 2025-09-10 22:52:12,408 - INFO - >>> {'loss': 2.1748, 'grad_norm': 1.0815179347991943, 'learning_rate': 1.3975760070888208e-05, 'epoch': 5.554621848739496}
>>> 2025-09-10 22:52:15,850 - INFO - >>> {'loss': 2.1835, 'grad_norm': 0.984411895275116, 'learning_rate': 1.3967681678767341e-05, 'epoch': 5.5588235294117645}
>>> 2025-09-10 22:52:18,704 - INFO - >>> {'loss': 2.2647, 'grad_norm': 1.1302299499511719, 'learning_rate': 1.395960021236631e-05, 'epoch': 5.563025210084033}
>>> 2025-09-10 22:52:22,398 - INFO - >>> {'loss': 2.2227, 'grad_norm': 0.9124181866645813, 'learning_rate': 1.395151567794687e-05, 'epoch': 5.567226890756302}
>>> 2025-09-10 22:52:26,493 - INFO - >>> {'loss': 2.2136, 'grad_norm': 1.1099438667297363, 'learning_rate': 1.3943428081773173e-05, 'epoch': 5.571428571428571}
>>> 2025-09-10 22:52:30,414 - INFO - >>> {'loss': 2.1802, 'grad_norm': 1.036162257194519, 'learning_rate': 1.3935337430111731e-05, 'epoch': 5.57563025210084}
>>> 2025-09-10 22:52:33,677 - INFO - >>> {'loss': 2.3143, 'grad_norm': 1.0701334476470947, 'learning_rate': 1.3927243729231425e-05, 'epoch': 5.579831932773109}
>>> 2025-09-10 22:52:36,720 - INFO - >>> {'loss': 2.2111, 'grad_norm': 1.176313042640686, 'learning_rate': 1.39191469854035e-05, 'epoch': 5.584033613445378}
>>> 2025-09-10 22:52:40,014 - INFO - >>> {'loss': 2.3168, 'grad_norm': 1.006954550743103, 'learning_rate': 1.391104720490156e-05, 'epoch': 5.588235294117647}
>>> 2025-09-10 22:52:43,955 - INFO - >>> {'loss': 2.2093, 'grad_norm': 1.011588215827942, 'learning_rate': 1.3902944394001562e-05, 'epoch': 5.592436974789916}
>>> 2025-09-10 22:52:47,105 - INFO - >>> {'loss': 2.0908, 'grad_norm': 1.172383427619934, 'learning_rate': 1.3894838558981811e-05, 'epoch': 5.5966386554621845}
>>> 2025-09-10 22:52:50,314 - INFO - >>> {'loss': 2.2422, 'grad_norm': 0.9885851144790649, 'learning_rate': 1.3886729706122948e-05, 'epoch': 5.600840336134453}
>>> 2025-09-10 22:52:53,896 - INFO - >>> {'loss': 2.2795, 'grad_norm': 1.0035176277160645, 'learning_rate': 1.3878617841707962e-05, 'epoch': 5.605042016806722}
>>> 2025-09-10 22:52:57,327 - INFO - >>> {'loss': 2.1816, 'grad_norm': 1.0460729598999023, 'learning_rate': 1.3870502972022175e-05, 'epoch': 5.609243697478991}
>>> 2025-09-10 22:53:01,170 - INFO - >>> {'loss': 2.225, 'grad_norm': 0.9400247931480408, 'learning_rate': 1.3862385103353224e-05, 'epoch': 5.61344537815126}
>>> 2025-09-10 22:53:04,425 - INFO - >>> {'loss': 2.2049, 'grad_norm': 1.0155755281448364, 'learning_rate': 1.3854264241991093e-05, 'epoch': 5.617647058823529}
>>> 2025-09-10 22:53:07,642 - INFO - >>> {'loss': 2.2057, 'grad_norm': 1.0401760339736938, 'learning_rate': 1.3846140394228062e-05, 'epoch': 5.621848739495798}
>>> 2025-09-10 22:53:10,785 - INFO - >>> {'loss': 2.2516, 'grad_norm': 1.0246448516845703, 'learning_rate': 1.3838013566358739e-05, 'epoch': 5.626050420168067}
>>> 2025-09-10 22:53:14,767 - INFO - >>> {'loss': 2.284, 'grad_norm': 1.1338107585906982, 'learning_rate': 1.3829883764680036e-05, 'epoch': 5.630252100840336}
>>> 2025-09-10 22:53:18,900 - INFO - >>> {'loss': 2.2646, 'grad_norm': 0.9400439262390137, 'learning_rate': 1.382175099549117e-05, 'epoch': 5.634453781512605}
>>> 2025-09-10 22:53:22,554 - INFO - >>> {'loss': 2.1737, 'grad_norm': 1.0010703802108765, 'learning_rate': 1.381361526509366e-05, 'epoch': 5.6386554621848735}
>>> 2025-09-10 22:53:26,050 - INFO - >>> {'loss': 2.2383, 'grad_norm': 1.012948751449585, 'learning_rate': 1.3805476579791313e-05, 'epoch': 5.642857142857143}
>>> 2025-09-10 22:53:29,247 - INFO - >>> {'loss': 2.1734, 'grad_norm': 1.1759343147277832, 'learning_rate': 1.379733494589023e-05, 'epoch': 5.647058823529412}
>>> 2025-09-10 22:53:32,354 - INFO - >>> {'loss': 2.168, 'grad_norm': 1.012780785560608, 'learning_rate': 1.3789190369698795e-05, 'epoch': 5.651260504201681}
>>> 2025-09-10 22:53:36,423 - INFO - >>> {'loss': 2.3107, 'grad_norm': 0.9943395853042603, 'learning_rate': 1.378104285752768e-05, 'epoch': 5.65546218487395}
>>> 2025-09-10 22:53:40,288 - INFO - >>> {'loss': 2.2195, 'grad_norm': 1.0036503076553345, 'learning_rate': 1.3772892415689811e-05, 'epoch': 5.659663865546219}
>>> 2025-09-10 22:53:43,721 - INFO - >>> {'loss': 2.209, 'grad_norm': 1.0941071510314941, 'learning_rate': 1.3764739050500415e-05, 'epoch': 5.663865546218488}
>>> 2025-09-10 22:53:47,416 - INFO - >>> {'loss': 2.1379, 'grad_norm': 1.0920698642730713, 'learning_rate': 1.3756582768276951e-05, 'epoch': 5.668067226890757}
>>> 2025-09-10 22:53:50,727 - INFO - >>> {'loss': 2.2359, 'grad_norm': 1.176043152809143, 'learning_rate': 1.3748423575339162e-05, 'epoch': 5.6722689075630255}
>>> 2025-09-10 22:53:54,431 - INFO - >>> {'loss': 2.2344, 'grad_norm': 1.0679346323013306, 'learning_rate': 1.3740261478009038e-05, 'epoch': 5.676470588235294}
>>> 2025-09-10 22:53:57,440 - INFO - >>> {'loss': 2.2758, 'grad_norm': 1.2092432975769043, 'learning_rate': 1.3732096482610819e-05, 'epoch': 5.680672268907563}
>>> 2025-09-10 22:54:01,555 - INFO - >>> {'loss': 2.1979, 'grad_norm': 1.0305917263031006, 'learning_rate': 1.372392859547099e-05, 'epoch': 5.684873949579832}
>>> 2025-09-10 22:54:04,552 - INFO - >>> {'loss': 2.1799, 'grad_norm': 0.9978861808776855, 'learning_rate': 1.3715757822918279e-05, 'epoch': 5.689075630252101}
>>> 2025-09-10 22:54:07,781 - INFO - >>> {'loss': 2.1519, 'grad_norm': 1.196765422821045, 'learning_rate': 1.3707584171283647e-05, 'epoch': 5.69327731092437}
>>> 2025-09-10 22:54:11,408 - INFO - >>> {'loss': 2.308, 'grad_norm': 1.0695340633392334, 'learning_rate': 1.3699407646900289e-05, 'epoch': 5.697478991596639}
>>> 2025-09-10 22:54:14,895 - INFO - >>> {'loss': 2.2392, 'grad_norm': 1.003261923789978, 'learning_rate': 1.3691228256103625e-05, 'epoch': 5.701680672268908}
>>> 2025-09-10 22:54:18,364 - INFO - >>> {'loss': 2.1964, 'grad_norm': 1.0707982778549194, 'learning_rate': 1.3683046005231293e-05, 'epoch': 5.705882352941177}
>>> 2025-09-10 22:54:21,542 - INFO - >>> {'loss': 2.2463, 'grad_norm': 1.2179243564605713, 'learning_rate': 1.367486090062315e-05, 'epoch': 5.7100840336134455}
>>> 2025-09-10 22:54:25,020 - INFO - >>> {'loss': 2.2263, 'grad_norm': 0.9554336667060852, 'learning_rate': 1.3666672948621265e-05, 'epoch': 5.714285714285714}
>>> 2025-09-10 22:54:28,847 - INFO - >>> {'loss': 2.2656, 'grad_norm': 0.9759127497673035, 'learning_rate': 1.3658482155569907e-05, 'epoch': 5.718487394957983}
>>> 2025-09-10 22:54:31,794 - INFO - >>> {'loss': 2.2827, 'grad_norm': 1.1333038806915283, 'learning_rate': 1.3650288527815558e-05, 'epoch': 5.722689075630252}
>>> 2025-09-10 22:54:34,396 - INFO - >>> {'loss': 2.2283, 'grad_norm': 1.1717159748077393, 'learning_rate': 1.3642092071706883e-05, 'epoch': 5.726890756302521}
>>> 2025-09-10 22:54:37,686 - INFO - >>> {'loss': 2.3329, 'grad_norm': 1.1109309196472168, 'learning_rate': 1.363389279359475e-05, 'epoch': 5.73109243697479}
>>> 2025-09-10 22:54:41,153 - INFO - >>> {'loss': 2.2604, 'grad_norm': 1.1686549186706543, 'learning_rate': 1.3625690699832204e-05, 'epoch': 5.735294117647059}
>>> 2025-09-10 22:54:43,987 - INFO - >>> {'loss': 2.2448, 'grad_norm': 1.0845783948898315, 'learning_rate': 1.361748579677448e-05, 'epoch': 5.739495798319328}
>>> 2025-09-10 22:54:47,822 - INFO - >>> {'loss': 2.2932, 'grad_norm': 0.9403418302536011, 'learning_rate': 1.3609278090778981e-05, 'epoch': 5.743697478991597}
>>> 2025-09-10 22:54:51,820 - INFO - >>> {'loss': 2.2096, 'grad_norm': 0.9197205901145935, 'learning_rate': 1.3601067588205295e-05, 'epoch': 5.7478991596638656}
>>> 2025-09-10 22:54:55,339 - INFO - >>> {'loss': 2.2293, 'grad_norm': 1.0341289043426514, 'learning_rate': 1.3592854295415157e-05, 'epoch': 5.7521008403361344}
>>> 2025-09-10 22:54:59,100 - INFO - >>> {'loss': 2.22, 'grad_norm': 0.9961309432983398, 'learning_rate': 1.3584638218772487e-05, 'epoch': 5.756302521008403}
>>> 2025-09-10 22:55:02,474 - INFO - >>> {'loss': 2.1228, 'grad_norm': 0.9768102765083313, 'learning_rate': 1.3576419364643343e-05, 'epoch': 5.760504201680672}
>>> 2025-09-10 22:55:06,554 - INFO - >>> {'loss': 2.2486, 'grad_norm': 1.0091112852096558, 'learning_rate': 1.3568197739395946e-05, 'epoch': 5.764705882352941}
>>> 2025-09-10 22:55:10,106 - INFO - >>> {'loss': 2.1919, 'grad_norm': 0.9942358732223511, 'learning_rate': 1.355997334940066e-05, 'epoch': 5.76890756302521}
>>> 2025-09-10 22:55:13,498 - INFO - >>> {'loss': 2.2488, 'grad_norm': 1.0526645183563232, 'learning_rate': 1.3551746201029989e-05, 'epoch': 5.773109243697479}
>>> 2025-09-10 22:55:17,116 - INFO - >>> {'loss': 2.1742, 'grad_norm': 1.0129895210266113, 'learning_rate': 1.3543516300658585e-05, 'epoch': 5.777310924369748}
>>> 2025-09-10 22:55:20,142 - INFO - >>> {'loss': 2.184, 'grad_norm': 1.0671404600143433, 'learning_rate': 1.3535283654663216e-05, 'epoch': 5.781512605042017}
>>> 2025-09-10 22:55:23,466 - INFO - >>> {'loss': 2.2983, 'grad_norm': 1.0206977128982544, 'learning_rate': 1.3527048269422789e-05, 'epoch': 5.785714285714286}
>>> 2025-09-10 22:55:26,333 - INFO - >>> {'loss': 2.2664, 'grad_norm': 1.1227364540100098, 'learning_rate': 1.351881015131833e-05, 'epoch': 5.7899159663865545}
>>> 2025-09-10 22:55:29,816 - INFO - >>> {'loss': 2.2454, 'grad_norm': 1.1042531728744507, 'learning_rate': 1.3510569306732988e-05, 'epoch': 5.794117647058823}
>>> 2025-09-10 22:55:33,850 - INFO - >>> {'loss': 2.2249, 'grad_norm': 1.0339875221252441, 'learning_rate': 1.3502325742052012e-05, 'epoch': 5.798319327731092}
>>> 2025-09-10 22:55:37,650 - INFO - >>> {'loss': 2.329, 'grad_norm': 1.0786964893341064, 'learning_rate': 1.349407946366277e-05, 'epoch': 5.802521008403361}
>>> 2025-09-10 22:55:40,993 - INFO - >>> {'loss': 2.1951, 'grad_norm': 1.0356417894363403, 'learning_rate': 1.3485830477954728e-05, 'epoch': 5.80672268907563}
>>> 2025-09-10 22:55:44,505 - INFO - >>> {'loss': 2.2725, 'grad_norm': 0.9950751066207886, 'learning_rate': 1.347757879131945e-05, 'epoch': 5.810924369747899}
>>> 2025-09-10 22:55:48,184 - INFO - >>> {'loss': 2.1902, 'grad_norm': 0.9547805190086365, 'learning_rate': 1.3469324410150594e-05, 'epoch': 5.815126050420168}
>>> 2025-09-10 22:55:51,942 - INFO - >>> {'loss': 2.2445, 'grad_norm': 1.0997815132141113, 'learning_rate': 1.3461067340843903e-05, 'epoch': 5.819327731092437}
>>> 2025-09-10 22:55:55,146 - INFO - >>> {'loss': 2.1951, 'grad_norm': 0.9936857223510742, 'learning_rate': 1.345280758979721e-05, 'epoch': 5.823529411764706}
>>> 2025-09-10 22:55:58,249 - INFO - >>> {'loss': 2.173, 'grad_norm': 1.0117923021316528, 'learning_rate': 1.3444545163410414e-05, 'epoch': 5.8277310924369745}
>>> 2025-09-10 22:56:01,816 - INFO - >>> {'loss': 2.2897, 'grad_norm': 0.9907788634300232, 'learning_rate': 1.3436280068085498e-05, 'epoch': 5.831932773109243}
>>> 2025-09-10 22:56:05,046 - INFO - >>> {'loss': 2.2149, 'grad_norm': 0.974223792552948, 'learning_rate': 1.3428012310226508e-05, 'epoch': 5.836134453781512}
>>> 2025-09-10 22:56:08,238 - INFO - >>> {'loss': 2.1529, 'grad_norm': 1.0769628286361694, 'learning_rate': 1.3419741896239556e-05, 'epoch': 5.840336134453781}
>>> 2025-09-10 22:56:11,356 - INFO - >>> {'loss': 2.2071, 'grad_norm': 1.0213608741760254, 'learning_rate': 1.3411468832532803e-05, 'epoch': 5.84453781512605}
>>> 2025-09-10 22:56:14,832 - INFO - >>> {'loss': 2.2325, 'grad_norm': 1.080558180809021, 'learning_rate': 1.3403193125516474e-05, 'epoch': 5.848739495798319}
>>> 2025-09-10 22:56:18,263 - INFO - >>> {'loss': 2.1737, 'grad_norm': 1.0493735074996948, 'learning_rate': 1.339491478160284e-05, 'epoch': 5.852941176470588}
>>> 2025-09-10 22:56:21,101 - INFO - >>> {'loss': 2.15, 'grad_norm': 1.077061414718628, 'learning_rate': 1.3386633807206209e-05, 'epoch': 5.857142857142857}
>>> 2025-09-10 22:56:24,950 - INFO - >>> {'loss': 2.1887, 'grad_norm': 1.0360716581344604, 'learning_rate': 1.3378350208742935e-05, 'epoch': 5.8613445378151265}
>>> 2025-09-10 22:56:28,340 - INFO - >>> {'loss': 2.1556, 'grad_norm': 1.0176632404327393, 'learning_rate': 1.3370063992631397e-05, 'epoch': 5.865546218487395}
>>> 2025-09-10 22:56:32,301 - INFO - >>> {'loss': 2.2595, 'grad_norm': 1.0927437543869019, 'learning_rate': 1.3361775165292013e-05, 'epoch': 5.869747899159664}
>>> 2025-09-10 22:56:36,319 - INFO - >>> {'loss': 2.1667, 'grad_norm': 1.043617844581604, 'learning_rate': 1.335348373314721e-05, 'epoch': 5.873949579831933}
>>> 2025-09-10 22:56:39,694 - INFO - >>> {'loss': 2.2893, 'grad_norm': 1.092050552368164, 'learning_rate': 1.3345189702621448e-05, 'epoch': 5.878151260504202}
>>> 2025-09-10 22:56:42,941 - INFO - >>> {'loss': 2.3434, 'grad_norm': 1.1645005941390991, 'learning_rate': 1.333689308014119e-05, 'epoch': 5.882352941176471}
>>> 2025-09-10 22:56:47,121 - INFO - >>> {'loss': 2.2853, 'grad_norm': 0.9642864465713501, 'learning_rate': 1.3328593872134913e-05, 'epoch': 5.88655462184874}
>>> 2025-09-10 22:56:51,009 - INFO - >>> {'loss': 2.2316, 'grad_norm': 1.0117169618606567, 'learning_rate': 1.3320292085033095e-05, 'epoch': 5.890756302521009}
>>> 2025-09-10 22:56:54,562 - INFO - >>> {'loss': 2.1802, 'grad_norm': 1.0275671482086182, 'learning_rate': 1.3311987725268209e-05, 'epoch': 5.894957983193278}
>>> 2025-09-10 22:56:57,515 - INFO - >>> {'loss': 2.2588, 'grad_norm': 1.1357476711273193, 'learning_rate': 1.3303680799274729e-05, 'epoch': 5.899159663865547}
>>> 2025-09-10 22:57:01,141 - INFO - >>> {'loss': 2.2316, 'grad_norm': 1.0831410884857178, 'learning_rate': 1.3295371313489112e-05, 'epoch': 5.9033613445378155}
>>> 2025-09-10 22:57:04,475 - INFO - >>> {'loss': 2.161, 'grad_norm': 1.0033537149429321, 'learning_rate': 1.3287059274349802e-05, 'epoch': 5.907563025210084}
>>> 2025-09-10 22:57:07,847 - INFO - >>> {'loss': 2.1295, 'grad_norm': 0.9907761216163635, 'learning_rate': 1.3278744688297212e-05, 'epoch': 5.911764705882353}
>>> 2025-09-10 22:57:11,294 - INFO - >>> {'loss': 2.2979, 'grad_norm': 1.0249475240707397, 'learning_rate': 1.3270427561773745e-05, 'epoch': 5.915966386554622}
>>> 2025-09-10 22:57:14,615 - INFO - >>> {'loss': 2.2176, 'grad_norm': 1.1213972568511963, 'learning_rate': 1.3262107901223756e-05, 'epoch': 5.920168067226891}
>>> 2025-09-10 22:57:18,595 - INFO - >>> {'loss': 2.2857, 'grad_norm': 0.9291369915008545, 'learning_rate': 1.3253785713093574e-05, 'epoch': 5.92436974789916}
>>> 2025-09-10 22:57:21,557 - INFO - >>> {'loss': 2.2379, 'grad_norm': 1.2111326456069946, 'learning_rate': 1.3245461003831483e-05, 'epoch': 5.928571428571429}
>>> 2025-09-10 22:57:25,383 - INFO - >>> {'loss': 2.1546, 'grad_norm': 0.960216224193573, 'learning_rate': 1.3237133779887716e-05, 'epoch': 5.932773109243698}
>>> 2025-09-10 22:57:28,959 - INFO - >>> {'loss': 2.2466, 'grad_norm': 0.9550646543502808, 'learning_rate': 1.3228804047714462e-05, 'epoch': 5.936974789915967}
>>> 2025-09-10 22:57:32,501 - INFO - >>> {'loss': 2.2606, 'grad_norm': 1.0782872438430786, 'learning_rate': 1.3220471813765854e-05, 'epoch': 5.9411764705882355}
>>> 2025-09-10 22:57:35,690 - INFO - >>> {'loss': 2.2274, 'grad_norm': 1.0963127613067627, 'learning_rate': 1.3212137084497953e-05, 'epoch': 5.945378151260504}
>>> 2025-09-10 22:57:38,689 - INFO - >>> {'loss': 2.2652, 'grad_norm': 1.1409679651260376, 'learning_rate': 1.3203799866368766e-05, 'epoch': 5.949579831932773}
>>> 2025-09-10 22:57:42,291 - INFO - >>> {'loss': 2.1702, 'grad_norm': 1.0447837114334106, 'learning_rate': 1.3195460165838219e-05, 'epoch': 5.953781512605042}
>>> 2025-09-10 22:57:45,398 - INFO - >>> {'loss': 2.288, 'grad_norm': 1.1165107488632202, 'learning_rate': 1.3187117989368163e-05, 'epoch': 5.957983193277311}
>>> 2025-09-10 22:57:49,097 - INFO - >>> {'loss': 2.2271, 'grad_norm': 0.9878535270690918, 'learning_rate': 1.3178773343422372e-05, 'epoch': 5.96218487394958}
>>> 2025-09-10 22:57:52,349 - INFO - >>> {'loss': 2.2681, 'grad_norm': 1.0720540285110474, 'learning_rate': 1.3170426234466538e-05, 'epoch': 5.966386554621849}
>>> 2025-09-10 22:57:56,205 - INFO - >>> {'loss': 2.2466, 'grad_norm': 0.9933871626853943, 'learning_rate': 1.316207666896824e-05, 'epoch': 5.970588235294118}
>>> 2025-09-10 22:57:59,999 - INFO - >>> {'loss': 2.1518, 'grad_norm': 1.126598596572876, 'learning_rate': 1.3153724653396987e-05, 'epoch': 5.974789915966387}
>>> 2025-09-10 22:58:03,999 - INFO - >>> {'loss': 2.2422, 'grad_norm': 0.9991965889930725, 'learning_rate': 1.3145370194224168e-05, 'epoch': 5.9789915966386555}
>>> 2025-09-10 22:58:07,076 - INFO - >>> {'loss': 2.0561, 'grad_norm': 1.039264440536499, 'learning_rate': 1.3137013297923072e-05, 'epoch': 5.983193277310924}
>>> 2025-09-10 22:58:10,837 - INFO - >>> {'loss': 2.1658, 'grad_norm': 1.0705119371414185, 'learning_rate': 1.3128653970968877e-05, 'epoch': 5.987394957983193}
>>> 2025-09-10 22:58:14,210 - INFO - >>> {'loss': 2.2974, 'grad_norm': 1.1646852493286133, 'learning_rate': 1.3120292219838642e-05, 'epoch': 5.991596638655462}
>>> 2025-09-10 22:58:17,307 - INFO - >>> {'loss': 2.1878, 'grad_norm': 1.1046972274780273, 'learning_rate': 1.311192805101131e-05, 'epoch': 5.995798319327731}
>>> 2025-09-10 22:58:20,609 - INFO - >>> {'loss': 2.198, 'grad_norm': 1.0609372854232788, 'learning_rate': 1.3103561470967687e-05, 'epoch': 6.0}
>>> 2025-09-10 22:58:24,586 - INFO - >>> {'loss': 2.1949, 'grad_norm': 0.9239596128463745, 'learning_rate': 1.3095192486190453e-05, 'epoch': 6.004201680672269}
>>> 2025-09-10 22:58:27,957 - INFO - >>> {'loss': 2.2904, 'grad_norm': 1.0119435787200928, 'learning_rate': 1.3086821103164158e-05, 'epoch': 6.008403361344538}
>>> 2025-09-10 22:58:30,990 - INFO - >>> {'loss': 2.1925, 'grad_norm': 1.10031259059906, 'learning_rate': 1.3078447328375199e-05, 'epoch': 6.012605042016807}
>>> 2025-09-10 22:58:34,534 - INFO - >>> {'loss': 2.328, 'grad_norm': 1.0317765474319458, 'learning_rate': 1.3070071168311826e-05, 'epoch': 6.016806722689076}
>>> 2025-09-10 22:58:38,025 - INFO - >>> {'loss': 2.1318, 'grad_norm': 1.0699350833892822, 'learning_rate': 1.3061692629464153e-05, 'epoch': 6.0210084033613445}
>>> 2025-09-10 22:58:42,026 - INFO - >>> {'loss': 2.2915, 'grad_norm': 1.0100045204162598, 'learning_rate': 1.3053311718324118e-05, 'epoch': 6.025210084033613}
>>> 2025-09-10 22:58:45,353 - INFO - >>> {'loss': 2.1714, 'grad_norm': 1.0143837928771973, 'learning_rate': 1.3044928441385507e-05, 'epoch': 6.029411764705882}
>>> 2025-09-10 22:58:48,691 - INFO - >>> {'loss': 2.1939, 'grad_norm': 1.0927773714065552, 'learning_rate': 1.3036542805143939e-05, 'epoch': 6.033613445378151}
>>> 2025-09-10 22:58:52,136 - INFO - >>> {'loss': 2.1891, 'grad_norm': 1.0560637712478638, 'learning_rate': 1.3028154816096859e-05, 'epoch': 6.03781512605042}
>>> 2025-09-10 22:58:55,155 - INFO - >>> {'loss': 2.1417, 'grad_norm': 1.1702760457992554, 'learning_rate': 1.3019764480743532e-05, 'epoch': 6.042016806722689}
>>> 2025-09-10 22:58:59,130 - INFO - >>> {'loss': 2.1958, 'grad_norm': 1.0373454093933105, 'learning_rate': 1.3011371805585051e-05, 'epoch': 6.046218487394958}
>>> 2025-09-10 22:59:02,686 - INFO - >>> {'loss': 2.2444, 'grad_norm': 1.0244451761245728, 'learning_rate': 1.300297679712431e-05, 'epoch': 6.050420168067227}
>>> 2025-09-10 22:59:06,631 - INFO - >>> {'loss': 2.1707, 'grad_norm': 1.1483938694000244, 'learning_rate': 1.299457946186602e-05, 'epoch': 6.054621848739496}
>>> 2025-09-10 22:59:10,327 - INFO - >>> {'loss': 2.3101, 'grad_norm': 1.0247074365615845, 'learning_rate': 1.2986179806316687e-05, 'epoch': 6.0588235294117645}
>>> 2025-09-10 22:59:13,373 - INFO - >>> {'loss': 2.218, 'grad_norm': 1.1985691785812378, 'learning_rate': 1.297777783698462e-05, 'epoch': 6.063025210084033}
>>> 2025-09-10 22:59:16,802 - INFO - >>> {'loss': 2.2022, 'grad_norm': 1.1147940158843994, 'learning_rate': 1.2969373560379926e-05, 'epoch': 6.067226890756302}
>>> 2025-09-10 22:59:20,022 - INFO - >>> {'loss': 2.2166, 'grad_norm': 1.0456862449645996, 'learning_rate': 1.2960966983014482e-05, 'epoch': 6.071428571428571}
>>> 2025-09-10 22:59:23,303 - INFO - >>> {'loss': 2.2637, 'grad_norm': 1.1011252403259277, 'learning_rate': 1.295255811140197e-05, 'epoch': 6.07563025210084}
>>> 2025-09-10 22:59:26,588 - INFO - >>> {'loss': 2.2178, 'grad_norm': 1.1148990392684937, 'learning_rate': 1.2944146952057834e-05, 'epoch': 6.079831932773109}
>>> 2025-09-10 22:59:30,333 - INFO - >>> {'loss': 2.1618, 'grad_norm': 1.0295847654342651, 'learning_rate': 1.2935733511499298e-05, 'epoch': 6.084033613445378}
>>> 2025-09-10 22:59:33,400 - INFO - >>> {'loss': 2.197, 'grad_norm': 1.1198712587356567, 'learning_rate': 1.2927317796245345e-05, 'epoch': 6.088235294117647}
>>> 2025-09-10 22:59:37,042 - INFO - >>> {'loss': 2.2429, 'grad_norm': 0.9861131310462952, 'learning_rate': 1.2918899812816734e-05, 'epoch': 6.092436974789916}
>>> 2025-09-10 22:59:40,513 - INFO - >>> {'loss': 2.1843, 'grad_norm': 1.07875657081604, 'learning_rate': 1.2910479567735971e-05, 'epoch': 6.0966386554621845}
>>> 2025-09-10 22:59:43,698 - INFO - >>> {'loss': 2.1518, 'grad_norm': 1.0625475645065308, 'learning_rate': 1.2902057067527315e-05, 'epoch': 6.100840336134453}
>>> 2025-09-10 22:59:47,661 - INFO - >>> {'loss': 2.297, 'grad_norm': 1.0480448007583618, 'learning_rate': 1.2893632318716782e-05, 'epoch': 6.105042016806722}
>>> 2025-09-10 22:59:51,115 - INFO - >>> {'loss': 2.3215, 'grad_norm': 0.9673243761062622, 'learning_rate': 1.288520532783211e-05, 'epoch': 6.109243697478991}
>>> 2025-09-10 22:59:54,447 - INFO - >>> {'loss': 2.2158, 'grad_norm': 1.0575751066207886, 'learning_rate': 1.2876776101402803e-05, 'epoch': 6.11344537815126}
>>> 2025-09-10 22:59:57,329 - INFO - >>> {'loss': 2.2509, 'grad_norm': 1.1513415575027466, 'learning_rate': 1.2868344645960069e-05, 'epoch': 6.117647058823529}
>>> 2025-09-10 23:00:00,707 - INFO - >>> {'loss': 2.1649, 'grad_norm': 1.0663843154907227, 'learning_rate': 1.2859910968036863e-05, 'epoch': 6.121848739495798}
>>> 2025-09-10 23:00:04,693 - INFO - >>> {'loss': 2.2034, 'grad_norm': 1.1043533086776733, 'learning_rate': 1.285147507416785e-05, 'epoch': 6.126050420168067}
>>> 2025-09-10 23:00:07,906 - INFO - >>> {'loss': 2.1912, 'grad_norm': 1.0963441133499146, 'learning_rate': 1.2843036970889422e-05, 'epoch': 6.130252100840337}
>>> 2025-09-10 23:00:11,424 - INFO - >>> {'loss': 2.1473, 'grad_norm': 1.0687388181686401, 'learning_rate': 1.2834596664739671e-05, 'epoch': 6.1344537815126055}
>>> 2025-09-10 23:00:14,967 - INFO - >>> {'loss': 2.2778, 'grad_norm': 1.1978057622909546, 'learning_rate': 1.2826154162258406e-05, 'epoch': 6.138655462184874}
>>> 2025-09-10 23:00:18,227 - INFO - >>> {'loss': 2.1835, 'grad_norm': 1.0507920980453491, 'learning_rate': 1.2817709469987133e-05, 'epoch': 6.142857142857143}
>>> 2025-09-10 23:00:21,630 - INFO - >>> {'loss': 2.1749, 'grad_norm': 0.9797520041465759, 'learning_rate': 1.2809262594469055e-05, 'epoch': 6.147058823529412}
>>> 2025-09-10 23:00:24,996 - INFO - >>> {'loss': 2.2107, 'grad_norm': 1.0272102355957031, 'learning_rate': 1.2800813542249073e-05, 'epoch': 6.151260504201681}
>>> 2025-09-10 23:00:28,074 - INFO - >>> {'loss': 2.1684, 'grad_norm': 1.0562001466751099, 'learning_rate': 1.2792362319873758e-05, 'epoch': 6.15546218487395}
>>> 2025-09-10 23:00:31,722 - INFO - >>> {'loss': 2.1917, 'grad_norm': 0.9980025887489319, 'learning_rate': 1.2783908933891387e-05, 'epoch': 6.159663865546219}
>>> 2025-09-10 23:00:34,745 - INFO - >>> {'loss': 2.2454, 'grad_norm': 1.1845251321792603, 'learning_rate': 1.2775453390851888e-05, 'epoch': 6.163865546218488}
>>> 2025-09-10 23:00:37,771 - INFO - >>> {'loss': 2.1988, 'grad_norm': 1.0066300630569458, 'learning_rate': 1.2766995697306882e-05, 'epoch': 6.168067226890757}
>>> 2025-09-10 23:00:41,246 - INFO - >>> {'loss': 2.2986, 'grad_norm': 0.9655895233154297, 'learning_rate': 1.2758535859809642e-05, 'epoch': 6.1722689075630255}
>>> 2025-09-10 23:00:44,693 - INFO - >>> {'loss': 2.1357, 'grad_norm': 0.9996141195297241, 'learning_rate': 1.2750073884915105e-05, 'epoch': 6.176470588235294}
>>> 2025-09-10 23:00:48,730 - INFO - >>> {'loss': 2.2504, 'grad_norm': 0.953373908996582, 'learning_rate': 1.2741609779179868e-05, 'epoch': 6.180672268907563}
>>> 2025-09-10 23:00:52,189 - INFO - >>> {'loss': 2.1958, 'grad_norm': 1.0055203437805176, 'learning_rate': 1.2733143549162178e-05, 'epoch': 6.184873949579832}
>>> 2025-09-10 23:00:55,343 - INFO - >>> {'loss': 2.2514, 'grad_norm': 0.9838154911994934, 'learning_rate': 1.2724675201421923e-05, 'epoch': 6.189075630252101}
>>> 2025-09-10 23:00:58,584 - INFO - >>> {'loss': 2.1432, 'grad_norm': 0.967685341835022, 'learning_rate': 1.2716204742520641e-05, 'epoch': 6.19327731092437}
>>> 2025-09-10 23:01:01,963 - INFO - >>> {'loss': 2.2505, 'grad_norm': 0.9678006172180176, 'learning_rate': 1.2707732179021502e-05, 'epoch': 6.197478991596639}
>>> 2025-09-10 23:01:05,694 - INFO - >>> {'loss': 2.2572, 'grad_norm': 0.9726576209068298, 'learning_rate': 1.2699257517489293e-05, 'epoch': 6.201680672268908}
>>> 2025-09-10 23:01:08,701 - INFO - >>> {'loss': 2.274, 'grad_norm': 1.194940447807312, 'learning_rate': 1.2690780764490456e-05, 'epoch': 6.205882352941177}
>>> 2025-09-10 23:01:11,666 - INFO - >>> {'loss': 2.2052, 'grad_norm': 1.1564236879348755, 'learning_rate': 1.2682301926593024e-05, 'epoch': 6.2100840336134455}
>>> 2025-09-10 23:01:15,126 - INFO - >>> {'loss': 2.1935, 'grad_norm': 1.0021839141845703, 'learning_rate': 1.2673821010366663e-05, 'epoch': 6.214285714285714}
>>> 2025-09-10 23:01:18,835 - INFO - >>> {'loss': 2.118, 'grad_norm': 1.0586639642715454, 'learning_rate': 1.2665338022382645e-05, 'epoch': 6.218487394957983}
>>> 2025-09-10 23:01:22,428 - INFO - >>> {'loss': 2.2272, 'grad_norm': 0.9647576808929443, 'learning_rate': 1.2656852969213843e-05, 'epoch': 6.222689075630252}
>>> 2025-09-10 23:01:25,350 - INFO - >>> {'loss': 2.2013, 'grad_norm': 1.103625774383545, 'learning_rate': 1.2648365857434734e-05, 'epoch': 6.226890756302521}
>>> 2025-09-10 23:01:29,183 - INFO - >>> {'loss': 2.2481, 'grad_norm': 1.0435082912445068, 'learning_rate': 1.2639876693621391e-05, 'epoch': 6.23109243697479}
>>> 2025-09-10 23:01:33,129 - INFO - >>> {'loss': 2.2953, 'grad_norm': 1.004305124282837, 'learning_rate': 1.2631385484351477e-05, 'epoch': 6.235294117647059}
>>> 2025-09-10 23:01:37,014 - INFO - >>> {'loss': 2.2412, 'grad_norm': 0.9801276922225952, 'learning_rate': 1.2622892236204238e-05, 'epoch': 6.239495798319328}
>>> 2025-09-10 23:01:40,123 - INFO - >>> {'loss': 2.1863, 'grad_norm': 1.1742312908172607, 'learning_rate': 1.2614396955760502e-05, 'epoch': 6.243697478991597}
>>> 2025-09-10 23:01:43,651 - INFO - >>> {'loss': 2.2624, 'grad_norm': 1.1157335042953491, 'learning_rate': 1.2605899649602662e-05, 'epoch': 6.2478991596638656}
>>> 2025-09-10 23:01:46,918 - INFO - >>> {'loss': 2.2401, 'grad_norm': 1.0329500436782837, 'learning_rate': 1.2597400324314701e-05, 'epoch': 6.2521008403361344}
>>> 2025-09-10 23:01:50,822 - INFO - >>> {'loss': 2.199, 'grad_norm': 0.9709329605102539, 'learning_rate': 1.2588898986482145e-05, 'epoch': 6.256302521008403}
>>> 2025-09-10 23:01:54,176 - INFO - >>> {'loss': 2.38, 'grad_norm': 1.1882566213607788, 'learning_rate': 1.2580395642692089e-05, 'epoch': 6.260504201680672}
>>> 2025-09-10 23:01:57,776 - INFO - >>> {'loss': 2.2795, 'grad_norm': 0.9588232636451721, 'learning_rate': 1.257189029953319e-05, 'epoch': 6.264705882352941}
>>> 2025-09-10 23:02:01,476 - INFO - >>> {'loss': 2.2825, 'grad_norm': 1.0204756259918213, 'learning_rate': 1.2563382963595636e-05, 'epoch': 6.26890756302521}
>>> 2025-09-10 23:02:05,028 - INFO - >>> {'loss': 2.1162, 'grad_norm': 1.0509322881698608, 'learning_rate': 1.2554873641471175e-05, 'epoch': 6.273109243697479}
>>> 2025-09-10 23:02:08,356 - INFO - >>> {'loss': 2.2223, 'grad_norm': 0.9939892292022705, 'learning_rate': 1.2546362339753084e-05, 'epoch': 6.277310924369748}
>>> 2025-09-10 23:02:12,139 - INFO - >>> {'loss': 2.203, 'grad_norm': 0.9111900925636292, 'learning_rate': 1.253784906503618e-05, 'epoch': 6.281512605042017}
>>> 2025-09-10 23:02:15,210 - INFO - >>> {'loss': 2.2815, 'grad_norm': 1.0630067586898804, 'learning_rate': 1.2529333823916807e-05, 'epoch': 6.285714285714286}
>>> 2025-09-10 23:02:18,403 - INFO - >>> {'loss': 2.172, 'grad_norm': 1.0501974821090698, 'learning_rate': 1.2520816622992835e-05, 'epoch': 6.2899159663865545}
>>> 2025-09-10 23:02:22,165 - INFO - >>> {'loss': 2.1589, 'grad_norm': 1.026503562927246, 'learning_rate': 1.2512297468863643e-05, 'epoch': 6.294117647058823}
>>> 2025-09-10 23:02:25,996 - INFO - >>> {'loss': 2.1245, 'grad_norm': 1.1636422872543335, 'learning_rate': 1.2503776368130136e-05, 'epoch': 6.298319327731092}
>>> 2025-09-10 23:02:29,135 - INFO - >>> {'loss': 2.1914, 'grad_norm': 1.0285158157348633, 'learning_rate': 1.2495253327394721e-05, 'epoch': 6.302521008403361}
>>> 2025-09-10 23:02:33,664 - INFO - >>> {'loss': 2.2532, 'grad_norm': 0.9755720496177673, 'learning_rate': 1.2486728353261301e-05, 'epoch': 6.30672268907563}
>>> 2025-09-10 23:02:36,880 - INFO - >>> {'loss': 2.1778, 'grad_norm': 1.0555499792099, 'learning_rate': 1.2478201452335294e-05, 'epoch': 6.310924369747899}
>>> 2025-09-10 23:02:40,206 - INFO - >>> {'loss': 2.2146, 'grad_norm': 1.116711139678955, 'learning_rate': 1.24696726312236e-05, 'epoch': 6.315126050420168}
>>> 2025-09-10 23:02:43,374 - INFO - >>> {'loss': 2.2017, 'grad_norm': 1.0249890089035034, 'learning_rate': 1.2461141896534604e-05, 'epoch': 6.319327731092437}
>>> 2025-09-10 23:02:46,688 - INFO - >>> {'loss': 2.1953, 'grad_norm': 1.0431543588638306, 'learning_rate': 1.2452609254878178e-05, 'epoch': 6.323529411764706}
>>> 2025-09-10 23:02:50,125 - INFO - >>> {'loss': 2.2244, 'grad_norm': 0.9917601346969604, 'learning_rate': 1.2444074712865677e-05, 'epoch': 6.3277310924369745}
>>> 2025-09-10 23:02:53,292 - INFO - >>> {'loss': 2.2511, 'grad_norm': 1.2830344438552856, 'learning_rate': 1.2435538277109919e-05, 'epoch': 6.331932773109243}
>>> 2025-09-10 23:02:57,342 - INFO - >>> {'loss': 2.2076, 'grad_norm': 0.9048174023628235, 'learning_rate': 1.2426999954225201e-05, 'epoch': 6.336134453781512}
>>> 2025-09-10 23:03:00,252 - INFO - >>> {'loss': 2.2656, 'grad_norm': 0.9982793927192688, 'learning_rate': 1.2418459750827263e-05, 'epoch': 6.340336134453781}
>>> 2025-09-10 23:03:04,054 - INFO - >>> {'loss': 2.1915, 'grad_norm': 1.0555170774459839, 'learning_rate': 1.2409917673533318e-05, 'epoch': 6.34453781512605}
>>> 2025-09-10 23:03:07,775 - INFO - >>> {'loss': 2.1299, 'grad_norm': 1.087664008140564, 'learning_rate': 1.2401373728962032e-05, 'epoch': 6.348739495798319}
>>> 2025-09-10 23:03:10,839 - INFO - >>> {'loss': 2.295, 'grad_norm': 1.1122827529907227, 'learning_rate': 1.2392827923733503e-05, 'epoch': 6.352941176470588}
>>> 2025-09-10 23:03:14,208 - INFO - >>> {'loss': 2.1267, 'grad_norm': 1.0340780019760132, 'learning_rate': 1.2384280264469293e-05, 'epoch': 6.357142857142857}
>>> 2025-09-10 23:03:17,600 - INFO - >>> {'loss': 2.2695, 'grad_norm': 0.9641199707984924, 'learning_rate': 1.2375730757792376e-05, 'epoch': 6.361344537815126}
>>> 2025-09-10 23:03:20,610 - INFO - >>> {'loss': 2.1498, 'grad_norm': 1.1462244987487793, 'learning_rate': 1.2367179410327175e-05, 'epoch': 6.3655462184873945}
>>> 2025-09-10 23:03:24,467 - INFO - >>> {'loss': 2.2062, 'grad_norm': 1.052120566368103, 'learning_rate': 1.2358626228699531e-05, 'epoch': 6.369747899159664}
>>> 2025-09-10 23:03:27,901 - INFO - >>> {'loss': 2.1756, 'grad_norm': 1.0382343530654907, 'learning_rate': 1.235007121953671e-05, 'epoch': 6.373949579831933}
>>> 2025-09-10 23:03:31,649 - INFO - >>> {'loss': 2.2387, 'grad_norm': 0.9868620038032532, 'learning_rate': 1.2341514389467398e-05, 'epoch': 6.378151260504202}
>>> 2025-09-10 23:03:35,315 - INFO - >>> {'loss': 2.3058, 'grad_norm': 1.0960322618484497, 'learning_rate': 1.2332955745121676e-05, 'epoch': 6.382352941176471}
>>> 2025-09-10 23:03:38,815 - INFO - >>> {'loss': 2.1454, 'grad_norm': 0.9680913090705872, 'learning_rate': 1.2324395293131047e-05, 'epoch': 6.38655462184874}
>>> 2025-09-10 23:03:42,159 - INFO - >>> {'loss': 2.2148, 'grad_norm': 1.0180339813232422, 'learning_rate': 1.2315833040128408e-05, 'epoch': 6.390756302521009}
>>> 2025-09-10 23:03:46,101 - INFO - >>> {'loss': 2.1675, 'grad_norm': 1.02902090549469, 'learning_rate': 1.2307268992748056e-05, 'epoch': 6.394957983193278}
>>> 2025-09-10 23:03:49,428 - INFO - >>> {'loss': 2.1801, 'grad_norm': 0.9650536179542542, 'learning_rate': 1.2298703157625667e-05, 'epoch': 6.399159663865547}
>>> 2025-09-10 23:03:52,836 - INFO - >>> {'loss': 2.1517, 'grad_norm': 1.1064406633377075, 'learning_rate': 1.229013554139832e-05, 'epoch': 6.4033613445378155}
>>> 2025-09-10 23:03:56,160 - INFO - >>> {'loss': 2.2348, 'grad_norm': 1.0297627449035645, 'learning_rate': 1.2281566150704456e-05, 'epoch': 6.407563025210084}
>>> 2025-09-10 23:04:00,186 - INFO - >>> {'loss': 2.3012, 'grad_norm': 1.0975239276885986, 'learning_rate': 1.2272994992183902e-05, 'epoch': 6.411764705882353}
>>> 2025-09-10 23:04:03,388 - INFO - >>> {'loss': 2.2465, 'grad_norm': 1.0701594352722168, 'learning_rate': 1.2264422072477852e-05, 'epoch': 6.415966386554622}
>>> 2025-09-10 23:04:06,940 - INFO - >>> {'loss': 2.1096, 'grad_norm': 1.0891448259353638, 'learning_rate': 1.2255847398228865e-05, 'epoch': 6.420168067226891}
>>> 2025-09-10 23:04:10,645 - INFO - >>> {'loss': 2.2948, 'grad_norm': 1.128462314605713, 'learning_rate': 1.2247270976080858e-05, 'epoch': 6.42436974789916}
>>> 2025-09-10 23:04:14,312 - INFO - >>> {'loss': 2.1395, 'grad_norm': 0.9696171879768372, 'learning_rate': 1.2238692812679102e-05, 'epoch': 6.428571428571429}
>>> 2025-09-10 23:04:18,130 - INFO - >>> {'loss': 2.2084, 'grad_norm': 1.048733115196228, 'learning_rate': 1.2230112914670218e-05, 'epoch': 6.432773109243698}
>>> 2025-09-10 23:04:21,533 - INFO - >>> {'loss': 2.1653, 'grad_norm': 1.01304292678833, 'learning_rate': 1.2221531288702172e-05, 'epoch': 6.436974789915967}
>>> 2025-09-10 23:04:25,150 - INFO - >>> {'loss': 2.3324, 'grad_norm': 1.0493289232254028, 'learning_rate': 1.2212947941424274e-05, 'epoch': 6.4411764705882355}
>>> 2025-09-10 23:04:28,094 - INFO - >>> {'loss': 2.2347, 'grad_norm': 1.0362378358840942, 'learning_rate': 1.220436287948715e-05, 'epoch': 6.445378151260504}
>>> 2025-09-10 23:04:31,378 - INFO - >>> {'loss': 2.2131, 'grad_norm': 1.0815486907958984, 'learning_rate': 1.2195776109542779e-05, 'epoch': 6.449579831932773}
>>> 2025-09-10 23:04:34,403 - INFO - >>> {'loss': 2.2412, 'grad_norm': 1.0264031887054443, 'learning_rate': 1.2187187638244442e-05, 'epoch': 6.453781512605042}
>>> 2025-09-10 23:04:38,119 - INFO - >>> {'loss': 2.2055, 'grad_norm': 1.0866032838821411, 'learning_rate': 1.2178597472246749e-05, 'epoch': 6.457983193277311}
>>> 2025-09-10 23:04:42,011 - INFO - >>> {'loss': 2.1874, 'grad_norm': 1.029237985610962, 'learning_rate': 1.2170005618205627e-05, 'epoch': 6.46218487394958}
>>> 2025-09-10 23:04:45,131 - INFO - >>> {'loss': 2.2486, 'grad_norm': 1.0238081216812134, 'learning_rate': 1.2161412082778297e-05, 'epoch': 6.466386554621849}
>>> 2025-09-10 23:04:48,321 - INFO - >>> {'loss': 2.1679, 'grad_norm': 1.2031358480453491, 'learning_rate': 1.2152816872623297e-05, 'epoch': 6.470588235294118}
>>> 2025-09-10 23:04:51,613 - INFO - >>> {'loss': 2.2197, 'grad_norm': 1.2365416288375854, 'learning_rate': 1.2144219994400453e-05, 'epoch': 6.474789915966387}
>>> 2025-09-10 23:04:55,315 - INFO - >>> {'loss': 2.2641, 'grad_norm': 1.064137578010559, 'learning_rate': 1.213562145477089e-05, 'epoch': 6.4789915966386555}
>>> 2025-09-10 23:04:59,242 - INFO - >>> {'loss': 2.267, 'grad_norm': 0.9840182065963745, 'learning_rate': 1.2127021260397013e-05, 'epoch': 6.483193277310924}
>>> 2025-09-10 23:05:02,796 - INFO - >>> {'loss': 2.1938, 'grad_norm': 0.9636024236679077, 'learning_rate': 1.2118419417942522e-05, 'epoch': 6.487394957983193}
>>> 2025-09-10 23:05:06,863 - INFO - >>> {'loss': 2.1908, 'grad_norm': 0.9925521612167358, 'learning_rate': 1.2109815934072376e-05, 'epoch': 6.491596638655462}
>>> 2025-09-10 23:05:10,144 - INFO - >>> {'loss': 2.209, 'grad_norm': 0.9987472295761108, 'learning_rate': 1.2101210815452822e-05, 'epoch': 6.495798319327731}
>>> 2025-09-10 23:05:13,691 - INFO - >>> {'loss': 2.3116, 'grad_norm': 0.9656564593315125, 'learning_rate': 1.2092604068751363e-05, 'epoch': 6.5}
>>> 2025-09-10 23:05:17,519 - INFO - >>> {'loss': 2.2122, 'grad_norm': 1.0154800415039062, 'learning_rate': 1.208399570063677e-05, 'epoch': 6.504201680672269}
>>> 2025-09-10 23:05:21,498 - INFO - >>> {'loss': 2.2599, 'grad_norm': 0.9711111783981323, 'learning_rate': 1.2075385717779069e-05, 'epoch': 6.508403361344538}
>>> 2025-09-10 23:05:25,505 - INFO - >>> {'loss': 2.1632, 'grad_norm': 1.033389925956726, 'learning_rate': 1.206677412684953e-05, 'epoch': 6.512605042016807}
>>> 2025-09-10 23:05:29,281 - INFO - >>> {'loss': 2.1761, 'grad_norm': 0.9435733556747437, 'learning_rate': 1.2058160934520685e-05, 'epoch': 6.516806722689076}
>>> 2025-09-10 23:05:32,990 - INFO - >>> {'loss': 2.1914, 'grad_norm': 0.9829372763633728, 'learning_rate': 1.2049546147466284e-05, 'epoch': 6.5210084033613445}
>>> 2025-09-10 23:05:36,902 - INFO - >>> {'loss': 2.2969, 'grad_norm': 1.3114004135131836, 'learning_rate': 1.2040929772361336e-05, 'epoch': 6.525210084033613}
>>> 2025-09-10 23:05:40,256 - INFO - >>> {'loss': 2.2539, 'grad_norm': 1.0543203353881836, 'learning_rate': 1.2032311815882066e-05, 'epoch': 6.529411764705882}
>>> 2025-09-10 23:05:43,900 - INFO - >>> {'loss': 2.2307, 'grad_norm': 1.0788743495941162, 'learning_rate': 1.2023692284705928e-05, 'epoch': 6.533613445378151}
>>> 2025-09-10 23:05:47,070 - INFO - >>> {'loss': 2.1815, 'grad_norm': 1.1703873872756958, 'learning_rate': 1.2015071185511595e-05, 'epoch': 6.53781512605042}
>>> 2025-09-10 23:05:51,089 - INFO - >>> {'loss': 2.1383, 'grad_norm': 1.0488637685775757, 'learning_rate': 1.2006448524978963e-05, 'epoch': 6.542016806722689}
>>> 2025-09-10 23:05:54,817 - INFO - >>> {'loss': 2.1923, 'grad_norm': 1.1055463552474976, 'learning_rate': 1.1997824309789122e-05, 'epoch': 6.546218487394958}
>>> 2025-09-10 23:05:58,417 - INFO - >>> {'loss': 2.1681, 'grad_norm': 1.155225396156311, 'learning_rate': 1.198919854662438e-05, 'epoch': 6.550420168067227}
>>> 2025-09-10 23:06:02,053 - INFO - >>> {'loss': 2.3092, 'grad_norm': 1.125158667564392, 'learning_rate': 1.198057124216824e-05, 'epoch': 6.554621848739496}
>>> 2025-09-10 23:06:05,241 - INFO - >>> {'loss': 2.1189, 'grad_norm': 1.184154748916626, 'learning_rate': 1.1971942403105397e-05, 'epoch': 6.5588235294117645}
>>> 2025-09-10 23:06:08,675 - INFO - >>> {'loss': 2.2305, 'grad_norm': 1.163283109664917, 'learning_rate': 1.1963312036121741e-05, 'epoch': 6.563025210084033}
>>> 2025-09-10 23:06:11,712 - INFO - >>> {'loss': 2.1059, 'grad_norm': 1.1255230903625488, 'learning_rate': 1.1954680147904343e-05, 'epoch': 6.567226890756302}
>>> 2025-09-10 23:06:14,966 - INFO - >>> {'loss': 2.1612, 'grad_norm': 1.1419861316680908, 'learning_rate': 1.1946046745141446e-05, 'epoch': 6.571428571428571}
>>> 2025-09-10 23:06:18,184 - INFO - >>> {'loss': 2.3617, 'grad_norm': 1.1087533235549927, 'learning_rate': 1.1937411834522476e-05, 'epoch': 6.57563025210084}
>>> 2025-09-10 23:06:21,490 - INFO - >>> {'loss': 2.1553, 'grad_norm': 1.1165577173233032, 'learning_rate': 1.1928775422738028e-05, 'epoch': 6.579831932773109}
>>> 2025-09-10 23:06:24,769 - INFO - >>> {'loss': 2.2771, 'grad_norm': 1.057222843170166, 'learning_rate': 1.1920137516479848e-05, 'epoch': 6.584033613445378}
>>> 2025-09-10 23:06:28,102 - INFO - >>> {'loss': 2.2916, 'grad_norm': 1.0750806331634521, 'learning_rate': 1.1911498122440853e-05, 'epoch': 6.588235294117647}
>>> 2025-09-10 23:06:32,207 - INFO - >>> {'loss': 2.199, 'grad_norm': 0.9598168730735779, 'learning_rate': 1.1902857247315104e-05, 'epoch': 6.592436974789916}
>>> 2025-09-10 23:06:36,177 - INFO - >>> {'loss': 2.2464, 'grad_norm': 0.9676440954208374, 'learning_rate': 1.1894214897797817e-05, 'epoch': 6.5966386554621845}
>>> 2025-09-10 23:06:39,831 - INFO - >>> {'loss': 2.2418, 'grad_norm': 0.9406696557998657, 'learning_rate': 1.1885571080585347e-05, 'epoch': 6.600840336134453}
>>> 2025-09-10 23:06:42,861 - INFO - >>> {'loss': 2.2337, 'grad_norm': 1.1228923797607422, 'learning_rate': 1.1876925802375183e-05, 'epoch': 6.605042016806722}
>>> 2025-09-10 23:06:45,747 - INFO - >>> {'loss': 2.2419, 'grad_norm': 1.1008846759796143, 'learning_rate': 1.186827906986595e-05, 'epoch': 6.609243697478991}
>>> 2025-09-10 23:06:49,763 - INFO - >>> {'loss': 2.2014, 'grad_norm': 1.0552788972854614, 'learning_rate': 1.1859630889757399e-05, 'epoch': 6.61344537815126}
>>> 2025-09-10 23:06:52,787 - INFO - >>> {'loss': 2.0847, 'grad_norm': 1.1667457818984985, 'learning_rate': 1.1850981268750398e-05, 'epoch': 6.617647058823529}
>>> 2025-09-10 23:06:56,374 - INFO - >>> {'loss': 2.1669, 'grad_norm': 1.0711123943328857, 'learning_rate': 1.1842330213546944e-05, 'epoch': 6.621848739495798}
>>> 2025-09-10 23:07:00,002 - INFO - >>> {'loss': 2.1088, 'grad_norm': 1.0530978441238403, 'learning_rate': 1.1833677730850134e-05, 'epoch': 6.626050420168067}
>>> 2025-09-10 23:07:03,494 - INFO - >>> {'loss': 2.3375, 'grad_norm': 0.9771549105644226, 'learning_rate': 1.1825023827364172e-05, 'epoch': 6.630252100840336}
>>> 2025-09-10 23:07:06,509 - INFO - >>> {'loss': 2.1695, 'grad_norm': 1.2389590740203857, 'learning_rate': 1.1816368509794365e-05, 'epoch': 6.634453781512605}
>>> 2025-09-10 23:07:10,196 - INFO - >>> {'loss': 2.1585, 'grad_norm': 0.9892165660858154, 'learning_rate': 1.1807711784847118e-05, 'epoch': 6.6386554621848735}
>>> 2025-09-10 23:07:13,803 - INFO - >>> {'loss': 2.2445, 'grad_norm': 0.9441226124763489, 'learning_rate': 1.1799053659229922e-05, 'epoch': 6.642857142857143}
>>> 2025-09-10 23:07:16,788 - INFO - >>> {'loss': 2.2588, 'grad_norm': 1.1305161714553833, 'learning_rate': 1.179039413965136e-05, 'epoch': 6.647058823529412}
>>> 2025-09-10 23:07:20,077 - INFO - >>> {'loss': 2.1902, 'grad_norm': 1.0095269680023193, 'learning_rate': 1.1781733232821085e-05, 'epoch': 6.651260504201681}
>>> 2025-09-10 23:07:23,776 - INFO - >>> {'loss': 2.1513, 'grad_norm': 1.0208170413970947, 'learning_rate': 1.1773070945449831e-05, 'epoch': 6.65546218487395}
>>> 2025-09-10 23:07:27,723 - INFO - >>> {'loss': 2.1817, 'grad_norm': 1.185282588005066, 'learning_rate': 1.1764407284249405e-05, 'epoch': 6.659663865546219}
>>> 2025-09-10 23:07:31,771 - INFO - >>> {'loss': 2.1372, 'grad_norm': 1.033530831336975, 'learning_rate': 1.1755742255932672e-05, 'epoch': 6.663865546218488}
>>> 2025-09-10 23:07:34,937 - INFO - >>> {'loss': 2.0659, 'grad_norm': 1.1254684925079346, 'learning_rate': 1.174707586721356e-05, 'epoch': 6.668067226890757}
>>> 2025-09-10 23:07:38,336 - INFO - >>> {'loss': 2.1842, 'grad_norm': 1.0485132932662964, 'learning_rate': 1.1738408124807048e-05, 'epoch': 6.6722689075630255}
>>> 2025-09-10 23:07:42,029 - INFO - >>> {'loss': 2.1356, 'grad_norm': 1.0886878967285156, 'learning_rate': 1.1729739035429166e-05, 'epoch': 6.676470588235294}
>>> 2025-09-10 23:07:45,687 - INFO - >>> {'loss': 2.253, 'grad_norm': 1.0374263525009155, 'learning_rate': 1.1721068605796988e-05, 'epoch': 6.680672268907563}
>>> 2025-09-10 23:07:49,160 - INFO - >>> {'loss': 2.178, 'grad_norm': 0.9491592645645142, 'learning_rate': 1.1712396842628629e-05, 'epoch': 6.684873949579832}
>>> 2025-09-10 23:07:52,466 - INFO - >>> {'loss': 2.1505, 'grad_norm': 1.0953254699707031, 'learning_rate': 1.170372375264323e-05, 'epoch': 6.689075630252101}
>>> 2025-09-10 23:07:55,856 - INFO - >>> {'loss': 2.2257, 'grad_norm': 1.1715495586395264, 'learning_rate': 1.1695049342560969e-05, 'epoch': 6.69327731092437}
>>> 2025-09-10 23:07:59,272 - INFO - >>> {'loss': 2.2804, 'grad_norm': 0.9995971322059631, 'learning_rate': 1.1686373619103037e-05, 'epoch': 6.697478991596639}
>>> 2025-09-10 23:08:02,297 - INFO - >>> {'loss': 2.1794, 'grad_norm': 1.0352578163146973, 'learning_rate': 1.167769658899165e-05, 'epoch': 6.701680672268908}
>>> 2025-09-10 23:08:05,405 - INFO - >>> {'loss': 2.1546, 'grad_norm': 1.0302865505218506, 'learning_rate': 1.1669018258950036e-05, 'epoch': 6.705882352941177}
>>> 2025-09-10 23:08:09,191 - INFO - >>> {'loss': 2.2355, 'grad_norm': 1.142281174659729, 'learning_rate': 1.1660338635702425e-05, 'epoch': 6.7100840336134455}
>>> 2025-09-10 23:08:13,243 - INFO - >>> {'loss': 2.1521, 'grad_norm': 1.0659563541412354, 'learning_rate': 1.1651657725974054e-05, 'epoch': 6.714285714285714}
>>> 2025-09-10 23:08:16,571 - INFO - >>> {'loss': 2.1494, 'grad_norm': 1.0337733030319214, 'learning_rate': 1.1642975536491157e-05, 'epoch': 6.718487394957983}
>>> 2025-09-10 23:08:19,564 - INFO - >>> {'loss': 2.2692, 'grad_norm': 1.148280382156372, 'learning_rate': 1.1634292073980955e-05, 'epoch': 6.722689075630252}
>>> 2025-09-10 23:08:24,165 - INFO - >>> {'loss': 2.3007, 'grad_norm': 1.0583560466766357, 'learning_rate': 1.162560734517166e-05, 'epoch': 6.726890756302521}
>>> 2025-09-10 23:08:27,355 - INFO - >>> {'loss': 2.2458, 'grad_norm': 1.1338061094284058, 'learning_rate': 1.1616921356792466e-05, 'epoch': 6.73109243697479}
>>> 2025-09-10 23:08:30,316 - INFO - >>> {'loss': 2.1254, 'grad_norm': 1.1063538789749146, 'learning_rate': 1.1608234115573534e-05, 'epoch': 6.735294117647059}
>>> 2025-09-10 23:08:34,320 - INFO - >>> {'loss': 2.2484, 'grad_norm': 1.02610445022583, 'learning_rate': 1.1599545628246007e-05, 'epoch': 6.739495798319328}
>>> 2025-09-10 23:08:37,348 - INFO - >>> {'loss': 2.26, 'grad_norm': 1.0443450212478638, 'learning_rate': 1.1590855901541989e-05, 'epoch': 6.743697478991597}
>>> 2025-09-10 23:08:41,146 - INFO - >>> {'loss': 2.2665, 'grad_norm': 1.076661467552185, 'learning_rate': 1.158216494219454e-05, 'epoch': 6.7478991596638656}
>>> 2025-09-10 23:08:44,461 - INFO - >>> {'loss': 2.1558, 'grad_norm': 1.1041033267974854, 'learning_rate': 1.1573472756937683e-05, 'epoch': 6.7521008403361344}
>>> 2025-09-10 23:08:47,486 - INFO - >>> {'loss': 2.1728, 'grad_norm': 1.0041269063949585, 'learning_rate': 1.1564779352506382e-05, 'epoch': 6.756302521008403}
>>> 2025-09-10 23:08:50,846 - INFO - >>> {'loss': 2.1935, 'grad_norm': 1.0571262836456299, 'learning_rate': 1.1556084735636557e-05, 'epoch': 6.760504201680672}
>>> 2025-09-10 23:08:54,611 - INFO - >>> {'loss': 2.172, 'grad_norm': 1.120162010192871, 'learning_rate': 1.1547388913065057e-05, 'epoch': 6.764705882352941}
>>> 2025-09-10 23:08:58,030 - INFO - >>> {'loss': 2.1722, 'grad_norm': 1.019857406616211, 'learning_rate': 1.153869189152967e-05, 'epoch': 6.76890756302521}
>>> 2025-09-10 23:09:01,128 - INFO - >>> {'loss': 2.1755, 'grad_norm': 1.1330969333648682, 'learning_rate': 1.152999367776911e-05, 'epoch': 6.773109243697479}
>>> 2025-09-10 23:09:04,940 - INFO - >>> {'loss': 2.1926, 'grad_norm': 1.0929054021835327, 'learning_rate': 1.1521294278523023e-05, 'epoch': 6.777310924369748}
>>> 2025-09-10 23:09:08,954 - INFO - >>> {'loss': 2.1728, 'grad_norm': 0.9692755937576294, 'learning_rate': 1.151259370053196e-05, 'epoch': 6.781512605042017}
>>> 2025-09-10 23:09:12,489 - INFO - >>> {'loss': 2.2126, 'grad_norm': 1.1011720895767212, 'learning_rate': 1.15038919505374e-05, 'epoch': 6.785714285714286}
>>> 2025-09-10 23:09:16,085 - INFO - >>> {'loss': 2.0655, 'grad_norm': 1.101211667060852, 'learning_rate': 1.1495189035281717e-05, 'epoch': 6.7899159663865545}
>>> 2025-09-10 23:09:19,380 - INFO - >>> {'loss': 2.2642, 'grad_norm': 1.0633646249771118, 'learning_rate': 1.1486484961508197e-05, 'epoch': 6.794117647058823}
>>> 2025-09-10 23:09:23,353 - INFO - >>> {'loss': 2.2217, 'grad_norm': 1.0116679668426514, 'learning_rate': 1.147777973596102e-05, 'epoch': 6.798319327731092}
>>> 2025-09-10 23:09:26,453 - INFO - >>> {'loss': 2.1415, 'grad_norm': 1.3655548095703125, 'learning_rate': 1.1469073365385258e-05, 'epoch': 6.802521008403361}
>>> 2025-09-10 23:09:30,182 - INFO - >>> {'loss': 2.2359, 'grad_norm': 1.1364747285842896, 'learning_rate': 1.1460365856526874e-05, 'epoch': 6.80672268907563}
>>> 2025-09-10 23:09:33,515 - INFO - >>> {'loss': 2.2106, 'grad_norm': 1.3079863786697388, 'learning_rate': 1.1451657216132706e-05, 'epoch': 6.810924369747899}
>>> 2025-09-10 23:09:37,015 - INFO - >>> {'loss': 2.2936, 'grad_norm': 1.106690526008606, 'learning_rate': 1.1442947450950475e-05, 'epoch': 6.815126050420168}
>>> 2025-09-10 23:09:40,326 - INFO - >>> {'loss': 2.144, 'grad_norm': 1.1487118005752563, 'learning_rate': 1.1434236567728771e-05, 'epoch': 6.819327731092437}
>>> 2025-09-10 23:09:44,067 - INFO - >>> {'loss': 2.3038, 'grad_norm': 1.0142637491226196, 'learning_rate': 1.1425524573217053e-05, 'epoch': 6.823529411764706}
>>> 2025-09-10 23:09:47,935 - INFO - >>> {'loss': 2.1868, 'grad_norm': 1.0391056537628174, 'learning_rate': 1.1416811474165633e-05, 'epoch': 6.8277310924369745}
>>> 2025-09-10 23:09:51,227 - INFO - >>> {'loss': 2.2905, 'grad_norm': 1.070397973060608, 'learning_rate': 1.1408097277325692e-05, 'epoch': 6.831932773109243}
>>> 2025-09-10 23:09:54,940 - INFO - >>> {'loss': 2.1454, 'grad_norm': 1.0077543258666992, 'learning_rate': 1.1399381989449248e-05, 'epoch': 6.836134453781512}
>>> 2025-09-10 23:09:58,263 - INFO - >>> {'loss': 2.1423, 'grad_norm': 1.0794755220413208, 'learning_rate': 1.1390665617289174e-05, 'epoch': 6.840336134453781}
>>> 2025-09-10 23:10:01,804 - INFO - >>> {'loss': 2.214, 'grad_norm': 1.0187926292419434, 'learning_rate': 1.138194816759918e-05, 'epoch': 6.84453781512605}
>>> 2025-09-10 23:10:05,591 - INFO - >>> {'loss': 2.1951, 'grad_norm': 1.169570803642273, 'learning_rate': 1.1373229647133803e-05, 'epoch': 6.848739495798319}
>>> 2025-09-10 23:10:08,895 - INFO - >>> {'loss': 2.2018, 'grad_norm': 1.0516632795333862, 'learning_rate': 1.136451006264843e-05, 'epoch': 6.852941176470588}
>>> 2025-09-10 23:10:12,512 - INFO - >>> {'loss': 2.2528, 'grad_norm': 0.9796978831291199, 'learning_rate': 1.135578942089925e-05, 'epoch': 6.857142857142857}
>>> 2025-09-10 23:10:15,536 - INFO - >>> {'loss': 2.2274, 'grad_norm': 1.1474460363388062, 'learning_rate': 1.1347067728643283e-05, 'epoch': 6.8613445378151265}
>>> 2025-09-10 23:10:18,936 - INFO - >>> {'loss': 2.2204, 'grad_norm': 1.0352822542190552, 'learning_rate': 1.1338344992638361e-05, 'epoch': 6.865546218487395}
>>> 2025-09-10 23:10:22,491 - INFO - >>> {'loss': 2.2341, 'grad_norm': 1.0485235452651978, 'learning_rate': 1.132962121964313e-05, 'epoch': 6.869747899159664}
>>> 2025-09-10 23:10:26,181 - INFO - >>> {'loss': 2.1972, 'grad_norm': 1.074869155883789, 'learning_rate': 1.1320896416417026e-05, 'epoch': 6.873949579831933}
>>> 2025-09-10 23:10:29,289 - INFO - >>> {'loss': 2.2065, 'grad_norm': 1.063101053237915, 'learning_rate': 1.1312170589720292e-05, 'epoch': 6.878151260504202}
>>> 2025-09-10 23:10:32,360 - INFO - >>> {'loss': 2.1841, 'grad_norm': 1.260254979133606, 'learning_rate': 1.1303443746313969e-05, 'epoch': 6.882352941176471}
>>> 2025-09-10 23:10:36,352 - INFO - >>> {'loss': 2.1904, 'grad_norm': 0.9956414699554443, 'learning_rate': 1.1294715892959878e-05, 'epoch': 6.88655462184874}
>>> 2025-09-10 23:10:39,435 - INFO - >>> {'loss': 2.2138, 'grad_norm': 1.098771095275879, 'learning_rate': 1.1285987036420622e-05, 'epoch': 6.890756302521009}
>>> 2025-09-10 23:10:42,755 - INFO - >>> {'loss': 2.1744, 'grad_norm': 1.1299009323120117, 'learning_rate': 1.1277257183459583e-05, 'epoch': 6.894957983193278}
>>> 2025-09-10 23:10:46,126 - INFO - >>> {'loss': 2.2364, 'grad_norm': 1.0531861782073975, 'learning_rate': 1.1268526340840923e-05, 'epoch': 6.899159663865547}
>>> 2025-09-10 23:10:49,595 - INFO - >>> {'loss': 2.2785, 'grad_norm': 1.1118766069412231, 'learning_rate': 1.1259794515329557e-05, 'epoch': 6.9033613445378155}
>>> 2025-09-10 23:10:53,133 - INFO - >>> {'loss': 2.2229, 'grad_norm': 0.990164577960968, 'learning_rate': 1.1251061713691172e-05, 'epoch': 6.907563025210084}
>>> 2025-09-10 23:10:56,417 - INFO - >>> {'loss': 2.2743, 'grad_norm': 1.1077868938446045, 'learning_rate': 1.1242327942692206e-05, 'epoch': 6.911764705882353}
>>> 2025-09-10 23:11:00,452 - INFO - >>> {'loss': 2.253, 'grad_norm': 0.9748514294624329, 'learning_rate': 1.1233593209099849e-05, 'epoch': 6.915966386554622}
>>> 2025-09-10 23:11:04,291 - INFO - >>> {'loss': 2.2146, 'grad_norm': 1.0432558059692383, 'learning_rate': 1.1224857519682039e-05, 'epoch': 6.920168067226891}
>>> 2025-09-10 23:11:07,637 - INFO - >>> {'loss': 2.1761, 'grad_norm': 1.1883816719055176, 'learning_rate': 1.1216120881207453e-05, 'epoch': 6.92436974789916}
>>> 2025-09-10 23:11:11,221 - INFO - >>> {'loss': 2.2031, 'grad_norm': 1.1286605596542358, 'learning_rate': 1.1207383300445504e-05, 'epoch': 6.928571428571429}
>>> 2025-09-10 23:11:15,176 - INFO - >>> {'loss': 2.3214, 'grad_norm': 0.9841418266296387, 'learning_rate': 1.1198644784166335e-05, 'epoch': 6.932773109243698}
>>> 2025-09-10 23:11:18,873 - INFO - >>> {'loss': 2.3635, 'grad_norm': 1.0532896518707275, 'learning_rate': 1.1189905339140816e-05, 'epoch': 6.936974789915967}
>>> 2025-09-10 23:11:22,465 - INFO - >>> {'loss': 2.1813, 'grad_norm': 0.9713724255561829, 'learning_rate': 1.1181164972140525e-05, 'epoch': 6.9411764705882355}
>>> 2025-09-10 23:11:26,191 - INFO - >>> {'loss': 2.1603, 'grad_norm': 1.087632417678833, 'learning_rate': 1.1172423689937775e-05, 'epoch': 6.945378151260504}
>>> 2025-09-10 23:11:29,480 - INFO - >>> {'loss': 2.1988, 'grad_norm': 1.0514129400253296, 'learning_rate': 1.1163681499305569e-05, 'epoch': 6.949579831932773}
>>> 2025-09-10 23:11:32,699 - INFO - >>> {'loss': 2.1967, 'grad_norm': 1.2688649892807007, 'learning_rate': 1.1154938407017627e-05, 'epoch': 6.953781512605042}
>>> 2025-09-10 23:11:36,455 - INFO - >>> {'loss': 2.3181, 'grad_norm': 1.0691838264465332, 'learning_rate': 1.1146194419848357e-05, 'epoch': 6.957983193277311}
>>> 2025-09-10 23:11:39,525 - INFO - >>> {'loss': 2.1893, 'grad_norm': 1.049312949180603, 'learning_rate': 1.1137449544572867e-05, 'epoch': 6.96218487394958}
>>> 2025-09-10 23:11:43,010 - INFO - >>> {'loss': 2.0772, 'grad_norm': 1.134483814239502, 'learning_rate': 1.1128703787966955e-05, 'epoch': 6.966386554621849}
>>> 2025-09-10 23:11:46,255 - INFO - >>> {'loss': 2.1488, 'grad_norm': 1.095025897026062, 'learning_rate': 1.1119957156807096e-05, 'epoch': 6.970588235294118}
>>> 2025-09-10 23:11:49,299 - INFO - >>> {'loss': 2.1079, 'grad_norm': 1.1685035228729248, 'learning_rate': 1.1111209657870443e-05, 'epoch': 6.974789915966387}
>>> 2025-09-10 23:11:53,037 - INFO - >>> {'loss': 2.192, 'grad_norm': 0.9775247573852539, 'learning_rate': 1.1102461297934828e-05, 'epoch': 6.9789915966386555}
>>> 2025-09-10 23:11:56,136 - INFO - >>> {'loss': 2.2447, 'grad_norm': 1.126837968826294, 'learning_rate': 1.1093712083778748e-05, 'epoch': 6.983193277310924}
>>> 2025-09-10 23:11:59,894 - INFO - >>> {'loss': 2.2514, 'grad_norm': 0.9962408542633057, 'learning_rate': 1.1084962022181347e-05, 'epoch': 6.987394957983193}
>>> 2025-09-10 23:12:03,558 - INFO - >>> {'loss': 2.1214, 'grad_norm': 1.0525323152542114, 'learning_rate': 1.1076211119922453e-05, 'epoch': 6.991596638655462}
>>> 2025-09-10 23:12:07,485 - INFO - >>> {'loss': 2.2685, 'grad_norm': 0.927878201007843, 'learning_rate': 1.1067459383782526e-05, 'epoch': 6.995798319327731}
>>> 2025-09-10 23:12:10,715 - INFO - >>> {'loss': 2.317, 'grad_norm': 1.074195146560669, 'learning_rate': 1.105870682054267e-05, 'epoch': 7.0}
>>> 2025-09-10 23:12:13,791 - INFO - >>> {'loss': 2.1666, 'grad_norm': 1.1209763288497925, 'learning_rate': 1.1049953436984645e-05, 'epoch': 7.004201680672269}
>>> 2025-09-10 23:12:17,870 - INFO - >>> {'loss': 2.3507, 'grad_norm': 1.0213719606399536, 'learning_rate': 1.1041199239890834e-05, 'epoch': 7.008403361344538}
>>> 2025-09-10 23:12:20,751 - INFO - >>> {'loss': 2.2146, 'grad_norm': 1.1062664985656738, 'learning_rate': 1.1032444236044257e-05, 'epoch': 7.012605042016807}
>>> 2025-09-10 23:12:24,455 - INFO - >>> {'loss': 2.2695, 'grad_norm': 1.0864297151565552, 'learning_rate': 1.1023688432228554e-05, 'epoch': 7.016806722689076}
>>> 2025-09-10 23:12:28,230 - INFO - >>> {'loss': 2.0541, 'grad_norm': 0.9931547045707703, 'learning_rate': 1.1014931835227988e-05, 'epoch': 7.0210084033613445}
>>> 2025-09-10 23:12:32,339 - INFO - >>> {'loss': 2.2482, 'grad_norm': 0.9765382409095764, 'learning_rate': 1.1006174451827439e-05, 'epoch': 7.025210084033613}
>>> 2025-09-10 23:12:35,715 - INFO - >>> {'loss': 2.1736, 'grad_norm': 0.99016273021698, 'learning_rate': 1.099741628881239e-05, 'epoch': 7.029411764705882}
>>> 2025-09-10 23:12:39,619 - INFO - >>> {'loss': 2.2653, 'grad_norm': 1.1153228282928467, 'learning_rate': 1.0988657352968932e-05, 'epoch': 7.033613445378151}
>>> 2025-09-10 23:12:43,250 - INFO - >>> {'loss': 2.3213, 'grad_norm': 1.0885279178619385, 'learning_rate': 1.0979897651083757e-05, 'epoch': 7.03781512605042}
>>> 2025-09-10 23:12:46,929 - INFO - >>> {'loss': 2.1397, 'grad_norm': 1.0327544212341309, 'learning_rate': 1.0971137189944147e-05, 'epoch': 7.042016806722689}
>>> 2025-09-10 23:12:50,471 - INFO - >>> {'loss': 2.2616, 'grad_norm': 1.2119824886322021, 'learning_rate': 1.0962375976337967e-05, 'epoch': 7.046218487394958}
>>> 2025-09-10 23:12:54,493 - INFO - >>> {'loss': 2.2089, 'grad_norm': 0.9776836633682251, 'learning_rate': 1.0953614017053682e-05, 'epoch': 7.050420168067227}
>>> 2025-09-10 23:12:58,222 - INFO - >>> {'loss': 2.2131, 'grad_norm': 1.0690836906433105, 'learning_rate': 1.0944851318880314e-05, 'epoch': 7.054621848739496}
>>> 2025-09-10 23:13:02,021 - INFO - >>> {'loss': 2.2637, 'grad_norm': 1.1359105110168457, 'learning_rate': 1.0936087888607474e-05, 'epoch': 7.0588235294117645}
>>> 2025-09-10 23:13:05,853 - INFO - >>> {'loss': 2.1618, 'grad_norm': 1.010910153388977, 'learning_rate': 1.0927323733025327e-05, 'epoch': 7.063025210084033}
>>> 2025-09-10 23:13:09,046 - INFO - >>> {'loss': 2.1658, 'grad_norm': 1.1724766492843628, 'learning_rate': 1.0918558858924615e-05, 'epoch': 7.067226890756302}
>>> 2025-09-10 23:13:12,808 - INFO - >>> {'loss': 2.1738, 'grad_norm': 1.0521180629730225, 'learning_rate': 1.0909793273096625e-05, 'epoch': 7.071428571428571}
>>> 2025-09-10 23:13:15,811 - INFO - >>> {'loss': 2.1279, 'grad_norm': 1.1627148389816284, 'learning_rate': 1.0901026982333198e-05, 'epoch': 7.07563025210084}
>>> 2025-09-10 23:13:19,163 - INFO - >>> {'loss': 2.1983, 'grad_norm': 1.0957293510437012, 'learning_rate': 1.089225999342672e-05, 'epoch': 7.079831932773109}
>>> 2025-09-10 23:13:23,163 - INFO - >>> {'loss': 2.1534, 'grad_norm': 1.0442103147506714, 'learning_rate': 1.0883492313170125e-05, 'epoch': 7.084033613445378}
>>> 2025-09-10 23:13:26,455 - INFO - >>> {'loss': 2.2138, 'grad_norm': 1.0187045335769653, 'learning_rate': 1.0874723948356876e-05, 'epoch': 7.088235294117647}
>>> 2025-09-10 23:13:29,625 - INFO - >>> {'loss': 2.3049, 'grad_norm': 1.0899473428726196, 'learning_rate': 1.0865954905780962e-05, 'epoch': 7.092436974789916}
>>> 2025-09-10 23:13:32,604 - INFO - >>> {'loss': 2.2147, 'grad_norm': 1.1707260608673096, 'learning_rate': 1.0857185192236915e-05, 'epoch': 7.0966386554621845}
>>> 2025-09-10 23:13:35,783 - INFO - >>> {'loss': 2.2038, 'grad_norm': 1.134077787399292, 'learning_rate': 1.0848414814519765e-05, 'epoch': 7.100840336134453}
>>> 2025-09-10 23:13:38,944 - INFO - >>> {'loss': 2.2841, 'grad_norm': 1.1590620279312134, 'learning_rate': 1.0839643779425073e-05, 'epoch': 7.105042016806722}
>>> 2025-09-10 23:13:42,780 - INFO - >>> {'loss': 2.1516, 'grad_norm': 1.1050671339035034, 'learning_rate': 1.0830872093748898e-05, 'epoch': 7.109243697478991}
>>> 2025-09-10 23:13:46,221 - INFO - >>> {'loss': 2.2695, 'grad_norm': 1.0373727083206177, 'learning_rate': 1.0822099764287813e-05, 'epoch': 7.11344537815126}
>>> 2025-09-10 23:13:50,055 - INFO - >>> {'loss': 2.1608, 'grad_norm': 0.9734213948249817, 'learning_rate': 1.0813326797838886e-05, 'epoch': 7.117647058823529}
>>> 2025-09-10 23:13:53,338 - INFO - >>> {'loss': 2.2228, 'grad_norm': 1.1287765502929688, 'learning_rate': 1.0804553201199671e-05, 'epoch': 7.121848739495798}
>>> 2025-09-10 23:13:56,254 - INFO - >>> {'loss': 2.2167, 'grad_norm': 1.1130386590957642, 'learning_rate': 1.079577898116822e-05, 'epoch': 7.126050420168067}
>>> 2025-09-10 23:13:59,512 - INFO - >>> {'loss': 2.1302, 'grad_norm': 1.0411452054977417, 'learning_rate': 1.0787004144543065e-05, 'epoch': 7.130252100840337}
>>> 2025-09-10 23:14:03,573 - INFO - >>> {'loss': 2.2405, 'grad_norm': 0.9742599725723267, 'learning_rate': 1.0778228698123221e-05, 'epoch': 7.1344537815126055}
>>> 2025-09-10 23:14:07,351 - INFO - >>> {'loss': 2.1717, 'grad_norm': 0.9841451644897461, 'learning_rate': 1.0769452648708161e-05, 'epoch': 7.138655462184874}
>>> 2025-09-10 23:14:10,402 - INFO - >>> {'loss': 2.2097, 'grad_norm': 1.125022292137146, 'learning_rate': 1.0760676003097841e-05, 'epoch': 7.142857142857143}
>>> 2025-09-10 23:14:14,829 - INFO - >>> {'loss': 2.1873, 'grad_norm': 1.0369597673416138, 'learning_rate': 1.075189876809267e-05, 'epoch': 7.147058823529412}
>>> 2025-09-10 23:14:17,845 - INFO - >>> {'loss': 2.2906, 'grad_norm': 1.25685453414917, 'learning_rate': 1.0743120950493516e-05, 'epoch': 7.151260504201681}
>>> 2025-09-10 23:14:21,384 - INFO - >>> {'loss': 2.2201, 'grad_norm': 1.0462636947631836, 'learning_rate': 1.0734342557101697e-05, 'epoch': 7.15546218487395}
>>> 2025-09-10 23:14:24,834 - INFO - >>> {'loss': 2.0492, 'grad_norm': 1.1259770393371582, 'learning_rate': 1.0725563594718985e-05, 'epoch': 7.159663865546219}
>>> 2025-09-10 23:14:28,342 - INFO - >>> {'loss': 2.1885, 'grad_norm': 1.0779458284378052, 'learning_rate': 1.0716784070147579e-05, 'epoch': 7.163865546218488}
>>> 2025-09-10 23:14:31,310 - INFO - >>> {'loss': 2.2305, 'grad_norm': 1.13417649269104, 'learning_rate': 1.0708003990190123e-05, 'epoch': 7.168067226890757}
>>> 2025-09-10 23:14:34,720 - INFO - >>> {'loss': 2.203, 'grad_norm': 1.073452353477478, 'learning_rate': 1.0699223361649692e-05, 'epoch': 7.1722689075630255}
>>> 2025-09-10 23:14:38,377 - INFO - >>> {'loss': 2.2346, 'grad_norm': 0.9881401658058167, 'learning_rate': 1.069044219132978e-05, 'epoch': 7.176470588235294}
>>> 2025-09-10 23:14:42,361 - INFO - >>> {'loss': 2.3456, 'grad_norm': 1.0007787942886353, 'learning_rate': 1.0681660486034309e-05, 'epoch': 7.180672268907563}
>>> 2025-09-10 23:14:46,035 - INFO - >>> {'loss': 2.2453, 'grad_norm': 1.194273829460144, 'learning_rate': 1.0672878252567604e-05, 'epoch': 7.184873949579832}
>>> 2025-09-10 23:14:49,762 - INFO - >>> {'loss': 2.3252, 'grad_norm': 1.0103883743286133, 'learning_rate': 1.0664095497734412e-05, 'epoch': 7.189075630252101}
>>> 2025-09-10 23:14:53,137 - INFO - >>> {'loss': 2.2644, 'grad_norm': 1.0478856563568115, 'learning_rate': 1.0655312228339876e-05, 'epoch': 7.19327731092437}
>>> 2025-09-10 23:14:56,079 - INFO - >>> {'loss': 2.2336, 'grad_norm': 1.0590764284133911, 'learning_rate': 1.0646528451189537e-05, 'epoch': 7.197478991596639}
>>> 2025-09-10 23:14:59,558 - INFO - >>> {'loss': 2.1567, 'grad_norm': 1.0642473697662354, 'learning_rate': 1.0637744173089336e-05, 'epoch': 7.201680672268908}
>>> 2025-09-10 23:15:03,056 - INFO - >>> {'loss': 2.1879, 'grad_norm': 1.0591976642608643, 'learning_rate': 1.0628959400845598e-05, 'epoch': 7.205882352941177}
>>> 2025-09-10 23:15:06,733 - INFO - >>> {'loss': 2.1687, 'grad_norm': 1.174587607383728, 'learning_rate': 1.0620174141265027e-05, 'epoch': 7.2100840336134455}
>>> 2025-09-10 23:15:09,902 - INFO - >>> {'loss': 2.2505, 'grad_norm': 1.1159430742263794, 'learning_rate': 1.0611388401154713e-05, 'epoch': 7.214285714285714}
>>> 2025-09-10 23:15:13,377 - INFO - >>> {'loss': 2.1931, 'grad_norm': 1.2133593559265137, 'learning_rate': 1.060260218732211e-05, 'epoch': 7.218487394957983}
>>> 2025-09-10 23:15:16,882 - INFO - >>> {'loss': 2.2312, 'grad_norm': 1.0578192472457886, 'learning_rate': 1.0593815506575048e-05, 'epoch': 7.222689075630252}
>>> 2025-09-10 23:15:20,849 - INFO - >>> {'loss': 2.205, 'grad_norm': 1.0385355949401855, 'learning_rate': 1.0585028365721714e-05, 'epoch': 7.226890756302521}
>>> 2025-09-10 23:15:23,850 - INFO - >>> {'loss': 2.1761, 'grad_norm': 1.2481316328048706, 'learning_rate': 1.0576240771570645e-05, 'epoch': 7.23109243697479}
>>> 2025-09-10 23:15:27,269 - INFO - >>> {'loss': 2.2424, 'grad_norm': 1.0519400835037231, 'learning_rate': 1.0567452730930743e-05, 'epoch': 7.235294117647059}
>>> 2025-09-10 23:15:30,124 - INFO - >>> {'loss': 2.1781, 'grad_norm': 1.1136342287063599, 'learning_rate': 1.0558664250611241e-05, 'epoch': 7.239495798319328}
>>> 2025-09-10 23:15:33,266 - INFO - >>> {'loss': 2.1726, 'grad_norm': 1.1078596115112305, 'learning_rate': 1.0549875337421726e-05, 'epoch': 7.243697478991597}
>>> 2025-09-10 23:15:36,559 - INFO - >>> {'loss': 2.1923, 'grad_norm': 1.123030662536621, 'learning_rate': 1.0541085998172116e-05, 'epoch': 7.2478991596638656}
>>> 2025-09-10 23:15:40,317 - INFO - >>> {'loss': 2.2207, 'grad_norm': 1.0942585468292236, 'learning_rate': 1.053229623967265e-05, 'epoch': 7.2521008403361344}
>>> 2025-09-10 23:15:43,304 - INFO - >>> {'loss': 2.2361, 'grad_norm': 1.1647841930389404, 'learning_rate': 1.0523506068733906e-05, 'epoch': 7.256302521008403}
>>> 2025-09-10 23:15:46,694 - INFO - >>> {'loss': 2.0447, 'grad_norm': 1.0930147171020508, 'learning_rate': 1.0514715492166777e-05, 'epoch': 7.260504201680672}
>>> 2025-09-10 23:15:50,180 - INFO - >>> {'loss': 2.1454, 'grad_norm': 1.0763013362884521, 'learning_rate': 1.050592451678246e-05, 'epoch': 7.264705882352941}
>>> 2025-09-10 23:15:54,311 - INFO - >>> {'loss': 2.1492, 'grad_norm': 1.0321886539459229, 'learning_rate': 1.0497133149392475e-05, 'epoch': 7.26890756302521}
>>> 2025-09-10 23:15:58,363 - INFO - >>> {'loss': 2.2974, 'grad_norm': 1.0657469034194946, 'learning_rate': 1.0488341396808645e-05, 'epoch': 7.273109243697479}
>>> 2025-09-10 23:16:02,054 - INFO - >>> {'loss': 2.2488, 'grad_norm': 1.0905892848968506, 'learning_rate': 1.0479549265843074e-05, 'epoch': 7.277310924369748}
>>> 2025-09-10 23:16:05,678 - INFO - >>> {'loss': 2.2265, 'grad_norm': 1.021427869796753, 'learning_rate': 1.0470756763308187e-05, 'epoch': 7.281512605042017}
>>> 2025-09-10 23:16:08,970 - INFO - >>> {'loss': 2.204, 'grad_norm': 1.0577354431152344, 'learning_rate': 1.0461963896016669e-05, 'epoch': 7.285714285714286}
>>> 2025-09-10 23:16:12,439 - INFO - >>> {'loss': 2.1386, 'grad_norm': 1.0032342672348022, 'learning_rate': 1.0453170670781508e-05, 'epoch': 7.2899159663865545}
>>> 2025-09-10 23:16:15,876 - INFO - >>> {'loss': 2.1322, 'grad_norm': 1.2428112030029297, 'learning_rate': 1.044437709441596e-05, 'epoch': 7.294117647058823}
>>> 2025-09-10 23:16:19,494 - INFO - >>> {'loss': 2.1088, 'grad_norm': 0.958191454410553, 'learning_rate': 1.0435583173733553e-05, 'epoch': 7.298319327731092}
>>> 2025-09-10 23:16:22,930 - INFO - >>> {'loss': 2.1616, 'grad_norm': 1.0174967050552368, 'learning_rate': 1.0426788915548082e-05, 'epoch': 7.302521008403361}
>>> 2025-09-10 23:16:26,061 - INFO - >>> {'loss': 2.192, 'grad_norm': 1.1114692687988281, 'learning_rate': 1.041799432667361e-05, 'epoch': 7.30672268907563}
>>> 2025-09-10 23:16:29,770 - INFO - >>> {'loss': 2.1557, 'grad_norm': 1.1682848930358887, 'learning_rate': 1.0409199413924451e-05, 'epoch': 7.310924369747899}
>>> 2025-09-10 23:16:32,615 - INFO - >>> {'loss': 2.231, 'grad_norm': 1.1500589847564697, 'learning_rate': 1.0400404184115166e-05, 'epoch': 7.315126050420168}
>>> 2025-09-10 23:16:35,850 - INFO - >>> {'loss': 2.177, 'grad_norm': 1.0713824033737183, 'learning_rate': 1.039160864406057e-05, 'epoch': 7.319327731092437}
>>> 2025-09-10 23:16:39,067 - INFO - >>> {'loss': 2.1575, 'grad_norm': 1.0846105813980103, 'learning_rate': 1.0382812800575714e-05, 'epoch': 7.323529411764706}
>>> 2025-09-10 23:16:42,345 - INFO - >>> {'loss': 2.1926, 'grad_norm': 1.1981641054153442, 'learning_rate': 1.037401666047588e-05, 'epoch': 7.3277310924369745}
>>> 2025-09-10 23:16:46,336 - INFO - >>> {'loss': 2.1962, 'grad_norm': 1.1927623748779297, 'learning_rate': 1.0365220230576592e-05, 'epoch': 7.331932773109243}
>>> 2025-09-10 23:16:50,147 - INFO - >>> {'loss': 2.1834, 'grad_norm': 1.0655730962753296, 'learning_rate': 1.0356423517693582e-05, 'epoch': 7.336134453781512}
>>> 2025-09-10 23:16:53,205 - INFO - >>> {'loss': 2.2551, 'grad_norm': 1.0437873601913452, 'learning_rate': 1.0347626528642815e-05, 'epoch': 7.340336134453781}
>>> 2025-09-10 23:16:57,126 - INFO - >>> {'loss': 2.1743, 'grad_norm': 1.089012622833252, 'learning_rate': 1.0338829270240467e-05, 'epoch': 7.34453781512605}
>>> 2025-09-10 23:17:01,139 - INFO - >>> {'loss': 2.3493, 'grad_norm': 1.044110655784607, 'learning_rate': 1.0330031749302915e-05, 'epoch': 7.348739495798319}
>>> 2025-09-10 23:17:04,999 - INFO - >>> {'loss': 2.3127, 'grad_norm': 1.0933340787887573, 'learning_rate': 1.032123397264675e-05, 'epoch': 7.352941176470588}
>>> 2025-09-10 23:17:08,474 - INFO - >>> {'loss': 2.1454, 'grad_norm': 1.1109789609909058, 'learning_rate': 1.0312435947088755e-05, 'epoch': 7.357142857142857}
>>> 2025-09-10 23:17:11,979 - INFO - >>> {'loss': 2.2509, 'grad_norm': 1.0841267108917236, 'learning_rate': 1.0303637679445905e-05, 'epoch': 7.361344537815126}
>>> 2025-09-10 23:17:15,245 - INFO - >>> {'loss': 2.1562, 'grad_norm': 1.1063237190246582, 'learning_rate': 1.0294839176535371e-05, 'epoch': 7.3655462184873945}
>>> 2025-09-10 23:17:18,913 - INFO - >>> {'loss': 2.2471, 'grad_norm': 1.096540927886963, 'learning_rate': 1.0286040445174495e-05, 'epoch': 7.369747899159664}
>>> 2025-09-10 23:17:22,172 - INFO - >>> {'loss': 2.2403, 'grad_norm': 1.0912691354751587, 'learning_rate': 1.0277241492180799e-05, 'epoch': 7.373949579831933}
>>> 2025-09-10 23:17:25,712 - INFO - >>> {'loss': 2.3256, 'grad_norm': 0.9844842553138733, 'learning_rate': 1.0268442324371985e-05, 'epoch': 7.378151260504202}
>>> 2025-09-10 23:17:29,434 - INFO - >>> {'loss': 2.1429, 'grad_norm': 1.1017013788223267, 'learning_rate': 1.0259642948565906e-05, 'epoch': 7.382352941176471}
>>> 2025-09-10 23:17:32,884 - INFO - >>> {'loss': 2.1904, 'grad_norm': 1.0701457262039185, 'learning_rate': 1.0250843371580599e-05, 'epoch': 7.38655462184874}
>>> 2025-09-10 23:17:36,535 - INFO - >>> {'loss': 2.1989, 'grad_norm': 1.056045413017273, 'learning_rate': 1.024204360023423e-05, 'epoch': 7.390756302521009}
>>> 2025-09-10 23:17:39,747 - INFO - >>> {'loss': 2.2655, 'grad_norm': 1.094129204750061, 'learning_rate': 1.0233243641345136e-05, 'epoch': 7.394957983193278}
>>> 2025-09-10 23:17:43,430 - INFO - >>> {'loss': 2.2143, 'grad_norm': 1.0650670528411865, 'learning_rate': 1.0224443501731792e-05, 'epoch': 7.399159663865547}
>>> 2025-09-10 23:17:47,524 - INFO - >>> {'loss': 2.3148, 'grad_norm': 1.0032680034637451, 'learning_rate': 1.021564318821281e-05, 'epoch': 7.4033613445378155}
>>> 2025-09-10 23:17:50,833 - INFO - >>> {'loss': 2.2298, 'grad_norm': 1.2297873497009277, 'learning_rate': 1.0206842707606943e-05, 'epoch': 7.407563025210084}
>>> 2025-09-10 23:17:54,812 - INFO - >>> {'loss': 2.2486, 'grad_norm': 1.1989731788635254, 'learning_rate': 1.019804206673307e-05, 'epoch': 7.411764705882353}
>>> 2025-09-10 23:17:58,676 - INFO - >>> {'loss': 2.2198, 'grad_norm': 1.0678855180740356, 'learning_rate': 1.0189241272410191e-05, 'epoch': 7.415966386554622}
>>> 2025-09-10 23:18:02,452 - INFO - >>> {'loss': 2.1597, 'grad_norm': 1.0308672189712524, 'learning_rate': 1.0180440331457432e-05, 'epoch': 7.420168067226891}
>>> 2025-09-10 23:18:06,398 - INFO - >>> {'loss': 2.1078, 'grad_norm': 1.1056320667266846, 'learning_rate': 1.0171639250694032e-05, 'epoch': 7.42436974789916}
>>> 2025-09-10 23:18:09,809 - INFO - >>> {'loss': 2.2183, 'grad_norm': 1.1228901147842407, 'learning_rate': 1.0162838036939328e-05, 'epoch': 7.428571428571429}
>>> 2025-09-10 23:18:13,541 - INFO - >>> {'loss': 2.1454, 'grad_norm': 1.108250379562378, 'learning_rate': 1.0154036697012775e-05, 'epoch': 7.432773109243698}
>>> 2025-09-10 23:18:17,160 - INFO - >>> {'loss': 2.2771, 'grad_norm': 1.189609408378601, 'learning_rate': 1.0145235237733913e-05, 'epoch': 7.436974789915967}
>>> 2025-09-10 23:18:20,415 - INFO - >>> {'loss': 2.2131, 'grad_norm': 1.0656312704086304, 'learning_rate': 1.013643366592238e-05, 'epoch': 7.4411764705882355}
>>> 2025-09-10 23:18:23,442 - INFO - >>> {'loss': 2.1017, 'grad_norm': 1.3801863193511963, 'learning_rate': 1.0127631988397905e-05, 'epoch': 7.445378151260504}
>>> 2025-09-10 23:18:27,015 - INFO - >>> {'loss': 2.1348, 'grad_norm': 0.9786957502365112, 'learning_rate': 1.011883021198029e-05, 'epoch': 7.449579831932773}
>>> 2025-09-10 23:18:30,768 - INFO - >>> {'loss': 2.1624, 'grad_norm': 1.0206960439682007, 'learning_rate': 1.0110028343489424e-05, 'epoch': 7.453781512605042}
>>> 2025-09-10 23:18:33,702 - INFO - >>> {'loss': 2.0766, 'grad_norm': 1.2634317874908447, 'learning_rate': 1.0101226389745255e-05, 'epoch': 7.457983193277311}
>>> 2025-09-10 23:18:37,595 - INFO - >>> {'loss': 2.1147, 'grad_norm': 1.0559879541397095, 'learning_rate': 1.0092424357567809e-05, 'epoch': 7.46218487394958}
>>> 2025-09-10 23:18:41,413 - INFO - >>> {'loss': 2.2823, 'grad_norm': 1.0138330459594727, 'learning_rate': 1.0083622253777166e-05, 'epoch': 7.466386554621849}
>>> 2025-09-10 23:18:45,056 - INFO - >>> {'loss': 2.2478, 'grad_norm': 1.0355621576309204, 'learning_rate': 1.0074820085193464e-05, 'epoch': 7.470588235294118}
>>> 2025-09-10 23:18:48,053 - INFO - >>> {'loss': 2.1898, 'grad_norm': 1.093731164932251, 'learning_rate': 1.0066017858636887e-05, 'epoch': 7.474789915966387}
>>> 2025-09-10 23:18:51,313 - INFO - >>> {'loss': 2.1197, 'grad_norm': 1.0899114608764648, 'learning_rate': 1.0057215580927672e-05, 'epoch': 7.4789915966386555}
>>> 2025-09-10 23:18:55,071 - INFO - >>> {'loss': 2.2006, 'grad_norm': 1.1168265342712402, 'learning_rate': 1.0048413258886089e-05, 'epoch': 7.483193277310924}
>>> 2025-09-10 23:18:58,806 - INFO - >>> {'loss': 2.1681, 'grad_norm': 1.0369782447814941, 'learning_rate': 1.0039610899332441e-05, 'epoch': 7.487394957983193}
>>> 2025-09-10 23:19:02,423 - INFO - >>> {'loss': 2.1809, 'grad_norm': 1.0449575185775757, 'learning_rate': 1.0030808509087072e-05, 'epoch': 7.491596638655462}
>>> 2025-09-10 23:19:05,476 - INFO - >>> {'loss': 2.2181, 'grad_norm': 1.044048547744751, 'learning_rate': 1.0022006094970326e-05, 'epoch': 7.495798319327731}
>>> 2025-09-10 23:19:08,500 - INFO - >>> {'loss': 2.1429, 'grad_norm': 1.1765557527542114, 'learning_rate': 1.0013203663802598e-05, 'epoch': 7.5}
>>> 2025-09-10 23:19:12,420 - INFO - >>> {'loss': 2.3116, 'grad_norm': 1.06863534450531, 'learning_rate': 1.0004401222404267e-05, 'epoch': 7.504201680672269}
>>> 2025-09-10 23:19:15,610 - INFO - >>> {'loss': 2.1375, 'grad_norm': 1.054233193397522, 'learning_rate': 9.995598777595733e-06, 'epoch': 7.508403361344538}
>>> 2025-09-10 23:19:19,586 - INFO - >>> {'loss': 2.2594, 'grad_norm': 1.0199079513549805, 'learning_rate': 9.986796336197405e-06, 'epoch': 7.512605042016807}
>>> 2025-09-10 23:19:22,804 - INFO - >>> {'loss': 2.2508, 'grad_norm': 1.1293402910232544, 'learning_rate': 9.977993905029675e-06, 'epoch': 7.516806722689076}
>>> 2025-09-10 23:19:26,190 - INFO - >>> {'loss': 2.255, 'grad_norm': 1.0226712226867676, 'learning_rate': 9.969191490912935e-06, 'epoch': 7.5210084033613445}
>>> 2025-09-10 23:19:29,723 - INFO - >>> {'loss': 2.1967, 'grad_norm': 1.2045657634735107, 'learning_rate': 9.960389100667559e-06, 'epoch': 7.525210084033613}
>>> 2025-09-10 23:19:33,687 - INFO - >>> {'loss': 2.2353, 'grad_norm': 1.0123486518859863, 'learning_rate': 9.951586741113915e-06, 'epoch': 7.529411764705882}
>>> 2025-09-10 23:19:36,717 - INFO - >>> {'loss': 2.2607, 'grad_norm': 1.1813684701919556, 'learning_rate': 9.94278441907233e-06, 'epoch': 7.533613445378151}
>>> 2025-09-10 23:19:39,556 - INFO - >>> {'loss': 2.2363, 'grad_norm': 1.252716302871704, 'learning_rate': 9.933982141363117e-06, 'epoch': 7.53781512605042}
>>> 2025-09-10 23:19:43,089 - INFO - >>> {'loss': 2.1808, 'grad_norm': 1.0862947702407837, 'learning_rate': 9.92517991480654e-06, 'epoch': 7.542016806722689}
>>> 2025-09-10 23:19:46,143 - INFO - >>> {'loss': 2.2451, 'grad_norm': 1.033023476600647, 'learning_rate': 9.916377746222836e-06, 'epoch': 7.546218487394958}
>>> 2025-09-10 23:19:50,182 - INFO - >>> {'loss': 2.1732, 'grad_norm': 1.028817057609558, 'learning_rate': 9.907575642432193e-06, 'epoch': 7.550420168067227}
>>> 2025-09-10 23:19:54,042 - INFO - >>> {'loss': 2.3064, 'grad_norm': 1.136533260345459, 'learning_rate': 9.898773610254747e-06, 'epoch': 7.554621848739496}
>>> 2025-09-10 23:19:58,049 - INFO - >>> {'loss': 2.1464, 'grad_norm': 1.069567322731018, 'learning_rate': 9.889971656510581e-06, 'epoch': 7.5588235294117645}
>>> 2025-09-10 23:20:01,286 - INFO - >>> {'loss': 2.3315, 'grad_norm': 1.1624374389648438, 'learning_rate': 9.88116978801971e-06, 'epoch': 7.563025210084033}
>>> 2025-09-10 23:20:05,124 - INFO - >>> {'loss': 2.2708, 'grad_norm': 1.0119903087615967, 'learning_rate': 9.872368011602096e-06, 'epoch': 7.567226890756302}
>>> 2025-09-10 23:20:08,410 - INFO - >>> {'loss': 2.1403, 'grad_norm': 1.0691232681274414, 'learning_rate': 9.863566334077623e-06, 'epoch': 7.571428571428571}
>>> 2025-09-10 23:20:12,069 - INFO - >>> {'loss': 2.179, 'grad_norm': 1.0118416547775269, 'learning_rate': 9.85476476226609e-06, 'epoch': 7.57563025210084}
>>> 2025-09-10 23:20:15,193 - INFO - >>> {'loss': 2.3069, 'grad_norm': 1.0844141244888306, 'learning_rate': 9.84596330298723e-06, 'epoch': 7.579831932773109}
>>> 2025-09-10 23:20:18,866 - INFO - >>> {'loss': 2.0785, 'grad_norm': 1.1398597955703735, 'learning_rate': 9.837161963060674e-06, 'epoch': 7.584033613445378}
>>> 2025-09-10 23:20:22,990 - INFO - >>> {'loss': 2.225, 'grad_norm': 0.9155454039573669, 'learning_rate': 9.82836074930597e-06, 'epoch': 7.588235294117647}
>>> 2025-09-10 23:20:26,807 - INFO - >>> {'loss': 2.1811, 'grad_norm': 1.0836573839187622, 'learning_rate': 9.81955966854257e-06, 'epoch': 7.592436974789916}
>>> 2025-09-10 23:20:30,330 - INFO - >>> {'loss': 2.2298, 'grad_norm': 1.1340515613555908, 'learning_rate': 9.810758727589814e-06, 'epoch': 7.5966386554621845}
>>> 2025-09-10 23:20:33,226 - INFO - >>> {'loss': 2.1552, 'grad_norm': 1.1935937404632568, 'learning_rate': 9.801957933266933e-06, 'epoch': 7.600840336134453}
>>> 2025-09-10 23:20:36,644 - INFO - >>> {'loss': 2.2937, 'grad_norm': 1.043378233909607, 'learning_rate': 9.79315729239306e-06, 'epoch': 7.605042016806722}
>>> 2025-09-10 23:20:39,801 - INFO - >>> {'loss': 2.2515, 'grad_norm': 1.1676768064498901, 'learning_rate': 9.784356811787193e-06, 'epoch': 7.609243697478991}
>>> 2025-09-10 23:20:43,273 - INFO - >>> {'loss': 2.1511, 'grad_norm': 1.1287707090377808, 'learning_rate': 9.775556498268213e-06, 'epoch': 7.61344537815126}
>>> 2025-09-10 23:20:46,904 - INFO - >>> {'loss': 2.1419, 'grad_norm': 1.0587021112442017, 'learning_rate': 9.766756358654869e-06, 'epoch': 7.617647058823529}
>>> 2025-09-10 23:20:50,186 - INFO - >>> {'loss': 2.2, 'grad_norm': 1.1235395669937134, 'learning_rate': 9.757956399765771e-06, 'epoch': 7.621848739495798}
>>> 2025-09-10 23:20:53,842 - INFO - >>> {'loss': 2.0682, 'grad_norm': 1.125850796699524, 'learning_rate': 9.749156628419405e-06, 'epoch': 7.626050420168067}
>>> 2025-09-10 23:20:57,265 - INFO - >>> {'loss': 2.1882, 'grad_norm': 1.004298448562622, 'learning_rate': 9.740357051434097e-06, 'epoch': 7.630252100840336}
>>> 2025-09-10 23:21:00,906 - INFO - >>> {'loss': 2.1164, 'grad_norm': 1.0846850872039795, 'learning_rate': 9.73155767562802e-06, 'epoch': 7.634453781512605}
>>> 2025-09-10 23:21:04,448 - INFO - >>> {'loss': 2.1584, 'grad_norm': 1.1262139081954956, 'learning_rate': 9.722758507819203e-06, 'epoch': 7.6386554621848735}
>>> 2025-09-10 23:21:07,784 - INFO - >>> {'loss': 2.2523, 'grad_norm': 1.163359522819519, 'learning_rate': 9.71395955482551e-06, 'epoch': 7.642857142857143}
>>> 2025-09-10 23:21:11,596 - INFO - >>> {'loss': 2.1706, 'grad_norm': 1.0102959871292114, 'learning_rate': 9.70516082346463e-06, 'epoch': 7.647058823529412}
>>> 2025-09-10 23:21:15,586 - INFO - >>> {'loss': 2.1179, 'grad_norm': 1.0747501850128174, 'learning_rate': 9.696362320554096e-06, 'epoch': 7.651260504201681}
>>> 2025-09-10 23:21:19,382 - INFO - >>> {'loss': 2.1907, 'grad_norm': 1.0883408784866333, 'learning_rate': 9.687564052911248e-06, 'epoch': 7.65546218487395}
>>> 2025-09-10 23:21:22,636 - INFO - >>> {'loss': 2.2012, 'grad_norm': 1.165938138961792, 'learning_rate': 9.67876602735325e-06, 'epoch': 7.659663865546219}
>>> 2025-09-10 23:21:26,341 - INFO - >>> {'loss': 2.3197, 'grad_norm': 1.0875141620635986, 'learning_rate': 9.669968250697086e-06, 'epoch': 7.663865546218488}
>>> 2025-09-10 23:21:29,330 - INFO - >>> {'loss': 2.2183, 'grad_norm': 1.153808355331421, 'learning_rate': 9.661170729759538e-06, 'epoch': 7.668067226890757}
>>> 2025-09-10 23:21:32,618 - INFO - >>> {'loss': 2.1972, 'grad_norm': 1.1053962707519531, 'learning_rate': 9.65237347135719e-06, 'epoch': 7.6722689075630255}
>>> 2025-09-10 23:21:36,697 - INFO - >>> {'loss': 2.2582, 'grad_norm': 1.0510077476501465, 'learning_rate': 9.64357648230642e-06, 'epoch': 7.676470588235294}
>>> 2025-09-10 23:21:40,109 - INFO - >>> {'loss': 2.3314, 'grad_norm': 1.1300454139709473, 'learning_rate': 9.634779769423412e-06, 'epoch': 7.680672268907563}
>>> 2025-09-10 23:21:43,641 - INFO - >>> {'loss': 2.2494, 'grad_norm': 1.1764386892318726, 'learning_rate': 9.625983339524122e-06, 'epoch': 7.684873949579832}
>>> 2025-09-10 23:21:46,634 - INFO - >>> {'loss': 2.2282, 'grad_norm': 1.056920051574707, 'learning_rate': 9.617187199424291e-06, 'epoch': 7.689075630252101}
>>> 2025-09-10 23:21:50,169 - INFO - >>> {'loss': 2.1631, 'grad_norm': 1.0109379291534424, 'learning_rate': 9.608391355939434e-06, 'epoch': 7.69327731092437}
>>> 2025-09-10 23:21:53,491 - INFO - >>> {'loss': 2.1845, 'grad_norm': 1.069745421409607, 'learning_rate': 9.599595815884835e-06, 'epoch': 7.697478991596639}
>>> 2025-09-10 23:21:56,512 - INFO - >>> {'loss': 2.2063, 'grad_norm': 1.144346833229065, 'learning_rate': 9.590800586075552e-06, 'epoch': 7.701680672268908}
>>> 2025-09-10 23:21:59,799 - INFO - >>> {'loss': 2.1706, 'grad_norm': 1.1554123163223267, 'learning_rate': 9.582005673326391e-06, 'epoch': 7.705882352941177}
>>> 2025-09-10 23:22:03,259 - INFO - >>> {'loss': 2.251, 'grad_norm': 1.217670202255249, 'learning_rate': 9.573211084451921e-06, 'epoch': 7.7100840336134455}
>>> 2025-09-10 23:22:06,210 - INFO - >>> {'loss': 2.1688, 'grad_norm': 1.2288049459457397, 'learning_rate': 9.56441682626645e-06, 'epoch': 7.714285714285714}
>>> 2025-09-10 23:22:09,805 - INFO - >>> {'loss': 2.1481, 'grad_norm': 1.0582869052886963, 'learning_rate': 9.555622905584043e-06, 'epoch': 7.718487394957983}
>>> 2025-09-10 23:22:13,320 - INFO - >>> {'loss': 2.1619, 'grad_norm': 1.0518238544464111, 'learning_rate': 9.546829329218495e-06, 'epoch': 7.722689075630252}
>>> 2025-09-10 23:22:17,067 - INFO - >>> {'loss': 2.1943, 'grad_norm': 1.1913697719573975, 'learning_rate': 9.538036103983333e-06, 'epoch': 7.726890756302521}
>>> 2025-09-10 23:22:20,797 - INFO - >>> {'loss': 2.127, 'grad_norm': 1.0878093242645264, 'learning_rate': 9.529243236691815e-06, 'epoch': 7.73109243697479}
>>> 2025-09-10 23:22:24,192 - INFO - >>> {'loss': 2.1745, 'grad_norm': 1.0147298574447632, 'learning_rate': 9.520450734156925e-06, 'epoch': 7.735294117647059}
>>> 2025-09-10 23:22:27,758 - INFO - >>> {'loss': 2.1598, 'grad_norm': 1.0804564952850342, 'learning_rate': 9.511658603191358e-06, 'epoch': 7.739495798319328}
>>> 2025-09-10 23:22:31,445 - INFO - >>> {'loss': 2.2224, 'grad_norm': 1.0484627485275269, 'learning_rate': 9.502866850607527e-06, 'epoch': 7.743697478991597}
>>> 2025-09-10 23:22:35,205 - INFO - >>> {'loss': 2.2376, 'grad_norm': 1.0486202239990234, 'learning_rate': 9.494075483217542e-06, 'epoch': 7.7478991596638656}
>>> 2025-09-10 23:22:38,666 - INFO - >>> {'loss': 2.1886, 'grad_norm': 1.1463323831558228, 'learning_rate': 9.485284507833225e-06, 'epoch': 7.7521008403361344}
>>> 2025-09-10 23:22:42,478 - INFO - >>> {'loss': 2.191, 'grad_norm': 1.1011884212493896, 'learning_rate': 9.476493931266095e-06, 'epoch': 7.756302521008403}
>>> 2025-09-10 23:22:46,243 - INFO - >>> {'loss': 2.258, 'grad_norm': 1.0299257040023804, 'learning_rate': 9.467703760327352e-06, 'epoch': 7.760504201680672}
>>> 2025-09-10 23:22:49,556 - INFO - >>> {'loss': 2.2019, 'grad_norm': 1.177245020866394, 'learning_rate': 9.458914001827889e-06, 'epoch': 7.764705882352941}
>>> 2025-09-10 23:22:53,295 - INFO - >>> {'loss': 2.2451, 'grad_norm': 1.1552753448486328, 'learning_rate': 9.450124662578274e-06, 'epoch': 7.76890756302521}
>>> 2025-09-10 23:22:56,322 - INFO - >>> {'loss': 2.1788, 'grad_norm': 1.063545823097229, 'learning_rate': 9.44133574938876e-06, 'epoch': 7.773109243697479}
>>> 2025-09-10 23:22:59,722 - INFO - >>> {'loss': 2.206, 'grad_norm': 1.038661241531372, 'learning_rate': 9.43254726906926e-06, 'epoch': 7.777310924369748}
>>> 2025-09-10 23:23:03,055 - INFO - >>> {'loss': 2.2638, 'grad_norm': 1.1128498315811157, 'learning_rate': 9.423759228429359e-06, 'epoch': 7.781512605042017}
>>> 2025-09-10 23:23:06,272 - INFO - >>> {'loss': 2.174, 'grad_norm': 1.181090235710144, 'learning_rate': 9.414971634278291e-06, 'epoch': 7.785714285714286}
>>> 2025-09-10 23:23:10,277 - INFO - >>> {'loss': 2.2242, 'grad_norm': 1.077750563621521, 'learning_rate': 9.406184493424952e-06, 'epoch': 7.7899159663865545}
>>> 2025-09-10 23:23:13,328 - INFO - >>> {'loss': 2.2745, 'grad_norm': 1.1794843673706055, 'learning_rate': 9.397397812677892e-06, 'epoch': 7.794117647058823}
>>> 2025-09-10 23:23:16,909 - INFO - >>> {'loss': 2.1578, 'grad_norm': 1.1868114471435547, 'learning_rate': 9.388611598845289e-06, 'epoch': 7.798319327731092}
>>> 2025-09-10 23:23:20,881 - INFO - >>> {'loss': 2.2233, 'grad_norm': 0.9836048483848572, 'learning_rate': 9.379825858734978e-06, 'epoch': 7.802521008403361}
>>> 2025-09-10 23:23:24,934 - INFO - >>> {'loss': 2.2761, 'grad_norm': 1.1655499935150146, 'learning_rate': 9.371040599154405e-06, 'epoch': 7.80672268907563}
>>> 2025-09-10 23:23:28,309 - INFO - >>> {'loss': 2.2406, 'grad_norm': 1.1224116086959839, 'learning_rate': 9.362255826910665e-06, 'epoch': 7.810924369747899}
>>> 2025-09-10 23:23:31,381 - INFO - >>> {'loss': 2.1531, 'grad_norm': 1.257934808731079, 'learning_rate': 9.353471548810465e-06, 'epoch': 7.815126050420168}
>>> 2025-09-10 23:23:35,354 - INFO - >>> {'loss': 2.2478, 'grad_norm': 1.0301469564437866, 'learning_rate': 9.344687771660129e-06, 'epoch': 7.819327731092437}
>>> 2025-09-10 23:23:38,540 - INFO - >>> {'loss': 2.2338, 'grad_norm': 1.066530466079712, 'learning_rate': 9.335904502265592e-06, 'epoch': 7.823529411764706}
>>> 2025-09-10 23:23:41,876 - INFO - >>> {'loss': 2.163, 'grad_norm': 1.061131477355957, 'learning_rate': 9.327121747432398e-06, 'epoch': 7.8277310924369745}
>>> 2025-09-10 23:23:45,323 - INFO - >>> {'loss': 2.2311, 'grad_norm': 1.0930471420288086, 'learning_rate': 9.318339513965693e-06, 'epoch': 7.831932773109243}
>>> 2025-09-10 23:23:49,183 - INFO - >>> {'loss': 2.2226, 'grad_norm': 1.0478515625, 'learning_rate': 9.309557808670222e-06, 'epoch': 7.836134453781512}
>>> 2025-09-10 23:23:53,186 - INFO - >>> {'loss': 2.2312, 'grad_norm': 0.9986141324043274, 'learning_rate': 9.300776638350311e-06, 'epoch': 7.840336134453781}
>>> 2025-09-10 23:23:56,564 - INFO - >>> {'loss': 2.1949, 'grad_norm': 1.1327217817306519, 'learning_rate': 9.291996009809876e-06, 'epoch': 7.84453781512605}
>>> 2025-09-10 23:23:59,953 - INFO - >>> {'loss': 2.2229, 'grad_norm': 1.1230250597000122, 'learning_rate': 9.283215929852423e-06, 'epoch': 7.848739495798319}
>>> 2025-09-10 23:24:03,075 - INFO - >>> {'loss': 2.1775, 'grad_norm': 1.1341814994812012, 'learning_rate': 9.274436405281019e-06, 'epoch': 7.852941176470588}
>>> 2025-09-10 23:24:06,814 - INFO - >>> {'loss': 2.2333, 'grad_norm': 1.0981374979019165, 'learning_rate': 9.265657442898304e-06, 'epoch': 7.857142857142857}
>>> 2025-09-10 23:24:10,006 - INFO - >>> {'loss': 2.2022, 'grad_norm': 1.2061848640441895, 'learning_rate': 9.25687904950649e-06, 'epoch': 7.8613445378151265}
>>> 2025-09-10 23:24:13,078 - INFO - >>> {'loss': 2.1458, 'grad_norm': 1.1207666397094727, 'learning_rate': 9.248101231907333e-06, 'epoch': 7.865546218487395}
>>> 2025-09-10 23:24:16,337 - INFO - >>> {'loss': 2.1678, 'grad_norm': 1.0970083475112915, 'learning_rate': 9.23932399690216e-06, 'epoch': 7.869747899159664}
>>> 2025-09-10 23:24:20,017 - INFO - >>> {'loss': 2.2054, 'grad_norm': 0.9656724333763123, 'learning_rate': 9.230547351291844e-06, 'epoch': 7.873949579831933}
>>> 2025-09-10 23:24:23,150 - INFO - >>> {'loss': 2.2762, 'grad_norm': 1.155600905418396, 'learning_rate': 9.221771301876784e-06, 'epoch': 7.878151260504202}
>>> 2025-09-10 23:24:26,136 - INFO - >>> {'loss': 2.2001, 'grad_norm': 1.10749351978302, 'learning_rate': 9.212995855456935e-06, 'epoch': 7.882352941176471}
>>> 2025-09-10 23:24:29,538 - INFO - >>> {'loss': 2.2801, 'grad_norm': 1.1320377588272095, 'learning_rate': 9.204221018831782e-06, 'epoch': 7.88655462184874}
>>> 2025-09-10 23:24:32,903 - INFO - >>> {'loss': 2.1649, 'grad_norm': 1.123354434967041, 'learning_rate': 9.195446798800332e-06, 'epoch': 7.890756302521009}
>>> 2025-09-10 23:24:36,779 - INFO - >>> {'loss': 2.2008, 'grad_norm': 1.0421860218048096, 'learning_rate': 9.18667320216112e-06, 'epoch': 7.894957983193278}
>>> 2025-09-10 23:24:40,558 - INFO - >>> {'loss': 2.177, 'grad_norm': 1.0169869661331177, 'learning_rate': 9.177900235712189e-06, 'epoch': 7.899159663865547}
>>> 2025-09-10 23:24:43,642 - INFO - >>> {'loss': 2.2069, 'grad_norm': 1.169020652770996, 'learning_rate': 9.169127906251101e-06, 'epoch': 7.9033613445378155}
>>> 2025-09-10 23:24:46,901 - INFO - >>> {'loss': 2.275, 'grad_norm': 0.9718849062919617, 'learning_rate': 9.16035622057493e-06, 'epoch': 7.907563025210084}
>>> 2025-09-10 23:24:49,909 - INFO - >>> {'loss': 2.1832, 'grad_norm': 1.0530645847320557, 'learning_rate': 9.151585185480237e-06, 'epoch': 7.911764705882353}
>>> 2025-09-10 23:24:52,972 - INFO - >>> {'loss': 2.1733, 'grad_norm': 1.198400855064392, 'learning_rate': 9.14281480776309e-06, 'epoch': 7.915966386554622}
>>> 2025-09-10 23:24:56,392 - INFO - >>> {'loss': 2.3025, 'grad_norm': 1.2241175174713135, 'learning_rate': 9.134045094219038e-06, 'epoch': 7.920168067226891}
>>> 2025-09-10 23:24:59,626 - INFO - >>> {'loss': 2.1893, 'grad_norm': 1.1710703372955322, 'learning_rate': 9.125276051643128e-06, 'epoch': 7.92436974789916}
>>> 2025-09-10 23:25:03,338 - INFO - >>> {'loss': 2.1632, 'grad_norm': 1.1270266771316528, 'learning_rate': 9.116507686829878e-06, 'epoch': 7.928571428571429}
>>> 2025-09-10 23:25:07,391 - INFO - >>> {'loss': 2.2524, 'grad_norm': 1.0798816680908203, 'learning_rate': 9.107740006573281e-06, 'epoch': 7.932773109243698}
>>> 2025-09-10 23:25:10,938 - INFO - >>> {'loss': 2.1999, 'grad_norm': 1.1017705202102661, 'learning_rate': 9.098973017666802e-06, 'epoch': 7.936974789915967}
>>> 2025-09-10 23:25:14,740 - INFO - >>> {'loss': 2.1314, 'grad_norm': 1.0984852313995361, 'learning_rate': 9.090206726903377e-06, 'epoch': 7.9411764705882355}
>>> 2025-09-10 23:25:18,033 - INFO - >>> {'loss': 2.2516, 'grad_norm': 1.1662700176239014, 'learning_rate': 9.081441141075387e-06, 'epoch': 7.945378151260504}
>>> 2025-09-10 23:25:21,714 - INFO - >>> {'loss': 2.2062, 'grad_norm': 1.237073302268982, 'learning_rate': 9.072676266974675e-06, 'epoch': 7.949579831932773}
>>> 2025-09-10 23:25:25,144 - INFO - >>> {'loss': 2.1413, 'grad_norm': 1.0739452838897705, 'learning_rate': 9.063912111392531e-06, 'epoch': 7.953781512605042}
>>> 2025-09-10 23:25:29,043 - INFO - >>> {'loss': 2.2258, 'grad_norm': 1.1066581010818481, 'learning_rate': 9.055148681119688e-06, 'epoch': 7.957983193277311}
>>> 2025-09-10 23:25:32,237 - INFO - >>> {'loss': 2.1466, 'grad_norm': 1.0336835384368896, 'learning_rate': 9.046385982946322e-06, 'epoch': 7.96218487394958}
>>> 2025-09-10 23:25:35,554 - INFO - >>> {'loss': 2.2159, 'grad_norm': 1.095888614654541, 'learning_rate': 9.037624023662035e-06, 'epoch': 7.966386554621849}
>>> 2025-09-10 23:25:38,588 - INFO - >>> {'loss': 2.2553, 'grad_norm': 1.2037429809570312, 'learning_rate': 9.028862810055858e-06, 'epoch': 7.970588235294118}
>>> 2025-09-10 23:25:41,854 - INFO - >>> {'loss': 2.247, 'grad_norm': 1.0607565641403198, 'learning_rate': 9.020102348916243e-06, 'epoch': 7.974789915966387}
>>> 2025-09-10 23:25:44,885 - INFO - >>> {'loss': 2.3315, 'grad_norm': 1.2024450302124023, 'learning_rate': 9.01134264703107e-06, 'epoch': 7.9789915966386555}
>>> 2025-09-10 23:25:48,953 - INFO - >>> {'loss': 2.2521, 'grad_norm': 1.0822114944458008, 'learning_rate': 9.002583711187612e-06, 'epoch': 7.983193277310924}
>>> 2025-09-10 23:25:53,014 - INFO - >>> {'loss': 2.1874, 'grad_norm': 1.1413239240646362, 'learning_rate': 8.993825548172564e-06, 'epoch': 7.987394957983193}
>>> 2025-09-10 23:25:57,017 - INFO - >>> {'loss': 2.1615, 'grad_norm': 1.0522186756134033, 'learning_rate': 8.985068164772015e-06, 'epoch': 7.991596638655462}
>>> 2025-09-10 23:26:00,285 - INFO - >>> {'loss': 2.1538, 'grad_norm': 1.0916541814804077, 'learning_rate': 8.976311567771447e-06, 'epoch': 7.995798319327731}
>>> 2025-09-10 23:26:03,222 - INFO - >>> {'loss': 2.2038, 'grad_norm': 1.2433152198791504, 'learning_rate': 8.967555763955746e-06, 'epoch': 8.0}
>>> 2025-09-10 23:26:06,649 - INFO - >>> {'loss': 2.126, 'grad_norm': 1.1145384311676025, 'learning_rate': 8.958800760109167e-06, 'epoch': 8.004201680672269}
>>> 2025-09-10 23:26:10,034 - INFO - >>> {'loss': 2.204, 'grad_norm': 1.0620801448822021, 'learning_rate': 8.95004656301536e-06, 'epoch': 8.008403361344538}
>>> 2025-09-10 23:26:13,158 - INFO - >>> {'loss': 2.2623, 'grad_norm': 1.1626713275909424, 'learning_rate': 8.941293179457331e-06, 'epoch': 8.012605042016807}
>>> 2025-09-10 23:26:16,213 - INFO - >>> {'loss': 2.1115, 'grad_norm': 1.1602824926376343, 'learning_rate': 8.932540616217476e-06, 'epoch': 8.016806722689076}
>>> 2025-09-10 23:26:19,928 - INFO - >>> {'loss': 2.0864, 'grad_norm': 0.9789975881576538, 'learning_rate': 8.923788880077549e-06, 'epoch': 8.021008403361344}
>>> 2025-09-10 23:26:22,918 - INFO - >>> {'loss': 2.0777, 'grad_norm': 1.084518551826477, 'learning_rate': 8.915037977818656e-06, 'epoch': 8.025210084033613}
>>> 2025-09-10 23:26:26,782 - INFO - >>> {'loss': 2.2297, 'grad_norm': 1.0220599174499512, 'learning_rate': 8.906287916221259e-06, 'epoch': 8.029411764705882}
>>> 2025-09-10 23:26:30,391 - INFO - >>> {'loss': 2.1579, 'grad_norm': 1.1058534383773804, 'learning_rate': 8.897538702065172e-06, 'epoch': 8.033613445378151}
>>> 2025-09-10 23:26:34,016 - INFO - >>> {'loss': 2.1955, 'grad_norm': 1.1280714273452759, 'learning_rate': 8.88879034212956e-06, 'epoch': 8.03781512605042}
>>> 2025-09-10 23:26:37,955 - INFO - >>> {'loss': 2.2089, 'grad_norm': 1.0648469924926758, 'learning_rate': 8.88004284319291e-06, 'epoch': 8.042016806722689}
>>> 2025-09-10 23:26:41,153 - INFO - >>> {'loss': 2.0976, 'grad_norm': 1.0797643661499023, 'learning_rate': 8.87129621203305e-06, 'epoch': 8.046218487394958}
>>> 2025-09-10 23:26:44,435 - INFO - >>> {'loss': 2.1097, 'grad_norm': 1.1849616765975952, 'learning_rate': 8.862550455427134e-06, 'epoch': 8.050420168067227}
>>> 2025-09-10 23:26:47,790 - INFO - >>> {'loss': 2.1299, 'grad_norm': 0.9891960024833679, 'learning_rate': 8.853805580151646e-06, 'epoch': 8.054621848739496}
>>> 2025-09-10 23:26:51,766 - INFO - >>> {'loss': 2.3134, 'grad_norm': 1.0912197828292847, 'learning_rate': 8.845061592982378e-06, 'epoch': 8.058823529411764}
>>> 2025-09-10 23:26:55,754 - INFO - >>> {'loss': 2.295, 'grad_norm': 0.9873841404914856, 'learning_rate': 8.836318500694434e-06, 'epoch': 8.063025210084033}
>>> 2025-09-10 23:26:58,719 - INFO - >>> {'loss': 2.2232, 'grad_norm': 1.2923394441604614, 'learning_rate': 8.82757631006223e-06, 'epoch': 8.067226890756302}
>>> 2025-09-10 23:27:01,825 - INFO - >>> {'loss': 2.1487, 'grad_norm': 1.1012006998062134, 'learning_rate': 8.818835027859476e-06, 'epoch': 8.071428571428571}
>>> 2025-09-10 23:27:05,421 - INFO - >>> {'loss': 2.3268, 'grad_norm': 1.0809457302093506, 'learning_rate': 8.810094660859187e-06, 'epoch': 8.07563025210084}
>>> 2025-09-10 23:27:08,759 - INFO - >>> {'loss': 2.3166, 'grad_norm': 1.1530482769012451, 'learning_rate': 8.801355215833667e-06, 'epoch': 8.079831932773109}
>>> 2025-09-10 23:27:12,492 - INFO - >>> {'loss': 2.1744, 'grad_norm': 1.000478982925415, 'learning_rate': 8.792616699554497e-06, 'epoch': 8.084033613445378}
>>> 2025-09-10 23:27:16,260 - INFO - >>> {'loss': 2.1381, 'grad_norm': 1.127157211303711, 'learning_rate': 8.783879118792547e-06, 'epoch': 8.088235294117647}
>>> 2025-09-10 23:27:19,818 - INFO - >>> {'loss': 2.2369, 'grad_norm': 1.0973496437072754, 'learning_rate': 8.775142480317963e-06, 'epoch': 8.092436974789916}
>>> 2025-09-10 23:27:23,068 - INFO - >>> {'loss': 2.2095, 'grad_norm': 1.0630487203598022, 'learning_rate': 8.766406790900155e-06, 'epoch': 8.096638655462185}
>>> 2025-09-10 23:27:26,266 - INFO - >>> {'loss': 2.1104, 'grad_norm': 1.1031200885772705, 'learning_rate': 8.757672057307799e-06, 'epoch': 8.100840336134453}
>>> 2025-09-10 23:27:29,304 - INFO - >>> {'loss': 2.2427, 'grad_norm': 1.081050992012024, 'learning_rate': 8.748938286308835e-06, 'epoch': 8.105042016806722}
>>> 2025-09-10 23:27:33,321 - INFO - >>> {'loss': 2.2314, 'grad_norm': 1.0427322387695312, 'learning_rate': 8.740205484670443e-06, 'epoch': 8.109243697478991}
>>> 2025-09-10 23:27:37,294 - INFO - >>> {'loss': 2.3255, 'grad_norm': 1.0703563690185547, 'learning_rate': 8.73147365915908e-06, 'epoch': 8.11344537815126}
>>> 2025-09-10 23:27:41,064 - INFO - >>> {'loss': 2.1198, 'grad_norm': 1.0495526790618896, 'learning_rate': 8.722742816540419e-06, 'epoch': 8.117647058823529}
>>> 2025-09-10 23:27:44,594 - INFO - >>> {'loss': 2.1708, 'grad_norm': 1.1316341161727905, 'learning_rate': 8.714012963579383e-06, 'epoch': 8.121848739495798}
>>> 2025-09-10 23:27:48,029 - INFO - >>> {'loss': 2.1524, 'grad_norm': 1.0990281105041504, 'learning_rate': 8.705284107040123e-06, 'epoch': 8.126050420168067}
>>> 2025-09-10 23:27:52,058 - INFO - >>> {'loss': 2.2358, 'grad_norm': 1.105125904083252, 'learning_rate': 8.696556253686033e-06, 'epoch': 8.130252100840336}
>>> 2025-09-10 23:27:55,785 - INFO - >>> {'loss': 2.2349, 'grad_norm': 1.10576593875885, 'learning_rate': 8.68782941027971e-06, 'epoch': 8.134453781512605}
>>> 2025-09-10 23:27:59,178 - INFO - >>> {'loss': 2.1591, 'grad_norm': 1.0289472341537476, 'learning_rate': 8.67910358358298e-06, 'epoch': 8.138655462184873}
>>> 2025-09-10 23:28:02,874 - INFO - >>> {'loss': 2.1673, 'grad_norm': 1.0588122606277466, 'learning_rate': 8.670378780356872e-06, 'epoch': 8.142857142857142}
>>> 2025-09-10 23:28:06,183 - INFO - >>> {'loss': 2.2498, 'grad_norm': 1.0731755495071411, 'learning_rate': 8.661655007361639e-06, 'epoch': 8.147058823529411}
>>> 2025-09-10 23:28:10,184 - INFO - >>> {'loss': 2.2006, 'grad_norm': 1.0544750690460205, 'learning_rate': 8.652932271356719e-06, 'epoch': 8.15126050420168}
>>> 2025-09-10 23:28:13,697 - INFO - >>> {'loss': 2.2566, 'grad_norm': 1.1230754852294922, 'learning_rate': 8.644210579100754e-06, 'epoch': 8.155462184873949}
>>> 2025-09-10 23:28:16,772 - INFO - >>> {'loss': 2.1791, 'grad_norm': 1.1251620054244995, 'learning_rate': 8.635489937351576e-06, 'epoch': 8.159663865546218}
>>> 2025-09-10 23:28:20,657 - INFO - >>> {'loss': 2.1651, 'grad_norm': 1.057369589805603, 'learning_rate': 8.626770352866197e-06, 'epoch': 8.163865546218487}
>>> 2025-09-10 23:28:23,778 - INFO - >>> {'loss': 2.1939, 'grad_norm': 1.2315689325332642, 'learning_rate': 8.618051832400823e-06, 'epoch': 8.168067226890756}
>>> 2025-09-10 23:28:27,025 - INFO - >>> {'loss': 2.1323, 'grad_norm': 1.1633007526397705, 'learning_rate': 8.60933438271083e-06, 'epoch': 8.172268907563025}
>>> 2025-09-10 23:28:30,115 - INFO - >>> {'loss': 2.0489, 'grad_norm': 1.2209242582321167, 'learning_rate': 8.600618010550754e-06, 'epoch': 8.176470588235293}
>>> 2025-09-10 23:28:33,745 - INFO - >>> {'loss': 2.2176, 'grad_norm': 1.133015751838684, 'learning_rate': 8.591902722674308e-06, 'epoch': 8.180672268907562}
>>> 2025-09-10 23:28:37,271 - INFO - >>> {'loss': 2.1012, 'grad_norm': 1.1482936143875122, 'learning_rate': 8.583188525834367e-06, 'epoch': 8.184873949579831}
>>> 2025-09-10 23:28:40,543 - INFO - >>> {'loss': 2.1301, 'grad_norm': 1.0733686685562134, 'learning_rate': 8.574475426782948e-06, 'epoch': 8.1890756302521}
>>> 2025-09-10 23:28:44,177 - INFO - >>> {'loss': 2.2591, 'grad_norm': 1.007766604423523, 'learning_rate': 8.56576343227123e-06, 'epoch': 8.193277310924369}
>>> 2025-09-10 23:28:47,394 - INFO - >>> {'loss': 2.3299, 'grad_norm': 1.0002286434173584, 'learning_rate': 8.55705254904953e-06, 'epoch': 8.197478991596638}
>>> 2025-09-10 23:28:50,461 - INFO - >>> {'loss': 2.2463, 'grad_norm': 1.0639559030532837, 'learning_rate': 8.548342783867297e-06, 'epoch': 8.201680672268907}
>>> 2025-09-10 23:28:53,734 - INFO - >>> {'loss': 2.2742, 'grad_norm': 1.1281323432922363, 'learning_rate': 8.539634143473129e-06, 'epoch': 8.205882352941176}
>>> 2025-09-10 23:28:57,345 - INFO - >>> {'loss': 2.2685, 'grad_norm': 1.0659165382385254, 'learning_rate': 8.530926634614746e-06, 'epoch': 8.210084033613445}
>>> 2025-09-10 23:29:01,000 - INFO - >>> {'loss': 2.2889, 'grad_norm': 1.1004260778427124, 'learning_rate': 8.522220264038983e-06, 'epoch': 8.214285714285714}
>>> 2025-09-10 23:29:04,299 - INFO - >>> {'loss': 2.1756, 'grad_norm': 1.0800246000289917, 'learning_rate': 8.513515038491805e-06, 'epoch': 8.218487394957982}
>>> 2025-09-10 23:29:07,554 - INFO - >>> {'loss': 2.1046, 'grad_norm': 1.0299633741378784, 'learning_rate': 8.504810964718286e-06, 'epoch': 8.222689075630251}
>>> 2025-09-10 23:29:11,279 - INFO - >>> {'loss': 2.2272, 'grad_norm': 1.1180633306503296, 'learning_rate': 8.496108049462604e-06, 'epoch': 8.22689075630252}
>>> 2025-09-10 23:29:14,513 - INFO - >>> {'loss': 2.1872, 'grad_norm': 0.9877254962921143, 'learning_rate': 8.487406299468044e-06, 'epoch': 8.231092436974789}
>>> 2025-09-10 23:29:18,299 - INFO - >>> {'loss': 2.1933, 'grad_norm': 1.0765163898468018, 'learning_rate': 8.478705721476982e-06, 'epoch': 8.235294117647058}
>>> 2025-09-10 23:29:22,069 - INFO - >>> {'loss': 2.2951, 'grad_norm': 1.0848643779754639, 'learning_rate': 8.470006322230891e-06, 'epoch': 8.239495798319327}
>>> 2025-09-10 23:29:25,597 - INFO - >>> {'loss': 2.2247, 'grad_norm': 1.0852564573287964, 'learning_rate': 8.461308108470334e-06, 'epoch': 8.243697478991596}
>>> 2025-09-10 23:29:29,577 - INFO - >>> {'loss': 2.308, 'grad_norm': 1.060465931892395, 'learning_rate': 8.452611086934946e-06, 'epoch': 8.247899159663866}
>>> 2025-09-10 23:29:33,069 - INFO - >>> {'loss': 2.1997, 'grad_norm': 1.1056095361709595, 'learning_rate': 8.443915264363447e-06, 'epoch': 8.252100840336134}
>>> 2025-09-10 23:29:36,440 - INFO - >>> {'loss': 2.2664, 'grad_norm': 1.0814123153686523, 'learning_rate': 8.43522064749362e-06, 'epoch': 8.256302521008404}
>>> 2025-09-10 23:29:40,250 - INFO - >>> {'loss': 2.1649, 'grad_norm': 1.0628896951675415, 'learning_rate': 8.426527243062318e-06, 'epoch': 8.260504201680673}
>>> 2025-09-10 23:29:43,780 - INFO - >>> {'loss': 2.2243, 'grad_norm': 0.9300136566162109, 'learning_rate': 8.417835057805464e-06, 'epoch': 8.264705882352942}
>>> 2025-09-10 23:29:47,552 - INFO - >>> {'loss': 2.1332, 'grad_norm': 1.0593647956848145, 'learning_rate': 8.409144098458015e-06, 'epoch': 8.268907563025211}
>>> 2025-09-10 23:29:51,473 - INFO - >>> {'loss': 2.2163, 'grad_norm': 1.1843535900115967, 'learning_rate': 8.400454371753997e-06, 'epoch': 8.27310924369748}
>>> 2025-09-10 23:29:54,697 - INFO - >>> {'loss': 2.1542, 'grad_norm': 1.0393669605255127, 'learning_rate': 8.391765884426468e-06, 'epoch': 8.277310924369749}
>>> 2025-09-10 23:29:58,545 - INFO - >>> {'loss': 2.193, 'grad_norm': 1.111973524093628, 'learning_rate': 8.383078643207536e-06, 'epoch': 8.281512605042018}
>>> 2025-09-10 23:30:01,730 - INFO - >>> {'loss': 2.1287, 'grad_norm': 1.1349122524261475, 'learning_rate': 8.374392654828341e-06, 'epoch': 8.285714285714286}
>>> 2025-09-10 23:30:05,411 - INFO - >>> {'loss': 2.3204, 'grad_norm': 1.0766311883926392, 'learning_rate': 8.365707926019048e-06, 'epoch': 8.289915966386555}
>>> 2025-09-10 23:30:08,593 - INFO - >>> {'loss': 2.2035, 'grad_norm': 1.0960744619369507, 'learning_rate': 8.357024463508847e-06, 'epoch': 8.294117647058824}
>>> 2025-09-10 23:30:11,911 - INFO - >>> {'loss': 2.354, 'grad_norm': 1.193440556526184, 'learning_rate': 8.348342274025947e-06, 'epoch': 8.298319327731093}
>>> 2025-09-10 23:30:15,190 - INFO - >>> {'loss': 2.2569, 'grad_norm': 1.0663877725601196, 'learning_rate': 8.33966136429758e-06, 'epoch': 8.302521008403362}
>>> 2025-09-10 23:30:18,096 - INFO - >>> {'loss': 2.2155, 'grad_norm': 1.1473644971847534, 'learning_rate': 8.330981741049969e-06, 'epoch': 8.306722689075631}
>>> 2025-09-10 23:30:21,391 - INFO - >>> {'loss': 2.1236, 'grad_norm': 1.1218022108078003, 'learning_rate': 8.322303411008355e-06, 'epoch': 8.3109243697479}
>>> 2025-09-10 23:30:25,129 - INFO - >>> {'loss': 2.2475, 'grad_norm': 1.0847922563552856, 'learning_rate': 8.313626380896966e-06, 'epoch': 8.315126050420169}
>>> 2025-09-10 23:30:28,198 - INFO - >>> {'loss': 2.229, 'grad_norm': 1.1882882118225098, 'learning_rate': 8.304950657439034e-06, 'epoch': 8.319327731092438}
>>> 2025-09-10 23:30:31,378 - INFO - >>> {'loss': 2.2486, 'grad_norm': 1.0488768815994263, 'learning_rate': 8.296276247356773e-06, 'epoch': 8.323529411764707}
>>> 2025-09-10 23:30:34,776 - INFO - >>> {'loss': 2.2664, 'grad_norm': 0.9818118810653687, 'learning_rate': 8.287603157371374e-06, 'epoch': 8.327731092436975}
>>> 2025-09-10 23:30:37,874 - INFO - >>> {'loss': 2.2253, 'grad_norm': 1.1319637298583984, 'learning_rate': 8.27893139420301e-06, 'epoch': 8.331932773109244}
>>> 2025-09-10 23:30:41,343 - INFO - >>> {'loss': 2.1607, 'grad_norm': 1.0213521718978882, 'learning_rate': 8.270260964570837e-06, 'epoch': 8.336134453781513}
>>> 2025-09-10 23:30:44,877 - INFO - >>> {'loss': 2.2244, 'grad_norm': 1.1340012550354004, 'learning_rate': 8.261591875192955e-06, 'epoch': 8.340336134453782}
>>> 2025-09-10 23:30:48,645 - INFO - >>> {'loss': 2.2115, 'grad_norm': 1.1216400861740112, 'learning_rate': 8.252924132786446e-06, 'epoch': 8.344537815126051}
>>> 2025-09-10 23:30:52,338 - INFO - >>> {'loss': 2.1714, 'grad_norm': 1.1747477054595947, 'learning_rate': 8.244257744067332e-06, 'epoch': 8.34873949579832}
>>> 2025-09-10 23:30:55,556 - INFO - >>> {'loss': 2.1156, 'grad_norm': 1.0886443853378296, 'learning_rate': 8.235592715750595e-06, 'epoch': 8.352941176470589}
>>> 2025-09-10 23:30:59,256 - INFO - >>> {'loss': 2.18, 'grad_norm': 1.0152195692062378, 'learning_rate': 8.22692905455017e-06, 'epoch': 8.357142857142858}
>>> 2025-09-10 23:31:02,477 - INFO - >>> {'loss': 2.2422, 'grad_norm': 1.125091791152954, 'learning_rate': 8.218266767178917e-06, 'epoch': 8.361344537815127}
>>> 2025-09-10 23:31:06,105 - INFO - >>> {'loss': 2.1535, 'grad_norm': 1.1128664016723633, 'learning_rate': 8.209605860348644e-06, 'epoch': 8.365546218487395}
>>> 2025-09-10 23:31:09,922 - INFO - >>> {'loss': 2.2959, 'grad_norm': 1.144895076751709, 'learning_rate': 8.200946340770078e-06, 'epoch': 8.369747899159664}
>>> 2025-09-10 23:31:13,598 - INFO - >>> {'loss': 2.1825, 'grad_norm': 1.0507583618164062, 'learning_rate': 8.192288215152886e-06, 'epoch': 8.373949579831933}
>>> 2025-09-10 23:31:17,242 - INFO - >>> {'loss': 2.1969, 'grad_norm': 1.28658127784729, 'learning_rate': 8.183631490205636e-06, 'epoch': 8.378151260504202}
>>> 2025-09-10 23:31:21,161 - INFO - >>> {'loss': 2.2785, 'grad_norm': 1.078993558883667, 'learning_rate': 8.174976172635833e-06, 'epoch': 8.382352941176471}
>>> 2025-09-10 23:31:24,549 - INFO - >>> {'loss': 2.2138, 'grad_norm': 1.0130728483200073, 'learning_rate': 8.166322269149868e-06, 'epoch': 8.38655462184874}
>>> 2025-09-10 23:31:27,901 - INFO - >>> {'loss': 2.1888, 'grad_norm': 1.038224220275879, 'learning_rate': 8.157669786453056e-06, 'epoch': 8.390756302521009}
>>> 2025-09-10 23:31:31,509 - INFO - >>> {'loss': 2.2943, 'grad_norm': 1.2420449256896973, 'learning_rate': 8.149018731249603e-06, 'epoch': 8.394957983193278}
>>> 2025-09-10 23:31:34,522 - INFO - >>> {'loss': 2.2873, 'grad_norm': 1.2275760173797607, 'learning_rate': 8.140369110242605e-06, 'epoch': 8.399159663865547}
>>> 2025-09-10 23:31:37,757 - INFO - >>> {'loss': 2.2092, 'grad_norm': 1.1159929037094116, 'learning_rate': 8.131720930134056e-06, 'epoch': 8.403361344537815}
>>> 2025-09-10 23:31:42,277 - INFO - >>> {'loss': 2.2627, 'grad_norm': 1.2578601837158203, 'learning_rate': 8.12307419762482e-06, 'epoch': 8.407563025210084}
>>> 2025-09-10 23:31:45,272 - INFO - >>> {'loss': 2.2391, 'grad_norm': 1.2903987169265747, 'learning_rate': 8.114428919414655e-06, 'epoch': 8.411764705882353}
>>> 2025-09-10 23:31:49,195 - INFO - >>> {'loss': 2.0838, 'grad_norm': 1.0345041751861572, 'learning_rate': 8.105785102202184e-06, 'epoch': 8.415966386554622}
>>> 2025-09-10 23:31:52,288 - INFO - >>> {'loss': 2.2221, 'grad_norm': 1.1690374612808228, 'learning_rate': 8.097142752684897e-06, 'epoch': 8.420168067226891}
>>> 2025-09-10 23:31:55,578 - INFO - >>> {'loss': 2.1376, 'grad_norm': 1.1836755275726318, 'learning_rate': 8.088501877559149e-06, 'epoch': 8.42436974789916}
>>> 2025-09-10 23:31:58,932 - INFO - >>> {'loss': 2.2275, 'grad_norm': 1.1392077207565308, 'learning_rate': 8.079862483520155e-06, 'epoch': 8.428571428571429}
>>> 2025-09-10 23:32:01,783 - INFO - >>> {'loss': 2.2721, 'grad_norm': 1.2515449523925781, 'learning_rate': 8.071224577261975e-06, 'epoch': 8.432773109243698}
>>> 2025-09-10 23:32:05,423 - INFO - >>> {'loss': 2.2119, 'grad_norm': 1.0761644840240479, 'learning_rate': 8.062588165477526e-06, 'epoch': 8.436974789915967}
>>> 2025-09-10 23:32:09,300 - INFO - >>> {'loss': 2.2124, 'grad_norm': 1.1371501684188843, 'learning_rate': 8.053953254858557e-06, 'epoch': 8.441176470588236}
>>> 2025-09-10 23:32:12,372 - INFO - >>> {'loss': 2.2584, 'grad_norm': 1.14470374584198, 'learning_rate': 8.045319852095659e-06, 'epoch': 8.445378151260504}
>>> 2025-09-10 23:32:16,075 - INFO - >>> {'loss': 2.1942, 'grad_norm': 1.1101467609405518, 'learning_rate': 8.03668796387826e-06, 'epoch': 8.449579831932773}
>>> 2025-09-10 23:32:20,177 - INFO - >>> {'loss': 2.2744, 'grad_norm': 1.0265655517578125, 'learning_rate': 8.028057596894607e-06, 'epoch': 8.453781512605042}
>>> 2025-09-10 23:32:23,848 - INFO - >>> {'loss': 2.1942, 'grad_norm': 1.0346169471740723, 'learning_rate': 8.019428757831765e-06, 'epoch': 8.457983193277311}
>>> 2025-09-10 23:32:27,587 - INFO - >>> {'loss': 2.215, 'grad_norm': 1.2028007507324219, 'learning_rate': 8.010801453375622e-06, 'epoch': 8.46218487394958}
>>> 2025-09-10 23:32:30,918 - INFO - >>> {'loss': 2.2617, 'grad_norm': 1.1695842742919922, 'learning_rate': 8.002175690210883e-06, 'epoch': 8.466386554621849}
>>> 2025-09-10 23:32:34,525 - INFO - >>> {'loss': 2.1541, 'grad_norm': 1.1356658935546875, 'learning_rate': 7.993551475021042e-06, 'epoch': 8.470588235294118}
>>> 2025-09-10 23:32:37,766 - INFO - >>> {'loss': 2.2552, 'grad_norm': 1.1135319471359253, 'learning_rate': 7.984928814488409e-06, 'epoch': 8.474789915966387}
>>> 2025-09-10 23:32:41,752 - INFO - >>> {'loss': 2.235, 'grad_norm': 1.0305598974227905, 'learning_rate': 7.976307715294077e-06, 'epoch': 8.478991596638656}
>>> 2025-09-10 23:32:44,938 - INFO - >>> {'loss': 2.2228, 'grad_norm': 1.0077944993972778, 'learning_rate': 7.967688184117936e-06, 'epoch': 8.483193277310924}
>>> 2025-09-10 23:32:48,763 - INFO - >>> {'loss': 2.206, 'grad_norm': 1.0133330821990967, 'learning_rate': 7.959070227638667e-06, 'epoch': 8.487394957983193}
>>> 2025-09-10 23:32:51,780 - INFO - >>> {'loss': 2.2464, 'grad_norm': 1.139041781425476, 'learning_rate': 7.950453852533719e-06, 'epoch': 8.491596638655462}
>>> 2025-09-10 23:32:54,692 - INFO - >>> {'loss': 2.1657, 'grad_norm': 1.1775048971176147, 'learning_rate': 7.941839065479322e-06, 'epoch': 8.495798319327731}
>>> 2025-09-10 23:32:57,795 - INFO - >>> {'loss': 2.2801, 'grad_norm': 1.0987986326217651, 'learning_rate': 7.93322587315047e-06, 'epoch': 8.5}
>>> 2025-09-10 23:33:01,209 - INFO - >>> {'loss': 2.15, 'grad_norm': 1.0982065200805664, 'learning_rate': 7.924614282220933e-06, 'epoch': 8.504201680672269}
>>> 2025-09-10 23:33:04,547 - INFO - >>> {'loss': 2.2916, 'grad_norm': 1.0651789903640747, 'learning_rate': 7.916004299363232e-06, 'epoch': 8.508403361344538}
>>> 2025-09-10 23:33:08,470 - INFO - >>> {'loss': 2.1748, 'grad_norm': 1.1183820962905884, 'learning_rate': 7.907395931248639e-06, 'epoch': 8.512605042016807}
>>> 2025-09-10 23:33:11,811 - INFO - >>> {'loss': 2.1763, 'grad_norm': 1.0445835590362549, 'learning_rate': 7.898789184547183e-06, 'epoch': 8.516806722689076}
>>> 2025-09-10 23:33:15,364 - INFO - >>> {'loss': 2.2457, 'grad_norm': 1.0672739744186401, 'learning_rate': 7.890184065927626e-06, 'epoch': 8.521008403361344}
>>> 2025-09-10 23:33:19,198 - INFO - >>> {'loss': 2.2063, 'grad_norm': 1.0351463556289673, 'learning_rate': 7.88158058205748e-06, 'epoch': 8.525210084033613}
>>> 2025-09-10 23:33:22,913 - INFO - >>> {'loss': 2.3323, 'grad_norm': 1.2341610193252563, 'learning_rate': 7.872978739602988e-06, 'epoch': 8.529411764705882}
>>> 2025-09-10 23:33:25,865 - INFO - >>> {'loss': 2.1775, 'grad_norm': 1.254392385482788, 'learning_rate': 7.864378545229113e-06, 'epoch': 8.533613445378151}
>>> 2025-09-10 23:33:28,926 - INFO - >>> {'loss': 2.2109, 'grad_norm': 1.1249034404754639, 'learning_rate': 7.855780005599547e-06, 'epoch': 8.53781512605042}
>>> 2025-09-10 23:33:32,030 - INFO - >>> {'loss': 2.1798, 'grad_norm': 1.1116586923599243, 'learning_rate': 7.847183127376705e-06, 'epoch': 8.542016806722689}
>>> 2025-09-10 23:33:35,231 - INFO - >>> {'loss': 2.1533, 'grad_norm': 1.0926847457885742, 'learning_rate': 7.838587917221707e-06, 'epoch': 8.546218487394958}
>>> 2025-09-10 23:33:38,867 - INFO - >>> {'loss': 2.1384, 'grad_norm': 1.1773128509521484, 'learning_rate': 7.829994381794378e-06, 'epoch': 8.550420168067227}
>>> 2025-09-10 23:33:42,138 - INFO - >>> {'loss': 2.1836, 'grad_norm': 1.1364402770996094, 'learning_rate': 7.821402527753254e-06, 'epoch': 8.554621848739496}
>>> 2025-09-10 23:33:45,554 - INFO - >>> {'loss': 2.3172, 'grad_norm': 1.0824168920516968, 'learning_rate': 7.812812361755561e-06, 'epoch': 8.558823529411764}
>>> 2025-09-10 23:33:49,373 - INFO - >>> {'loss': 2.142, 'grad_norm': 1.1164480447769165, 'learning_rate': 7.804223890457225e-06, 'epoch': 8.563025210084033}
>>> 2025-09-10 23:33:52,431 - INFO - >>> {'loss': 2.2134, 'grad_norm': 1.3868920803070068, 'learning_rate': 7.795637120512853e-06, 'epoch': 8.567226890756302}
>>> 2025-09-10 23:33:55,840 - INFO - >>> {'loss': 2.2182, 'grad_norm': 1.0616533756256104, 'learning_rate': 7.78705205857573e-06, 'epoch': 8.571428571428571}
>>> 2025-09-10 23:33:59,150 - INFO - >>> {'loss': 2.2375, 'grad_norm': 1.103400468826294, 'learning_rate': 7.778468711297826e-06, 'epoch': 8.57563025210084}
>>> 2025-09-10 23:34:02,439 - INFO - >>> {'loss': 2.2332, 'grad_norm': 1.1238373517990112, 'learning_rate': 7.769887085329783e-06, 'epoch': 8.579831932773109}
>>> 2025-09-10 23:34:06,165 - INFO - >>> {'loss': 2.1814, 'grad_norm': 1.119214653968811, 'learning_rate': 7.761307187320901e-06, 'epoch': 8.584033613445378}
>>> 2025-09-10 23:34:09,286 - INFO - >>> {'loss': 2.1687, 'grad_norm': 1.109963297843933, 'learning_rate': 7.752729023919147e-06, 'epoch': 8.588235294117647}
>>> 2025-09-10 23:34:13,212 - INFO - >>> {'loss': 2.2324, 'grad_norm': 1.1228740215301514, 'learning_rate': 7.744152601771135e-06, 'epoch': 8.592436974789916}
>>> 2025-09-10 23:34:16,874 - INFO - >>> {'loss': 2.2474, 'grad_norm': 1.1727672815322876, 'learning_rate': 7.735577927522148e-06, 'epoch': 8.596638655462185}
>>> 2025-09-10 23:34:20,309 - INFO - >>> {'loss': 2.2692, 'grad_norm': 1.2681230306625366, 'learning_rate': 7.7270050078161e-06, 'epoch': 8.600840336134453}
>>> 2025-09-10 23:34:23,558 - INFO - >>> {'loss': 2.1919, 'grad_norm': 1.0746383666992188, 'learning_rate': 7.718433849295546e-06, 'epoch': 8.605042016806722}
>>> 2025-09-10 23:34:26,824 - INFO - >>> {'loss': 2.1214, 'grad_norm': 1.269997000694275, 'learning_rate': 7.709864458601685e-06, 'epoch': 8.609243697478991}
>>> 2025-09-10 23:34:29,880 - INFO - >>> {'loss': 2.1002, 'grad_norm': 1.2663462162017822, 'learning_rate': 7.701296842374333e-06, 'epoch': 8.61344537815126}
>>> 2025-09-10 23:34:32,830 - INFO - >>> {'loss': 2.032, 'grad_norm': 1.1352503299713135, 'learning_rate': 7.692731007251947e-06, 'epoch': 8.617647058823529}
>>> 2025-09-10 23:34:36,471 - INFO - >>> {'loss': 2.1658, 'grad_norm': 1.2287434339523315, 'learning_rate': 7.684166959871595e-06, 'epoch': 8.621848739495798}
>>> 2025-09-10 23:34:39,621 - INFO - >>> {'loss': 2.1186, 'grad_norm': 1.1776148080825806, 'learning_rate': 7.675604706868958e-06, 'epoch': 8.626050420168067}
>>> 2025-09-10 23:34:42,965 - INFO - >>> {'loss': 2.28, 'grad_norm': 1.168756365776062, 'learning_rate': 7.667044254878326e-06, 'epoch': 8.630252100840336}
>>> 2025-09-10 23:34:46,234 - INFO - >>> {'loss': 2.1928, 'grad_norm': 1.0476171970367432, 'learning_rate': 7.658485610532607e-06, 'epoch': 8.634453781512605}
>>> 2025-09-10 23:34:50,254 - INFO - >>> {'loss': 2.199, 'grad_norm': 1.0404298305511475, 'learning_rate': 7.649928780463292e-06, 'epoch': 8.638655462184873}
>>> 2025-09-10 23:34:54,211 - INFO - >>> {'loss': 2.2355, 'grad_norm': 0.9730587601661682, 'learning_rate': 7.64137377130047e-06, 'epoch': 8.642857142857142}
>>> 2025-09-10 23:34:57,603 - INFO - >>> {'loss': 2.1342, 'grad_norm': 1.1202367544174194, 'learning_rate': 7.63282058967283e-06, 'epoch': 8.647058823529411}
>>> 2025-09-10 23:35:01,108 - INFO - >>> {'loss': 2.1804, 'grad_norm': 1.111598253250122, 'learning_rate': 7.624269242207627e-06, 'epoch': 8.65126050420168}
>>> 2025-09-10 23:35:04,461 - INFO - >>> {'loss': 2.1785, 'grad_norm': 1.1622278690338135, 'learning_rate': 7.61571973553071e-06, 'epoch': 8.655462184873949}
>>> 2025-09-10 23:35:08,276 - INFO - >>> {'loss': 2.211, 'grad_norm': 1.1167738437652588, 'learning_rate': 7.607172076266498e-06, 'epoch': 8.659663865546218}
>>> 2025-09-10 23:35:11,572 - INFO - >>> {'loss': 2.2183, 'grad_norm': 1.1663031578063965, 'learning_rate': 7.598626271037972e-06, 'epoch': 8.663865546218487}
>>> 2025-09-10 23:35:14,131 - INFO - >>> {'loss': 2.1403, 'grad_norm': 1.3419783115386963, 'learning_rate': 7.590082326466682e-06, 'epoch': 8.668067226890756}
>>> 2025-09-10 23:35:17,561 - INFO - >>> {'loss': 2.2119, 'grad_norm': 1.067104458808899, 'learning_rate': 7.5815402491727405e-06, 'epoch': 8.672268907563025}
>>> 2025-09-10 23:35:21,372 - INFO - >>> {'loss': 2.2246, 'grad_norm': 0.9488223195075989, 'learning_rate': 7.573000045774803e-06, 'epoch': 8.676470588235293}
>>> 2025-09-10 23:35:24,579 - INFO - >>> {'loss': 2.2533, 'grad_norm': 1.1885830163955688, 'learning_rate': 7.564461722890082e-06, 'epoch': 8.680672268907562}
>>> 2025-09-10 23:35:28,019 - INFO - >>> {'loss': 2.1709, 'grad_norm': 1.1092826128005981, 'learning_rate': 7.555925287134325e-06, 'epoch': 8.684873949579831}
>>> 2025-09-10 23:35:31,836 - INFO - >>> {'loss': 2.1114, 'grad_norm': 1.0456429719924927, 'learning_rate': 7.547390745121822e-06, 'epoch': 8.6890756302521}
>>> 2025-09-10 23:35:34,854 - INFO - >>> {'loss': 2.1719, 'grad_norm': 1.2254328727722168, 'learning_rate': 7.538858103465401e-06, 'epoch': 8.693277310924369}
>>> 2025-09-10 23:35:38,178 - INFO - >>> {'loss': 2.2238, 'grad_norm': 1.1013370752334595, 'learning_rate': 7.530327368776405e-06, 'epoch': 8.697478991596638}
>>> 2025-09-10 23:35:42,249 - INFO - >>> {'loss': 2.2832, 'grad_norm': 1.1577695608139038, 'learning_rate': 7.52179854766471e-06, 'epoch': 8.701680672268907}
>>> 2025-09-10 23:35:46,284 - INFO - >>> {'loss': 2.1756, 'grad_norm': 1.1800557374954224, 'learning_rate': 7.5132716467387e-06, 'epoch': 8.705882352941176}
>>> 2025-09-10 23:35:50,219 - INFO - >>> {'loss': 2.1999, 'grad_norm': 0.987118661403656, 'learning_rate': 7.504746672605282e-06, 'epoch': 8.710084033613445}
>>> 2025-09-10 23:35:53,552 - INFO - >>> {'loss': 2.1344, 'grad_norm': 1.074009895324707, 'learning_rate': 7.496223631869867e-06, 'epoch': 8.714285714285714}
>>> 2025-09-10 23:35:57,629 - INFO - >>> {'loss': 2.3163, 'grad_norm': 1.081221580505371, 'learning_rate': 7.487702531136361e-06, 'epoch': 8.718487394957982}
>>> 2025-09-10 23:36:01,178 - INFO - >>> {'loss': 2.1429, 'grad_norm': 1.056266188621521, 'learning_rate': 7.479183377007169e-06, 'epoch': 8.722689075630251}
>>> 2025-09-10 23:36:04,548 - INFO - >>> {'loss': 2.2014, 'grad_norm': 1.1417933702468872, 'learning_rate': 7.470666176083193e-06, 'epoch': 8.72689075630252}
>>> 2025-09-10 23:36:07,436 - INFO - >>> {'loss': 2.0834, 'grad_norm': 1.1504844427108765, 'learning_rate': 7.462150934963822e-06, 'epoch': 8.731092436974789}
>>> 2025-09-10 23:36:10,978 - INFO - >>> {'loss': 2.0874, 'grad_norm': 1.1500681638717651, 'learning_rate': 7.453637660246919e-06, 'epoch': 8.735294117647058}
>>> 2025-09-10 23:36:14,343 - INFO - >>> {'loss': 2.145, 'grad_norm': 1.002969741821289, 'learning_rate': 7.445126358528832e-06, 'epoch': 8.739495798319329}
>>> 2025-09-10 23:36:17,789 - INFO - >>> {'loss': 2.2035, 'grad_norm': 1.243050217628479, 'learning_rate': 7.436617036404366e-06, 'epoch': 8.743697478991596}
>>> 2025-09-10 23:36:21,284 - INFO - >>> {'loss': 2.2275, 'grad_norm': 1.1025141477584839, 'learning_rate': 7.428109700466813e-06, 'epoch': 8.747899159663866}
>>> 2025-09-10 23:36:25,024 - INFO - >>> {'loss': 2.1833, 'grad_norm': 1.1286180019378662, 'learning_rate': 7.419604357307912e-06, 'epoch': 8.752100840336134}
>>> 2025-09-10 23:36:28,453 - INFO - >>> {'loss': 2.3435, 'grad_norm': 1.1335272789001465, 'learning_rate': 7.411101013517858e-06, 'epoch': 8.756302521008404}
>>> 2025-09-10 23:36:31,972 - INFO - >>> {'loss': 2.1879, 'grad_norm': 1.0957945585250854, 'learning_rate': 7.4025996756853046e-06, 'epoch': 8.760504201680671}
>>> 2025-09-10 23:36:35,205 - INFO - >>> {'loss': 2.2196, 'grad_norm': 1.1869670152664185, 'learning_rate': 7.394100350397337e-06, 'epoch': 8.764705882352942}
>>> 2025-09-10 23:36:38,953 - INFO - >>> {'loss': 2.1815, 'grad_norm': 1.1063196659088135, 'learning_rate': 7.385603044239501e-06, 'epoch': 8.768907563025211}
>>> 2025-09-10 23:36:42,359 - INFO - >>> {'loss': 2.239, 'grad_norm': 1.2069144248962402, 'learning_rate': 7.377107763795764e-06, 'epoch': 8.77310924369748}
>>> 2025-09-10 23:36:45,918 - INFO - >>> {'loss': 2.1486, 'grad_norm': 1.0816066265106201, 'learning_rate': 7.368614515648524e-06, 'epoch': 8.777310924369749}
>>> 2025-09-10 23:36:49,630 - INFO - >>> {'loss': 2.1255, 'grad_norm': 1.0660606622695923, 'learning_rate': 7.360123306378608e-06, 'epoch': 8.781512605042018}
>>> 2025-09-10 23:36:52,709 - INFO - >>> {'loss': 2.1004, 'grad_norm': 1.1512566804885864, 'learning_rate': 7.35163414256527e-06, 'epoch': 8.785714285714286}
>>> 2025-09-10 23:36:56,478 - INFO - >>> {'loss': 2.1888, 'grad_norm': 1.0446689128875732, 'learning_rate': 7.34314703078616e-06, 'epoch': 8.789915966386555}
>>> 2025-09-10 23:36:59,505 - INFO - >>> {'loss': 2.1548, 'grad_norm': 1.0735589265823364, 'learning_rate': 7.33466197761736e-06, 'epoch': 8.794117647058824}
>>> 2025-09-10 23:37:02,637 - INFO - >>> {'loss': 2.2735, 'grad_norm': 1.2216094732284546, 'learning_rate': 7.326178989633337e-06, 'epoch': 8.798319327731093}
>>> 2025-09-10 23:37:06,193 - INFO - >>> {'loss': 2.1572, 'grad_norm': 1.103325366973877, 'learning_rate': 7.317698073406978e-06, 'epoch': 8.802521008403362}
>>> 2025-09-10 23:37:09,960 - INFO - >>> {'loss': 2.1931, 'grad_norm': 0.993082582950592, 'learning_rate': 7.309219235509546e-06, 'epoch': 8.806722689075631}
>>> 2025-09-10 23:37:14,107 - INFO - >>> {'loss': 2.1337, 'grad_norm': 1.066638708114624, 'learning_rate': 7.3007424825107075e-06, 'epoch': 8.8109243697479}
>>> 2025-09-10 23:37:17,358 - INFO - >>> {'loss': 2.2396, 'grad_norm': 1.2005246877670288, 'learning_rate': 7.2922678209785045e-06, 'epoch': 8.815126050420169}
>>> 2025-09-10 23:37:21,009 - INFO - >>> {'loss': 2.1336, 'grad_norm': 1.07840895652771, 'learning_rate': 7.283795257479359e-06, 'epoch': 8.819327731092438}
>>> 2025-09-10 23:37:24,444 - INFO - >>> {'loss': 2.2712, 'grad_norm': 1.0894016027450562, 'learning_rate': 7.275324798578079e-06, 'epoch': 8.823529411764707}
>>> 2025-09-10 23:37:28,654 - INFO - >>> {'loss': 2.2477, 'grad_norm': 1.117271065711975, 'learning_rate': 7.266856450837826e-06, 'epoch': 8.827731092436975}
>>> 2025-09-10 23:37:32,542 - INFO - >>> {'loss': 2.2777, 'grad_norm': 1.1366779804229736, 'learning_rate': 7.2583902208201375e-06, 'epoch': 8.831932773109244}
>>> 2025-09-10 23:37:36,064 - INFO - >>> {'loss': 2.2136, 'grad_norm': 1.0422109365463257, 'learning_rate': 7.249926115084898e-06, 'epoch': 8.836134453781513}
>>> 2025-09-10 23:37:39,948 - INFO - >>> {'loss': 2.1994, 'grad_norm': 1.065879464149475, 'learning_rate': 7.241464140190362e-06, 'epoch': 8.840336134453782}
>>> 2025-09-10 23:37:43,631 - INFO - >>> {'loss': 2.1597, 'grad_norm': 1.0998196601867676, 'learning_rate': 7.233004302693122e-06, 'epoch': 8.844537815126051}
>>> 2025-09-10 23:37:47,146 - INFO - >>> {'loss': 2.2298, 'grad_norm': 1.0793479681015015, 'learning_rate': 7.224546609148113e-06, 'epoch': 8.84873949579832}
>>> 2025-09-10 23:37:50,771 - INFO - >>> {'loss': 2.1447, 'grad_norm': 1.1118484735488892, 'learning_rate': 7.216091066108619e-06, 'epoch': 8.852941176470589}
>>> 2025-09-10 23:37:54,059 - INFO - >>> {'loss': 2.1648, 'grad_norm': 1.1412192583084106, 'learning_rate': 7.207637680126242e-06, 'epoch': 8.857142857142858}
>>> 2025-09-10 23:37:57,619 - INFO - >>> {'loss': 2.1787, 'grad_norm': 1.0629206895828247, 'learning_rate': 7.199186457750931e-06, 'epoch': 8.861344537815127}
>>> 2025-09-10 23:38:01,398 - INFO - >>> {'loss': 2.1256, 'grad_norm': 1.038313627243042, 'learning_rate': 7.190737405530948e-06, 'epoch': 8.865546218487395}
>>> 2025-09-10 23:38:05,029 - INFO - >>> {'loss': 2.1599, 'grad_norm': 1.0548336505889893, 'learning_rate': 7.18229053001287e-06, 'epoch': 8.869747899159664}
>>> 2025-09-10 23:38:08,678 - INFO - >>> {'loss': 2.1996, 'grad_norm': 1.2702680826187134, 'learning_rate': 7.173845837741595e-06, 'epoch': 8.873949579831933}
>>> 2025-09-10 23:38:12,687 - INFO - >>> {'loss': 2.1279, 'grad_norm': 1.0627918243408203, 'learning_rate': 7.165403335260331e-06, 'epoch': 8.878151260504202}
>>> 2025-09-10 23:38:16,672 - INFO - >>> {'loss': 2.2362, 'grad_norm': 1.216394066810608, 'learning_rate': 7.156963029110581e-06, 'epoch': 8.882352941176471}
>>> 2025-09-10 23:38:19,854 - INFO - >>> {'loss': 2.1983, 'grad_norm': 1.1285592317581177, 'learning_rate': 7.148524925832152e-06, 'epoch': 8.88655462184874}
>>> 2025-09-10 23:38:23,379 - INFO - >>> {'loss': 2.1629, 'grad_norm': 1.0348219871520996, 'learning_rate': 7.140089031963142e-06, 'epoch': 8.890756302521009}
>>> 2025-09-10 23:38:26,974 - INFO - >>> {'loss': 2.2151, 'grad_norm': 1.1120778322219849, 'learning_rate': 7.131655354039933e-06, 'epoch': 8.894957983193278}
>>> 2025-09-10 23:38:29,935 - INFO - >>> {'loss': 2.1563, 'grad_norm': 1.1237549781799316, 'learning_rate': 7.1232238985972005e-06, 'epoch': 8.899159663865547}
>>> 2025-09-10 23:38:33,913 - INFO - >>> {'loss': 2.2357, 'grad_norm': 1.1150898933410645, 'learning_rate': 7.114794672167891e-06, 'epoch': 8.903361344537815}
>>> 2025-09-10 23:38:36,783 - INFO - >>> {'loss': 2.2185, 'grad_norm': 1.2169443368911743, 'learning_rate': 7.1063676812832235e-06, 'epoch': 8.907563025210084}
>>> 2025-09-10 23:38:40,875 - INFO - >>> {'loss': 2.2734, 'grad_norm': 1.0318762063980103, 'learning_rate': 7.097942932472686e-06, 'epoch': 8.911764705882353}
>>> 2025-09-10 23:38:44,927 - INFO - >>> {'loss': 2.1051, 'grad_norm': 0.9765294194221497, 'learning_rate': 7.089520432264032e-06, 'epoch': 8.915966386554622}
>>> 2025-09-10 23:38:48,145 - INFO - >>> {'loss': 2.2621, 'grad_norm': 1.0543957948684692, 'learning_rate': 7.081100187183267e-06, 'epoch': 8.920168067226891}
>>> 2025-09-10 23:38:51,197 - INFO - >>> {'loss': 2.2147, 'grad_norm': 1.0419189929962158, 'learning_rate': 7.072682203754658e-06, 'epoch': 8.92436974789916}
>>> 2025-09-10 23:38:54,189 - INFO - >>> {'loss': 2.2625, 'grad_norm': 1.1840548515319824, 'learning_rate': 7.064266488500707e-06, 'epoch': 8.928571428571429}
>>> 2025-09-10 23:38:57,257 - INFO - >>> {'loss': 2.2229, 'grad_norm': 1.1409869194030762, 'learning_rate': 7.0558530479421676e-06, 'epoch': 8.932773109243698}
>>> 2025-09-10 23:39:00,667 - INFO - >>> {'loss': 2.1792, 'grad_norm': 1.1694645881652832, 'learning_rate': 7.047441888598032e-06, 'epoch': 8.936974789915967}
>>> 2025-09-10 23:39:04,535 - INFO - >>> {'loss': 2.2595, 'grad_norm': 1.232322335243225, 'learning_rate': 7.03903301698552e-06, 'epoch': 8.941176470588236}
>>> 2025-09-10 23:39:07,381 - INFO - >>> {'loss': 2.1061, 'grad_norm': 1.1576523780822754, 'learning_rate': 7.030626439620081e-06, 'epoch': 8.945378151260504}
>>> 2025-09-10 23:39:10,689 - INFO - >>> {'loss': 2.2814, 'grad_norm': 1.2669068574905396, 'learning_rate': 7.022222163015381e-06, 'epoch': 8.949579831932773}
>>> 2025-09-10 23:39:14,673 - INFO - >>> {'loss': 2.106, 'grad_norm': 0.9827777147293091, 'learning_rate': 7.013820193683315e-06, 'epoch': 8.953781512605042}
>>> 2025-09-10 23:39:18,470 - INFO - >>> {'loss': 2.2783, 'grad_norm': 1.1932027339935303, 'learning_rate': 7.005420538133985e-06, 'epoch': 8.957983193277311}
>>> 2025-09-10 23:39:21,378 - INFO - >>> {'loss': 2.1326, 'grad_norm': 1.4263670444488525, 'learning_rate': 6.9970232028756925e-06, 'epoch': 8.96218487394958}
>>> 2025-09-10 23:39:25,106 - INFO - >>> {'loss': 2.2334, 'grad_norm': 1.0782709121704102, 'learning_rate': 6.988628194414953e-06, 'epoch': 8.966386554621849}
>>> 2025-09-10 23:39:28,856 - INFO - >>> {'loss': 2.2271, 'grad_norm': 1.029926061630249, 'learning_rate': 6.980235519256468e-06, 'epoch': 8.970588235294118}
>>> 2025-09-10 23:39:32,146 - INFO - >>> {'loss': 2.1909, 'grad_norm': 1.068615436553955, 'learning_rate': 6.9718451839031455e-06, 'epoch': 8.974789915966387}
>>> 2025-09-10 23:39:35,483 - INFO - >>> {'loss': 2.2215, 'grad_norm': 1.1325907707214355, 'learning_rate': 6.963457194856064e-06, 'epoch': 8.978991596638656}
>>> 2025-09-10 23:39:39,573 - INFO - >>> {'loss': 2.2635, 'grad_norm': 1.060660481452942, 'learning_rate': 6.955071558614497e-06, 'epoch': 8.983193277310924}
>>> 2025-09-10 23:39:42,579 - INFO - >>> {'loss': 2.1752, 'grad_norm': 1.1239076852798462, 'learning_rate': 6.946688281675884e-06, 'epoch': 8.987394957983193}
>>> 2025-09-10 23:39:45,579 - INFO - >>> {'loss': 2.2253, 'grad_norm': 1.1860949993133545, 'learning_rate': 6.9383073705358495e-06, 'epoch': 8.991596638655462}
>>> 2025-09-10 23:39:48,876 - INFO - >>> {'loss': 2.1961, 'grad_norm': 1.1195855140686035, 'learning_rate': 6.929928831688177e-06, 'epoch': 8.995798319327731}
>>> 2025-09-10 23:39:52,291 - INFO - >>> {'loss': 2.0405, 'grad_norm': 1.190096139907837, 'learning_rate': 6.921552671624807e-06, 'epoch': 9.0}
>>> 2025-09-10 23:39:56,349 - INFO - >>> {'loss': 2.4095, 'grad_norm': 1.0656309127807617, 'learning_rate': 6.9131788968358435e-06, 'epoch': 9.004201680672269}
>>> 2025-09-10 23:39:59,772 - INFO - >>> {'loss': 2.1031, 'grad_norm': 1.045452356338501, 'learning_rate': 6.904807513809548e-06, 'epoch': 9.008403361344538}
>>> 2025-09-10 23:40:03,137 - INFO - >>> {'loss': 2.2361, 'grad_norm': 1.1061079502105713, 'learning_rate': 6.896438529032317e-06, 'epoch': 9.012605042016807}
>>> 2025-09-10 23:40:07,203 - INFO - >>> {'loss': 2.3135, 'grad_norm': 1.237962245941162, 'learning_rate': 6.888071948988695e-06, 'epoch': 9.016806722689076}
>>> 2025-09-10 23:40:10,317 - INFO - >>> {'loss': 2.1914, 'grad_norm': 1.0844157934188843, 'learning_rate': 6.87970778016136e-06, 'epoch': 9.021008403361344}
>>> 2025-09-10 23:40:14,046 - INFO - >>> {'loss': 2.2341, 'grad_norm': 1.1510334014892578, 'learning_rate': 6.871346029031124e-06, 'epoch': 9.025210084033613}
>>> 2025-09-10 23:40:17,140 - INFO - >>> {'loss': 2.1326, 'grad_norm': 1.151558756828308, 'learning_rate': 6.862986702076931e-06, 'epoch': 9.029411764705882}
>>> 2025-09-10 23:40:20,333 - INFO - >>> {'loss': 2.3124, 'grad_norm': 1.2085061073303223, 'learning_rate': 6.854629805775836e-06, 'epoch': 9.033613445378151}
>>> 2025-09-10 23:40:23,802 - INFO - >>> {'loss': 2.2621, 'grad_norm': 1.0122687816619873, 'learning_rate': 6.846275346603017e-06, 'epoch': 9.03781512605042}
>>> 2025-09-10 23:40:27,873 - INFO - >>> {'loss': 2.2576, 'grad_norm': 0.9716600179672241, 'learning_rate': 6.837923331031761e-06, 'epoch': 9.042016806722689}
>>> 2025-09-10 23:40:31,368 - INFO - >>> {'loss': 2.1376, 'grad_norm': 1.0942977666854858, 'learning_rate': 6.829573765533466e-06, 'epoch': 9.046218487394958}
>>> 2025-09-10 23:40:34,486 - INFO - >>> {'loss': 2.0392, 'grad_norm': 1.294793725013733, 'learning_rate': 6.821226656577628e-06, 'epoch': 9.050420168067227}
>>> 2025-09-10 23:40:37,721 - INFO - >>> {'loss': 2.2654, 'grad_norm': 1.1930986642837524, 'learning_rate': 6.812882010631839e-06, 'epoch': 9.054621848739496}
>>> 2025-09-10 23:40:41,222 - INFO - >>> {'loss': 2.1977, 'grad_norm': 1.1328743696212769, 'learning_rate': 6.804539834161788e-06, 'epoch': 9.058823529411764}
>>> 2025-09-10 23:40:44,729 - INFO - >>> {'loss': 2.201, 'grad_norm': 1.172045111656189, 'learning_rate': 6.796200133631237e-06, 'epoch': 9.063025210084033}
>>> 2025-09-10 23:40:48,707 - INFO - >>> {'loss': 2.2424, 'grad_norm': 0.9559938311576843, 'learning_rate': 6.787862915502049e-06, 'epoch': 9.067226890756302}
>>> 2025-09-10 23:40:52,769 - INFO - >>> {'loss': 2.1334, 'grad_norm': 1.0627795457839966, 'learning_rate': 6.77952818623415e-06, 'epoch': 9.071428571428571}
>>> 2025-09-10 23:40:55,960 - INFO - >>> {'loss': 2.3187, 'grad_norm': 1.1416873931884766, 'learning_rate': 6.771195952285541e-06, 'epoch': 9.07563025210084}
>>> 2025-09-10 23:40:59,296 - INFO - >>> {'loss': 2.3424, 'grad_norm': 1.1892765760421753, 'learning_rate': 6.762866220112286e-06, 'epoch': 9.079831932773109}
>>> 2025-09-10 23:41:03,233 - INFO - >>> {'loss': 2.1695, 'grad_norm': 1.1278127431869507, 'learning_rate': 6.7545389961685205e-06, 'epoch': 9.084033613445378}
>>> 2025-09-10 23:41:06,982 - INFO - >>> {'loss': 2.274, 'grad_norm': 1.1261136531829834, 'learning_rate': 6.74621428690643e-06, 'epoch': 9.088235294117647}
>>> 2025-09-10 23:41:10,750 - INFO - >>> {'loss': 2.2977, 'grad_norm': 1.1076397895812988, 'learning_rate': 6.737892098776247e-06, 'epoch': 9.092436974789916}
>>> 2025-09-10 23:41:14,558 - INFO - >>> {'loss': 2.2549, 'grad_norm': 1.0887221097946167, 'learning_rate': 6.7295724382262595e-06, 'epoch': 9.096638655462185}
>>> 2025-09-10 23:41:17,459 - INFO - >>> {'loss': 2.1184, 'grad_norm': 1.2240816354751587, 'learning_rate': 6.721255311702789e-06, 'epoch': 9.100840336134453}
>>> 2025-09-10 23:41:20,408 - INFO - >>> {'loss': 2.1395, 'grad_norm': 1.1925126314163208, 'learning_rate': 6.712940725650201e-06, 'epoch': 9.105042016806722}
>>> 2025-09-10 23:41:23,928 - INFO - >>> {'loss': 2.2174, 'grad_norm': 1.099764347076416, 'learning_rate': 6.70462868651089e-06, 'epoch': 9.109243697478991}
>>> 2025-09-10 23:41:27,938 - INFO - >>> {'loss': 2.3287, 'grad_norm': 0.9884406328201294, 'learning_rate': 6.696319200725273e-06, 'epoch': 9.11344537815126}
>>> 2025-09-10 23:41:31,353 - INFO - >>> {'loss': 2.1298, 'grad_norm': 1.1162554025650024, 'learning_rate': 6.688012274731791e-06, 'epoch': 9.117647058823529}
>>> 2025-09-10 23:41:34,601 - INFO - >>> {'loss': 2.1935, 'grad_norm': 1.1044625043869019, 'learning_rate': 6.679707914966909e-06, 'epoch': 9.121848739495798}
>>> 2025-09-10 23:41:38,151 - INFO - >>> {'loss': 2.1623, 'grad_norm': 1.1465805768966675, 'learning_rate': 6.6714061278650875e-06, 'epoch': 9.126050420168067}
>>> 2025-09-10 23:41:41,822 - INFO - >>> {'loss': 2.2193, 'grad_norm': 0.9984817504882812, 'learning_rate': 6.663106919858813e-06, 'epoch': 9.130252100840336}
>>> 2025-09-10 23:41:45,158 - INFO - >>> {'loss': 2.1913, 'grad_norm': 1.1462675333023071, 'learning_rate': 6.654810297378556e-06, 'epoch': 9.134453781512605}
>>> 2025-09-10 23:41:48,675 - INFO - >>> {'loss': 2.1293, 'grad_norm': 1.0563580989837646, 'learning_rate': 6.6465162668527895e-06, 'epoch': 9.138655462184873}
>>> 2025-09-10 23:41:52,637 - INFO - >>> {'loss': 2.2384, 'grad_norm': 1.258374810218811, 'learning_rate': 6.638224834707991e-06, 'epoch': 9.142857142857142}
>>> 2025-09-10 23:41:55,676 - INFO - >>> {'loss': 2.2828, 'grad_norm': 1.2709437608718872, 'learning_rate': 6.629936007368607e-06, 'epoch': 9.147058823529411}
>>> 2025-09-10 23:41:59,523 - INFO - >>> {'loss': 2.1652, 'grad_norm': 1.1575324535369873, 'learning_rate': 6.6216497912570696e-06, 'epoch': 9.15126050420168}
>>> 2025-09-10 23:42:03,223 - INFO - >>> {'loss': 2.2106, 'grad_norm': 1.1613377332687378, 'learning_rate': 6.613366192793792e-06, 'epoch': 9.155462184873949}
>>> 2025-09-10 23:42:06,661 - INFO - >>> {'loss': 2.1141, 'grad_norm': 1.057453989982605, 'learning_rate': 6.605085218397164e-06, 'epoch': 9.159663865546218}
>>> 2025-09-10 23:42:09,971 - INFO - >>> {'loss': 2.1352, 'grad_norm': 1.117872953414917, 'learning_rate': 6.596806874483528e-06, 'epoch': 9.163865546218487}
>>> 2025-09-10 23:42:13,303 - INFO - >>> {'loss': 2.1153, 'grad_norm': 1.1238607168197632, 'learning_rate': 6.588531167467203e-06, 'epoch': 9.168067226890756}
>>> 2025-09-10 23:42:16,321 - INFO - >>> {'loss': 2.1224, 'grad_norm': 1.2699954509735107, 'learning_rate': 6.58025810376045e-06, 'epoch': 9.172268907563025}
>>> 2025-09-10 23:42:20,204 - INFO - >>> {'loss': 2.2531, 'grad_norm': 1.0557667016983032, 'learning_rate': 6.571987689773492e-06, 'epoch': 9.176470588235293}
>>> 2025-09-10 23:42:23,178 - INFO - >>> {'loss': 2.2388, 'grad_norm': 1.2328208684921265, 'learning_rate': 6.563719931914504e-06, 'epoch': 9.180672268907562}
>>> 2025-09-10 23:42:26,591 - INFO - >>> {'loss': 2.2677, 'grad_norm': 1.1075717210769653, 'learning_rate': 6.555454836589588e-06, 'epoch': 9.184873949579831}
>>> 2025-09-10 23:42:30,260 - INFO - >>> {'loss': 2.0951, 'grad_norm': 1.1316883563995361, 'learning_rate': 6.547192410202795e-06, 'epoch': 9.1890756302521}
>>> 2025-09-10 23:42:33,471 - INFO - >>> {'loss': 2.2123, 'grad_norm': 1.278320550918579, 'learning_rate': 6.538932659156097e-06, 'epoch': 9.193277310924369}
>>> 2025-09-10 23:42:36,783 - INFO - >>> {'loss': 2.1868, 'grad_norm': 1.176246166229248, 'learning_rate': 6.530675589849407e-06, 'epoch': 9.197478991596638}
>>> 2025-09-10 23:42:40,726 - INFO - >>> {'loss': 2.2559, 'grad_norm': 1.0802825689315796, 'learning_rate': 6.522421208680553e-06, 'epoch': 9.201680672268907}
>>> 2025-09-10 23:42:43,753 - INFO - >>> {'loss': 2.2226, 'grad_norm': 1.2363002300262451, 'learning_rate': 6.514169522045275e-06, 'epoch': 9.205882352941176}
>>> 2025-09-10 23:42:46,755 - INFO - >>> {'loss': 2.1593, 'grad_norm': 1.0554651021957397, 'learning_rate': 6.50592053633723e-06, 'epoch': 9.210084033613445}
>>> 2025-09-10 23:42:50,563 - INFO - >>> {'loss': 2.2284, 'grad_norm': 0.9259711503982544, 'learning_rate': 6.49767425794799e-06, 'epoch': 9.214285714285714}
>>> 2025-09-10 23:42:54,630 - INFO - >>> {'loss': 2.1756, 'grad_norm': 1.11854088306427, 'learning_rate': 6.489430693267014e-06, 'epoch': 9.218487394957982}
>>> 2025-09-10 23:42:57,959 - INFO - >>> {'loss': 2.1036, 'grad_norm': 1.0930050611495972, 'learning_rate': 6.48118984868167e-06, 'epoch': 9.222689075630251}
>>> 2025-09-10 23:43:01,478 - INFO - >>> {'loss': 2.2661, 'grad_norm': 1.032450556755066, 'learning_rate': 6.4729517305772146e-06, 'epoch': 9.22689075630252}
>>> 2025-09-10 23:43:05,328 - INFO - >>> {'loss': 2.2432, 'grad_norm': 1.083119511604309, 'learning_rate': 6.464716345336784e-06, 'epoch': 9.231092436974789}
>>> 2025-09-10 23:43:09,049 - INFO - >>> {'loss': 2.2922, 'grad_norm': 1.1433573961257935, 'learning_rate': 6.456483699341418e-06, 'epoch': 9.235294117647058}
>>> 2025-09-10 23:43:12,863 - INFO - >>> {'loss': 2.2598, 'grad_norm': 1.1500695943832397, 'learning_rate': 6.448253798970012e-06, 'epoch': 9.239495798319327}
>>> 2025-09-10 23:43:15,921 - INFO - >>> {'loss': 2.2749, 'grad_norm': 1.1551909446716309, 'learning_rate': 6.440026650599344e-06, 'epoch': 9.243697478991596}
>>> 2025-09-10 23:43:20,466 - INFO - >>> {'loss': 2.2666, 'grad_norm': 1.04273521900177, 'learning_rate': 6.431802260604054e-06, 'epoch': 9.247899159663866}
>>> 2025-09-10 23:43:23,714 - INFO - >>> {'loss': 2.2254, 'grad_norm': 1.08427095413208, 'learning_rate': 6.423580635356658e-06, 'epoch': 9.252100840336134}
>>> 2025-09-10 23:43:26,926 - INFO - >>> {'loss': 2.2311, 'grad_norm': 1.1233841180801392, 'learning_rate': 6.415361781227516e-06, 'epoch': 9.256302521008404}
>>> 2025-09-10 23:43:30,171 - INFO - >>> {'loss': 2.2204, 'grad_norm': 1.083132028579712, 'learning_rate': 6.407145704584845e-06, 'epoch': 9.260504201680673}
>>> 2025-09-10 23:43:33,063 - INFO - >>> {'loss': 2.2109, 'grad_norm': 1.0722676515579224, 'learning_rate': 6.398932411794711e-06, 'epoch': 9.264705882352942}
>>> 2025-09-10 23:43:36,050 - INFO - >>> {'loss': 2.2488, 'grad_norm': 1.275552749633789, 'learning_rate': 6.390721909221019e-06, 'epoch': 9.268907563025211}
>>> 2025-09-10 23:43:39,227 - INFO - >>> {'loss': 2.1969, 'grad_norm': 1.0418128967285156, 'learning_rate': 6.382514203225525e-06, 'epoch': 9.27310924369748}
>>> 2025-09-10 23:43:42,179 - INFO - >>> {'loss': 2.1924, 'grad_norm': 1.1758075952529907, 'learning_rate': 6.374309300167799e-06, 'epoch': 9.277310924369749}
>>> 2025-09-10 23:43:46,213 - INFO - >>> {'loss': 2.2176, 'grad_norm': 1.125627875328064, 'learning_rate': 6.366107206405255e-06, 'epoch': 9.281512605042018}
>>> 2025-09-10 23:43:49,633 - INFO - >>> {'loss': 2.1898, 'grad_norm': 1.2044891119003296, 'learning_rate': 6.357907928293119e-06, 'epoch': 9.285714285714286}
>>> 2025-09-10 23:43:52,844 - INFO - >>> {'loss': 2.2043, 'grad_norm': 1.0861833095550537, 'learning_rate': 6.3497114721844445e-06, 'epoch': 9.289915966386555}
>>> 2025-09-10 23:43:56,124 - INFO - >>> {'loss': 2.1919, 'grad_norm': 1.0701888799667358, 'learning_rate': 6.3415178444300955e-06, 'epoch': 9.294117647058824}
>>> 2025-09-10 23:44:00,145 - INFO - >>> {'loss': 2.2081, 'grad_norm': 1.1491410732269287, 'learning_rate': 6.333327051378739e-06, 'epoch': 9.298319327731093}
>>> 2025-09-10 23:44:03,503 - INFO - >>> {'loss': 2.2569, 'grad_norm': 1.1379810571670532, 'learning_rate': 6.325139099376854e-06, 'epoch': 9.302521008403362}
>>> 2025-09-10 23:44:07,530 - INFO - >>> {'loss': 2.2252, 'grad_norm': 1.1993186473846436, 'learning_rate': 6.316953994768708e-06, 'epoch': 9.306722689075631}
>>> 2025-09-10 23:44:11,096 - INFO - >>> {'loss': 2.1799, 'grad_norm': 1.2129594087600708, 'learning_rate': 6.308771743896376e-06, 'epoch': 9.3109243697479}
>>> 2025-09-10 23:44:14,855 - INFO - >>> {'loss': 2.1835, 'grad_norm': 0.9945082068443298, 'learning_rate': 6.300592353099713e-06, 'epoch': 9.315126050420169}
>>> 2025-09-10 23:44:18,705 - INFO - >>> {'loss': 2.2324, 'grad_norm': 1.1183384656906128, 'learning_rate': 6.2924158287163564e-06, 'epoch': 9.319327731092438}
>>> 2025-09-10 23:44:22,514 - INFO - >>> {'loss': 2.1605, 'grad_norm': 1.2909133434295654, 'learning_rate': 6.284242177081723e-06, 'epoch': 9.323529411764707}
>>> 2025-09-10 23:44:26,534 - INFO - >>> {'loss': 2.1952, 'grad_norm': 1.2172127962112427, 'learning_rate': 6.276071404529012e-06, 'epoch': 9.327731092436975}
>>> 2025-09-10 23:44:30,027 - INFO - >>> {'loss': 2.2098, 'grad_norm': 1.029563069343567, 'learning_rate': 6.267903517389184e-06, 'epoch': 9.331932773109244}
>>> 2025-09-10 23:44:33,090 - INFO - >>> {'loss': 2.2822, 'grad_norm': 1.1193121671676636, 'learning_rate': 6.259738521990965e-06, 'epoch': 9.336134453781513}
>>> 2025-09-10 23:44:37,000 - INFO - >>> {'loss': 2.2245, 'grad_norm': 1.0990310907363892, 'learning_rate': 6.251576424660841e-06, 'epoch': 9.340336134453782}
>>> 2025-09-10 23:44:40,075 - INFO - >>> {'loss': 2.242, 'grad_norm': 1.1626129150390625, 'learning_rate': 6.243417231723052e-06, 'epoch': 9.344537815126051}
>>> 2025-09-10 23:44:43,465 - INFO - >>> {'loss': 2.2812, 'grad_norm': 1.2344682216644287, 'learning_rate': 6.2352609494995884e-06, 'epoch': 9.34873949579832}
>>> 2025-09-10 23:44:47,395 - INFO - >>> {'loss': 2.2773, 'grad_norm': 1.043330430984497, 'learning_rate': 6.22710758431019e-06, 'epoch': 9.352941176470589}
>>> 2025-09-10 23:44:50,609 - INFO - >>> {'loss': 2.1848, 'grad_norm': 1.092997431755066, 'learning_rate': 6.218957142472326e-06, 'epoch': 9.357142857142858}
>>> 2025-09-10 23:44:54,571 - INFO - >>> {'loss': 2.1811, 'grad_norm': 1.0892162322998047, 'learning_rate': 6.210809630301204e-06, 'epoch': 9.361344537815127}
>>> 2025-09-10 23:44:57,925 - INFO - >>> {'loss': 2.11, 'grad_norm': 1.1106154918670654, 'learning_rate': 6.202665054109773e-06, 'epoch': 9.365546218487395}
>>> 2025-09-10 23:45:01,807 - INFO - >>> {'loss': 2.0447, 'grad_norm': 1.0563563108444214, 'learning_rate': 6.19452342020869e-06, 'epoch': 9.369747899159664}
>>> 2025-09-10 23:45:05,852 - INFO - >>> {'loss': 2.2302, 'grad_norm': 1.059630036354065, 'learning_rate': 6.186384734906344e-06, 'epoch': 9.373949579831933}
>>> 2025-09-10 23:45:09,074 - INFO - >>> {'loss': 2.1844, 'grad_norm': 1.1094409227371216, 'learning_rate': 6.178249004508832e-06, 'epoch': 9.378151260504202}
>>> 2025-09-10 23:45:12,435 - INFO - >>> {'loss': 2.2143, 'grad_norm': 1.1641534566879272, 'learning_rate': 6.170116235319965e-06, 'epoch': 9.382352941176471}
>>> 2025-09-10 23:45:16,165 - INFO - >>> {'loss': 2.2006, 'grad_norm': 1.098880410194397, 'learning_rate': 6.161986433641263e-06, 'epoch': 9.38655462184874}
>>> 2025-09-10 23:45:19,504 - INFO - >>> {'loss': 2.2669, 'grad_norm': 1.213771104812622, 'learning_rate': 6.1538596057719405e-06, 'epoch': 9.390756302521009}
>>> 2025-09-10 23:45:22,500 - INFO - >>> {'loss': 2.2888, 'grad_norm': 1.1335829496383667, 'learning_rate': 6.145735758008913e-06, 'epoch': 9.394957983193278}
>>> 2025-09-10 23:45:25,428 - INFO - >>> {'loss': 2.1314, 'grad_norm': 1.1061718463897705, 'learning_rate': 6.137614896646775e-06, 'epoch': 9.399159663865547}
>>> 2025-09-10 23:45:28,643 - INFO - >>> {'loss': 2.1961, 'grad_norm': 1.1671937704086304, 'learning_rate': 6.129497027977829e-06, 'epoch': 9.403361344537815}
>>> 2025-09-10 23:45:32,586 - INFO - >>> {'loss': 2.2549, 'grad_norm': 1.070115566253662, 'learning_rate': 6.121382158292039e-06, 'epoch': 9.407563025210084}
>>> 2025-09-10 23:45:35,820 - INFO - >>> {'loss': 2.178, 'grad_norm': 1.256565809249878, 'learning_rate': 6.113270293877056e-06, 'epoch': 9.411764705882353}
>>> 2025-09-10 23:45:39,461 - INFO - >>> {'loss': 2.2348, 'grad_norm': 1.0581053495407104, 'learning_rate': 6.105161441018192e-06, 'epoch': 9.415966386554622}
>>> 2025-09-10 23:45:42,459 - INFO - >>> {'loss': 2.166, 'grad_norm': 1.1391671895980835, 'learning_rate': 6.097055605998439e-06, 'epoch': 9.420168067226891}
>>> 2025-09-10 23:45:46,049 - INFO - >>> {'loss': 2.2812, 'grad_norm': 1.223203182220459, 'learning_rate': 6.088952795098442e-06, 'epoch': 9.42436974789916}
>>> 2025-09-10 23:45:48,987 - INFO - >>> {'loss': 2.2351, 'grad_norm': 1.0606772899627686, 'learning_rate': 6.080853014596503e-06, 'epoch': 9.428571428571429}
>>> 2025-09-10 23:45:52,266 - INFO - >>> {'loss': 2.1244, 'grad_norm': 1.2290929555892944, 'learning_rate': 6.0727562707685804e-06, 'epoch': 9.432773109243698}
>>> 2025-09-10 23:45:55,294 - INFO - >>> {'loss': 2.182, 'grad_norm': 1.1530061960220337, 'learning_rate': 6.064662569888272e-06, 'epoch': 9.436974789915967}
>>> 2025-09-10 23:45:58,550 - INFO - >>> {'loss': 2.1891, 'grad_norm': 1.129194736480713, 'learning_rate': 6.056571918226829e-06, 'epoch': 9.441176470588236}
>>> 2025-09-10 23:46:01,938 - INFO - >>> {'loss': 2.2535, 'grad_norm': 1.1759734153747559, 'learning_rate': 6.048484322053132e-06, 'epoch': 9.445378151260504}
>>> 2025-09-10 23:46:05,772 - INFO - >>> {'loss': 2.1666, 'grad_norm': 1.0696403980255127, 'learning_rate': 6.040399787633695e-06, 'epoch': 9.449579831932773}
>>> 2025-09-10 23:46:09,492 - INFO - >>> {'loss': 2.1253, 'grad_norm': 1.0968655347824097, 'learning_rate': 6.032318321232658e-06, 'epoch': 9.453781512605042}
>>> 2025-09-10 23:46:13,174 - INFO - >>> {'loss': 2.1892, 'grad_norm': 1.047967553138733, 'learning_rate': 6.0242399291117924e-06, 'epoch': 9.457983193277311}
>>> 2025-09-10 23:46:16,522 - INFO - >>> {'loss': 2.127, 'grad_norm': 1.077918529510498, 'learning_rate': 6.016164617530478e-06, 'epoch': 9.46218487394958}
>>> 2025-09-10 23:46:20,037 - INFO - >>> {'loss': 2.0847, 'grad_norm': 1.0464859008789062, 'learning_rate': 6.008092392745712e-06, 'epoch': 9.466386554621849}
>>> 2025-09-10 23:46:23,385 - INFO - >>> {'loss': 2.1564, 'grad_norm': 1.1377191543579102, 'learning_rate': 6.000023261012099e-06, 'epoch': 9.470588235294118}
>>> 2025-09-10 23:46:26,531 - INFO - >>> {'loss': 2.135, 'grad_norm': 1.1617428064346313, 'learning_rate': 5.991957228581844e-06, 'epoch': 9.474789915966387}
>>> 2025-09-10 23:46:29,532 - INFO - >>> {'loss': 2.0427, 'grad_norm': 1.204047441482544, 'learning_rate': 5.98389430170476e-06, 'epoch': 9.478991596638656}
>>> 2025-09-10 23:46:32,800 - INFO - >>> {'loss': 2.2472, 'grad_norm': 1.094079613685608, 'learning_rate': 5.975834486628242e-06, 'epoch': 9.483193277310924}
>>> 2025-09-10 23:46:36,307 - INFO - >>> {'loss': 2.0995, 'grad_norm': 1.0522501468658447, 'learning_rate': 5.967777789597284e-06, 'epoch': 9.487394957983193}
>>> 2025-09-10 23:46:40,331 - INFO - >>> {'loss': 2.1655, 'grad_norm': 1.1113474369049072, 'learning_rate': 5.9597242168544534e-06, 'epoch': 9.491596638655462}
>>> 2025-09-10 23:46:43,961 - INFO - >>> {'loss': 2.2123, 'grad_norm': 1.0030381679534912, 'learning_rate': 5.951673774639909e-06, 'epoch': 9.495798319327731}
>>> 2025-09-10 23:46:47,620 - INFO - >>> {'loss': 2.1938, 'grad_norm': 1.13080632686615, 'learning_rate': 5.943626469191372e-06, 'epoch': 9.5}
>>> 2025-09-10 23:46:51,238 - INFO - >>> {'loss': 2.2277, 'grad_norm': 1.0956029891967773, 'learning_rate': 5.935582306744145e-06, 'epoch': 9.504201680672269}
>>> 2025-09-10 23:46:55,212 - INFO - >>> {'loss': 2.1825, 'grad_norm': 1.2490768432617188, 'learning_rate': 5.927541293531085e-06, 'epoch': 9.508403361344538}
>>> 2025-09-10 23:46:58,587 - INFO - >>> {'loss': 2.1394, 'grad_norm': 1.2440522909164429, 'learning_rate': 5.91950343578261e-06, 'epoch': 9.512605042016807}
>>> 2025-09-10 23:47:01,602 - INFO - >>> {'loss': 2.1452, 'grad_norm': 1.112878680229187, 'learning_rate': 5.9114687397267046e-06, 'epoch': 9.516806722689076}
>>> 2025-09-10 23:47:05,536 - INFO - >>> {'loss': 2.3197, 'grad_norm': 1.1557505130767822, 'learning_rate': 5.903437211588888e-06, 'epoch': 9.521008403361344}
>>> 2025-09-10 23:47:09,086 - INFO - >>> {'loss': 2.2475, 'grad_norm': 1.087811827659607, 'learning_rate': 5.8954088575922376e-06, 'epoch': 9.525210084033613}
>>> 2025-09-10 23:47:12,789 - INFO - >>> {'loss': 2.1307, 'grad_norm': 1.104468584060669, 'learning_rate': 5.887383683957357e-06, 'epoch': 9.529411764705882}
>>> 2025-09-10 23:47:15,821 - INFO - >>> {'loss': 2.2627, 'grad_norm': 1.2331899404525757, 'learning_rate': 5.8793616969024034e-06, 'epoch': 9.533613445378151}
>>> 2025-09-10 23:47:18,793 - INFO - >>> {'loss': 2.1659, 'grad_norm': 1.2724897861480713, 'learning_rate': 5.871342902643054e-06, 'epoch': 9.53781512605042}
>>> 2025-09-10 23:47:21,780 - INFO - >>> {'loss': 2.2765, 'grad_norm': 1.268121600151062, 'learning_rate': 5.863327307392509e-06, 'epoch': 9.542016806722689}
>>> 2025-09-10 23:47:25,198 - INFO - >>> {'loss': 2.2206, 'grad_norm': 1.1469368934631348, 'learning_rate': 5.855314917361501e-06, 'epoch': 9.546218487394958}
>>> 2025-09-10 23:47:29,043 - INFO - >>> {'loss': 2.0785, 'grad_norm': 1.3374814987182617, 'learning_rate': 5.847305738758266e-06, 'epoch': 9.550420168067227}
>>> 2025-09-10 23:47:32,789 - INFO - >>> {'loss': 2.1834, 'grad_norm': 1.1039317846298218, 'learning_rate': 5.839299777788563e-06, 'epoch': 9.554621848739496}
>>> 2025-09-10 23:47:36,313 - INFO - >>> {'loss': 2.3281, 'grad_norm': 1.151829481124878, 'learning_rate': 5.831297040655655e-06, 'epoch': 9.558823529411764}
>>> 2025-09-10 23:47:39,311 - INFO - >>> {'loss': 2.1452, 'grad_norm': 1.3661059141159058, 'learning_rate': 5.8232975335603056e-06, 'epoch': 9.563025210084033}
>>> 2025-09-10 23:47:42,657 - INFO - >>> {'loss': 2.112, 'grad_norm': 1.0373393297195435, 'learning_rate': 5.815301262700768e-06, 'epoch': 9.567226890756302}
>>> 2025-09-10 23:47:45,601 - INFO - >>> {'loss': 2.1954, 'grad_norm': 1.083984375, 'learning_rate': 5.807308234272803e-06, 'epoch': 9.571428571428571}
>>> 2025-09-10 23:47:49,028 - INFO - >>> {'loss': 2.1864, 'grad_norm': 1.2216811180114746, 'learning_rate': 5.799318454469656e-06, 'epoch': 9.57563025210084}
>>> 2025-09-10 23:47:52,159 - INFO - >>> {'loss': 2.1682, 'grad_norm': 1.1334208250045776, 'learning_rate': 5.7913319294820405e-06, 'epoch': 9.579831932773109}
>>> 2025-09-10 23:47:55,901 - INFO - >>> {'loss': 2.2076, 'grad_norm': 1.0296417474746704, 'learning_rate': 5.78334866549816e-06, 'epoch': 9.584033613445378}
>>> 2025-09-10 23:47:58,756 - INFO - >>> {'loss': 2.1491, 'grad_norm': 1.0465933084487915, 'learning_rate': 5.7753686687036956e-06, 'epoch': 9.588235294117647}
>>> 2025-09-10 23:48:01,990 - INFO - >>> {'loss': 2.2477, 'grad_norm': 1.1406365633010864, 'learning_rate': 5.767391945281786e-06, 'epoch': 9.592436974789916}
>>> 2025-09-10 23:48:05,099 - INFO - >>> {'loss': 2.2851, 'grad_norm': 1.1207839250564575, 'learning_rate': 5.759418501413041e-06, 'epoch': 9.596638655462185}
>>> 2025-09-10 23:48:08,540 - INFO - >>> {'loss': 2.0938, 'grad_norm': 1.1210613250732422, 'learning_rate': 5.75144834327553e-06, 'epoch': 9.600840336134453}
>>> 2025-09-10 23:48:11,915 - INFO - >>> {'loss': 2.1923, 'grad_norm': 1.1313421726226807, 'learning_rate': 5.74348147704476e-06, 'epoch': 9.605042016806722}
>>> 2025-09-10 23:48:14,741 - INFO - >>> {'loss': 2.2723, 'grad_norm': 1.2619339227676392, 'learning_rate': 5.735517908893715e-06, 'epoch': 9.609243697478991}
>>> 2025-09-10 23:48:18,365 - INFO - >>> {'loss': 2.1373, 'grad_norm': 1.1188527345657349, 'learning_rate': 5.727557644992809e-06, 'epoch': 9.61344537815126}
>>> 2025-09-10 23:48:21,377 - INFO - >>> {'loss': 2.2955, 'grad_norm': 1.1782642602920532, 'learning_rate': 5.719600691509888e-06, 'epoch': 9.617647058823529}
>>> 2025-09-10 23:48:24,839 - INFO - >>> {'loss': 2.2092, 'grad_norm': 1.1016627550125122, 'learning_rate': 5.711647054610238e-06, 'epoch': 9.621848739495798}
>>> 2025-09-10 23:48:28,281 - INFO - >>> {'loss': 2.2884, 'grad_norm': 1.1678855419158936, 'learning_rate': 5.7036967404565945e-06, 'epoch': 9.626050420168067}
>>> 2025-09-10 23:48:31,337 - INFO - >>> {'loss': 2.2657, 'grad_norm': 1.2580434083938599, 'learning_rate': 5.695749755209089e-06, 'epoch': 9.630252100840336}
>>> 2025-09-10 23:48:34,649 - INFO - >>> {'loss': 2.1842, 'grad_norm': 1.1496208906173706, 'learning_rate': 5.6878061050252894e-06, 'epoch': 9.634453781512605}
>>> 2025-09-10 23:48:37,880 - INFO - >>> {'loss': 2.2046, 'grad_norm': 1.2146425247192383, 'learning_rate': 5.679865796060178e-06, 'epoch': 9.638655462184873}
>>> 2025-09-10 23:48:41,338 - INFO - >>> {'loss': 2.1696, 'grad_norm': 1.0878199338912964, 'learning_rate': 5.671928834466147e-06, 'epoch': 9.642857142857142}
>>> 2025-09-10 23:48:44,651 - INFO - >>> {'loss': 2.2412, 'grad_norm': 1.1338136196136475, 'learning_rate': 5.663995226392995e-06, 'epoch': 9.647058823529411}
>>> 2025-09-10 23:48:48,260 - INFO - >>> {'loss': 2.2367, 'grad_norm': 1.199738621711731, 'learning_rate': 5.656064977987923e-06, 'epoch': 9.65126050420168}
>>> 2025-09-10 23:48:51,546 - INFO - >>> {'loss': 2.28, 'grad_norm': 1.0065271854400635, 'learning_rate': 5.64813809539553e-06, 'epoch': 9.655462184873949}
>>> 2025-09-10 23:48:55,362 - INFO - >>> {'loss': 2.2513, 'grad_norm': 1.0926576852798462, 'learning_rate': 5.640214584757795e-06, 'epoch': 9.659663865546218}
>>> 2025-09-10 23:48:59,311 - INFO - >>> {'loss': 2.1979, 'grad_norm': 1.0469129085540771, 'learning_rate': 5.632294452214106e-06, 'epoch': 9.663865546218487}
>>> 2025-09-10 23:49:03,168 - INFO - >>> {'loss': 2.1874, 'grad_norm': 1.2558995485305786, 'learning_rate': 5.624377703901223e-06, 'epoch': 9.668067226890756}
>>> 2025-09-10 23:49:06,850 - INFO - >>> {'loss': 2.1877, 'grad_norm': 1.0533815622329712, 'learning_rate': 5.616464345953274e-06, 'epoch': 9.672268907563025}
>>> 2025-09-10 23:49:10,766 - INFO - >>> {'loss': 2.1753, 'grad_norm': 1.1162222623825073, 'learning_rate': 5.608554384501772e-06, 'epoch': 9.676470588235293}
>>> 2025-09-10 23:49:14,169 - INFO - >>> {'loss': 2.1857, 'grad_norm': 1.0332392454147339, 'learning_rate': 5.6006478256755955e-06, 'epoch': 9.680672268907562}
>>> 2025-09-10 23:49:17,242 - INFO - >>> {'loss': 2.1698, 'grad_norm': 1.1212009191513062, 'learning_rate': 5.592744675600987e-06, 'epoch': 9.684873949579831}
>>> 2025-09-10 23:49:21,277 - INFO - >>> {'loss': 2.214, 'grad_norm': 1.1028366088867188, 'learning_rate': 5.584844940401548e-06, 'epoch': 9.6890756302521}
>>> 2025-09-10 23:49:24,399 - INFO - >>> {'loss': 2.1886, 'grad_norm': 1.162272334098816, 'learning_rate': 5.576948626198235e-06, 'epoch': 9.693277310924369}
>>> 2025-09-10 23:49:27,406 - INFO - >>> {'loss': 2.2251, 'grad_norm': 1.0993351936340332, 'learning_rate': 5.569055739109341e-06, 'epoch': 9.697478991596638}
>>> 2025-09-10 23:49:31,158 - INFO - >>> {'loss': 2.2161, 'grad_norm': 1.090141773223877, 'learning_rate': 5.561166285250526e-06, 'epoch': 9.701680672268907}
>>> 2025-09-10 23:49:34,540 - INFO - >>> {'loss': 2.2372, 'grad_norm': 1.1135814189910889, 'learning_rate': 5.553280270734778e-06, 'epoch': 9.705882352941176}
>>> 2025-09-10 23:49:37,607 - INFO - >>> {'loss': 2.1613, 'grad_norm': 1.1675254106521606, 'learning_rate': 5.545397701672412e-06, 'epoch': 9.710084033613445}
>>> 2025-09-10 23:49:41,673 - INFO - >>> {'loss': 2.1832, 'grad_norm': 1.3766998052597046, 'learning_rate': 5.537518584171088e-06, 'epoch': 9.714285714285714}
>>> 2025-09-10 23:49:44,980 - INFO - >>> {'loss': 2.2494, 'grad_norm': 1.1741055250167847, 'learning_rate': 5.529642924335783e-06, 'epoch': 9.718487394957982}
>>> 2025-09-10 23:49:48,479 - INFO - >>> {'loss': 2.1499, 'grad_norm': 1.0724023580551147, 'learning_rate': 5.521770728268798e-06, 'epoch': 9.722689075630251}
>>> 2025-09-10 23:49:52,442 - INFO - >>> {'loss': 2.197, 'grad_norm': 1.072845220565796, 'learning_rate': 5.51390200206975e-06, 'epoch': 9.72689075630252}
>>> 2025-09-10 23:49:55,695 - INFO - >>> {'loss': 2.1877, 'grad_norm': 1.0897855758666992, 'learning_rate': 5.506036751835566e-06, 'epoch': 9.731092436974789}
>>> 2025-09-10 23:49:59,122 - INFO - >>> {'loss': 2.1108, 'grad_norm': 1.1622456312179565, 'learning_rate': 5.4981749836604804e-06, 'epoch': 9.735294117647058}
>>> 2025-09-10 23:50:02,263 - INFO - >>> {'loss': 2.1631, 'grad_norm': 1.1378191709518433, 'learning_rate': 5.490316703636029e-06, 'epoch': 9.739495798319329}
>>> 2025-09-10 23:50:05,315 - INFO - >>> {'loss': 2.1101, 'grad_norm': 1.1867936849594116, 'learning_rate': 5.48246191785105e-06, 'epoch': 9.743697478991596}
>>> 2025-09-10 23:50:09,035 - INFO - >>> {'loss': 2.15, 'grad_norm': 1.1246898174285889, 'learning_rate': 5.474610632391668e-06, 'epoch': 9.747899159663866}
>>> 2025-09-10 23:50:12,432 - INFO - >>> {'loss': 2.2368, 'grad_norm': 1.0100315809249878, 'learning_rate': 5.466762853341292e-06, 'epoch': 9.752100840336134}
>>> 2025-09-10 23:50:15,695 - INFO - >>> {'loss': 2.1, 'grad_norm': 1.0922011137008667, 'learning_rate': 5.458918586780624e-06, 'epoch': 9.756302521008404}
>>> 2025-09-10 23:50:18,814 - INFO - >>> {'loss': 2.1409, 'grad_norm': 1.0624202489852905, 'learning_rate': 5.451077838787639e-06, 'epoch': 9.760504201680671}
>>> 2025-09-10 23:50:22,551 - INFO - >>> {'loss': 2.1689, 'grad_norm': 1.073102593421936, 'learning_rate': 5.443240615437586e-06, 'epoch': 9.764705882352942}
>>> 2025-09-10 23:50:26,563 - INFO - >>> {'loss': 2.2065, 'grad_norm': 1.0870441198349, 'learning_rate': 5.435406922802984e-06, 'epoch': 9.768907563025211}
>>> 2025-09-10 23:50:29,799 - INFO - >>> {'loss': 2.1119, 'grad_norm': 1.135872483253479, 'learning_rate': 5.427576766953615e-06, 'epoch': 9.77310924369748}
>>> 2025-09-10 23:50:33,923 - INFO - >>> {'loss': 2.2159, 'grad_norm': 1.047922134399414, 'learning_rate': 5.419750153956522e-06, 'epoch': 9.777310924369749}
>>> 2025-09-10 23:50:37,573 - INFO - >>> {'loss': 2.1702, 'grad_norm': 1.1492611169815063, 'learning_rate': 5.411927089876003e-06, 'epoch': 9.781512605042018}
>>> 2025-09-10 23:50:40,966 - INFO - >>> {'loss': 2.1498, 'grad_norm': 1.1032288074493408, 'learning_rate': 5.40410758077361e-06, 'epoch': 9.785714285714286}
>>> 2025-09-10 23:50:44,379 - INFO - >>> {'loss': 2.1916, 'grad_norm': 1.1002117395401, 'learning_rate': 5.396291632708126e-06, 'epoch': 9.789915966386555}
>>> 2025-09-10 23:50:48,231 - INFO - >>> {'loss': 2.2071, 'grad_norm': 1.1691441535949707, 'learning_rate': 5.388479251735584e-06, 'epoch': 9.794117647058824}
>>> 2025-09-10 23:50:52,369 - INFO - >>> {'loss': 2.1561, 'grad_norm': 1.0388734340667725, 'learning_rate': 5.380670443909268e-06, 'epoch': 9.798319327731093}
>>> 2025-09-10 23:50:56,064 - INFO - >>> {'loss': 2.1591, 'grad_norm': 1.0851894617080688, 'learning_rate': 5.372865215279668e-06, 'epoch': 9.802521008403362}
>>> 2025-09-10 23:50:59,494 - INFO - >>> {'loss': 2.2577, 'grad_norm': 1.1119328737258911, 'learning_rate': 5.365063571894513e-06, 'epoch': 9.806722689075631}
>>> 2025-09-10 23:51:02,848 - INFO - >>> {'loss': 2.1697, 'grad_norm': 1.1018297672271729, 'learning_rate': 5.357265519798754e-06, 'epoch': 9.8109243697479}
>>> 2025-09-10 23:51:06,282 - INFO - >>> {'loss': 2.1186, 'grad_norm': 1.2059670686721802, 'learning_rate': 5.34947106503456e-06, 'epoch': 9.815126050420169}
>>> 2025-09-10 23:51:09,920 - INFO - >>> {'loss': 2.168, 'grad_norm': 1.0330227613449097, 'learning_rate': 5.341680213641311e-06, 'epoch': 9.819327731092438}
>>> 2025-09-10 23:51:13,217 - INFO - >>> {'loss': 2.1386, 'grad_norm': 1.0993599891662598, 'learning_rate': 5.333892971655593e-06, 'epoch': 9.823529411764707}
>>> 2025-09-10 23:51:17,137 - INFO - >>> {'loss': 2.1322, 'grad_norm': 1.1360257863998413, 'learning_rate': 5.326109345111204e-06, 'epoch': 9.827731092436975}
>>> 2025-09-10 23:51:20,496 - INFO - >>> {'loss': 2.2049, 'grad_norm': 1.1322300434112549, 'learning_rate': 5.318329340039119e-06, 'epoch': 9.831932773109244}
>>> 2025-09-10 23:51:23,880 - INFO - >>> {'loss': 2.1999, 'grad_norm': 1.1339356899261475, 'learning_rate': 5.310552962467539e-06, 'epoch': 9.836134453781513}
>>> 2025-09-10 23:51:27,194 - INFO - >>> {'loss': 2.1659, 'grad_norm': 1.1518820524215698, 'learning_rate': 5.302780218421828e-06, 'epoch': 9.840336134453782}
>>> 2025-09-10 23:51:30,267 - INFO - >>> {'loss': 2.1105, 'grad_norm': 1.1798291206359863, 'learning_rate': 5.295011113924544e-06, 'epoch': 9.844537815126051}
>>> 2025-09-10 23:51:34,176 - INFO - >>> {'loss': 2.1756, 'grad_norm': 1.0910881757736206, 'learning_rate': 5.287245654995428e-06, 'epoch': 9.84873949579832}
>>> 2025-09-10 23:51:37,749 - INFO - >>> {'loss': 2.1868, 'grad_norm': 1.162725567817688, 'learning_rate': 5.2794838476513925e-06, 'epoch': 9.852941176470589}
>>> 2025-09-10 23:51:41,041 - INFO - >>> {'loss': 2.1811, 'grad_norm': 1.117172122001648, 'learning_rate': 5.27172569790652e-06, 'epoch': 9.857142857142858}
>>> 2025-09-10 23:51:44,762 - INFO - >>> {'loss': 2.2371, 'grad_norm': 1.1135931015014648, 'learning_rate': 5.263971211772062e-06, 'epoch': 9.861344537815127}
>>> 2025-09-10 23:51:48,911 - INFO - >>> {'loss': 2.1296, 'grad_norm': 1.0374709367752075, 'learning_rate': 5.256220395256428e-06, 'epoch': 9.865546218487395}
>>> 2025-09-10 23:51:52,179 - INFO - >>> {'loss': 2.1478, 'grad_norm': 1.1142791509628296, 'learning_rate': 5.2484732543651885e-06, 'epoch': 9.869747899159664}
>>> 2025-09-10 23:51:55,474 - INFO - >>> {'loss': 2.2402, 'grad_norm': 1.2359695434570312, 'learning_rate': 5.240729795101061e-06, 'epoch': 9.873949579831933}
>>> 2025-09-10 23:51:59,225 - INFO - >>> {'loss': 2.1784, 'grad_norm': 1.0956403017044067, 'learning_rate': 5.232990023463918e-06, 'epoch': 9.878151260504202}
>>> 2025-09-10 23:52:02,852 - INFO - >>> {'loss': 2.1993, 'grad_norm': 1.1694374084472656, 'learning_rate': 5.225253945450759e-06, 'epoch': 9.882352941176471}
>>> 2025-09-10 23:52:06,595 - INFO - >>> {'loss': 2.1424, 'grad_norm': 1.166725993156433, 'learning_rate': 5.217521567055734e-06, 'epoch': 9.88655462184874}
>>> 2025-09-10 23:52:09,705 - INFO - >>> {'loss': 2.2513, 'grad_norm': 1.1171077489852905, 'learning_rate': 5.209792894270138e-06, 'epoch': 9.890756302521009}
>>> 2025-09-10 23:52:13,465 - INFO - >>> {'loss': 2.1553, 'grad_norm': 1.0638619661331177, 'learning_rate': 5.2020679330823665e-06, 'epoch': 9.894957983193278}
>>> 2025-09-10 23:52:17,284 - INFO - >>> {'loss': 2.2327, 'grad_norm': 1.1725060939788818, 'learning_rate': 5.194346689477959e-06, 'epoch': 9.899159663865547}
>>> 2025-09-10 23:52:21,051 - INFO - >>> {'loss': 2.2744, 'grad_norm': 1.0400182008743286, 'learning_rate': 5.18662916943957e-06, 'epoch': 9.903361344537815}
>>> 2025-09-10 23:52:24,345 - INFO - >>> {'loss': 2.1929, 'grad_norm': 1.029836893081665, 'learning_rate': 5.178915378946967e-06, 'epoch': 9.907563025210084}
>>> 2025-09-10 23:52:28,107 - INFO - >>> {'loss': 2.2215, 'grad_norm': 1.170900583267212, 'learning_rate': 5.1712053239770284e-06, 'epoch': 9.911764705882353}
>>> 2025-09-10 23:52:31,199 - INFO - >>> {'loss': 2.1823, 'grad_norm': 1.2556548118591309, 'learning_rate': 5.163499010503741e-06, 'epoch': 9.915966386554622}
>>> 2025-09-10 23:52:34,741 - INFO - >>> {'loss': 2.1326, 'grad_norm': 1.1754285097122192, 'learning_rate': 5.155796444498191e-06, 'epoch': 9.920168067226891}
>>> 2025-09-10 23:52:37,741 - INFO - >>> {'loss': 2.2026, 'grad_norm': 1.2159757614135742, 'learning_rate': 5.1480976319285525e-06, 'epoch': 9.92436974789916}
>>> 2025-09-10 23:52:41,069 - INFO - >>> {'loss': 2.1876, 'grad_norm': 1.1553431749343872, 'learning_rate': 5.140402578760112e-06, 'epoch': 9.928571428571429}
>>> 2025-09-10 23:52:44,939 - INFO - >>> {'loss': 2.1553, 'grad_norm': 1.0268266201019287, 'learning_rate': 5.132711290955219e-06, 'epoch': 9.932773109243698}
>>> 2025-09-10 23:52:48,541 - INFO - >>> {'loss': 2.1453, 'grad_norm': 1.0659565925598145, 'learning_rate': 5.125023774473321e-06, 'epoch': 9.936974789915967}
>>> 2025-09-10 23:52:52,294 - INFO - >>> {'loss': 2.1213, 'grad_norm': 1.0939034223556519, 'learning_rate': 5.11734003527094e-06, 'epoch': 9.941176470588236}
>>> 2025-09-10 23:52:55,881 - INFO - >>> {'loss': 2.2444, 'grad_norm': 1.0368014574050903, 'learning_rate': 5.109660079301668e-06, 'epoch': 9.945378151260504}
>>> 2025-09-10 23:52:59,397 - INFO - >>> {'loss': 2.1964, 'grad_norm': 1.0790456533432007, 'learning_rate': 5.1019839125161706e-06, 'epoch': 9.949579831932773}
>>> 2025-09-10 23:53:03,037 - INFO - >>> {'loss': 2.2058, 'grad_norm': 1.0868276357650757, 'learning_rate': 5.0943115408621716e-06, 'epoch': 9.953781512605042}
>>> 2025-09-10 23:53:06,391 - INFO - >>> {'loss': 2.1061, 'grad_norm': 1.296362042427063, 'learning_rate': 5.086642970284464e-06, 'epoch': 9.957983193277311}
>>> 2025-09-10 23:53:09,727 - INFO - >>> {'loss': 2.1701, 'grad_norm': 1.1216957569122314, 'learning_rate': 5.078978206724876e-06, 'epoch': 9.96218487394958}
>>> 2025-09-10 23:53:12,812 - INFO - >>> {'loss': 2.1758, 'grad_norm': 1.1123435497283936, 'learning_rate': 5.0713172561223115e-06, 'epoch': 9.966386554621849}
>>> 2025-09-10 23:53:15,875 - INFO - >>> {'loss': 2.1776, 'grad_norm': 1.1029332876205444, 'learning_rate': 5.063660124412706e-06, 'epoch': 9.970588235294118}
>>> 2025-09-10 23:53:19,849 - INFO - >>> {'loss': 2.2799, 'grad_norm': 1.044236660003662, 'learning_rate': 5.056006817529031e-06, 'epoch': 9.974789915966387}
>>> 2025-09-10 23:53:22,965 - INFO - >>> {'loss': 2.2484, 'grad_norm': 1.2912187576293945, 'learning_rate': 5.048357341401302e-06, 'epoch': 9.978991596638656}
>>> 2025-09-10 23:53:26,687 - INFO - >>> {'loss': 2.1885, 'grad_norm': 1.092682123184204, 'learning_rate': 5.0407117019565745e-06, 'epoch': 9.983193277310924}
>>> 2025-09-10 23:53:30,743 - INFO - >>> {'loss': 2.2022, 'grad_norm': 1.0689959526062012, 'learning_rate': 5.0330699051189104e-06, 'epoch': 9.987394957983193}
>>> 2025-09-10 23:53:34,621 - INFO - >>> {'loss': 2.2406, 'grad_norm': 1.2382892370224, 'learning_rate': 5.0254319568094125e-06, 'epoch': 9.991596638655462}
>>> 2025-09-10 23:53:38,357 - INFO - >>> {'loss': 2.1823, 'grad_norm': 1.0686098337173462, 'learning_rate': 5.017797862946192e-06, 'epoch': 9.995798319327731}
>>> 2025-09-10 23:53:41,416 - INFO - >>> {'loss': 2.3281, 'grad_norm': 1.184715986251831, 'learning_rate': 5.010167629444378e-06, 'epoch': 10.0}
>>> 2025-09-10 23:53:45,263 - INFO - >>> {'loss': 2.1206, 'grad_norm': 1.1346688270568848, 'learning_rate': 5.002541262216106e-06, 'epoch': 10.004201680672269}
>>> 2025-09-10 23:53:48,610 - INFO - >>> {'loss': 2.2278, 'grad_norm': 1.0803173780441284, 'learning_rate': 4.9949187671705215e-06, 'epoch': 10.008403361344538}
>>> 2025-09-10 23:53:52,147 - INFO - >>> {'loss': 2.2315, 'grad_norm': 1.1871777772903442, 'learning_rate': 4.987300150213756e-06, 'epoch': 10.012605042016807}
>>> 2025-09-10 23:53:55,261 - INFO - >>> {'loss': 2.1224, 'grad_norm': 1.1491622924804688, 'learning_rate': 4.979685417248947e-06, 'epoch': 10.016806722689076}
>>> 2025-09-10 23:53:59,112 - INFO - >>> {'loss': 2.1741, 'grad_norm': 1.0319740772247314, 'learning_rate': 4.9720745741762325e-06, 'epoch': 10.021008403361344}
>>> 2025-09-10 23:54:02,909 - INFO - >>> {'loss': 2.3198, 'grad_norm': 1.1225639581680298, 'learning_rate': 4.96446762689271e-06, 'epoch': 10.025210084033613}
>>> 2025-09-10 23:54:06,000 - INFO - >>> {'loss': 2.1372, 'grad_norm': 1.1961002349853516, 'learning_rate': 4.956864581292479e-06, 'epoch': 10.029411764705882}
>>> 2025-09-10 23:54:09,322 - INFO - >>> {'loss': 2.2484, 'grad_norm': 1.1384153366088867, 'learning_rate': 4.94926544326661e-06, 'epoch': 10.033613445378151}
>>> 2025-09-10 23:54:13,078 - INFO - >>> {'loss': 2.2543, 'grad_norm': 1.2399296760559082, 'learning_rate': 4.941670218703145e-06, 'epoch': 10.03781512605042}
>>> 2025-09-10 23:54:16,295 - INFO - >>> {'loss': 2.1898, 'grad_norm': 1.1495839357376099, 'learning_rate': 4.9340789134870956e-06, 'epoch': 10.042016806722689}
>>> 2025-09-10 23:54:19,701 - INFO - >>> {'loss': 2.1678, 'grad_norm': 1.195348858833313, 'learning_rate': 4.9264915335004345e-06, 'epoch': 10.046218487394958}
>>> 2025-09-10 23:54:23,118 - INFO - >>> {'loss': 2.1011, 'grad_norm': 1.2038536071777344, 'learning_rate': 4.9189080846221e-06, 'epoch': 10.050420168067227}
>>> 2025-09-10 23:54:26,604 - INFO - >>> {'loss': 2.2127, 'grad_norm': 1.2445476055145264, 'learning_rate': 4.911328572727963e-06, 'epoch': 10.054621848739496}
>>> 2025-09-10 23:54:29,623 - INFO - >>> {'loss': 2.206, 'grad_norm': 1.1971805095672607, 'learning_rate': 4.903753003690873e-06, 'epoch': 10.058823529411764}
>>> 2025-09-10 23:54:32,620 - INFO - >>> {'loss': 2.2893, 'grad_norm': 1.1335248947143555, 'learning_rate': 4.896181383380611e-06, 'epoch': 10.063025210084033}
>>> 2025-09-10 23:54:35,718 - INFO - >>> {'loss': 2.2928, 'grad_norm': 1.1435078382492065, 'learning_rate': 4.888613717663891e-06, 'epoch': 10.067226890756302}
>>> 2025-09-10 23:54:38,688 - INFO - >>> {'loss': 2.1498, 'grad_norm': 1.204162359237671, 'learning_rate': 4.881050012404367e-06, 'epoch': 10.071428571428571}
>>> 2025-09-10 23:54:42,312 - INFO - >>> {'loss': 2.1449, 'grad_norm': 1.0914905071258545, 'learning_rate': 4.873490273462643e-06, 'epoch': 10.07563025210084}
>>> 2025-09-10 23:54:45,508 - INFO - >>> {'loss': 2.1792, 'grad_norm': 1.1185044050216675, 'learning_rate': 4.86593450669622e-06, 'epoch': 10.079831932773109}
>>> 2025-09-10 23:54:48,869 - INFO - >>> {'loss': 2.2217, 'grad_norm': 1.1873430013656616, 'learning_rate': 4.85838271795954e-06, 'epoch': 10.084033613445378}
>>> 2025-09-10 23:54:52,572 - INFO - >>> {'loss': 2.1974, 'grad_norm': 1.1731252670288086, 'learning_rate': 4.850834913103962e-06, 'epoch': 10.088235294117647}
>>> 2025-09-10 23:54:56,586 - INFO - >>> {'loss': 2.1394, 'grad_norm': 1.0712603330612183, 'learning_rate': 4.843291097977743e-06, 'epoch': 10.092436974789916}
>>> 2025-09-10 23:54:59,646 - INFO - >>> {'loss': 2.133, 'grad_norm': 1.1031007766723633, 'learning_rate': 4.835751278426071e-06, 'epoch': 10.096638655462185}
>>> 2025-09-10 23:55:03,036 - INFO - >>> {'loss': 2.1468, 'grad_norm': 1.2346677780151367, 'learning_rate': 4.828215460291026e-06, 'epoch': 10.100840336134453}
>>> 2025-09-10 23:55:06,278 - INFO - >>> {'loss': 2.1249, 'grad_norm': 1.2396365404129028, 'learning_rate': 4.820683649411582e-06, 'epoch': 10.105042016806722}
>>> 2025-09-10 23:55:09,719 - INFO - >>> {'loss': 2.1667, 'grad_norm': 1.1999261379241943, 'learning_rate': 4.813155851623615e-06, 'epoch': 10.109243697478991}
>>> 2025-09-10 23:55:13,491 - INFO - >>> {'loss': 2.086, 'grad_norm': 1.114516258239746, 'learning_rate': 4.8056320727599026e-06, 'epoch': 10.11344537815126}
>>> 2025-09-10 23:55:17,148 - INFO - >>> {'loss': 2.2054, 'grad_norm': 1.0420691967010498, 'learning_rate': 4.798112318650084e-06, 'epoch': 10.117647058823529}
>>> 2025-09-10 23:55:20,812 - INFO - >>> {'loss': 2.1248, 'grad_norm': 1.0872471332550049, 'learning_rate': 4.790596595120699e-06, 'epoch': 10.121848739495798}
>>> 2025-09-10 23:55:24,518 - INFO - >>> {'loss': 2.1835, 'grad_norm': 1.1936591863632202, 'learning_rate': 4.783084907995156e-06, 'epoch': 10.126050420168067}
>>> 2025-09-10 23:55:28,624 - INFO - >>> {'loss': 2.2595, 'grad_norm': 1.1701414585113525, 'learning_rate': 4.775577263093739e-06, 'epoch': 10.130252100840336}
>>> 2025-09-10 23:55:31,955 - INFO - >>> {'loss': 2.2261, 'grad_norm': 1.212114930152893, 'learning_rate': 4.7680736662336e-06, 'epoch': 10.134453781512605}
>>> 2025-09-10 23:55:35,255 - INFO - >>> {'loss': 2.1483, 'grad_norm': 1.1610528230667114, 'learning_rate': 4.760574123228753e-06, 'epoch': 10.138655462184873}
>>> 2025-09-10 23:55:39,164 - INFO - >>> {'loss': 2.2015, 'grad_norm': 1.0576856136322021, 'learning_rate': 4.7530786398900745e-06, 'epoch': 10.142857142857142}
>>> 2025-09-10 23:55:42,519 - INFO - >>> {'loss': 2.2298, 'grad_norm': 1.0573869943618774, 'learning_rate': 4.745587222025282e-06, 'epoch': 10.147058823529411}
>>> 2025-09-10 23:55:45,539 - INFO - >>> {'loss': 2.2114, 'grad_norm': 1.1379085779190063, 'learning_rate': 4.738099875438964e-06, 'epoch': 10.15126050420168}
>>> 2025-09-10 23:55:49,194 - INFO - >>> {'loss': 2.2397, 'grad_norm': 1.1025974750518799, 'learning_rate': 4.730616605932545e-06, 'epoch': 10.155462184873949}
>>> 2025-09-10 23:55:52,866 - INFO - >>> {'loss': 2.1266, 'grad_norm': 1.11202871799469, 'learning_rate': 4.72313741930428e-06, 'epoch': 10.159663865546218}
>>> 2025-09-10 23:55:56,838 - INFO - >>> {'loss': 2.246, 'grad_norm': 1.09386146068573, 'learning_rate': 4.715662321349276e-06, 'epoch': 10.163865546218487}
>>> 2025-09-10 23:56:00,253 - INFO - >>> {'loss': 2.1084, 'grad_norm': 1.1940220594406128, 'learning_rate': 4.708191317859463e-06, 'epoch': 10.168067226890756}
>>> 2025-09-10 23:56:03,452 - INFO - >>> {'loss': 2.1844, 'grad_norm': 1.2742780447006226, 'learning_rate': 4.700724414623603e-06, 'epoch': 10.172268907563025}
>>> 2025-09-10 23:56:06,946 - INFO - >>> {'loss': 2.2456, 'grad_norm': 1.1630632877349854, 'learning_rate': 4.6932616174272794e-06, 'epoch': 10.176470588235293}
>>> 2025-09-10 23:56:10,912 - INFO - >>> {'loss': 2.1751, 'grad_norm': 0.978152871131897, 'learning_rate': 4.6858029320528965e-06, 'epoch': 10.180672268907562}
>>> 2025-09-10 23:56:14,611 - INFO - >>> {'loss': 2.2102, 'grad_norm': 1.2152256965637207, 'learning_rate': 4.678348364279659e-06, 'epoch': 10.184873949579831}
>>> 2025-09-10 23:56:17,857 - INFO - >>> {'loss': 2.2617, 'grad_norm': 1.1487964391708374, 'learning_rate': 4.670897919883605e-06, 'epoch': 10.1890756302521}
>>> 2025-09-10 23:56:20,872 - INFO - >>> {'loss': 2.3088, 'grad_norm': 1.1520613431930542, 'learning_rate': 4.663451604637562e-06, 'epoch': 10.193277310924369}
>>> 2025-09-10 23:56:24,686 - INFO - >>> {'loss': 2.2585, 'grad_norm': 1.1027828454971313, 'learning_rate': 4.656009424311156e-06, 'epoch': 10.197478991596638}
>>> 2025-09-10 23:56:28,590 - INFO - >>> {'loss': 2.2645, 'grad_norm': 1.1343164443969727, 'learning_rate': 4.648571384670818e-06, 'epoch': 10.201680672268907}
>>> 2025-09-10 23:56:31,943 - INFO - >>> {'loss': 2.2341, 'grad_norm': 1.165021300315857, 'learning_rate': 4.6411374914797655e-06, 'epoch': 10.205882352941176}
>>> 2025-09-10 23:56:35,693 - INFO - >>> {'loss': 2.1943, 'grad_norm': 0.970964252948761, 'learning_rate': 4.633707750498004e-06, 'epoch': 10.210084033613445}
>>> 2025-09-10 23:56:39,660 - INFO - >>> {'loss': 2.1758, 'grad_norm': 1.1173323392868042, 'learning_rate': 4.6262821674823246e-06, 'epoch': 10.214285714285714}
>>> 2025-09-10 23:56:43,423 - INFO - >>> {'loss': 2.2346, 'grad_norm': 1.0831485986709595, 'learning_rate': 4.6188607481862925e-06, 'epoch': 10.218487394957982}
>>> 2025-09-10 23:56:46,455 - INFO - >>> {'loss': 2.2337, 'grad_norm': 1.1645606756210327, 'learning_rate': 4.6114434983602495e-06, 'epoch': 10.222689075630251}
>>> 2025-09-10 23:56:50,193 - INFO - >>> {'loss': 2.288, 'grad_norm': 0.9942561984062195, 'learning_rate': 4.604030423751306e-06, 'epoch': 10.22689075630252}
>>> 2025-09-10 23:56:53,419 - INFO - >>> {'loss': 2.136, 'grad_norm': 1.0602061748504639, 'learning_rate': 4.596621530103336e-06, 'epoch': 10.231092436974789}
>>> 2025-09-10 23:56:56,943 - INFO - >>> {'loss': 2.1519, 'grad_norm': 1.0979200601577759, 'learning_rate': 4.589216823156979e-06, 'epoch': 10.235294117647058}
>>> 2025-09-10 23:57:00,575 - INFO - >>> {'loss': 2.2004, 'grad_norm': 1.0700064897537231, 'learning_rate': 4.58181630864962e-06, 'epoch': 10.239495798319327}
>>> 2025-09-10 23:57:04,497 - INFO - >>> {'loss': 2.1169, 'grad_norm': 1.2375015020370483, 'learning_rate': 4.574419992315402e-06, 'epoch': 10.243697478991596}
>>> 2025-09-10 23:57:08,138 - INFO - >>> {'loss': 2.1524, 'grad_norm': 1.1271181106567383, 'learning_rate': 4.567027879885226e-06, 'epoch': 10.247899159663866}
>>> 2025-09-10 23:57:11,375 - INFO - >>> {'loss': 2.1701, 'grad_norm': 1.1635873317718506, 'learning_rate': 4.559639977086716e-06, 'epoch': 10.252100840336134}
>>> 2025-09-10 23:57:14,782 - INFO - >>> {'loss': 2.1944, 'grad_norm': 1.1011520624160767, 'learning_rate': 4.552256289644244e-06, 'epoch': 10.256302521008404}
>>> 2025-09-10 23:57:18,174 - INFO - >>> {'loss': 2.1552, 'grad_norm': 1.0386499166488647, 'learning_rate': 4.544876823278917e-06, 'epoch': 10.260504201680673}
>>> 2025-09-10 23:57:22,126 - INFO - >>> {'loss': 2.2539, 'grad_norm': 1.0654363632202148, 'learning_rate': 4.537501583708568e-06, 'epoch': 10.264705882352942}
>>> 2025-09-10 23:57:25,729 - INFO - >>> {'loss': 2.1039, 'grad_norm': 1.1550363302230835, 'learning_rate': 4.5301305766477574e-06, 'epoch': 10.268907563025211}
>>> 2025-09-10 23:57:28,614 - INFO - >>> {'loss': 2.2078, 'grad_norm': 1.1711866855621338, 'learning_rate': 4.522763807807769e-06, 'epoch': 10.27310924369748}
>>> 2025-09-10 23:57:31,967 - INFO - >>> {'loss': 2.2118, 'grad_norm': 1.3233492374420166, 'learning_rate': 4.515401282896588e-06, 'epoch': 10.277310924369749}
>>> 2025-09-10 23:57:35,913 - INFO - >>> {'loss': 2.2576, 'grad_norm': 1.0402183532714844, 'learning_rate': 4.508043007618932e-06, 'epoch': 10.281512605042018}
>>> 2025-09-10 23:57:39,698 - INFO - >>> {'loss': 2.1246, 'grad_norm': 1.0701674222946167, 'learning_rate': 4.500688987676217e-06, 'epoch': 10.285714285714286}
>>> 2025-09-10 23:57:43,436 - INFO - >>> {'loss': 2.2334, 'grad_norm': 1.0602524280548096, 'learning_rate': 4.493339228766553e-06, 'epoch': 10.289915966386555}
>>> 2025-09-10 23:57:46,756 - INFO - >>> {'loss': 2.1208, 'grad_norm': 1.0636669397354126, 'learning_rate': 4.485993736584762e-06, 'epoch': 10.294117647058824}
>>> 2025-09-10 23:57:50,034 - INFO - >>> {'loss': 2.1681, 'grad_norm': 1.22091805934906, 'learning_rate': 4.47865251682235e-06, 'epoch': 10.298319327731093}
>>> 2025-09-10 23:57:53,275 - INFO - >>> {'loss': 2.1989, 'grad_norm': 1.2087639570236206, 'learning_rate': 4.4713155751675195e-06, 'epoch': 10.302521008403362}
>>> 2025-09-10 23:57:56,970 - INFO - >>> {'loss': 2.2266, 'grad_norm': 1.1183364391326904, 'learning_rate': 4.463982917305155e-06, 'epoch': 10.306722689075631}
>>> 2025-09-10 23:58:00,251 - INFO - >>> {'loss': 2.3221, 'grad_norm': 1.1791044473648071, 'learning_rate': 4.456654548916823e-06, 'epoch': 10.3109243697479}
>>> 2025-09-10 23:58:03,354 - INFO - >>> {'loss': 2.1622, 'grad_norm': 1.1583448648452759, 'learning_rate': 4.449330475680767e-06, 'epoch': 10.315126050420169}
>>> 2025-09-10 23:58:07,371 - INFO - >>> {'loss': 2.1766, 'grad_norm': 1.0630053281784058, 'learning_rate': 4.442010703271897e-06, 'epoch': 10.319327731092438}
>>> 2025-09-10 23:58:11,356 - INFO - >>> {'loss': 2.2139, 'grad_norm': 1.0461355447769165, 'learning_rate': 4.434695237361798e-06, 'epoch': 10.323529411764707}
>>> 2025-09-10 23:58:15,211 - INFO - >>> {'loss': 2.1799, 'grad_norm': 1.08821702003479, 'learning_rate': 4.427384083618718e-06, 'epoch': 10.327731092436975}
>>> 2025-09-10 23:58:19,226 - INFO - >>> {'loss': 2.2923, 'grad_norm': 1.0483518838882446, 'learning_rate': 4.420077247707554e-06, 'epoch': 10.331932773109244}
>>> 2025-09-10 23:58:22,501 - INFO - >>> {'loss': 2.0453, 'grad_norm': 1.2275315523147583, 'learning_rate': 4.412774735289862e-06, 'epoch': 10.336134453781513}
>>> 2025-09-10 23:58:25,530 - INFO - >>> {'loss': 2.1185, 'grad_norm': 1.1032156944274902, 'learning_rate': 4.405476552023864e-06, 'epoch': 10.340336134453782}
>>> 2025-09-10 23:58:28,935 - INFO - >>> {'loss': 2.1658, 'grad_norm': 1.0692306756973267, 'learning_rate': 4.398182703564401e-06, 'epoch': 10.344537815126051}
>>> 2025-09-10 23:58:32,715 - INFO - >>> {'loss': 2.2671, 'grad_norm': 1.0892342329025269, 'learning_rate': 4.390893195562973e-06, 'epoch': 10.34873949579832}
>>> 2025-09-10 23:58:36,111 - INFO - >>> {'loss': 2.1986, 'grad_norm': 1.2149673700332642, 'learning_rate': 4.3836080336677115e-06, 'epoch': 10.352941176470589}
>>> 2025-09-10 23:58:39,584 - INFO - >>> {'loss': 2.1366, 'grad_norm': 1.3132485151290894, 'learning_rate': 4.37632722352338e-06, 'epoch': 10.357142857142858}
>>> 2025-09-10 23:58:43,456 - INFO - >>> {'loss': 2.1994, 'grad_norm': 1.0297592878341675, 'learning_rate': 4.369050770771373e-06, 'epoch': 10.361344537815127}
>>> 2025-09-10 23:58:46,668 - INFO - >>> {'loss': 2.2166, 'grad_norm': 1.1235897541046143, 'learning_rate': 4.36177868104971e-06, 'epoch': 10.365546218487395}
>>> 2025-09-10 23:58:50,409 - INFO - >>> {'loss': 2.2012, 'grad_norm': 1.081670880317688, 'learning_rate': 4.354510959993018e-06, 'epoch': 10.369747899159664}
>>> 2025-09-10 23:58:54,017 - INFO - >>> {'loss': 2.1812, 'grad_norm': 1.1622676849365234, 'learning_rate': 4.347247613232549e-06, 'epoch': 10.373949579831933}
>>> 2025-09-10 23:58:57,613 - INFO - >>> {'loss': 2.1361, 'grad_norm': 1.0593959093093872, 'learning_rate': 4.3399886463961764e-06, 'epoch': 10.378151260504202}
>>> 2025-09-10 23:59:01,275 - INFO - >>> {'loss': 2.1677, 'grad_norm': 1.1137239933013916, 'learning_rate': 4.332734065108355e-06, 'epoch': 10.382352941176471}
>>> 2025-09-10 23:59:04,391 - INFO - >>> {'loss': 2.1773, 'grad_norm': 1.0509759187698364, 'learning_rate': 4.32548387499016e-06, 'epoch': 10.38655462184874}
>>> 2025-09-10 23:59:07,505 - INFO - >>> {'loss': 2.1945, 'grad_norm': 1.1868937015533447, 'learning_rate': 4.3182380816592576e-06, 'epoch': 10.390756302521009}
>>> 2025-09-10 23:59:10,699 - INFO - >>> {'loss': 2.1808, 'grad_norm': 1.1682591438293457, 'learning_rate': 4.31099669072991e-06, 'epoch': 10.394957983193278}
>>> 2025-09-10 23:59:13,988 - INFO - >>> {'loss': 2.2831, 'grad_norm': 1.1273939609527588, 'learning_rate': 4.303759707812963e-06, 'epoch': 10.399159663865547}
>>> 2025-09-10 23:59:17,203 - INFO - >>> {'loss': 2.3073, 'grad_norm': 1.1607638597488403, 'learning_rate': 4.2965271385158535e-06, 'epoch': 10.403361344537815}
>>> 2025-09-10 23:59:20,917 - INFO - >>> {'loss': 2.0584, 'grad_norm': 1.1583143472671509, 'learning_rate': 4.289298988442599e-06, 'epoch': 10.407563025210084}
>>> 2025-09-10 23:59:24,248 - INFO - >>> {'loss': 2.2478, 'grad_norm': 1.1326967477798462, 'learning_rate': 4.282075263193779e-06, 'epoch': 10.411764705882353}
>>> 2025-09-10 23:59:28,204 - INFO - >>> {'loss': 2.2768, 'grad_norm': 1.0144301652908325, 'learning_rate': 4.274855968366564e-06, 'epoch': 10.415966386554622}
>>> 2025-09-10 23:59:31,501 - INFO - >>> {'loss': 2.1735, 'grad_norm': 1.0120158195495605, 'learning_rate': 4.267641109554686e-06, 'epoch': 10.420168067226891}
>>> 2025-09-10 23:59:35,031 - INFO - >>> {'loss': 2.182, 'grad_norm': 1.0299047231674194, 'learning_rate': 4.260430692348426e-06, 'epoch': 10.42436974789916}
>>> 2025-09-10 23:59:38,381 - INFO - >>> {'loss': 2.1488, 'grad_norm': 1.0442476272583008, 'learning_rate': 4.2532247223346355e-06, 'epoch': 10.428571428571429}
>>> 2025-09-10 23:59:42,268 - INFO - >>> {'loss': 2.1932, 'grad_norm': 1.071089744567871, 'learning_rate': 4.2460232050967285e-06, 'epoch': 10.432773109243698}
>>> 2025-09-10 23:59:45,966 - INFO - >>> {'loss': 2.2122, 'grad_norm': 1.0120419263839722, 'learning_rate': 4.238826146214651e-06, 'epoch': 10.436974789915967}
>>> 2025-09-10 23:59:49,264 - INFO - >>> {'loss': 2.1266, 'grad_norm': 1.223510503768921, 'learning_rate': 4.231633551264903e-06, 'epoch': 10.441176470588236}
>>> 2025-09-10 23:59:52,730 - INFO - >>> {'loss': 2.2674, 'grad_norm': 1.202027678489685, 'learning_rate': 4.224445425820531e-06, 'epoch': 10.445378151260504}
>>> 2025-09-10 23:59:56,122 - INFO - >>> {'loss': 2.2586, 'grad_norm': 1.163004755973816, 'learning_rate': 4.2172617754511e-06, 'epoch': 10.449579831932773}
>>> 2025-09-10 23:59:59,887 - INFO - >>> {'loss': 2.1217, 'grad_norm': 1.1354243755340576, 'learning_rate': 4.210082605722734e-06, 'epoch': 10.453781512605042}
>>> 2025-09-11 00:00:03,678 - INFO - >>> {'loss': 2.19, 'grad_norm': 1.0925594568252563, 'learning_rate': 4.20290792219807e-06, 'epoch': 10.457983193277311}
>>> 2025-09-11 00:00:06,719 - INFO - >>> {'loss': 2.2224, 'grad_norm': 1.1167434453964233, 'learning_rate': 4.195737730436262e-06, 'epoch': 10.46218487394958}
>>> 2025-09-11 00:00:10,652 - INFO - >>> {'loss': 2.1852, 'grad_norm': 1.1193221807479858, 'learning_rate': 4.188572035992994e-06, 'epoch': 10.466386554621849}
>>> 2025-09-11 00:00:13,990 - INFO - >>> {'loss': 2.183, 'grad_norm': 1.2807824611663818, 'learning_rate': 4.181410844420473e-06, 'epoch': 10.470588235294118}
>>> 2025-09-11 00:00:17,900 - INFO - >>> {'loss': 2.3085, 'grad_norm': 1.069612741470337, 'learning_rate': 4.174254161267399e-06, 'epoch': 10.474789915966387}
>>> 2025-09-11 00:00:21,709 - INFO - >>> {'loss': 2.0681, 'grad_norm': 1.1046665906906128, 'learning_rate': 4.167101992078988e-06, 'epoch': 10.478991596638656}
>>> 2025-09-11 00:00:25,472 - INFO - >>> {'loss': 2.1353, 'grad_norm': 1.027740240097046, 'learning_rate': 4.15995434239696e-06, 'epoch': 10.483193277310924}
>>> 2025-09-11 00:00:28,097 - INFO - >>> {'loss': 2.2737, 'grad_norm': 1.4359416961669922, 'learning_rate': 4.152811217759529e-06, 'epoch': 10.487394957983193}
>>> 2025-09-11 00:00:31,533 - INFO - >>> {'loss': 2.1799, 'grad_norm': 1.0739641189575195, 'learning_rate': 4.145672623701406e-06, 'epoch': 10.491596638655462}
>>> 2025-09-11 00:00:35,504 - INFO - >>> {'loss': 2.1744, 'grad_norm': 1.1216315031051636, 'learning_rate': 4.138538565753791e-06, 'epoch': 10.495798319327731}
>>> 2025-09-11 00:00:39,361 - INFO - >>> {'loss': 2.2753, 'grad_norm': 1.0999534130096436, 'learning_rate': 4.1314090494443705e-06, 'epoch': 10.5}
>>> 2025-09-11 00:00:43,261 - INFO - >>> {'loss': 2.1538, 'grad_norm': 1.2023154497146606, 'learning_rate': 4.1242840802973e-06, 'epoch': 10.504201680672269}
>>> 2025-09-11 00:00:46,888 - INFO - >>> {'loss': 2.1152, 'grad_norm': 1.1274287700653076, 'learning_rate': 4.117163663833232e-06, 'epoch': 10.508403361344538}
>>> 2025-09-11 00:00:50,233 - INFO - >>> {'loss': 2.2693, 'grad_norm': 1.2038180828094482, 'learning_rate': 4.1100478055692825e-06, 'epoch': 10.512605042016807}
>>> 2025-09-11 00:00:53,248 - INFO - >>> {'loss': 2.2199, 'grad_norm': 1.348914384841919, 'learning_rate': 4.102936511019027e-06, 'epoch': 10.516806722689076}
>>> 2025-09-11 00:00:56,431 - INFO - >>> {'loss': 2.2129, 'grad_norm': 1.0824410915374756, 'learning_rate': 4.095829785692511e-06, 'epoch': 10.521008403361344}
>>> 2025-09-11 00:00:59,627 - INFO - >>> {'loss': 2.1074, 'grad_norm': 1.034747838973999, 'learning_rate': 4.088727635096253e-06, 'epoch': 10.525210084033613}
>>> 2025-09-11 00:01:02,515 - INFO - >>> {'loss': 2.1601, 'grad_norm': 1.1689029932022095, 'learning_rate': 4.081630064733204e-06, 'epoch': 10.529411764705882}
>>> 2025-09-11 00:01:05,775 - INFO - >>> {'loss': 2.1738, 'grad_norm': 1.117653727531433, 'learning_rate': 4.074537080102779e-06, 'epoch': 10.533613445378151}
>>> 2025-09-11 00:01:09,805 - INFO - >>> {'loss': 2.1456, 'grad_norm': 1.1233478784561157, 'learning_rate': 4.0674486867008435e-06, 'epoch': 10.53781512605042}
>>> 2025-09-11 00:01:13,210 - INFO - >>> {'loss': 2.1798, 'grad_norm': 1.1033352613449097, 'learning_rate': 4.060364890019688e-06, 'epoch': 10.542016806722689}
>>> 2025-09-11 00:01:17,308 - INFO - >>> {'loss': 2.2156, 'grad_norm': 1.0491516590118408, 'learning_rate': 4.0532856955480635e-06, 'epoch': 10.546218487394958}
>>> 2025-09-11 00:01:20,306 - INFO - >>> {'loss': 2.1602, 'grad_norm': 1.1035497188568115, 'learning_rate': 4.046211108771145e-06, 'epoch': 10.550420168067227}
>>> 2025-09-11 00:01:23,604 - INFO - >>> {'loss': 2.0911, 'grad_norm': 1.1376796960830688, 'learning_rate': 4.0391411351705304e-06, 'epoch': 10.554621848739496}
>>> 2025-09-11 00:01:26,839 - INFO - >>> {'loss': 2.1989, 'grad_norm': 1.1610227823257446, 'learning_rate': 4.032075780224248e-06, 'epoch': 10.558823529411764}
>>> 2025-09-11 00:01:30,550 - INFO - >>> {'loss': 2.1696, 'grad_norm': 1.075666904449463, 'learning_rate': 4.025015049406763e-06, 'epoch': 10.563025210084033}
>>> 2025-09-11 00:01:33,576 - INFO - >>> {'loss': 2.2203, 'grad_norm': 1.2441447973251343, 'learning_rate': 4.01795894818893e-06, 'epoch': 10.567226890756302}
>>> 2025-09-11 00:01:36,944 - INFO - >>> {'loss': 2.2788, 'grad_norm': 1.2328232526779175, 'learning_rate': 4.010907482038037e-06, 'epoch': 10.571428571428571}
>>> 2025-09-11 00:01:40,639 - INFO - >>> {'loss': 2.1782, 'grad_norm': 0.9859670996665955, 'learning_rate': 4.003860656417771e-06, 'epoch': 10.57563025210084}
>>> 2025-09-11 00:01:44,191 - INFO - >>> {'loss': 2.2773, 'grad_norm': 1.1527432203292847, 'learning_rate': 3.996818476788229e-06, 'epoch': 10.579831932773109}
>>> 2025-09-11 00:01:47,723 - INFO - >>> {'loss': 2.1938, 'grad_norm': 1.0648101568222046, 'learning_rate': 3.989780948605902e-06, 'epoch': 10.584033613445378}
>>> 2025-09-11 00:01:50,740 - INFO - >>> {'loss': 2.2182, 'grad_norm': 1.186160922050476, 'learning_rate': 3.982748077323685e-06, 'epoch': 10.588235294117647}
>>> 2025-09-11 00:01:54,270 - INFO - >>> {'loss': 2.2783, 'grad_norm': 1.1162890195846558, 'learning_rate': 3.97571986839086e-06, 'epoch': 10.592436974789916}
>>> 2025-09-11 00:01:57,652 - INFO - >>> {'loss': 2.2353, 'grad_norm': 1.1744126081466675, 'learning_rate': 3.968696327253087e-06, 'epoch': 10.596638655462185}
>>> 2025-09-11 00:02:01,408 - INFO - >>> {'loss': 2.2598, 'grad_norm': 1.097870945930481, 'learning_rate': 3.96167745935243e-06, 'epoch': 10.600840336134453}
>>> 2025-09-11 00:02:04,212 - INFO - >>> {'loss': 2.1811, 'grad_norm': 1.3706583976745605, 'learning_rate': 3.9546632701273205e-06, 'epoch': 10.605042016806722}
>>> 2025-09-11 00:02:07,756 - INFO - >>> {'loss': 2.1919, 'grad_norm': 1.0658886432647705, 'learning_rate': 3.947653765012558e-06, 'epoch': 10.609243697478991}
>>> 2025-09-11 00:02:11,409 - INFO - >>> {'loss': 2.1038, 'grad_norm': 1.0935046672821045, 'learning_rate': 3.9406489494393225e-06, 'epoch': 10.61344537815126}
>>> 2025-09-11 00:02:15,341 - INFO - >>> {'loss': 2.2215, 'grad_norm': 1.148585319519043, 'learning_rate': 3.93364882883516e-06, 'epoch': 10.617647058823529}
>>> 2025-09-11 00:02:18,659 - INFO - >>> {'loss': 2.1714, 'grad_norm': 1.061342477798462, 'learning_rate': 3.926653408623972e-06, 'epoch': 10.621848739495798}
>>> 2025-09-11 00:02:22,693 - INFO - >>> {'loss': 2.2656, 'grad_norm': 1.152361273765564, 'learning_rate': 3.9196626942260285e-06, 'epoch': 10.626050420168067}
>>> 2025-09-11 00:02:26,441 - INFO - >>> {'loss': 2.2154, 'grad_norm': 1.2548049688339233, 'learning_rate': 3.9126766910579465e-06, 'epoch': 10.630252100840336}
>>> 2025-09-11 00:02:29,627 - INFO - >>> {'loss': 2.1513, 'grad_norm': 1.2539699077606201, 'learning_rate': 3.9056954045326825e-06, 'epoch': 10.634453781512605}
>>> 2025-09-11 00:02:33,657 - INFO - >>> {'loss': 2.2376, 'grad_norm': 1.0036488771438599, 'learning_rate': 3.898718840059561e-06, 'epoch': 10.638655462184873}
>>> 2025-09-11 00:02:37,555 - INFO - >>> {'loss': 2.2685, 'grad_norm': 1.07290518283844, 'learning_rate': 3.891747003044236e-06, 'epoch': 10.642857142857142}
>>> 2025-09-11 00:02:40,746 - INFO - >>> {'loss': 2.1809, 'grad_norm': 1.2788195610046387, 'learning_rate': 3.884779898888691e-06, 'epoch': 10.647058823529411}
>>> 2025-09-11 00:02:44,454 - INFO - >>> {'loss': 2.3107, 'grad_norm': 1.189794898033142, 'learning_rate': 3.877817532991254e-06, 'epoch': 10.65126050420168}
>>> 2025-09-11 00:02:47,644 - INFO - >>> {'loss': 2.1913, 'grad_norm': 1.1590240001678467, 'learning_rate': 3.870859910746575e-06, 'epoch': 10.655462184873949}
>>> 2025-09-11 00:02:51,394 - INFO - >>> {'loss': 2.1875, 'grad_norm': 1.0560996532440186, 'learning_rate': 3.863907037545634e-06, 'epoch': 10.659663865546218}
>>> 2025-09-11 00:02:55,127 - INFO - >>> {'loss': 2.1764, 'grad_norm': 1.0363589525222778, 'learning_rate': 3.856958918775726e-06, 'epoch': 10.663865546218487}
>>> 2025-09-11 00:02:58,875 - INFO - >>> {'loss': 2.1612, 'grad_norm': 1.1772425174713135, 'learning_rate': 3.850015559820465e-06, 'epoch': 10.668067226890756}
>>> 2025-09-11 00:03:02,483 - INFO - >>> {'loss': 2.1634, 'grad_norm': 1.2025409936904907, 'learning_rate': 3.843076966059775e-06, 'epoch': 10.672268907563025}
>>> 2025-09-11 00:03:06,003 - INFO - >>> {'loss': 2.1756, 'grad_norm': 1.197170376777649, 'learning_rate': 3.836143142869891e-06, 'epoch': 10.676470588235293}
>>> 2025-09-11 00:03:09,347 - INFO - >>> {'loss': 2.2505, 'grad_norm': 1.3063477277755737, 'learning_rate': 3.829214095623349e-06, 'epoch': 10.680672268907562}
>>> 2025-09-11 00:03:13,144 - INFO - >>> {'loss': 2.1597, 'grad_norm': 0.9423025250434875, 'learning_rate': 3.8222898296889885e-06, 'epoch': 10.684873949579831}
>>> 2025-09-11 00:03:16,681 - INFO - >>> {'loss': 2.2084, 'grad_norm': 1.0608551502227783, 'learning_rate': 3.81537035043193e-06, 'epoch': 10.6890756302521}
>>> 2025-09-11 00:03:19,788 - INFO - >>> {'loss': 2.3392, 'grad_norm': 1.142804503440857, 'learning_rate': 3.808455663213607e-06, 'epoch': 10.693277310924369}
>>> 2025-09-11 00:03:23,830 - INFO - >>> {'loss': 2.174, 'grad_norm': 1.1326048374176025, 'learning_rate': 3.8015457733917285e-06, 'epoch': 10.697478991596638}
>>> 2025-09-11 00:03:26,959 - INFO - >>> {'loss': 2.0965, 'grad_norm': 1.1237778663635254, 'learning_rate': 3.79464068632028e-06, 'epoch': 10.701680672268907}
>>> 2025-09-11 00:03:30,326 - INFO - >>> {'loss': 2.3016, 'grad_norm': 1.1917777061462402, 'learning_rate': 3.787740407349536e-06, 'epoch': 10.705882352941176}
>>> 2025-09-11 00:03:33,651 - INFO - >>> {'loss': 2.1815, 'grad_norm': 1.1416068077087402, 'learning_rate': 3.780844941826042e-06, 'epoch': 10.710084033613445}
>>> 2025-09-11 00:03:37,506 - INFO - >>> {'loss': 2.1237, 'grad_norm': 1.1705384254455566, 'learning_rate': 3.7739542950926134e-06, 'epoch': 10.714285714285714}
>>> 2025-09-11 00:03:40,740 - INFO - >>> {'loss': 2.2052, 'grad_norm': 1.116007685661316, 'learning_rate': 3.7670684724883334e-06, 'epoch': 10.718487394957982}
>>> 2025-09-11 00:03:43,801 - INFO - >>> {'loss': 2.2374, 'grad_norm': 1.1951020956039429, 'learning_rate': 3.7601874793485503e-06, 'epoch': 10.722689075630251}
>>> 2025-09-11 00:03:47,497 - INFO - >>> {'loss': 2.218, 'grad_norm': 1.1042269468307495, 'learning_rate': 3.7533113210048544e-06, 'epoch': 10.72689075630252}
>>> 2025-09-11 00:03:50,755 - INFO - >>> {'loss': 2.1479, 'grad_norm': 1.1208947896957397, 'learning_rate': 3.746440002785112e-06, 'epoch': 10.731092436974789}
>>> 2025-09-11 00:03:54,717 - INFO - >>> {'loss': 2.2536, 'grad_norm': 1.096991777420044, 'learning_rate': 3.7395735300134306e-06, 'epoch': 10.735294117647058}
>>> 2025-09-11 00:03:57,968 - INFO - >>> {'loss': 2.2201, 'grad_norm': 1.13507878780365, 'learning_rate': 3.732711908010155e-06, 'epoch': 10.739495798319329}
>>> 2025-09-11 00:04:00,756 - INFO - >>> {'loss': 2.1249, 'grad_norm': 1.2819936275482178, 'learning_rate': 3.7258551420918797e-06, 'epoch': 10.743697478991596}
>>> 2025-09-11 00:04:04,271 - INFO - >>> {'loss': 2.2546, 'grad_norm': 1.060619592666626, 'learning_rate': 3.7190032375714356e-06, 'epoch': 10.747899159663866}
>>> 2025-09-11 00:04:07,265 - INFO - >>> {'loss': 2.1248, 'grad_norm': 1.1872929334640503, 'learning_rate': 3.712156199757887e-06, 'epoch': 10.752100840336134}
>>> 2025-09-11 00:04:10,353 - INFO - >>> {'loss': 2.0961, 'grad_norm': 1.174075961112976, 'learning_rate': 3.7053140339565262e-06, 'epoch': 10.756302521008404}
>>> 2025-09-11 00:04:13,390 - INFO - >>> {'loss': 2.1556, 'grad_norm': 1.2586162090301514, 'learning_rate': 3.698476745468871e-06, 'epoch': 10.760504201680671}
>>> 2025-09-11 00:04:17,365 - INFO - >>> {'loss': 2.2887, 'grad_norm': 1.0244946479797363, 'learning_rate': 3.6916443395926604e-06, 'epoch': 10.764705882352942}
>>> 2025-09-11 00:04:20,906 - INFO - >>> {'loss': 2.1216, 'grad_norm': 1.108451008796692, 'learning_rate': 3.684816821621848e-06, 'epoch': 10.768907563025211}
>>> 2025-09-11 00:04:24,207 - INFO - >>> {'loss': 2.2128, 'grad_norm': 1.1889073848724365, 'learning_rate': 3.6779941968466047e-06, 'epoch': 10.77310924369748}
>>> 2025-09-11 00:04:27,965 - INFO - >>> {'loss': 2.2186, 'grad_norm': 1.0938911437988281, 'learning_rate': 3.6711764705533083e-06, 'epoch': 10.777310924369749}
>>> 2025-09-11 00:04:31,297 - INFO - >>> {'loss': 2.1644, 'grad_norm': 1.2113186120986938, 'learning_rate': 3.6643636480245337e-06, 'epoch': 10.781512605042018}
>>> 2025-09-11 00:04:34,371 - INFO - >>> {'loss': 2.2281, 'grad_norm': 1.2204113006591797, 'learning_rate': 3.6575557345390667e-06, 'epoch': 10.785714285714286}
>>> 2025-09-11 00:04:38,487 - INFO - >>> {'loss': 2.2428, 'grad_norm': 1.227554440498352, 'learning_rate': 3.650752735371885e-06, 'epoch': 10.789915966386555}
>>> 2025-09-11 00:04:41,831 - INFO - >>> {'loss': 2.1922, 'grad_norm': 1.194819688796997, 'learning_rate': 3.6439546557941587e-06, 'epoch': 10.794117647058824}
>>> 2025-09-11 00:04:45,491 - INFO - >>> {'loss': 2.2683, 'grad_norm': 1.1225128173828125, 'learning_rate': 3.637161501073245e-06, 'epoch': 10.798319327731093}
>>> 2025-09-11 00:04:49,292 - INFO - >>> {'loss': 2.2106, 'grad_norm': 1.1952108144760132, 'learning_rate': 3.6303732764726897e-06, 'epoch': 10.802521008403362}
>>> 2025-09-11 00:04:52,329 - INFO - >>> {'loss': 2.265, 'grad_norm': 1.1823025941848755, 'learning_rate': 3.6235899872522105e-06, 'epoch': 10.806722689075631}
>>> 2025-09-11 00:04:56,446 - INFO - >>> {'loss': 2.2951, 'grad_norm': 1.057827115058899, 'learning_rate': 3.61681163866771e-06, 'epoch': 10.8109243697479}
>>> 2025-09-11 00:05:00,115 - INFO - >>> {'loss': 2.2907, 'grad_norm': 1.0726372003555298, 'learning_rate': 3.6100382359712593e-06, 'epoch': 10.815126050420169}
>>> 2025-09-11 00:05:03,186 - INFO - >>> {'loss': 2.0743, 'grad_norm': 1.3486692905426025, 'learning_rate': 3.6032697844110896e-06, 'epoch': 10.819327731092438}
>>> 2025-09-11 00:05:07,119 - INFO - >>> {'loss': 2.1011, 'grad_norm': 1.028098225593567, 'learning_rate': 3.596506289231604e-06, 'epoch': 10.823529411764707}
>>> 2025-09-11 00:05:10,794 - INFO - >>> {'loss': 2.1257, 'grad_norm': 1.1269279718399048, 'learning_rate': 3.589747755673373e-06, 'epoch': 10.827731092436975}
>>> 2025-09-11 00:05:14,473 - INFO - >>> {'loss': 2.2176, 'grad_norm': 1.0540152788162231, 'learning_rate': 3.5829941889731036e-06, 'epoch': 10.831932773109244}
>>> 2025-09-11 00:05:17,544 - INFO - >>> {'loss': 2.1484, 'grad_norm': 1.173334002494812, 'learning_rate': 3.5762455943636666e-06, 'epoch': 10.836134453781513}
>>> 2025-09-11 00:05:21,589 - INFO - >>> {'loss': 2.2216, 'grad_norm': 1.0958881378173828, 'learning_rate': 3.5695019770740804e-06, 'epoch': 10.840336134453782}
>>> 2025-09-11 00:05:25,025 - INFO - >>> {'loss': 2.145, 'grad_norm': 1.0614484548568726, 'learning_rate': 3.5627633423295015e-06, 'epoch': 10.844537815126051}
>>> 2025-09-11 00:05:28,300 - INFO - >>> {'loss': 2.1957, 'grad_norm': 1.17229163646698, 'learning_rate': 3.5560296953512296e-06, 'epoch': 10.84873949579832}
>>> 2025-09-11 00:05:31,379 - INFO - >>> {'loss': 2.2273, 'grad_norm': 1.196114182472229, 'learning_rate': 3.5493010413567e-06, 'epoch': 10.852941176470589}
>>> 2025-09-11 00:05:34,913 - INFO - >>> {'loss': 2.0938, 'grad_norm': 1.058144211769104, 'learning_rate': 3.542577385559479e-06, 'epoch': 10.857142857142858}
>>> 2025-09-11 00:05:38,205 - INFO - >>> {'loss': 2.2045, 'grad_norm': 1.1308077573776245, 'learning_rate': 3.5358587331692516e-06, 'epoch': 10.861344537815127}
>>> 2025-09-11 00:05:41,638 - INFO - >>> {'loss': 2.1455, 'grad_norm': 1.2091302871704102, 'learning_rate': 3.5291450893918423e-06, 'epoch': 10.865546218487395}
>>> 2025-09-11 00:05:45,641 - INFO - >>> {'loss': 2.2067, 'grad_norm': 1.0561617612838745, 'learning_rate': 3.522436459429186e-06, 'epoch': 10.869747899159664}
>>> 2025-09-11 00:05:49,599 - INFO - >>> {'loss': 2.2592, 'grad_norm': 1.1031686067581177, 'learning_rate': 3.515732848479325e-06, 'epoch': 10.873949579831933}
>>> 2025-09-11 00:05:53,094 - INFO - >>> {'loss': 2.1839, 'grad_norm': 1.1688224077224731, 'learning_rate': 3.509034261736426e-06, 'epoch': 10.878151260504202}
>>> 2025-09-11 00:05:56,411 - INFO - >>> {'loss': 2.1432, 'grad_norm': 1.1954487562179565, 'learning_rate': 3.5023407043907564e-06, 'epoch': 10.882352941176471}
>>> 2025-09-11 00:06:00,137 - INFO - >>> {'loss': 2.1482, 'grad_norm': 1.089148759841919, 'learning_rate': 3.495652181628688e-06, 'epoch': 10.88655462184874}
>>> 2025-09-11 00:06:03,691 - INFO - >>> {'loss': 2.1787, 'grad_norm': 1.180128574371338, 'learning_rate': 3.4889686986326897e-06, 'epoch': 10.890756302521009}
>>> 2025-09-11 00:06:07,653 - INFO - >>> {'loss': 2.2206, 'grad_norm': 1.0820109844207764, 'learning_rate': 3.482290260581328e-06, 'epoch': 10.894957983193278}
>>> 2025-09-11 00:06:10,936 - INFO - >>> {'loss': 2.2238, 'grad_norm': 1.0981042385101318, 'learning_rate': 3.47561687264926e-06, 'epoch': 10.899159663865547}
>>> 2025-09-11 00:06:14,006 - INFO - >>> {'loss': 2.1802, 'grad_norm': 1.2115669250488281, 'learning_rate': 3.4689485400072276e-06, 'epoch': 10.903361344537815}
>>> 2025-09-11 00:06:17,316 - INFO - >>> {'loss': 2.3165, 'grad_norm': 1.375848650932312, 'learning_rate': 3.462285267822062e-06, 'epoch': 10.907563025210084}
>>> 2025-09-11 00:06:20,318 - INFO - >>> {'loss': 2.264, 'grad_norm': 1.1351455450057983, 'learning_rate': 3.4556270612566622e-06, 'epoch': 10.911764705882353}
>>> 2025-09-11 00:06:23,432 - INFO - >>> {'loss': 2.2946, 'grad_norm': 1.0414516925811768, 'learning_rate': 3.4489739254700072e-06, 'epoch': 10.915966386554622}
>>> 2025-09-11 00:06:27,164 - INFO - >>> {'loss': 2.159, 'grad_norm': 1.1873005628585815, 'learning_rate': 3.4423258656171586e-06, 'epoch': 10.920168067226891}
>>> 2025-09-11 00:06:30,921 - INFO - >>> {'loss': 2.1532, 'grad_norm': 1.082375407218933, 'learning_rate': 3.4356828868492263e-06, 'epoch': 10.92436974789916}
>>> 2025-09-11 00:06:35,431 - INFO - >>> {'loss': 2.1972, 'grad_norm': 1.1997662782669067, 'learning_rate': 3.429044994313395e-06, 'epoch': 10.928571428571429}
>>> 2025-09-11 00:06:38,764 - INFO - >>> {'loss': 2.1494, 'grad_norm': 1.2045376300811768, 'learning_rate': 3.4224121931529052e-06, 'epoch': 10.932773109243698}
>>> 2025-09-11 00:06:42,513 - INFO - >>> {'loss': 2.1712, 'grad_norm': 1.1274158954620361, 'learning_rate': 3.4157844885070513e-06, 'epoch': 10.936974789915967}
>>> 2025-09-11 00:06:45,798 - INFO - >>> {'loss': 2.2903, 'grad_norm': 1.2828776836395264, 'learning_rate': 3.4091618855111806e-06, 'epoch': 10.941176470588236}
>>> 2025-09-11 00:06:49,679 - INFO - >>> {'loss': 2.1141, 'grad_norm': 1.1404606103897095, 'learning_rate': 3.402544389296687e-06, 'epoch': 10.945378151260504}
>>> 2025-09-11 00:06:53,021 - INFO - >>> {'loss': 2.1678, 'grad_norm': 1.1850868463516235, 'learning_rate': 3.3959320049910116e-06, 'epoch': 10.949579831932773}
>>> 2025-09-11 00:06:56,617 - INFO - >>> {'loss': 2.121, 'grad_norm': 1.2048594951629639, 'learning_rate': 3.3893247377176187e-06, 'epoch': 10.953781512605042}
>>> 2025-09-11 00:07:00,660 - INFO - >>> {'loss': 2.2545, 'grad_norm': 1.0287011861801147, 'learning_rate': 3.382722592596034e-06, 'epoch': 10.957983193277311}
>>> 2025-09-11 00:07:04,127 - INFO - >>> {'loss': 2.2264, 'grad_norm': 1.205268144607544, 'learning_rate': 3.37612557474179e-06, 'epoch': 10.96218487394958}
>>> 2025-09-11 00:07:08,189 - INFO - >>> {'loss': 2.1496, 'grad_norm': 0.9576516151428223, 'learning_rate': 3.3695336892664586e-06, 'epoch': 10.966386554621849}
>>> 2025-09-11 00:07:12,127 - INFO - >>> {'loss': 2.2322, 'grad_norm': 1.1084359884262085, 'learning_rate': 3.362946941277634e-06, 'epoch': 10.970588235294118}
>>> 2025-09-11 00:07:15,368 - INFO - >>> {'loss': 2.2243, 'grad_norm': 1.2924116849899292, 'learning_rate': 3.356365335878927e-06, 'epoch': 10.974789915966387}
>>> 2025-09-11 00:07:18,421 - INFO - >>> {'loss': 2.1856, 'grad_norm': 1.1249219179153442, 'learning_rate': 3.3497888781699663e-06, 'epoch': 10.978991596638656}
>>> 2025-09-11 00:07:22,221 - INFO - >>> {'loss': 2.1842, 'grad_norm': 1.0665967464447021, 'learning_rate': 3.343217573246391e-06, 'epoch': 10.983193277310924}
>>> 2025-09-11 00:07:25,599 - INFO - >>> {'loss': 2.1024, 'grad_norm': 1.0852247476577759, 'learning_rate': 3.3366514261998506e-06, 'epoch': 10.987394957983193}
>>> 2025-09-11 00:07:28,602 - INFO - >>> {'loss': 2.1732, 'grad_norm': 1.0667351484298706, 'learning_rate': 3.330090442117985e-06, 'epoch': 10.991596638655462}
>>> 2025-09-11 00:07:32,247 - INFO - >>> {'loss': 2.1781, 'grad_norm': 1.111144781112671, 'learning_rate': 3.3235346260844526e-06, 'epoch': 10.995798319327731}
>>> 2025-09-11 00:07:35,722 - INFO - >>> {'loss': 2.2801, 'grad_norm': 1.0777180194854736, 'learning_rate': 3.3169839831789008e-06, 'epoch': 11.0}
>>> 2025-09-11 00:07:38,796 - INFO - >>> {'loss': 2.164, 'grad_norm': 1.141102910041809, 'learning_rate': 3.310438518476958e-06, 'epoch': 11.004201680672269}
>>> 2025-09-11 00:07:42,840 - INFO - >>> {'loss': 2.2966, 'grad_norm': 1.125461220741272, 'learning_rate': 3.3038982370502503e-06, 'epoch': 11.008403361344538}
>>> 2025-09-11 00:07:46,103 - INFO - >>> {'loss': 2.2243, 'grad_norm': 1.1450556516647339, 'learning_rate': 3.2973631439663958e-06, 'epoch': 11.012605042016807}
>>> 2025-09-11 00:07:49,216 - INFO - >>> {'loss': 2.2621, 'grad_norm': 1.0706220865249634, 'learning_rate': 3.2908332442889723e-06, 'epoch': 11.016806722689076}
>>> 2025-09-11 00:07:52,554 - INFO - >>> {'loss': 2.2225, 'grad_norm': 1.1100671291351318, 'learning_rate': 3.2843085430775478e-06, 'epoch': 11.021008403361344}
>>> 2025-09-11 00:07:56,219 - INFO - >>> {'loss': 2.1733, 'grad_norm': 1.0586135387420654, 'learning_rate': 3.277789045387659e-06, 'epoch': 11.025210084033613}
>>> 2025-09-11 00:07:59,669 - INFO - >>> {'loss': 2.2407, 'grad_norm': 1.1160346269607544, 'learning_rate': 3.2712747562708115e-06, 'epoch': 11.029411764705882}
>>> 2025-09-11 00:08:03,635 - INFO - >>> {'loss': 2.2057, 'grad_norm': 1.2043811082839966, 'learning_rate': 3.264765680774472e-06, 'epoch': 11.033613445378151}
>>> 2025-09-11 00:08:07,066 - INFO - >>> {'loss': 2.1564, 'grad_norm': 1.2155267000198364, 'learning_rate': 3.2582618239420716e-06, 'epoch': 11.03781512605042}
>>> 2025-09-11 00:08:10,618 - INFO - >>> {'loss': 2.2593, 'grad_norm': 1.2100398540496826, 'learning_rate': 3.2517631908129988e-06, 'epoch': 11.042016806722689}
>>> 2025-09-11 00:08:14,654 - INFO - >>> {'loss': 2.2699, 'grad_norm': 1.146729826927185, 'learning_rate': 3.24526978642258e-06, 'epoch': 11.046218487394958}
>>> 2025-09-11 00:08:18,018 - INFO - >>> {'loss': 2.2454, 'grad_norm': 1.0665860176086426, 'learning_rate': 3.238781615802119e-06, 'epoch': 11.050420168067227}
>>> 2025-09-11 00:08:21,907 - INFO - >>> {'loss': 2.1656, 'grad_norm': 1.0858993530273438, 'learning_rate': 3.2322986839788338e-06, 'epoch': 11.054621848739496}
>>> 2025-09-11 00:08:25,280 - INFO - >>> {'loss': 2.1993, 'grad_norm': 1.0486103296279907, 'learning_rate': 3.2258209959759024e-06, 'epoch': 11.058823529411764}
>>> 2025-09-11 00:08:29,218 - INFO - >>> {'loss': 2.231, 'grad_norm': 1.116760015487671, 'learning_rate': 3.2193485568124327e-06, 'epoch': 11.063025210084033}
>>> 2025-09-11 00:08:32,516 - INFO - >>> {'loss': 2.2557, 'grad_norm': 1.1266525983810425, 'learning_rate': 3.2128813715034667e-06, 'epoch': 11.067226890756302}
>>> 2025-09-11 00:08:36,331 - INFO - >>> {'loss': 2.1392, 'grad_norm': 1.092182993888855, 'learning_rate': 3.206419445059977e-06, 'epoch': 11.071428571428571}
>>> 2025-09-11 00:08:39,747 - INFO - >>> {'loss': 2.2489, 'grad_norm': 1.2779788970947266, 'learning_rate': 3.199962782488859e-06, 'epoch': 11.07563025210084}
>>> 2025-09-11 00:08:43,436 - INFO - >>> {'loss': 2.2325, 'grad_norm': 1.101790189743042, 'learning_rate': 3.1935113887929357e-06, 'epoch': 11.079831932773109}
>>> 2025-09-11 00:08:46,573 - INFO - >>> {'loss': 2.1803, 'grad_norm': 1.1258189678192139, 'learning_rate': 3.1870652689709324e-06, 'epoch': 11.084033613445378}
>>> 2025-09-11 00:08:50,176 - INFO - >>> {'loss': 2.0761, 'grad_norm': 0.9900816679000854, 'learning_rate': 3.180624428017507e-06, 'epoch': 11.088235294117647}
>>> 2025-09-11 00:08:54,109 - INFO - >>> {'loss': 2.2109, 'grad_norm': 1.1182446479797363, 'learning_rate': 3.1741888709232194e-06, 'epoch': 11.092436974789916}
>>> 2025-09-11 00:08:57,557 - INFO - >>> {'loss': 2.1845, 'grad_norm': 1.1538382768630981, 'learning_rate': 3.167758602674528e-06, 'epoch': 11.096638655462185}
>>> 2025-09-11 00:09:00,933 - INFO - >>> {'loss': 2.1877, 'grad_norm': 1.0316466093063354, 'learning_rate': 3.1613336282538e-06, 'epoch': 11.100840336134453}
>>> 2025-09-11 00:09:04,333 - INFO - >>> {'loss': 2.1422, 'grad_norm': 1.1563968658447266, 'learning_rate': 3.154913952639309e-06, 'epoch': 11.105042016806722}
>>> 2025-09-11 00:09:08,240 - INFO - >>> {'loss': 2.1979, 'grad_norm': 1.1085513830184937, 'learning_rate': 3.1484995808052066e-06, 'epoch': 11.109243697478991}
>>> 2025-09-11 00:09:11,849 - INFO - >>> {'loss': 2.2332, 'grad_norm': 1.0566164255142212, 'learning_rate': 3.1420905177215443e-06, 'epoch': 11.11344537815126}
>>> 2025-09-11 00:09:14,726 - INFO - >>> {'loss': 2.028, 'grad_norm': 1.2477260828018188, 'learning_rate': 3.1356867683542603e-06, 'epoch': 11.117647058823529}
>>> 2025-09-11 00:09:18,710 - INFO - >>> {'loss': 2.3284, 'grad_norm': 1.2361321449279785, 'learning_rate': 3.1292883376651726e-06, 'epoch': 11.121848739495798}
>>> 2025-09-11 00:09:21,654 - INFO - >>> {'loss': 2.1996, 'grad_norm': 1.1681156158447266, 'learning_rate': 3.122895230611981e-06, 'epoch': 11.126050420168067}
>>> 2025-09-11 00:09:25,550 - INFO - >>> {'loss': 2.2929, 'grad_norm': 1.0898995399475098, 'learning_rate': 3.116507452148261e-06, 'epoch': 11.130252100840336}
>>> 2025-09-11 00:09:28,584 - INFO - >>> {'loss': 2.2725, 'grad_norm': 1.1784307956695557, 'learning_rate': 3.110125007223449e-06, 'epoch': 11.134453781512605}
>>> 2025-09-11 00:09:31,996 - INFO - >>> {'loss': 2.1801, 'grad_norm': 1.0469651222229004, 'learning_rate': 3.1037479007828606e-06, 'epoch': 11.138655462184873}
>>> 2025-09-11 00:09:35,860 - INFO - >>> {'loss': 2.1532, 'grad_norm': 1.0956270694732666, 'learning_rate': 3.097376137767678e-06, 'epoch': 11.142857142857142}
>>> 2025-09-11 00:09:39,186 - INFO - >>> {'loss': 2.1929, 'grad_norm': 1.290786623954773, 'learning_rate': 3.0910097231149296e-06, 'epoch': 11.147058823529411}
>>> 2025-09-11 00:09:42,856 - INFO - >>> {'loss': 2.107, 'grad_norm': 1.170128345489502, 'learning_rate': 3.084648661757508e-06, 'epoch': 11.15126050420168}
>>> 2025-09-11 00:09:45,785 - INFO - >>> {'loss': 2.2353, 'grad_norm': 1.248940348625183, 'learning_rate': 3.078292958624156e-06, 'epoch': 11.155462184873949}
>>> 2025-09-11 00:09:48,670 - INFO - >>> {'loss': 2.3004, 'grad_norm': 1.3187811374664307, 'learning_rate': 3.0719426186394675e-06, 'epoch': 11.159663865546218}
>>> 2025-09-11 00:09:52,427 - INFO - >>> {'loss': 2.2018, 'grad_norm': 1.1326791048049927, 'learning_rate': 3.065597646723877e-06, 'epoch': 11.163865546218487}
>>> 2025-09-11 00:09:56,097 - INFO - >>> {'loss': 2.2421, 'grad_norm': 1.0104645490646362, 'learning_rate': 3.0592580477936606e-06, 'epoch': 11.168067226890756}
>>> 2025-09-11 00:09:59,633 - INFO - >>> {'loss': 2.2259, 'grad_norm': 1.1220327615737915, 'learning_rate': 3.0529238267609374e-06, 'epoch': 11.172268907563025}
>>> 2025-09-11 00:10:03,302 - INFO - >>> {'loss': 2.2526, 'grad_norm': 1.1315826177597046, 'learning_rate': 3.0465949885336433e-06, 'epoch': 11.176470588235293}
>>> 2025-09-11 00:10:07,434 - INFO - >>> {'loss': 2.2159, 'grad_norm': 0.9446847438812256, 'learning_rate': 3.0402715380155636e-06, 'epoch': 11.180672268907562}
>>> 2025-09-11 00:10:10,773 - INFO - >>> {'loss': 2.2081, 'grad_norm': 1.1172889471054077, 'learning_rate': 3.0339534801062996e-06, 'epoch': 11.184873949579831}
>>> 2025-09-11 00:10:14,314 - INFO - >>> {'loss': 2.1166, 'grad_norm': 1.2757270336151123, 'learning_rate': 3.0276408197012687e-06, 'epoch': 11.1890756302521}
>>> 2025-09-11 00:10:18,027 - INFO - >>> {'loss': 2.2519, 'grad_norm': 1.090671181678772, 'learning_rate': 3.021333561691715e-06, 'epoch': 11.193277310924369}
>>> 2025-09-11 00:10:21,370 - INFO - >>> {'loss': 2.1668, 'grad_norm': 1.1830661296844482, 'learning_rate': 3.015031710964692e-06, 'epoch': 11.197478991596638}
>>> 2025-09-11 00:10:24,008 - INFO - >>> {'loss': 2.2237, 'grad_norm': 1.2011973857879639, 'learning_rate': 3.008735272403065e-06, 'epoch': 11.201680672268907}
>>> 2025-09-11 00:10:27,306 - INFO - >>> {'loss': 2.3175, 'grad_norm': 1.125795841217041, 'learning_rate': 3.0024442508855078e-06, 'epoch': 11.205882352941176}
>>> 2025-09-11 00:10:30,684 - INFO - >>> {'loss': 2.2529, 'grad_norm': 1.1665421724319458, 'learning_rate': 2.9961586512864947e-06, 'epoch': 11.210084033613445}
>>> 2025-09-11 00:10:34,480 - INFO - >>> {'loss': 2.2218, 'grad_norm': 1.1067724227905273, 'learning_rate': 2.9898784784762934e-06, 'epoch': 11.214285714285714}
>>> 2025-09-11 00:10:38,270 - INFO - >>> {'loss': 2.1768, 'grad_norm': 1.1253892183303833, 'learning_rate': 2.983603737320978e-06, 'epoch': 11.218487394957982}
>>> 2025-09-11 00:10:42,134 - INFO - >>> {'loss': 2.2051, 'grad_norm': 1.1375035047531128, 'learning_rate': 2.9773344326824115e-06, 'epoch': 11.222689075630251}
>>> 2025-09-11 00:10:45,412 - INFO - >>> {'loss': 2.1037, 'grad_norm': 1.110105276107788, 'learning_rate': 2.9710705694182328e-06, 'epoch': 11.22689075630252}
>>> 2025-09-11 00:10:49,256 - INFO - >>> {'loss': 2.2086, 'grad_norm': 1.2262389659881592, 'learning_rate': 2.9648121523818785e-06, 'epoch': 11.231092436974789}
>>> 2025-09-11 00:10:53,188 - INFO - >>> {'loss': 2.2828, 'grad_norm': 1.009284257888794, 'learning_rate': 2.958559186422557e-06, 'epoch': 11.235294117647058}
>>> 2025-09-11 00:10:56,295 - INFO - >>> {'loss': 2.0182, 'grad_norm': 1.1332699060440063, 'learning_rate': 2.95231167638526e-06, 'epoch': 11.239495798319327}
>>> 2025-09-11 00:10:59,548 - INFO - >>> {'loss': 2.2075, 'grad_norm': 1.3972480297088623, 'learning_rate': 2.9460696271107446e-06, 'epoch': 11.243697478991596}
>>> 2025-09-11 00:11:03,186 - INFO - >>> {'loss': 2.1931, 'grad_norm': 1.1237910985946655, 'learning_rate': 2.9398330434355417e-06, 'epoch': 11.247899159663866}
>>> 2025-09-11 00:11:06,954 - INFO - >>> {'loss': 2.2379, 'grad_norm': 1.124701738357544, 'learning_rate': 2.9336019301919438e-06, 'epoch': 11.252100840336134}
>>> 2025-09-11 00:11:10,395 - INFO - >>> {'loss': 2.0911, 'grad_norm': 1.127284049987793, 'learning_rate': 2.9273762922080094e-06, 'epoch': 11.256302521008404}
>>> 2025-09-11 00:11:14,026 - INFO - >>> {'loss': 2.1282, 'grad_norm': 1.1766713857650757, 'learning_rate': 2.9211561343075488e-06, 'epoch': 11.260504201680673}
>>> 2025-09-11 00:11:17,441 - INFO - >>> {'loss': 2.2553, 'grad_norm': 1.2084803581237793, 'learning_rate': 2.9149414613101345e-06, 'epoch': 11.264705882352942}
>>> 2025-09-11 00:11:20,474 - INFO - >>> {'loss': 2.2452, 'grad_norm': 1.2803266048431396, 'learning_rate': 2.9087322780310766e-06, 'epoch': 11.268907563025211}
>>> 2025-09-11 00:11:23,854 - INFO - >>> {'loss': 2.1043, 'grad_norm': 1.157008171081543, 'learning_rate': 2.902528589281438e-06, 'epoch': 11.27310924369748}
>>> 2025-09-11 00:11:27,807 - INFO - >>> {'loss': 2.2199, 'grad_norm': 1.0886166095733643, 'learning_rate': 2.896330399868036e-06, 'epoch': 11.277310924369749}
>>> 2025-09-11 00:11:31,385 - INFO - >>> {'loss': 2.2309, 'grad_norm': 0.9843114614486694, 'learning_rate': 2.890137714593405e-06, 'epoch': 11.281512605042018}
>>> 2025-09-11 00:11:34,614 - INFO - >>> {'loss': 2.1834, 'grad_norm': 1.1875758171081543, 'learning_rate': 2.8839505382558296e-06, 'epoch': 11.285714285714286}
>>> 2025-09-11 00:11:38,593 - INFO - >>> {'loss': 2.2129, 'grad_norm': 0.9616950154304504, 'learning_rate': 2.8777688756493207e-06, 'epoch': 11.289915966386555}
>>> 2025-09-11 00:11:41,623 - INFO - >>> {'loss': 2.1262, 'grad_norm': 1.1772011518478394, 'learning_rate': 2.8715927315636184e-06, 'epoch': 11.294117647058824}
>>> 2025-09-11 00:11:45,017 - INFO - >>> {'loss': 2.1999, 'grad_norm': 1.1198095083236694, 'learning_rate': 2.8654221107841874e-06, 'epoch': 11.298319327731093}
>>> 2025-09-11 00:11:48,378 - INFO - >>> {'loss': 2.1526, 'grad_norm': 1.3789644241333008, 'learning_rate': 2.859257018092214e-06, 'epoch': 11.302521008403362}
>>> 2025-09-11 00:11:52,138 - INFO - >>> {'loss': 2.1837, 'grad_norm': 1.0125699043273926, 'learning_rate': 2.853097458264594e-06, 'epoch': 11.306722689075631}
>>> 2025-09-11 00:11:55,363 - INFO - >>> {'loss': 2.2868, 'grad_norm': 1.1926000118255615, 'learning_rate': 2.846943436073941e-06, 'epoch': 11.3109243697479}
>>> 2025-09-11 00:11:58,363 - INFO - >>> {'loss': 2.1426, 'grad_norm': 1.324789047241211, 'learning_rate': 2.840794956288586e-06, 'epoch': 11.315126050420169}
>>> 2025-09-11 00:12:01,650 - INFO - >>> {'loss': 2.2137, 'grad_norm': 1.1771471500396729, 'learning_rate': 2.8346520236725504e-06, 'epoch': 11.319327731092438}
>>> 2025-09-11 00:12:05,227 - INFO - >>> {'loss': 2.2797, 'grad_norm': 1.1348369121551514, 'learning_rate': 2.828514642985567e-06, 'epoch': 11.323529411764707}
>>> 2025-09-11 00:12:08,582 - INFO - >>> {'loss': 2.2531, 'grad_norm': 1.1763391494750977, 'learning_rate': 2.822382818983064e-06, 'epoch': 11.327731092436975}
>>> 2025-09-11 00:12:11,878 - INFO - >>> {'loss': 2.0866, 'grad_norm': 1.1975346803665161, 'learning_rate': 2.816256556416166e-06, 'epoch': 11.331932773109244}
>>> 2025-09-11 00:12:15,044 - INFO - >>> {'loss': 2.237, 'grad_norm': 1.1347218751907349, 'learning_rate': 2.8101358600316854e-06, 'epoch': 11.336134453781513}
>>> 2025-09-11 00:12:18,940 - INFO - >>> {'loss': 2.1083, 'grad_norm': 1.1763875484466553, 'learning_rate': 2.804020734572126e-06, 'epoch': 11.340336134453782}
>>> 2025-09-11 00:12:22,225 - INFO - >>> {'loss': 2.1476, 'grad_norm': 1.0254795551300049, 'learning_rate': 2.7979111847756703e-06, 'epoch': 11.344537815126051}
>>> 2025-09-11 00:12:25,761 - INFO - >>> {'loss': 2.2063, 'grad_norm': 1.191854476928711, 'learning_rate': 2.791807215376183e-06, 'epoch': 11.34873949579832}
>>> 2025-09-11 00:12:29,130 - INFO - >>> {'loss': 2.0914, 'grad_norm': 1.0134884119033813, 'learning_rate': 2.7857088311032065e-06, 'epoch': 11.352941176470589}
>>> 2025-09-11 00:12:32,336 - INFO - >>> {'loss': 2.1772, 'grad_norm': 1.3218412399291992, 'learning_rate': 2.779616036681957e-06, 'epoch': 11.357142857142858}
>>> 2025-09-11 00:12:35,638 - INFO - >>> {'loss': 2.1945, 'grad_norm': 1.1002711057662964, 'learning_rate': 2.773528836833308e-06, 'epoch': 11.361344537815127}
>>> 2025-09-11 00:12:38,840 - INFO - >>> {'loss': 2.1583, 'grad_norm': 1.0945899486541748, 'learning_rate': 2.767447236273809e-06, 'epoch': 11.365546218487395}
>>> 2025-09-11 00:12:41,878 - INFO - >>> {'loss': 2.1546, 'grad_norm': 1.1473798751831055, 'learning_rate': 2.7613712397156767e-06, 'epoch': 11.369747899159664}
>>> 2025-09-11 00:12:45,064 - INFO - >>> {'loss': 2.2276, 'grad_norm': 1.2060819864273071, 'learning_rate': 2.755300851866769e-06, 'epoch': 11.373949579831933}
>>> 2025-09-11 00:12:48,550 - INFO - >>> {'loss': 2.1619, 'grad_norm': 1.1047917604446411, 'learning_rate': 2.7492360774306093e-06, 'epoch': 11.378151260504202}
>>> 2025-09-11 00:12:52,186 - INFO - >>> {'loss': 2.1933, 'grad_norm': 1.1055244207382202, 'learning_rate': 2.7431769211063686e-06, 'epoch': 11.382352941176471}
>>> 2025-09-11 00:12:54,854 - INFO - >>> {'loss': 2.24, 'grad_norm': 1.3184176683425903, 'learning_rate': 2.737123387588867e-06, 'epoch': 11.38655462184874}
>>> 2025-09-11 00:12:58,199 - INFO - >>> {'loss': 2.1362, 'grad_norm': 1.1584073305130005, 'learning_rate': 2.7310754815685627e-06, 'epoch': 11.390756302521009}
>>> 2025-09-11 00:13:01,636 - INFO - >>> {'loss': 2.1775, 'grad_norm': 1.2212557792663574, 'learning_rate': 2.725033207731562e-06, 'epoch': 11.394957983193278}
>>> 2025-09-11 00:13:05,432 - INFO - >>> {'loss': 2.1556, 'grad_norm': 1.1283116340637207, 'learning_rate': 2.7189965707595965e-06, 'epoch': 11.399159663865547}
>>> 2025-09-11 00:13:08,778 - INFO - >>> {'loss': 2.1747, 'grad_norm': 1.1936558485031128, 'learning_rate': 2.7129655753300344e-06, 'epoch': 11.403361344537815}
>>> 2025-09-11 00:13:12,133 - INFO - >>> {'loss': 2.1213, 'grad_norm': 1.1455278396606445, 'learning_rate': 2.706940226115883e-06, 'epoch': 11.407563025210084}
>>> 2025-09-11 00:13:15,435 - INFO - >>> {'loss': 2.259, 'grad_norm': 1.093685507774353, 'learning_rate': 2.7009205277857576e-06, 'epoch': 11.411764705882353}
>>> 2025-09-11 00:13:18,740 - INFO - >>> {'loss': 2.0797, 'grad_norm': 1.1755337715148926, 'learning_rate': 2.694906485003905e-06, 'epoch': 11.415966386554622}
>>> 2025-09-11 00:13:22,283 - INFO - >>> {'loss': 2.0642, 'grad_norm': 1.0744661092758179, 'learning_rate': 2.6888981024301897e-06, 'epoch': 11.420168067226891}
>>> 2025-09-11 00:13:25,894 - INFO - >>> {'loss': 2.1832, 'grad_norm': 1.2244901657104492, 'learning_rate': 2.682895384720087e-06, 'epoch': 11.42436974789916}
>>> 2025-09-11 00:13:29,594 - INFO - >>> {'loss': 2.242, 'grad_norm': 1.1650851964950562, 'learning_rate': 2.6768983365246857e-06, 'epoch': 11.428571428571429}
>>> 2025-09-11 00:13:33,352 - INFO - >>> {'loss': 2.15, 'grad_norm': 1.2090599536895752, 'learning_rate': 2.6709069624906813e-06, 'epoch': 11.432773109243698}
>>> 2025-09-11 00:13:37,114 - INFO - >>> {'loss': 2.0882, 'grad_norm': 1.1635284423828125, 'learning_rate': 2.664921267260374e-06, 'epoch': 11.436974789915967}
>>> 2025-09-11 00:13:40,749 - INFO - >>> {'loss': 2.157, 'grad_norm': 1.1143276691436768, 'learning_rate': 2.658941255471654e-06, 'epoch': 11.441176470588236}
>>> 2025-09-11 00:13:44,271 - INFO - >>> {'loss': 2.1052, 'grad_norm': 1.0699186325073242, 'learning_rate': 2.652966931758024e-06, 'epoch': 11.445378151260504}
>>> 2025-09-11 00:13:47,467 - INFO - >>> {'loss': 2.0999, 'grad_norm': 1.1514781713485718, 'learning_rate': 2.646998300748572e-06, 'epoch': 11.449579831932773}
>>> 2025-09-11 00:13:51,480 - INFO - >>> {'loss': 2.2277, 'grad_norm': 1.0287288427352905, 'learning_rate': 2.6410353670679687e-06, 'epoch': 11.453781512605042}
>>> 2025-09-11 00:13:54,807 - INFO - >>> {'loss': 2.1605, 'grad_norm': 1.1019726991653442, 'learning_rate': 2.635078135336476e-06, 'epoch': 11.457983193277311}
>>> 2025-09-11 00:13:58,026 - INFO - >>> {'loss': 2.3102, 'grad_norm': 1.2442800998687744, 'learning_rate': 2.6291266101699454e-06, 'epoch': 11.46218487394958}
>>> 2025-09-11 00:14:01,695 - INFO - >>> {'loss': 2.1073, 'grad_norm': 1.1477831602096558, 'learning_rate': 2.6231807961797916e-06, 'epoch': 11.466386554621849}
>>> 2025-09-11 00:14:04,758 - INFO - >>> {'loss': 2.1741, 'grad_norm': 1.2151011228561401, 'learning_rate': 2.617240697973016e-06, 'epoch': 11.470588235294118}
>>> 2025-09-11 00:14:07,965 - INFO - >>> {'loss': 2.1278, 'grad_norm': 1.1709927320480347, 'learning_rate': 2.611306320152186e-06, 'epoch': 11.474789915966387}
>>> 2025-09-11 00:14:11,261 - INFO - >>> {'loss': 2.1598, 'grad_norm': 1.1027964353561401, 'learning_rate': 2.6053776673154317e-06, 'epoch': 11.478991596638656}
>>> 2025-09-11 00:14:15,257 - INFO - >>> {'loss': 2.1848, 'grad_norm': 1.247233271598816, 'learning_rate': 2.599454744056462e-06, 'epoch': 11.483193277310924}
>>> 2025-09-11 00:14:18,304 - INFO - >>> {'loss': 2.1711, 'grad_norm': 1.162655234336853, 'learning_rate': 2.593537554964537e-06, 'epoch': 11.487394957983193}
>>> 2025-09-11 00:14:22,377 - INFO - >>> {'loss': 2.2599, 'grad_norm': 1.094647765159607, 'learning_rate': 2.587626104624469e-06, 'epoch': 11.491596638655462}
>>> 2025-09-11 00:14:26,165 - INFO - >>> {'loss': 2.1638, 'grad_norm': 1.0836000442504883, 'learning_rate': 2.581720397616628e-06, 'epoch': 11.495798319327731}
>>> 2025-09-11 00:14:30,031 - INFO - >>> {'loss': 2.2783, 'grad_norm': 1.036243200302124, 'learning_rate': 2.575820438516944e-06, 'epoch': 11.5}
>>> 2025-09-11 00:14:34,008 - INFO - >>> {'loss': 2.1656, 'grad_norm': 1.1029213666915894, 'learning_rate': 2.5699262318968763e-06, 'epoch': 11.504201680672269}
>>> 2025-09-11 00:14:38,030 - INFO - >>> {'loss': 2.3071, 'grad_norm': 1.1001629829406738, 'learning_rate': 2.5640377823234387e-06, 'epoch': 11.508403361344538}
>>> 2025-09-11 00:14:41,827 - INFO - >>> {'loss': 2.1956, 'grad_norm': 1.0424840450286865, 'learning_rate': 2.5581550943591783e-06, 'epoch': 11.512605042016807}
>>> 2025-09-11 00:14:45,535 - INFO - >>> {'loss': 2.1348, 'grad_norm': 1.0630431175231934, 'learning_rate': 2.5522781725621814e-06, 'epoch': 11.516806722689076}
>>> 2025-09-11 00:14:48,899 - INFO - >>> {'loss': 2.1837, 'grad_norm': 1.1244155168533325, 'learning_rate': 2.546407021486067e-06, 'epoch': 11.521008403361344}
>>> 2025-09-11 00:14:52,721 - INFO - >>> {'loss': 2.1642, 'grad_norm': 1.1683374643325806, 'learning_rate': 2.540541645679978e-06, 'epoch': 11.525210084033613}
>>> 2025-09-11 00:14:56,319 - INFO - >>> {'loss': 2.1454, 'grad_norm': 1.1158783435821533, 'learning_rate': 2.5346820496885917e-06, 'epoch': 11.529411764705882}
>>> 2025-09-11 00:14:59,886 - INFO - >>> {'loss': 2.2033, 'grad_norm': 1.088067650794983, 'learning_rate': 2.528828238052089e-06, 'epoch': 11.533613445378151}
>>> 2025-09-11 00:15:03,263 - INFO - >>> {'loss': 2.145, 'grad_norm': 1.1960577964782715, 'learning_rate': 2.5229802153061924e-06, 'epoch': 11.53781512605042}
>>> 2025-09-11 00:15:06,947 - INFO - >>> {'loss': 2.2241, 'grad_norm': 1.0990673303604126, 'learning_rate': 2.5171379859821254e-06, 'epoch': 11.542016806722689}
>>> 2025-09-11 00:15:10,828 - INFO - >>> {'loss': 2.2216, 'grad_norm': 1.111470103263855, 'learning_rate': 2.5113015546066187e-06, 'epoch': 11.546218487394958}
>>> 2025-09-11 00:15:14,152 - INFO - >>> {'loss': 2.153, 'grad_norm': 1.2276761531829834, 'learning_rate': 2.5054709257019183e-06, 'epoch': 11.550420168067227}
>>> 2025-09-11 00:15:17,962 - INFO - >>> {'loss': 2.1431, 'grad_norm': 1.1560513973236084, 'learning_rate': 2.49964610378578e-06, 'epoch': 11.554621848739496}
>>> 2025-09-11 00:15:21,203 - INFO - >>> {'loss': 2.1973, 'grad_norm': 1.1876078844070435, 'learning_rate': 2.4938270933714435e-06, 'epoch': 11.558823529411764}
>>> 2025-09-11 00:15:24,816 - INFO - >>> {'loss': 2.2021, 'grad_norm': 1.1630128622055054, 'learning_rate': 2.488013898967657e-06, 'epoch': 11.563025210084033}
>>> 2025-09-11 00:15:28,511 - INFO - >>> {'loss': 2.2557, 'grad_norm': 1.1941676139831543, 'learning_rate': 2.482206525078665e-06, 'epoch': 11.567226890756302}
>>> 2025-09-11 00:15:32,493 - INFO - >>> {'loss': 2.1801, 'grad_norm': 1.1075199842453003, 'learning_rate': 2.4764049762041874e-06, 'epoch': 11.571428571428571}
>>> 2025-09-11 00:15:36,265 - INFO - >>> {'loss': 2.1925, 'grad_norm': 1.1054757833480835, 'learning_rate': 2.470609256839448e-06, 'epoch': 11.57563025210084}
>>> 2025-09-11 00:15:39,201 - INFO - >>> {'loss': 2.2187, 'grad_norm': 1.2179155349731445, 'learning_rate': 2.4648193714751467e-06, 'epoch': 11.579831932773109}
>>> 2025-09-11 00:15:43,210 - INFO - >>> {'loss': 2.2573, 'grad_norm': 1.1850098371505737, 'learning_rate': 2.4590353245974563e-06, 'epoch': 11.584033613445378}
>>> 2025-09-11 00:15:46,194 - INFO - >>> {'loss': 2.2102, 'grad_norm': 1.2315067052841187, 'learning_rate': 2.4532571206880328e-06, 'epoch': 11.588235294117647}
>>> 2025-09-11 00:15:49,537 - INFO - >>> {'loss': 2.2265, 'grad_norm': 1.1713664531707764, 'learning_rate': 2.447484764224012e-06, 'epoch': 11.592436974789916}
>>> 2025-09-11 00:15:52,916 - INFO - >>> {'loss': 2.2058, 'grad_norm': 1.1570833921432495, 'learning_rate': 2.4417182596779822e-06, 'epoch': 11.596638655462185}
>>> 2025-09-11 00:15:56,788 - INFO - >>> {'loss': 2.2105, 'grad_norm': 1.1299563646316528, 'learning_rate': 2.4359576115180082e-06, 'epoch': 11.600840336134453}
>>> 2025-09-11 00:16:00,269 - INFO - >>> {'loss': 2.1897, 'grad_norm': 1.2325752973556519, 'learning_rate': 2.430202824207616e-06, 'epoch': 11.605042016806722}
>>> 2025-09-11 00:16:04,114 - INFO - >>> {'loss': 2.1687, 'grad_norm': 1.1582088470458984, 'learning_rate': 2.42445390220579e-06, 'epoch': 11.609243697478991}
>>> 2025-09-11 00:16:07,874 - INFO - >>> {'loss': 2.1846, 'grad_norm': 1.0793564319610596, 'learning_rate': 2.4187108499669674e-06, 'epoch': 11.61344537815126}
>>> 2025-09-11 00:16:11,726 - INFO - >>> {'loss': 2.2201, 'grad_norm': 1.1084781885147095, 'learning_rate': 2.4129736719410412e-06, 'epoch': 11.617647058823529}
>>> 2025-09-11 00:16:14,890 - INFO - >>> {'loss': 2.1154, 'grad_norm': 1.1084778308868408, 'learning_rate': 2.407242372573354e-06, 'epoch': 11.621848739495798}
>>> 2025-09-11 00:16:18,153 - INFO - >>> {'loss': 2.2623, 'grad_norm': 1.1462223529815674, 'learning_rate': 2.40151695630468e-06, 'epoch': 11.626050420168067}
>>> 2025-09-11 00:16:21,647 - INFO - >>> {'loss': 2.2427, 'grad_norm': 1.1836777925491333, 'learning_rate': 2.395797427571256e-06, 'epoch': 11.630252100840336}
>>> 2025-09-11 00:16:25,261 - INFO - >>> {'loss': 2.1859, 'grad_norm': 1.1746439933776855, 'learning_rate': 2.3900837908047457e-06, 'epoch': 11.634453781512605}
>>> 2025-09-11 00:16:28,672 - INFO - >>> {'loss': 2.2264, 'grad_norm': 1.0820577144622803, 'learning_rate': 2.3843760504322423e-06, 'epoch': 11.638655462184873}
>>> 2025-09-11 00:16:32,321 - INFO - >>> {'loss': 2.1745, 'grad_norm': 1.0282140970230103, 'learning_rate': 2.3786742108762793e-06, 'epoch': 11.642857142857142}
>>> 2025-09-11 00:16:36,489 - INFO - >>> {'loss': 2.1271, 'grad_norm': 1.0541564226150513, 'learning_rate': 2.372978276554815e-06, 'epoch': 11.647058823529411}
>>> 2025-09-11 00:16:40,487 - INFO - >>> {'loss': 2.1516, 'grad_norm': 0.9913187623023987, 'learning_rate': 2.3672882518812324e-06, 'epoch': 11.65126050420168}
>>> 2025-09-11 00:16:44,017 - INFO - >>> {'loss': 2.1439, 'grad_norm': 1.1657952070236206, 'learning_rate': 2.361604141264334e-06, 'epoch': 11.655462184873949}
>>> 2025-09-11 00:16:47,303 - INFO - >>> {'loss': 2.3067, 'grad_norm': 1.1720490455627441, 'learning_rate': 2.3559259491083463e-06, 'epoch': 11.659663865546218}
>>> 2025-09-11 00:16:50,338 - INFO - >>> {'loss': 2.2367, 'grad_norm': 1.1878467798233032, 'learning_rate': 2.350253679812895e-06, 'epoch': 11.663865546218487}
>>> 2025-09-11 00:16:53,484 - INFO - >>> {'loss': 2.2498, 'grad_norm': 1.174753189086914, 'learning_rate': 2.3445873377730354e-06, 'epoch': 11.668067226890756}
>>> 2025-09-11 00:16:56,950 - INFO - >>> {'loss': 2.2142, 'grad_norm': 1.217736005783081, 'learning_rate': 2.33892692737922e-06, 'epoch': 11.672268907563025}
>>> 2025-09-11 00:17:00,973 - INFO - >>> {'loss': 2.1756, 'grad_norm': 1.2265995740890503, 'learning_rate': 2.3332724530173033e-06, 'epoch': 11.676470588235293}
>>> 2025-09-11 00:17:04,102 - INFO - >>> {'loss': 2.1174, 'grad_norm': 1.128050684928894, 'learning_rate': 2.3276239190685436e-06, 'epoch': 11.680672268907562}
>>> 2025-09-11 00:17:07,103 - INFO - >>> {'loss': 2.2705, 'grad_norm': 1.1881556510925293, 'learning_rate': 2.3219813299095985e-06, 'epoch': 11.684873949579831}
>>> 2025-09-11 00:17:10,620 - INFO - >>> {'loss': 2.2033, 'grad_norm': 1.172873616218567, 'learning_rate': 2.3163446899125175e-06, 'epoch': 11.6890756302521}
>>> 2025-09-11 00:17:14,884 - INFO - >>> {'loss': 2.2316, 'grad_norm': 1.0161794424057007, 'learning_rate': 2.3107140034447383e-06, 'epoch': 11.693277310924369}
>>> 2025-09-11 00:17:17,922 - INFO - >>> {'loss': 2.099, 'grad_norm': 1.2470078468322754, 'learning_rate': 2.30508927486909e-06, 'epoch': 11.697478991596638}
>>> 2025-09-11 00:17:21,967 - INFO - >>> {'loss': 2.1052, 'grad_norm': 1.1457781791687012, 'learning_rate': 2.2994705085437807e-06, 'epoch': 11.701680672268907}
>>> 2025-09-11 00:17:25,742 - INFO - >>> {'loss': 2.2037, 'grad_norm': 1.1572682857513428, 'learning_rate': 2.293857708822402e-06, 'epoch': 11.705882352941176}
>>> 2025-09-11 00:17:29,721 - INFO - >>> {'loss': 2.2159, 'grad_norm': 1.0459368228912354, 'learning_rate': 2.288250880053923e-06, 'epoch': 11.710084033613445}
>>> 2025-09-11 00:17:33,545 - INFO - >>> {'loss': 2.1323, 'grad_norm': 1.0793673992156982, 'learning_rate': 2.2826500265826858e-06, 'epoch': 11.714285714285714}
>>> 2025-09-11 00:17:37,573 - INFO - >>> {'loss': 2.3081, 'grad_norm': 1.032178282737732, 'learning_rate': 2.2770551527483965e-06, 'epoch': 11.718487394957982}
>>> 2025-09-11 00:17:40,948 - INFO - >>> {'loss': 2.1763, 'grad_norm': 1.352988362312317, 'learning_rate': 2.271466262886133e-06, 'epoch': 11.722689075630251}
>>> 2025-09-11 00:17:45,035 - INFO - >>> {'loss': 2.2647, 'grad_norm': 1.0332013368606567, 'learning_rate': 2.2658833613263443e-06, 'epoch': 11.72689075630252}
>>> 2025-09-11 00:17:48,120 - INFO - >>> {'loss': 2.2091, 'grad_norm': 1.1081253290176392, 'learning_rate': 2.260306452394826e-06, 'epoch': 11.731092436974789}
>>> 2025-09-11 00:17:51,906 - INFO - >>> {'loss': 2.1436, 'grad_norm': 1.1195955276489258, 'learning_rate': 2.254735540412737e-06, 'epoch': 11.735294117647058}
>>> 2025-09-11 00:17:55,073 - INFO - >>> {'loss': 2.2663, 'grad_norm': 1.0828875303268433, 'learning_rate': 2.24917062969659e-06, 'epoch': 11.739495798319329}
>>> 2025-09-11 00:17:58,329 - INFO - >>> {'loss': 2.1562, 'grad_norm': 1.1507765054702759, 'learning_rate': 2.2436117245582457e-06, 'epoch': 11.743697478991596}
>>> 2025-09-11 00:18:00,974 - INFO - >>> {'loss': 2.1819, 'grad_norm': 1.3736205101013184, 'learning_rate': 2.238058829304913e-06, 'epoch': 11.747899159663866}
>>> 2025-09-11 00:18:04,575 - INFO - >>> {'loss': 2.1741, 'grad_norm': 1.0134791135787964, 'learning_rate': 2.2325119482391466e-06, 'epoch': 11.752100840336134}
>>> 2025-09-11 00:18:08,283 - INFO - >>> {'loss': 2.1432, 'grad_norm': 1.1950931549072266, 'learning_rate': 2.226971085658831e-06, 'epoch': 11.756302521008404}
>>> 2025-09-11 00:18:11,620 - INFO - >>> {'loss': 2.0852, 'grad_norm': 1.2369369268417358, 'learning_rate': 2.2214362458572013e-06, 'epoch': 11.760504201680671}
>>> 2025-09-11 00:18:14,790 - INFO - >>> {'loss': 2.1489, 'grad_norm': 1.2367440462112427, 'learning_rate': 2.2159074331228194e-06, 'epoch': 11.764705882352942}
>>> 2025-09-11 00:18:18,585 - INFO - >>> {'loss': 2.2303, 'grad_norm': 1.0587024688720703, 'learning_rate': 2.2103846517395732e-06, 'epoch': 11.768907563025211}
>>> 2025-09-11 00:18:22,022 - INFO - >>> {'loss': 2.2388, 'grad_norm': 1.1044278144836426, 'learning_rate': 2.2048679059866818e-06, 'epoch': 11.77310924369748}
>>> 2025-09-11 00:18:25,096 - INFO - >>> {'loss': 2.2591, 'grad_norm': 1.0957900285720825, 'learning_rate': 2.1993572001386896e-06, 'epoch': 11.777310924369749}
>>> 2025-09-11 00:18:29,192 - INFO - >>> {'loss': 2.1485, 'grad_norm': 1.1513118743896484, 'learning_rate': 2.1938525384654576e-06, 'epoch': 11.781512605042018}
>>> 2025-09-11 00:18:33,300 - INFO - >>> {'loss': 2.208, 'grad_norm': 1.0466357469558716, 'learning_rate': 2.188353925232164e-06, 'epoch': 11.785714285714286}
>>> 2025-09-11 00:18:36,617 - INFO - >>> {'loss': 2.176, 'grad_norm': 1.118053913116455, 'learning_rate': 2.1828613646993023e-06, 'epoch': 11.789915966386555}
>>> 2025-09-11 00:18:40,055 - INFO - >>> {'loss': 2.1946, 'grad_norm': 1.2583367824554443, 'learning_rate': 2.177374861122674e-06, 'epoch': 11.794117647058824}
>>> 2025-09-11 00:18:43,641 - INFO - >>> {'loss': 2.1706, 'grad_norm': 1.1348292827606201, 'learning_rate': 2.17189441875339e-06, 'epoch': 11.798319327731093}
>>> 2025-09-11 00:18:47,645 - INFO - >>> {'loss': 2.3015, 'grad_norm': 1.1990702152252197, 'learning_rate': 2.1664200418378624e-06, 'epoch': 11.802521008403362}
>>> 2025-09-11 00:18:50,982 - INFO - >>> {'loss': 2.1609, 'grad_norm': 1.167681336402893, 'learning_rate': 2.160951734617808e-06, 'epoch': 11.806722689075631}
>>> 2025-09-11 00:18:54,557 - INFO - >>> {'loss': 2.1016, 'grad_norm': 1.1339545249938965, 'learning_rate': 2.15548950133023e-06, 'epoch': 11.8109243697479}
>>> 2025-09-11 00:18:58,199 - INFO - >>> {'loss': 2.1571, 'grad_norm': 1.1768516302108765, 'learning_rate': 2.150033346207435e-06, 'epoch': 11.815126050420169}
>>> 2025-09-11 00:19:01,964 - INFO - >>> {'loss': 2.2467, 'grad_norm': 1.060534954071045, 'learning_rate': 2.1445832734770235e-06, 'epoch': 11.819327731092438}
>>> 2025-09-11 00:19:05,379 - INFO - >>> {'loss': 2.1805, 'grad_norm': 1.1020495891571045, 'learning_rate': 2.139139287361871e-06, 'epoch': 11.823529411764707}
>>> 2025-09-11 00:19:08,587 - INFO - >>> {'loss': 2.2794, 'grad_norm': 1.156899094581604, 'learning_rate': 2.133701392080144e-06, 'epoch': 11.827731092436975}
>>> 2025-09-11 00:19:11,896 - INFO - >>> {'loss': 2.215, 'grad_norm': 1.1029291152954102, 'learning_rate': 2.12826959184529e-06, 'epoch': 11.831932773109244}
>>> 2025-09-11 00:19:15,359 - INFO - >>> {'loss': 2.2681, 'grad_norm': 1.2227119207382202, 'learning_rate': 2.122843890866033e-06, 'epoch': 11.836134453781513}
>>> 2025-09-11 00:19:18,735 - INFO - >>> {'loss': 2.2115, 'grad_norm': 1.2267868518829346, 'learning_rate': 2.1174242933463697e-06, 'epoch': 11.840336134453782}
>>> 2025-09-11 00:19:21,880 - INFO - >>> {'loss': 2.1913, 'grad_norm': 1.1807481050491333, 'learning_rate': 2.112010803485571e-06, 'epoch': 11.844537815126051}
>>> 2025-09-11 00:19:25,623 - INFO - >>> {'loss': 2.1245, 'grad_norm': 1.2120589017868042, 'learning_rate': 2.10660342547817e-06, 'epoch': 11.84873949579832}
>>> 2025-09-11 00:19:28,951 - INFO - >>> {'loss': 2.2322, 'grad_norm': 1.2225394248962402, 'learning_rate': 2.1012021635139646e-06, 'epoch': 11.852941176470589}
>>> 2025-09-11 00:19:33,136 - INFO - >>> {'loss': 2.1621, 'grad_norm': 1.1404668092727661, 'learning_rate': 2.095807021778026e-06, 'epoch': 11.857142857142858}
>>> 2025-09-11 00:19:36,541 - INFO - >>> {'loss': 2.1918, 'grad_norm': 1.0972046852111816, 'learning_rate': 2.090418004450665e-06, 'epoch': 11.861344537815127}
>>> 2025-09-11 00:19:40,675 - INFO - >>> {'loss': 2.2658, 'grad_norm': 1.0447800159454346, 'learning_rate': 2.08503511570746e-06, 'epoch': 11.865546218487395}
>>> 2025-09-11 00:19:44,545 - INFO - >>> {'loss': 2.2417, 'grad_norm': 1.1113487482070923, 'learning_rate': 2.0796583597192356e-06, 'epoch': 11.869747899159664}
>>> 2025-09-11 00:19:47,947 - INFO - >>> {'loss': 2.1132, 'grad_norm': 1.2148897647857666, 'learning_rate': 2.0742877406520644e-06, 'epoch': 11.873949579831933}
>>> 2025-09-11 00:19:51,441 - INFO - >>> {'loss': 2.222, 'grad_norm': 1.164145588874817, 'learning_rate': 2.068923262667266e-06, 'epoch': 11.878151260504202}
>>> 2025-09-11 00:19:55,280 - INFO - >>> {'loss': 2.3858, 'grad_norm': 1.078352689743042, 'learning_rate': 2.063564929921401e-06, 'epoch': 11.882352941176471}
>>> 2025-09-11 00:19:58,722 - INFO - >>> {'loss': 2.2921, 'grad_norm': 1.1035847663879395, 'learning_rate': 2.0582127465662707e-06, 'epoch': 11.88655462184874}
>>> 2025-09-11 00:20:02,500 - INFO - >>> {'loss': 2.1652, 'grad_norm': 1.0697479248046875, 'learning_rate': 2.0528667167489015e-06, 'epoch': 11.890756302521009}
>>> 2025-09-11 00:20:06,011 - INFO - >>> {'loss': 2.3216, 'grad_norm': 1.1491070985794067, 'learning_rate': 2.047526844611566e-06, 'epoch': 11.894957983193278}
>>> 2025-09-11 00:20:09,255 - INFO - >>> {'loss': 2.2444, 'grad_norm': 1.2568814754486084, 'learning_rate': 2.0421931342917613e-06, 'epoch': 11.899159663865547}
>>> 2025-09-11 00:20:13,100 - INFO - >>> {'loss': 2.1526, 'grad_norm': 1.0429248809814453, 'learning_rate': 2.036865589922201e-06, 'epoch': 11.903361344537815}
>>> 2025-09-11 00:20:16,745 - INFO - >>> {'loss': 2.1019, 'grad_norm': 1.2164149284362793, 'learning_rate': 2.031544215630832e-06, 'epoch': 11.907563025210084}
>>> 2025-09-11 00:20:20,202 - INFO - >>> {'loss': 2.1644, 'grad_norm': 1.1789228916168213, 'learning_rate': 2.026229015540816e-06, 'epoch': 11.911764705882353}
>>> 2025-09-11 00:20:23,681 - INFO - >>> {'loss': 2.095, 'grad_norm': 1.1768994331359863, 'learning_rate': 2.02091999377053e-06, 'epoch': 11.915966386554622}
>>> 2025-09-11 00:20:27,670 - INFO - >>> {'loss': 2.2375, 'grad_norm': 1.2651019096374512, 'learning_rate': 2.015617154433569e-06, 'epoch': 11.920168067226891}
>>> 2025-09-11 00:20:30,978 - INFO - >>> {'loss': 2.1158, 'grad_norm': 1.1388543844223022, 'learning_rate': 2.010320501638733e-06, 'epoch': 11.92436974789916}
>>> 2025-09-11 00:20:34,561 - INFO - >>> {'loss': 2.0725, 'grad_norm': 1.1539440155029297, 'learning_rate': 2.0050300394900214e-06, 'epoch': 11.928571428571429}
>>> 2025-09-11 00:20:38,307 - INFO - >>> {'loss': 2.2372, 'grad_norm': 1.1162327527999878, 'learning_rate': 1.9997457720866554e-06, 'epoch': 11.932773109243698}
>>> 2025-09-11 00:20:41,681 - INFO - >>> {'loss': 2.1754, 'grad_norm': 1.0743650197982788, 'learning_rate': 1.9944677035230432e-06, 'epoch': 11.936974789915967}
>>> 2025-09-11 00:20:45,557 - INFO - >>> {'loss': 2.2258, 'grad_norm': 1.0985959768295288, 'learning_rate': 1.989195837888788e-06, 'epoch': 11.941176470588236}
>>> 2025-09-11 00:20:49,227 - INFO - >>> {'loss': 2.2304, 'grad_norm': 1.1151094436645508, 'learning_rate': 1.9839301792686916e-06, 'epoch': 11.945378151260504}
>>> 2025-09-11 00:20:52,313 - INFO - >>> {'loss': 2.0502, 'grad_norm': 1.1555237770080566, 'learning_rate': 1.9786707317427543e-06, 'epoch': 11.949579831932773}
>>> 2025-09-11 00:20:55,517 - INFO - >>> {'loss': 2.1839, 'grad_norm': 1.1501911878585815, 'learning_rate': 1.9734174993861467e-06, 'epoch': 11.953781512605042}
>>> 2025-09-11 00:20:59,042 - INFO - >>> {'loss': 2.2827, 'grad_norm': 1.137138843536377, 'learning_rate': 1.968170486269236e-06, 'epoch': 11.957983193277311}
>>> 2025-09-11 00:21:03,091 - INFO - >>> {'loss': 2.2613, 'grad_norm': 1.0997613668441772, 'learning_rate': 1.9629296964575672e-06, 'epoch': 11.96218487394958}
>>> 2025-09-11 00:21:06,280 - INFO - >>> {'loss': 2.2231, 'grad_norm': 1.267513394355774, 'learning_rate': 1.9576951340118634e-06, 'epoch': 11.966386554621849}
>>> 2025-09-11 00:21:10,217 - INFO - >>> {'loss': 2.1798, 'grad_norm': 1.0310943126678467, 'learning_rate': 1.952466802988021e-06, 'epoch': 11.970588235294118}
>>> 2025-09-11 00:21:13,099 - INFO - >>> {'loss': 2.1221, 'grad_norm': 1.258553147315979, 'learning_rate': 1.9472447074371116e-06, 'epoch': 11.974789915966387}
>>> 2025-09-11 00:21:17,147 - INFO - >>> {'loss': 2.171, 'grad_norm': 1.1047837734222412, 'learning_rate': 1.9420288514053742e-06, 'epoch': 11.978991596638656}
>>> 2025-09-11 00:21:20,532 - INFO - >>> {'loss': 2.1566, 'grad_norm': 1.1223230361938477, 'learning_rate': 1.9368192389342066e-06, 'epoch': 11.983193277310924}
>>> 2025-09-11 00:21:23,852 - INFO - >>> {'loss': 2.1715, 'grad_norm': 1.200534701347351, 'learning_rate': 1.9316158740601796e-06, 'epoch': 11.987394957983193}
>>> 2025-09-11 00:21:27,599 - INFO - >>> {'loss': 2.2284, 'grad_norm': 1.0772960186004639, 'learning_rate': 1.9264187608150208e-06, 'epoch': 11.991596638655462}
>>> 2025-09-11 00:21:30,655 - INFO - >>> {'loss': 2.1634, 'grad_norm': 1.2429404258728027, 'learning_rate': 1.9212279032256043e-06, 'epoch': 11.995798319327731}
>>> 2025-09-11 00:21:33,998 - INFO - >>> {'loss': 2.248, 'grad_norm': 1.1952300071716309, 'learning_rate': 1.9160433053139682e-06, 'epoch': 12.0}
>>> 2025-09-11 00:21:37,649 - INFO - >>> {'loss': 2.1794, 'grad_norm': 1.159942626953125, 'learning_rate': 1.9108649710972947e-06, 'epoch': 12.004201680672269}
>>> 2025-09-11 00:21:40,992 - INFO - >>> {'loss': 2.2656, 'grad_norm': 1.1592544317245483, 'learning_rate': 1.9056929045879157e-06, 'epoch': 12.008403361344538}
>>> 2025-09-11 00:21:44,284 - INFO - >>> {'loss': 2.2784, 'grad_norm': 1.3730744123458862, 'learning_rate': 1.9005271097933042e-06, 'epoch': 12.012605042016807}
>>> 2025-09-11 00:21:47,890 - INFO - >>> {'loss': 2.1461, 'grad_norm': 1.3078633546829224, 'learning_rate': 1.8953675907160785e-06, 'epoch': 12.016806722689076}
>>> 2025-09-11 00:21:51,690 - INFO - >>> {'loss': 2.1137, 'grad_norm': 0.9992188215255737, 'learning_rate': 1.8902143513539816e-06, 'epoch': 12.021008403361344}
>>> 2025-09-11 00:21:54,833 - INFO - >>> {'loss': 2.2127, 'grad_norm': 1.3248742818832397, 'learning_rate': 1.8850673956999077e-06, 'epoch': 12.025210084033613}
>>> 2025-09-11 00:21:58,779 - INFO - >>> {'loss': 2.1676, 'grad_norm': 1.1144622564315796, 'learning_rate': 1.8799267277418743e-06, 'epoch': 12.029411764705882}
>>> 2025-09-11 00:22:02,051 - INFO - >>> {'loss': 2.1417, 'grad_norm': 1.1802490949630737, 'learning_rate': 1.8747923514630229e-06, 'epoch': 12.033613445378151}
>>> 2025-09-11 00:22:05,899 - INFO - >>> {'loss': 2.2096, 'grad_norm': 1.11720609664917, 'learning_rate': 1.8696642708416224e-06, 'epoch': 12.03781512605042}
>>> 2025-09-11 00:22:08,658 - INFO - >>> {'loss': 2.2134, 'grad_norm': 1.237641453742981, 'learning_rate': 1.8645424898510745e-06, 'epoch': 12.042016806722689}
>>> 2025-09-11 00:22:11,930 - INFO - >>> {'loss': 2.2087, 'grad_norm': 1.2143163681030273, 'learning_rate': 1.8594270124598822e-06, 'epoch': 12.046218487394958}
>>> 2025-09-11 00:22:14,965 - INFO - >>> {'loss': 2.1602, 'grad_norm': 1.1354461908340454, 'learning_rate': 1.8543178426316743e-06, 'epoch': 12.050420168067227}
>>> 2025-09-11 00:22:18,692 - INFO - >>> {'loss': 2.2659, 'grad_norm': 1.1674251556396484, 'learning_rate': 1.8492149843251916e-06, 'epoch': 12.054621848739496}
>>> 2025-09-11 00:22:22,225 - INFO - >>> {'loss': 2.2314, 'grad_norm': 1.1621865034103394, 'learning_rate': 1.8441184414942837e-06, 'epoch': 12.058823529411764}
>>> 2025-09-11 00:22:25,632 - INFO - >>> {'loss': 2.1806, 'grad_norm': 1.1024501323699951, 'learning_rate': 1.8390282180879059e-06, 'epoch': 12.063025210084033}
>>> 2025-09-11 00:22:28,982 - INFO - >>> {'loss': 2.1618, 'grad_norm': 1.103358268737793, 'learning_rate': 1.8339443180501183e-06, 'epoch': 12.067226890756302}
>>> 2025-09-11 00:22:32,202 - INFO - >>> {'loss': 2.1641, 'grad_norm': 1.0692639350891113, 'learning_rate': 1.8288667453200827e-06, 'epoch': 12.071428571428571}
>>> 2025-09-11 00:22:35,815 - INFO - >>> {'loss': 2.2149, 'grad_norm': 1.0235016345977783, 'learning_rate': 1.823795503832051e-06, 'epoch': 12.07563025210084}
>>> 2025-09-11 00:22:39,179 - INFO - >>> {'loss': 2.1814, 'grad_norm': 1.1120983362197876, 'learning_rate': 1.8187305975153825e-06, 'epoch': 12.079831932773109}
>>> 2025-09-11 00:22:42,434 - INFO - >>> {'loss': 2.1763, 'grad_norm': 1.037767767906189, 'learning_rate': 1.8136720302945143e-06, 'epoch': 12.084033613445378}
>>> 2025-09-11 00:22:46,264 - INFO - >>> {'loss': 2.1046, 'grad_norm': 1.1500581502914429, 'learning_rate': 1.8086198060889793e-06, 'epoch': 12.088235294117647}
>>> 2025-09-11 00:22:49,991 - INFO - >>> {'loss': 2.2443, 'grad_norm': 1.020667552947998, 'learning_rate': 1.8035739288133958e-06, 'epoch': 12.092436974789916}
>>> 2025-09-11 00:22:52,951 - INFO - >>> {'loss': 2.2567, 'grad_norm': 1.2085447311401367, 'learning_rate': 1.7985344023774608e-06, 'epoch': 12.096638655462185}
>>> 2025-09-11 00:22:56,630 - INFO - >>> {'loss': 2.1345, 'grad_norm': 1.0851093530654907, 'learning_rate': 1.7935012306859534e-06, 'epoch': 12.100840336134453}
>>> 2025-09-11 00:22:59,790 - INFO - >>> {'loss': 2.2281, 'grad_norm': 1.2194033861160278, 'learning_rate': 1.788474417638728e-06, 'epoch': 12.105042016806722}
>>> 2025-09-11 00:23:02,590 - INFO - >>> {'loss': 2.2845, 'grad_norm': 1.2492527961730957, 'learning_rate': 1.7834539671307128e-06, 'epoch': 12.109243697478991}
>>> 2025-09-11 00:23:06,008 - INFO - >>> {'loss': 2.1469, 'grad_norm': 1.0841903686523438, 'learning_rate': 1.7784398830519002e-06, 'epoch': 12.11344537815126}
>>> 2025-09-11 00:23:09,933 - INFO - >>> {'loss': 2.2483, 'grad_norm': 1.056650161743164, 'learning_rate': 1.773432169287359e-06, 'epoch': 12.117647058823529}
>>> 2025-09-11 00:23:12,870 - INFO - >>> {'loss': 2.2232, 'grad_norm': 1.3177447319030762, 'learning_rate': 1.7684308297172203e-06, 'epoch': 12.121848739495798}
>>> 2025-09-11 00:23:16,578 - INFO - >>> {'loss': 2.1443, 'grad_norm': 1.1132465600967407, 'learning_rate': 1.7634358682166685e-06, 'epoch': 12.126050420168067}
>>> 2025-09-11 00:23:19,820 - INFO - >>> {'loss': 2.2602, 'grad_norm': 1.2162749767303467, 'learning_rate': 1.7584472886559533e-06, 'epoch': 12.130252100840336}
>>> 2025-09-11 00:23:23,150 - INFO - >>> {'loss': 2.1537, 'grad_norm': 1.1672701835632324, 'learning_rate': 1.7534650949003772e-06, 'epoch': 12.134453781512605}
>>> 2025-09-11 00:23:26,980 - INFO - >>> {'loss': 2.2427, 'grad_norm': 1.1469653844833374, 'learning_rate': 1.7484892908102956e-06, 'epoch': 12.138655462184873}
>>> 2025-09-11 00:23:30,367 - INFO - >>> {'loss': 2.2541, 'grad_norm': 1.2287297248840332, 'learning_rate': 1.7435198802411124e-06, 'epoch': 12.142857142857142}
>>> 2025-09-11 00:23:33,853 - INFO - >>> {'loss': 2.1876, 'grad_norm': 1.0233566761016846, 'learning_rate': 1.7385568670432784e-06, 'epoch': 12.147058823529411}
>>> 2025-09-11 00:23:37,129 - INFO - >>> {'loss': 2.1444, 'grad_norm': 1.2736445665359497, 'learning_rate': 1.7336002550622855e-06, 'epoch': 12.15126050420168}
>>> 2025-09-11 00:23:40,199 - INFO - >>> {'loss': 2.2093, 'grad_norm': 1.156832218170166, 'learning_rate': 1.7286500481386681e-06, 'epoch': 12.155462184873949}
>>> 2025-09-11 00:23:43,580 - INFO - >>> {'loss': 2.246, 'grad_norm': 1.1232575178146362, 'learning_rate': 1.723706250107998e-06, 'epoch': 12.159663865546218}
>>> 2025-09-11 00:23:47,501 - INFO - >>> {'loss': 2.1848, 'grad_norm': 1.0260636806488037, 'learning_rate': 1.7187688648008772e-06, 'epoch': 12.163865546218487}
>>> 2025-09-11 00:23:50,559 - INFO - >>> {'loss': 2.1755, 'grad_norm': 1.1384516954421997, 'learning_rate': 1.7138378960429391e-06, 'epoch': 12.168067226890756}
>>> 2025-09-11 00:23:53,814 - INFO - >>> {'loss': 2.2194, 'grad_norm': 1.2003456354141235, 'learning_rate': 1.708913347654857e-06, 'epoch': 12.172268907563025}
>>> 2025-09-11 00:23:57,205 - INFO - >>> {'loss': 2.167, 'grad_norm': 1.0921502113342285, 'learning_rate': 1.7039952234523127e-06, 'epoch': 12.176470588235293}
>>> 2025-09-11 00:24:00,323 - INFO - >>> {'loss': 2.1571, 'grad_norm': 1.3353652954101562, 'learning_rate': 1.6990835272460205e-06, 'epoch': 12.180672268907562}
>>> 2025-09-11 00:24:03,580 - INFO - >>> {'loss': 2.1733, 'grad_norm': 1.150076150894165, 'learning_rate': 1.6941782628417114e-06, 'epoch': 12.184873949579831}
>>> 2025-09-11 00:24:07,495 - INFO - >>> {'loss': 2.1731, 'grad_norm': 1.1523078680038452, 'learning_rate': 1.689279434040133e-06, 'epoch': 12.1890756302521}
>>> 2025-09-11 00:24:10,554 - INFO - >>> {'loss': 2.1411, 'grad_norm': 1.0598948001861572, 'learning_rate': 1.6843870446370481e-06, 'epoch': 12.193277310924369}
>>> 2025-09-11 00:24:13,740 - INFO - >>> {'loss': 2.1351, 'grad_norm': 1.3308508396148682, 'learning_rate': 1.6795010984232263e-06, 'epoch': 12.197478991596638}
>>> 2025-09-11 00:24:17,131 - INFO - >>> {'loss': 2.1141, 'grad_norm': 1.0777064561843872, 'learning_rate': 1.6746215991844505e-06, 'epoch': 12.201680672268907}
>>> 2025-09-11 00:24:20,380 - INFO - >>> {'loss': 2.2306, 'grad_norm': 1.2800580263137817, 'learning_rate': 1.6697485507014966e-06, 'epoch': 12.205882352941176}
>>> 2025-09-11 00:24:24,090 - INFO - >>> {'loss': 2.1308, 'grad_norm': 1.1744909286499023, 'learning_rate': 1.6648819567501596e-06, 'epoch': 12.210084033613445}
>>> 2025-09-11 00:24:27,704 - INFO - >>> {'loss': 2.1469, 'grad_norm': 1.22963285446167, 'learning_rate': 1.660021821101222e-06, 'epoch': 12.214285714285714}
>>> 2025-09-11 00:24:30,888 - INFO - >>> {'loss': 2.0965, 'grad_norm': 1.2412704229354858, 'learning_rate': 1.6551681475204607e-06, 'epoch': 12.218487394957982}
>>> 2025-09-11 00:24:34,753 - INFO - >>> {'loss': 2.1685, 'grad_norm': 1.1056665182113647, 'learning_rate': 1.6503209397686527e-06, 'epoch': 12.222689075630251}
>>> 2025-09-11 00:24:38,559 - INFO - >>> {'loss': 2.1748, 'grad_norm': 1.061875581741333, 'learning_rate': 1.6454802016015592e-06, 'epoch': 12.22689075630252}
>>> 2025-09-11 00:24:42,226 - INFO - >>> {'loss': 2.2035, 'grad_norm': 1.1088812351226807, 'learning_rate': 1.6406459367699335e-06, 'epoch': 12.231092436974789}
>>> 2025-09-11 00:24:45,962 - INFO - >>> {'loss': 2.2158, 'grad_norm': 1.2022552490234375, 'learning_rate': 1.635818149019509e-06, 'epoch': 12.235294117647058}
>>> 2025-09-11 00:24:49,466 - INFO - >>> {'loss': 2.2099, 'grad_norm': 1.1158427000045776, 'learning_rate': 1.6309968420910028e-06, 'epoch': 12.239495798319327}
>>> 2025-09-11 00:24:52,514 - INFO - >>> {'loss': 2.1134, 'grad_norm': 1.2517995834350586, 'learning_rate': 1.6261820197201094e-06, 'epoch': 12.243697478991596}
>>> 2025-09-11 00:24:56,244 - INFO - >>> {'loss': 2.1938, 'grad_norm': 1.2327932119369507, 'learning_rate': 1.6213736856374994e-06, 'epoch': 12.247899159663866}
>>> 2025-09-11 00:24:59,527 - INFO - >>> {'loss': 2.233, 'grad_norm': 1.1815423965454102, 'learning_rate': 1.6165718435688194e-06, 'epoch': 12.252100840336134}
>>> 2025-09-11 00:25:02,584 - INFO - >>> {'loss': 2.2049, 'grad_norm': 1.1818279027938843, 'learning_rate': 1.6117764972346761e-06, 'epoch': 12.256302521008404}
>>> 2025-09-11 00:25:06,267 - INFO - >>> {'loss': 2.1248, 'grad_norm': 1.109262466430664, 'learning_rate': 1.606987650350652e-06, 'epoch': 12.260504201680673}
>>> 2025-09-11 00:25:10,189 - INFO - >>> {'loss': 2.2005, 'grad_norm': 1.0564649105072021, 'learning_rate': 1.6022053066272926e-06, 'epoch': 12.264705882352942}
>>> 2025-09-11 00:25:13,582 - INFO - >>> {'loss': 2.0951, 'grad_norm': 1.1838024854660034, 'learning_rate': 1.597429469770101e-06, 'epoch': 12.268907563025211}
>>> 2025-09-11 00:25:17,501 - INFO - >>> {'loss': 2.2618, 'grad_norm': 1.1701661348342896, 'learning_rate': 1.5926601434795409e-06, 'epoch': 12.27310924369748}
>>> 2025-09-11 00:25:20,507 - INFO - >>> {'loss': 2.1381, 'grad_norm': 1.2619425058364868, 'learning_rate': 1.5878973314510315e-06, 'epoch': 12.277310924369749}
>>> 2025-09-11 00:25:24,201 - INFO - >>> {'loss': 2.2373, 'grad_norm': 1.101447343826294, 'learning_rate': 1.5831410373749445e-06, 'epoch': 12.281512605042018}
>>> 2025-09-11 00:25:27,939 - INFO - >>> {'loss': 2.1584, 'grad_norm': 1.260054349899292, 'learning_rate': 1.5783912649366006e-06, 'epoch': 12.285714285714286}
>>> 2025-09-11 00:25:31,186 - INFO - >>> {'loss': 2.1289, 'grad_norm': 1.1169778108596802, 'learning_rate': 1.573648017816267e-06, 'epoch': 12.289915966386555}
>>> 2025-09-11 00:25:34,918 - INFO - >>> {'loss': 2.2896, 'grad_norm': 1.0886051654815674, 'learning_rate': 1.5689112996891576e-06, 'epoch': 12.294117647058824}
>>> 2025-09-11 00:25:38,133 - INFO - >>> {'loss': 2.2001, 'grad_norm': 1.1865571737289429, 'learning_rate': 1.5641811142254216e-06, 'epoch': 12.298319327731093}
>>> 2025-09-11 00:25:41,634 - INFO - >>> {'loss': 2.182, 'grad_norm': 1.1359617710113525, 'learning_rate': 1.5594574650901506e-06, 'epoch': 12.302521008403362}
>>> 2025-09-11 00:25:45,591 - INFO - >>> {'loss': 2.2007, 'grad_norm': 1.0891497135162354, 'learning_rate': 1.554740355943377e-06, 'epoch': 12.306722689075631}
>>> 2025-09-11 00:25:49,558 - INFO - >>> {'loss': 2.2298, 'grad_norm': 1.106701135635376, 'learning_rate': 1.550029790440054e-06, 'epoch': 12.3109243697479}
>>> 2025-09-11 00:25:53,175 - INFO - >>> {'loss': 2.1384, 'grad_norm': 1.0753880739212036, 'learning_rate': 1.545325772230073e-06, 'epoch': 12.315126050420169}
>>> 2025-09-11 00:25:56,615 - INFO - >>> {'loss': 2.1585, 'grad_norm': 1.0850239992141724, 'learning_rate': 1.5406283049582493e-06, 'epoch': 12.319327731092438}
>>> 2025-09-11 00:26:00,146 - INFO - >>> {'loss': 2.1598, 'grad_norm': 1.1543197631835938, 'learning_rate': 1.535937392264324e-06, 'epoch': 12.323529411764707}
>>> 2025-09-11 00:26:04,222 - INFO - >>> {'loss': 2.2733, 'grad_norm': 1.1648638248443604, 'learning_rate': 1.531253037782957e-06, 'epoch': 12.327731092436975}
>>> 2025-09-11 00:26:07,344 - INFO - >>> {'loss': 2.1974, 'grad_norm': 1.2540323734283447, 'learning_rate': 1.5265752451437332e-06, 'epoch': 12.331932773109244}
>>> 2025-09-11 00:26:11,345 - INFO - >>> {'loss': 2.2072, 'grad_norm': 1.0923477411270142, 'learning_rate': 1.521904017971142e-06, 'epoch': 12.336134453781513}
>>> 2025-09-11 00:26:14,801 - INFO - >>> {'loss': 2.1472, 'grad_norm': 1.0307538509368896, 'learning_rate': 1.5172393598845925e-06, 'epoch': 12.340336134453782}
>>> 2025-09-11 00:26:18,846 - INFO - >>> {'loss': 2.2009, 'grad_norm': 1.196405291557312, 'learning_rate': 1.5125812744984103e-06, 'epoch': 12.344537815126051}
>>> 2025-09-11 00:26:22,387 - INFO - >>> {'loss': 2.1983, 'grad_norm': 1.1648294925689697, 'learning_rate': 1.5079297654218127e-06, 'epoch': 12.34873949579832}
>>> 2025-09-11 00:26:26,080 - INFO - >>> {'loss': 2.0751, 'grad_norm': 1.2151556015014648, 'learning_rate': 1.5032848362589358e-06, 'epoch': 12.352941176470589}
>>> 2025-09-11 00:26:29,302 - INFO - >>> {'loss': 2.2512, 'grad_norm': 1.0699445009231567, 'learning_rate': 1.4986464906088083e-06, 'epoch': 12.357142857142858}
>>> 2025-09-11 00:26:32,526 - INFO - >>> {'loss': 2.1331, 'grad_norm': 1.3085901737213135, 'learning_rate': 1.494014732065363e-06, 'epoch': 12.361344537815127}
>>> 2025-09-11 00:26:35,782 - INFO - >>> {'loss': 2.282, 'grad_norm': 1.1328855752944946, 'learning_rate': 1.489389564217426e-06, 'epoch': 12.365546218487395}
>>> 2025-09-11 00:26:39,377 - INFO - >>> {'loss': 2.111, 'grad_norm': 1.2310889959335327, 'learning_rate': 1.4847709906487196e-06, 'epoch': 12.369747899159664}
>>> 2025-09-11 00:26:42,378 - INFO - >>> {'loss': 2.3525, 'grad_norm': 1.1901788711547852, 'learning_rate': 1.480159014937852e-06, 'epoch': 12.373949579831933}
>>> 2025-09-11 00:26:45,888 - INFO - >>> {'loss': 2.1708, 'grad_norm': 1.0749461650848389, 'learning_rate': 1.4755536406583238e-06, 'epoch': 12.378151260504202}
>>> 2025-09-11 00:26:49,268 - INFO - >>> {'loss': 2.2176, 'grad_norm': 1.0394583940505981, 'learning_rate': 1.4709548713785181e-06, 'epoch': 12.382352941176471}
>>> 2025-09-11 00:26:53,108 - INFO - >>> {'loss': 2.2134, 'grad_norm': 1.0696320533752441, 'learning_rate': 1.466362710661704e-06, 'epoch': 12.38655462184874}
>>> 2025-09-11 00:26:56,332 - INFO - >>> {'loss': 2.153, 'grad_norm': 1.1185967922210693, 'learning_rate': 1.4617771620660214e-06, 'epoch': 12.390756302521009}
>>> 2025-09-11 00:26:59,599 - INFO - >>> {'loss': 2.1919, 'grad_norm': 1.039749026298523, 'learning_rate': 1.457198229144493e-06, 'epoch': 12.394957983193278}
>>> 2025-09-11 00:27:03,032 - INFO - >>> {'loss': 2.2017, 'grad_norm': 1.193090558052063, 'learning_rate': 1.4526259154450217e-06, 'epoch': 12.399159663865547}
>>> 2025-09-11 00:27:06,751 - INFO - >>> {'loss': 2.1911, 'grad_norm': 1.0812739133834839, 'learning_rate': 1.4480602245103681e-06, 'epoch': 12.403361344537815}
>>> 2025-09-11 00:27:10,724 - INFO - >>> {'loss': 2.2312, 'grad_norm': 1.0772918462753296, 'learning_rate': 1.44350115987817e-06, 'epoch': 12.407563025210084}
>>> 2025-09-11 00:27:14,269 - INFO - >>> {'loss': 2.165, 'grad_norm': 1.086376667022705, 'learning_rate': 1.4389487250809297e-06, 'epoch': 12.411764705882353}
>>> 2025-09-11 00:27:17,997 - INFO - >>> {'loss': 2.2135, 'grad_norm': 1.243787169456482, 'learning_rate': 1.4344029236460112e-06, 'epoch': 12.415966386554622}
>>> 2025-09-11 00:27:21,219 - INFO - >>> {'loss': 2.1304, 'grad_norm': 1.1046419143676758, 'learning_rate': 1.42986375909564e-06, 'epoch': 12.420168067226891}
>>> 2025-09-11 00:27:25,074 - INFO - >>> {'loss': 2.1294, 'grad_norm': 1.1363238096237183, 'learning_rate': 1.4253312349469006e-06, 'epoch': 12.42436974789916}
>>> 2025-09-11 00:27:28,162 - INFO - >>> {'loss': 2.0846, 'grad_norm': 1.2207491397857666, 'learning_rate': 1.4208053547117251e-06, 'epoch': 12.428571428571429}
>>> 2025-09-11 00:27:31,598 - INFO - >>> {'loss': 2.2769, 'grad_norm': 1.3677445650100708, 'learning_rate': 1.4162861218969038e-06, 'epoch': 12.432773109243698}
>>> 2025-09-11 00:27:35,180 - INFO - >>> {'loss': 2.1836, 'grad_norm': 1.235113263130188, 'learning_rate': 1.4117735400040811e-06, 'epoch': 12.436974789915967}
>>> 2025-09-11 00:27:38,754 - INFO - >>> {'loss': 2.2741, 'grad_norm': 1.0125409364700317, 'learning_rate': 1.4072676125297368e-06, 'epoch': 12.441176470588236}
>>> 2025-09-11 00:27:42,350 - INFO - >>> {'loss': 2.2896, 'grad_norm': 1.1081339120864868, 'learning_rate': 1.4027683429652017e-06, 'epoch': 12.445378151260504}
>>> 2025-09-11 00:27:46,421 - INFO - >>> {'loss': 2.1024, 'grad_norm': 1.0739079713821411, 'learning_rate': 1.3982757347966469e-06, 'epoch': 12.449579831932773}
>>> 2025-09-11 00:27:49,817 - INFO - >>> {'loss': 2.2281, 'grad_norm': 1.1102896928787231, 'learning_rate': 1.3937897915050813e-06, 'epoch': 12.453781512605042}
>>> 2025-09-11 00:27:53,594 - INFO - >>> {'loss': 2.2157, 'grad_norm': 1.1205410957336426, 'learning_rate': 1.3893105165663502e-06, 'epoch': 12.457983193277311}
>>> 2025-09-11 00:27:56,848 - INFO - >>> {'loss': 2.1736, 'grad_norm': 1.0559097528457642, 'learning_rate': 1.3848379134511313e-06, 'epoch': 12.46218487394958}
>>> 2025-09-11 00:28:00,619 - INFO - >>> {'loss': 2.1768, 'grad_norm': 1.1371644735336304, 'learning_rate': 1.3803719856249353e-06, 'epoch': 12.466386554621849}
>>> 2025-09-11 00:28:04,123 - INFO - >>> {'loss': 2.2615, 'grad_norm': 1.2046877145767212, 'learning_rate': 1.3759127365480929e-06, 'epoch': 12.470588235294118}
>>> 2025-09-11 00:28:06,981 - INFO - >>> {'loss': 2.1573, 'grad_norm': 1.1912140846252441, 'learning_rate': 1.3714601696757713e-06, 'epoch': 12.474789915966387}
>>> 2025-09-11 00:28:10,365 - INFO - >>> {'loss': 2.2477, 'grad_norm': 1.1723339557647705, 'learning_rate': 1.367014288457955e-06, 'epoch': 12.478991596638656}
>>> 2025-09-11 00:28:14,052 - INFO - >>> {'loss': 2.1501, 'grad_norm': 1.2661174535751343, 'learning_rate': 1.3625750963394425e-06, 'epoch': 12.483193277310924}
>>> 2025-09-11 00:28:18,033 - INFO - >>> {'loss': 2.207, 'grad_norm': 1.166577935218811, 'learning_rate': 1.358142596759855e-06, 'epoch': 12.487394957983193}
>>> 2025-09-11 00:28:21,969 - INFO - >>> {'loss': 2.1811, 'grad_norm': 1.2408885955810547, 'learning_rate': 1.3537167931536343e-06, 'epoch': 12.491596638655462}
>>> 2025-09-11 00:28:25,252 - INFO - >>> {'loss': 2.2295, 'grad_norm': 1.2127230167388916, 'learning_rate': 1.349297688950022e-06, 'epoch': 12.495798319327731}
>>> 2025-09-11 00:28:29,027 - INFO - >>> {'loss': 2.1973, 'grad_norm': 0.9888131618499756, 'learning_rate': 1.3448852875730734e-06, 'epoch': 12.5}
>>> 2025-09-11 00:28:33,137 - INFO - >>> {'loss': 2.2476, 'grad_norm': 1.1387032270431519, 'learning_rate': 1.340479592441656e-06, 'epoch': 12.504201680672269}
>>> 2025-09-11 00:28:35,998 - INFO - >>> {'loss': 2.2739, 'grad_norm': 1.1662647724151611, 'learning_rate': 1.3360806069694287e-06, 'epoch': 12.508403361344538}
>>> 2025-09-11 00:28:39,687 - INFO - >>> {'loss': 2.1169, 'grad_norm': 1.0788387060165405, 'learning_rate': 1.3316883345648646e-06, 'epoch': 12.512605042016807}
>>> 2025-09-11 00:28:43,430 - INFO - >>> {'loss': 2.2378, 'grad_norm': 1.245793104171753, 'learning_rate': 1.3273027786312298e-06, 'epoch': 12.516806722689076}
>>> 2025-09-11 00:28:47,097 - INFO - >>> {'loss': 2.1448, 'grad_norm': 1.1145063638687134, 'learning_rate': 1.322923942566583e-06, 'epoch': 12.521008403361344}
>>> 2025-09-11 00:28:50,605 - INFO - >>> {'loss': 2.2323, 'grad_norm': 1.1066148281097412, 'learning_rate': 1.318551829763779e-06, 'epoch': 12.525210084033613}
>>> 2025-09-11 00:28:54,174 - INFO - >>> {'loss': 2.2382, 'grad_norm': 1.0986970663070679, 'learning_rate': 1.314186443610468e-06, 'epoch': 12.529411764705882}
>>> 2025-09-11 00:28:58,180 - INFO - >>> {'loss': 2.1702, 'grad_norm': 1.0948166847229004, 'learning_rate': 1.3098277874890796e-06, 'epoch': 12.533613445378151}
>>> 2025-09-11 00:29:01,594 - INFO - >>> {'loss': 2.1025, 'grad_norm': 1.1049798727035522, 'learning_rate': 1.305475864776834e-06, 'epoch': 12.53781512605042}
>>> 2025-09-11 00:29:05,486 - INFO - >>> {'loss': 2.2043, 'grad_norm': 1.1276389360427856, 'learning_rate': 1.3011306788457334e-06, 'epoch': 12.542016806722689}
>>> 2025-09-11 00:29:08,824 - INFO - >>> {'loss': 2.2409, 'grad_norm': 1.229341745376587, 'learning_rate': 1.2967922330625582e-06, 'epoch': 12.546218487394958}
>>> 2025-09-11 00:29:11,974 - INFO - >>> {'loss': 2.2071, 'grad_norm': 1.176495909690857, 'learning_rate': 1.2924605307888705e-06, 'epoch': 12.550420168067227}
>>> 2025-09-11 00:29:15,412 - INFO - >>> {'loss': 2.149, 'grad_norm': 1.161379337310791, 'learning_rate': 1.2881355753810022e-06, 'epoch': 12.554621848739496}
>>> 2025-09-11 00:29:18,466 - INFO - >>> {'loss': 2.1829, 'grad_norm': 1.2205933332443237, 'learning_rate': 1.2838173701900636e-06, 'epoch': 12.558823529411764}
>>> 2025-09-11 00:29:22,429 - INFO - >>> {'loss': 2.1639, 'grad_norm': 1.0299614667892456, 'learning_rate': 1.279505918561923e-06, 'epoch': 12.563025210084033}
>>> 2025-09-11 00:29:26,116 - INFO - >>> {'loss': 2.1783, 'grad_norm': 1.2641679048538208, 'learning_rate': 1.2752012238372325e-06, 'epoch': 12.567226890756302}
>>> 2025-09-11 00:29:30,120 - INFO - >>> {'loss': 2.3318, 'grad_norm': 1.0937480926513672, 'learning_rate': 1.2709032893513996e-06, 'epoch': 12.571428571428571}
>>> 2025-09-11 00:29:33,835 - INFO - >>> {'loss': 2.1622, 'grad_norm': 1.083537220954895, 'learning_rate': 1.2666121184345893e-06, 'epoch': 12.57563025210084}
>>> 2025-09-11 00:29:37,460 - INFO - >>> {'loss': 2.128, 'grad_norm': 1.1579029560089111, 'learning_rate': 1.2623277144117318e-06, 'epoch': 12.579831932773109}
>>> 2025-09-11 00:29:40,701 - INFO - >>> {'loss': 2.2223, 'grad_norm': 1.154654860496521, 'learning_rate': 1.2580500806025198e-06, 'epoch': 12.584033613445378}
>>> 2025-09-11 00:29:44,069 - INFO - >>> {'loss': 2.1745, 'grad_norm': 1.059361219406128, 'learning_rate': 1.253779220321386e-06, 'epoch': 12.588235294117647}
>>> 2025-09-11 00:29:46,934 - INFO - >>> {'loss': 2.2141, 'grad_norm': 1.261480450630188, 'learning_rate': 1.249515136877527e-06, 'epoch': 12.592436974789916}
>>> 2025-09-11 00:29:50,160 - INFO - >>> {'loss': 2.2701, 'grad_norm': 1.2217111587524414, 'learning_rate': 1.2452578335748844e-06, 'epoch': 12.596638655462185}
>>> 2025-09-11 00:29:53,685 - INFO - >>> {'loss': 2.2847, 'grad_norm': 1.081911563873291, 'learning_rate': 1.2410073137121392e-06, 'epoch': 12.600840336134453}
>>> 2025-09-11 00:29:57,643 - INFO - >>> {'loss': 2.2081, 'grad_norm': 1.1771992444992065, 'learning_rate': 1.2367635805827315e-06, 'epoch': 12.605042016806722}
>>> 2025-09-11 00:30:01,284 - INFO - >>> {'loss': 2.1569, 'grad_norm': 1.132856011390686, 'learning_rate': 1.2325266374748336e-06, 'epoch': 12.609243697478991}
>>> 2025-09-11 00:30:04,664 - INFO - >>> {'loss': 2.2863, 'grad_norm': 1.1989145278930664, 'learning_rate': 1.2282964876713522e-06, 'epoch': 12.61344537815126}
>>> 2025-09-11 00:30:08,527 - INFO - >>> {'loss': 2.1944, 'grad_norm': 1.231262445449829, 'learning_rate': 1.224073134449938e-06, 'epoch': 12.617647058823529}
>>> 2025-09-11 00:30:11,483 - INFO - >>> {'loss': 2.241, 'grad_norm': 1.514571189880371, 'learning_rate': 1.2198565810829776e-06, 'epoch': 12.621848739495798}
>>> 2025-09-11 00:30:15,476 - INFO - >>> {'loss': 2.2076, 'grad_norm': 1.0633026361465454, 'learning_rate': 1.215646830837579e-06, 'epoch': 12.626050420168067}
>>> 2025-09-11 00:30:18,487 - INFO - >>> {'loss': 2.237, 'grad_norm': 1.1042397022247314, 'learning_rate': 1.2114438869755873e-06, 'epoch': 12.630252100840336}
>>> 2025-09-11 00:30:22,583 - INFO - >>> {'loss': 2.1598, 'grad_norm': 1.0694069862365723, 'learning_rate': 1.2072477527535697e-06, 'epoch': 12.634453781512605}
>>> 2025-09-11 00:30:25,961 - INFO - >>> {'loss': 2.1178, 'grad_norm': 1.1352187395095825, 'learning_rate': 1.2030584314228177e-06, 'epoch': 12.638655462184873}
>>> 2025-09-11 00:30:29,582 - INFO - >>> {'loss': 2.1184, 'grad_norm': 1.1459214687347412, 'learning_rate': 1.198875926229347e-06, 'epoch': 12.642857142857142}
>>> 2025-09-11 00:30:33,372 - INFO - >>> {'loss': 2.1404, 'grad_norm': 1.0170905590057373, 'learning_rate': 1.1947002404138874e-06, 'epoch': 12.647058823529411}
>>> 2025-09-11 00:30:36,823 - INFO - >>> {'loss': 2.1845, 'grad_norm': 1.1484867334365845, 'learning_rate': 1.1905313772118909e-06, 'epoch': 12.65126050420168}
>>> 2025-09-11 00:30:39,682 - INFO - >>> {'loss': 2.246, 'grad_norm': 1.5991225242614746, 'learning_rate': 1.1863693398535115e-06, 'epoch': 12.655462184873949}
>>> 2025-09-11 00:30:42,752 - INFO - >>> {'loss': 2.2432, 'grad_norm': 1.2964106798171997, 'learning_rate': 1.1822141315636282e-06, 'epoch': 12.659663865546218}
>>> 2025-09-11 00:30:46,074 - INFO - >>> {'loss': 2.1967, 'grad_norm': 1.144703984260559, 'learning_rate': 1.1780657555618236e-06, 'epoch': 12.663865546218487}
>>> 2025-09-11 00:30:49,876 - INFO - >>> {'loss': 2.1042, 'grad_norm': 1.1218347549438477, 'learning_rate': 1.1739242150623797e-06, 'epoch': 12.668067226890756}
>>> 2025-09-11 00:30:53,837 - INFO - >>> {'loss': 2.2121, 'grad_norm': 1.0727251768112183, 'learning_rate': 1.1697895132742908e-06, 'epoch': 12.672268907563025}
>>> 2025-09-11 00:30:57,631 - INFO - >>> {'loss': 2.128, 'grad_norm': 1.116555094718933, 'learning_rate': 1.16566165340125e-06, 'epoch': 12.676470588235293}
>>> 2025-09-11 00:31:00,698 - INFO - >>> {'loss': 2.1799, 'grad_norm': 1.1310532093048096, 'learning_rate': 1.1615406386416472e-06, 'epoch': 12.680672268907562}
>>> 2025-09-11 00:31:04,308 - INFO - >>> {'loss': 2.2195, 'grad_norm': 1.0740574598312378, 'learning_rate': 1.157426472188571e-06, 'epoch': 12.684873949579831}
>>> 2025-09-11 00:31:08,045 - INFO - >>> {'loss': 2.1651, 'grad_norm': 1.0817029476165771, 'learning_rate': 1.153319157229803e-06, 'epoch': 12.6890756302521}
>>> 2025-09-11 00:31:11,851 - INFO - >>> {'loss': 2.3087, 'grad_norm': 1.0548404455184937, 'learning_rate': 1.1492186969478091e-06, 'epoch': 12.693277310924369}
>>> 2025-09-11 00:31:14,819 - INFO - >>> {'loss': 2.2613, 'grad_norm': 1.1849949359893799, 'learning_rate': 1.1451250945197579e-06, 'epoch': 12.697478991596638}
>>> 2025-09-11 00:31:18,183 - INFO - >>> {'loss': 2.208, 'grad_norm': 1.09926176071167, 'learning_rate': 1.1410383531174962e-06, 'epoch': 12.701680672268907}
>>> 2025-09-11 00:31:21,304 - INFO - >>> {'loss': 2.2291, 'grad_norm': 1.145715355873108, 'learning_rate': 1.1369584759075515e-06, 'epoch': 12.705882352941176}
>>> 2025-09-11 00:31:24,243 - INFO - >>> {'loss': 2.1879, 'grad_norm': 1.3237794637680054, 'learning_rate': 1.132885466051138e-06, 'epoch': 12.710084033613445}
>>> 2025-09-11 00:31:28,251 - INFO - >>> {'loss': 2.1968, 'grad_norm': 1.1352319717407227, 'learning_rate': 1.1288193267041469e-06, 'epoch': 12.714285714285714}
>>> 2025-09-11 00:31:32,099 - INFO - >>> {'loss': 2.0881, 'grad_norm': 1.105767846107483, 'learning_rate': 1.1247600610171472e-06, 'epoch': 12.718487394957982}
>>> 2025-09-11 00:31:35,674 - INFO - >>> {'loss': 2.0908, 'grad_norm': 1.1267988681793213, 'learning_rate': 1.1207076721353815e-06, 'epoch': 12.722689075630251}
>>> 2025-09-11 00:31:39,383 - INFO - >>> {'loss': 2.1529, 'grad_norm': 1.1332449913024902, 'learning_rate': 1.116662163198763e-06, 'epoch': 12.72689075630252}
>>> 2025-09-11 00:31:42,446 - INFO - >>> {'loss': 2.1559, 'grad_norm': 1.139359712600708, 'learning_rate': 1.1126235373418736e-06, 'epoch': 12.731092436974789}
>>> 2025-09-11 00:31:45,845 - INFO - >>> {'loss': 2.2322, 'grad_norm': 1.248824954032898, 'learning_rate': 1.1085917976939653e-06, 'epoch': 12.735294117647058}
>>> 2025-09-11 00:31:49,199 - INFO - >>> {'loss': 2.1829, 'grad_norm': 1.0852540731430054, 'learning_rate': 1.1045669473789522e-06, 'epoch': 12.739495798319329}
>>> 2025-09-11 00:31:52,160 - INFO - >>> {'loss': 2.1884, 'grad_norm': 1.101670742034912, 'learning_rate': 1.100548989515411e-06, 'epoch': 12.743697478991596}
>>> 2025-09-11 00:31:55,426 - INFO - >>> {'loss': 2.2679, 'grad_norm': 1.1927073001861572, 'learning_rate': 1.0965379272165743e-06, 'epoch': 12.747899159663866}
>>> 2025-09-11 00:31:58,963 - INFO - >>> {'loss': 2.1641, 'grad_norm': 1.0770937204360962, 'learning_rate': 1.0925337635903344e-06, 'epoch': 12.752100840336134}
>>> 2025-09-11 00:32:02,400 - INFO - >>> {'loss': 2.237, 'grad_norm': 1.1543928384780884, 'learning_rate': 1.0885365017392446e-06, 'epoch': 12.756302521008404}
>>> 2025-09-11 00:32:05,737 - INFO - >>> {'loss': 2.266, 'grad_norm': 1.1228235960006714, 'learning_rate': 1.0845461447605e-06, 'epoch': 12.760504201680671}
>>> 2025-09-11 00:32:09,415 - INFO - >>> {'loss': 2.157, 'grad_norm': 1.084173321723938, 'learning_rate': 1.0805626957459514e-06, 'epoch': 12.764705882352942}
>>> 2025-09-11 00:32:12,761 - INFO - >>> {'loss': 2.1469, 'grad_norm': 1.1661643981933594, 'learning_rate': 1.0765861577820957e-06, 'epoch': 12.768907563025211}
>>> 2025-09-11 00:32:16,745 - INFO - >>> {'loss': 2.2605, 'grad_norm': 1.0368127822875977, 'learning_rate': 1.072616533950076e-06, 'epoch': 12.77310924369748}
>>> 2025-09-11 00:32:20,010 - INFO - >>> {'loss': 2.178, 'grad_norm': 1.143965244293213, 'learning_rate': 1.0686538273256763e-06, 'epoch': 12.777310924369749}
>>> 2025-09-11 00:32:24,006 - INFO - >>> {'loss': 2.1603, 'grad_norm': 1.077213168144226, 'learning_rate': 1.0646980409793251e-06, 'epoch': 12.781512605042018}
>>> 2025-09-11 00:32:28,069 - INFO - >>> {'loss': 2.2546, 'grad_norm': 1.0282708406448364, 'learning_rate': 1.0607491779760804e-06, 'epoch': 12.785714285714286}
>>> 2025-09-11 00:32:31,356 - INFO - >>> {'loss': 2.2495, 'grad_norm': 1.208537220954895, 'learning_rate': 1.0568072413756414e-06, 'epoch': 12.789915966386555}
>>> 2025-09-11 00:32:34,874 - INFO - >>> {'loss': 2.2261, 'grad_norm': 1.1383012533187866, 'learning_rate': 1.0528722342323473e-06, 'epoch': 12.794117647058824}
>>> 2025-09-11 00:32:38,228 - INFO - >>> {'loss': 2.2841, 'grad_norm': 1.1045215129852295, 'learning_rate': 1.0489441595951544e-06, 'epoch': 12.798319327731093}
>>> 2025-09-11 00:32:41,756 - INFO - >>> {'loss': 2.2289, 'grad_norm': 1.1205698251724243, 'learning_rate': 1.045023020507656e-06, 'epoch': 12.802521008403362}
>>> 2025-09-11 00:32:45,097 - INFO - >>> {'loss': 2.2332, 'grad_norm': 1.1207025051116943, 'learning_rate': 1.04110882000807e-06, 'epoch': 12.806722689075631}
>>> 2025-09-11 00:32:48,234 - INFO - >>> {'loss': 2.2079, 'grad_norm': 1.1139596700668335, 'learning_rate': 1.0372015611292363e-06, 'epoch': 12.8109243697479}
>>> 2025-09-11 00:32:51,634 - INFO - >>> {'loss': 2.2277, 'grad_norm': 1.153649091720581, 'learning_rate': 1.0333012468986191e-06, 'epoch': 12.815126050420169}
>>> 2025-09-11 00:32:54,629 - INFO - >>> {'loss': 2.1284, 'grad_norm': 1.218829870223999, 'learning_rate': 1.0294078803383001e-06, 'epoch': 12.819327731092438}
>>> 2025-09-11 00:32:57,990 - INFO - >>> {'loss': 2.225, 'grad_norm': 1.1709352731704712, 'learning_rate': 1.025521464464978e-06, 'epoch': 12.823529411764707}
>>> 2025-09-11 00:33:01,929 - INFO - >>> {'loss': 2.2121, 'grad_norm': 1.2580701112747192, 'learning_rate': 1.0216420022899654e-06, 'epoch': 12.827731092436975}
>>> 2025-09-11 00:33:05,623 - INFO - >>> {'loss': 2.1889, 'grad_norm': 1.2349753379821777, 'learning_rate': 1.0177694968191864e-06, 'epoch': 12.831932773109244}
>>> 2025-09-11 00:33:09,665 - INFO - >>> {'loss': 2.2213, 'grad_norm': 1.0874390602111816, 'learning_rate': 1.01390395105318e-06, 'epoch': 12.836134453781513}
>>> 2025-09-11 00:33:13,090 - INFO - >>> {'loss': 2.2189, 'grad_norm': 1.2576148509979248, 'learning_rate': 1.0100453679870824e-06, 'epoch': 12.840336134453782}
>>> 2025-09-11 00:33:16,353 - INFO - >>> {'loss': 2.1526, 'grad_norm': 1.2014034986495972, 'learning_rate': 1.006193750610641e-06, 'epoch': 12.844537815126051}
>>> 2025-09-11 00:33:20,347 - INFO - >>> {'loss': 2.2142, 'grad_norm': 1.0579659938812256, 'learning_rate': 1.0023491019082131e-06, 'epoch': 12.84873949579832}
>>> 2025-09-11 00:33:23,982 - INFO - >>> {'loss': 2.2414, 'grad_norm': 1.1699419021606445, 'learning_rate': 9.985114248587423e-07, 'epoch': 12.852941176470589}
>>> 2025-09-11 00:33:27,486 - INFO - >>> {'loss': 2.1858, 'grad_norm': 1.0787506103515625, 'learning_rate': 9.946807224357791e-07, 'epoch': 12.857142857142858}
>>> 2025-09-11 00:33:31,074 - INFO - >>> {'loss': 2.2357, 'grad_norm': 1.1079347133636475, 'learning_rate': 9.908569976074678e-07, 'epoch': 12.861344537815127}
>>> 2025-09-11 00:33:34,830 - INFO - >>> {'loss': 2.2315, 'grad_norm': 1.0692421197891235, 'learning_rate': 9.870402533365474e-07, 'epoch': 12.865546218487395}
>>> 2025-09-11 00:33:37,820 - INFO - >>> {'loss': 2.2035, 'grad_norm': 1.372546911239624, 'learning_rate': 9.83230492580347e-07, 'epoch': 12.869747899159664}
>>> 2025-09-11 00:33:41,668 - INFO - >>> {'loss': 2.2658, 'grad_norm': 1.057927131652832, 'learning_rate': 9.794277182907852e-07, 'epoch': 12.873949579831933}
>>> 2025-09-11 00:33:45,082 - INFO - >>> {'loss': 2.241, 'grad_norm': 1.3026500940322876, 'learning_rate': 9.756319334143661e-07, 'epoch': 12.878151260504202}
>>> 2025-09-11 00:33:48,432 - INFO - >>> {'loss': 2.032, 'grad_norm': 1.2267783880233765, 'learning_rate': 9.718431408921757e-07, 'epoch': 12.882352941176471}
>>> 2025-09-11 00:33:51,818 - INFO - >>> {'loss': 2.1335, 'grad_norm': 1.186099648475647, 'learning_rate': 9.680613436598928e-07, 'epoch': 12.88655462184874}
>>> 2025-09-11 00:33:55,509 - INFO - >>> {'loss': 2.2285, 'grad_norm': 1.1355477571487427, 'learning_rate': 9.642865446477635e-07, 'epoch': 12.890756302521009}
>>> 2025-09-11 00:33:59,087 - INFO - >>> {'loss': 2.1903, 'grad_norm': 1.1145379543304443, 'learning_rate': 9.605187467806177e-07, 'epoch': 12.894957983193278}
>>> 2025-09-11 00:34:02,848 - INFO - >>> {'loss': 2.2627, 'grad_norm': 1.045088291168213, 'learning_rate': 9.567579529778591e-07, 'epoch': 12.899159663865547}
>>> 2025-09-11 00:34:06,170 - INFO - >>> {'loss': 2.083, 'grad_norm': 1.1769845485687256, 'learning_rate': 9.530041661534662e-07, 'epoch': 12.903361344537815}
>>> 2025-09-11 00:34:09,970 - INFO - >>> {'loss': 2.2644, 'grad_norm': 1.1106318235397339, 'learning_rate': 9.492573892159851e-07, 'epoch': 12.907563025210084}
>>> 2025-09-11 00:34:12,991 - INFO - >>> {'loss': 2.1831, 'grad_norm': 1.4125075340270996, 'learning_rate': 9.455176250685338e-07, 'epoch': 12.911764705882353}
>>> 2025-09-11 00:34:16,370 - INFO - >>> {'loss': 2.2011, 'grad_norm': 1.1582108736038208, 'learning_rate': 9.417848766087967e-07, 'epoch': 12.915966386554622}
>>> 2025-09-11 00:34:19,217 - INFO - >>> {'loss': 2.2958, 'grad_norm': 1.2825206518173218, 'learning_rate': 9.380591467290146e-07, 'epoch': 12.920168067226891}
>>> 2025-09-11 00:34:22,517 - INFO - >>> {'loss': 2.197, 'grad_norm': 1.1453229188919067, 'learning_rate': 9.343404383160016e-07, 'epoch': 12.92436974789916}
>>> 2025-09-11 00:34:25,793 - INFO - >>> {'loss': 2.1616, 'grad_norm': 1.1538268327713013, 'learning_rate': 9.30628754251126e-07, 'epoch': 12.928571428571429}
>>> 2025-09-11 00:34:29,027 - INFO - >>> {'loss': 2.1493, 'grad_norm': 1.1292781829833984, 'learning_rate': 9.2692409741031e-07, 'epoch': 12.932773109243698}
>>> 2025-09-11 00:34:32,859 - INFO - >>> {'loss': 2.1582, 'grad_norm': 1.2941278219223022, 'learning_rate': 9.232264706640337e-07, 'epoch': 12.936974789915967}
>>> 2025-09-11 00:34:35,947 - INFO - >>> {'loss': 2.186, 'grad_norm': 1.2686806917190552, 'learning_rate': 9.195358768773355e-07, 'epoch': 12.941176470588236}
>>> 2025-09-11 00:34:39,210 - INFO - >>> {'loss': 2.1129, 'grad_norm': 1.15227472782135, 'learning_rate': 9.158523189097945e-07, 'epoch': 12.945378151260504}
>>> 2025-09-11 00:34:42,859 - INFO - >>> {'loss': 2.1471, 'grad_norm': 1.2174607515335083, 'learning_rate': 9.121757996155456e-07, 'epoch': 12.949579831932773}
>>> 2025-09-11 00:34:46,277 - INFO - >>> {'loss': 2.2243, 'grad_norm': 1.0821938514709473, 'learning_rate': 9.085063218432677e-07, 'epoch': 12.953781512605042}
>>> 2025-09-11 00:34:49,377 - INFO - >>> {'loss': 2.1968, 'grad_norm': 1.2011967897415161, 'learning_rate': 9.048438884361799e-07, 'epoch': 12.957983193277311}
>>> 2025-09-11 00:34:52,857 - INFO - >>> {'loss': 2.2032, 'grad_norm': 1.042627215385437, 'learning_rate': 9.011885022320521e-07, 'epoch': 12.96218487394958}
>>> 2025-09-11 00:34:55,856 - INFO - >>> {'loss': 2.1626, 'grad_norm': 1.176668643951416, 'learning_rate': 8.975401660631866e-07, 'epoch': 12.966386554621849}
>>> 2025-09-11 00:34:59,235 - INFO - >>> {'loss': 2.1667, 'grad_norm': 1.2648134231567383, 'learning_rate': 8.938988827564232e-07, 'epoch': 12.970588235294118}
>>> 2025-09-11 00:35:03,026 - INFO - >>> {'loss': 2.1591, 'grad_norm': 1.189460277557373, 'learning_rate': 8.902646551331374e-07, 'epoch': 12.974789915966387}
>>> 2025-09-11 00:35:06,712 - INFO - >>> {'loss': 2.2169, 'grad_norm': 1.1925806999206543, 'learning_rate': 8.866374860092453e-07, 'epoch': 12.978991596638656}
>>> 2025-09-11 00:35:10,467 - INFO - >>> {'loss': 2.1858, 'grad_norm': 1.0671913623809814, 'learning_rate': 8.830173781951823e-07, 'epoch': 12.983193277310924}
>>> 2025-09-11 00:35:13,779 - INFO - >>> {'loss': 2.1122, 'grad_norm': 1.2080543041229248, 'learning_rate': 8.794043344959191e-07, 'epoch': 12.987394957983193}
>>> 2025-09-11 00:35:16,879 - INFO - >>> {'loss': 2.1474, 'grad_norm': 1.2268013954162598, 'learning_rate': 8.757983577109519e-07, 'epoch': 12.991596638655462}
>>> 2025-09-11 00:35:21,017 - INFO - >>> {'loss': 2.1345, 'grad_norm': 1.1298043727874756, 'learning_rate': 8.721994506343013e-07, 'epoch': 12.995798319327731}
>>> 2025-09-11 00:35:24,818 - INFO - >>> {'loss': 2.1831, 'grad_norm': 1.1487795114517212, 'learning_rate': 8.686076160545087e-07, 'epoch': 13.0}
>>> 2025-09-11 00:35:28,616 - INFO - >>> {'loss': 2.2136, 'grad_norm': 1.0322171449661255, 'learning_rate': 8.65022856754637e-07, 'epoch': 13.004201680672269}
>>> 2025-09-11 00:35:32,409 - INFO - >>> {'loss': 2.2487, 'grad_norm': 1.1014366149902344, 'learning_rate': 8.614451755122677e-07, 'epoch': 13.008403361344538}
>>> 2025-09-11 00:35:35,861 - INFO - >>> {'loss': 2.1923, 'grad_norm': 1.1909618377685547, 'learning_rate': 8.578745750994932e-07, 'epoch': 13.012605042016807}
>>> 2025-09-11 00:35:39,277 - INFO - >>> {'loss': 2.2621, 'grad_norm': 1.199594497680664, 'learning_rate': 8.543110582829272e-07, 'epoch': 13.016806722689076}
>>> 2025-09-11 00:35:42,980 - INFO - >>> {'loss': 2.2416, 'grad_norm': 1.0981507301330566, 'learning_rate': 8.5075462782369e-07, 'epoch': 13.021008403361344}
>>> 2025-09-11 00:35:46,403 - INFO - >>> {'loss': 2.1253, 'grad_norm': 1.2426389455795288, 'learning_rate': 8.472052864774094e-07, 'epoch': 13.025210084033613}
>>> 2025-09-11 00:35:50,098 - INFO - >>> {'loss': 2.2239, 'grad_norm': 1.1360856294631958, 'learning_rate': 8.436630369942244e-07, 'epoch': 13.029411764705882}
>>> 2025-09-11 00:35:53,780 - INFO - >>> {'loss': 2.2364, 'grad_norm': 1.1500823497772217, 'learning_rate': 8.401278821187775e-07, 'epoch': 13.033613445378151}
>>> 2025-09-11 00:35:57,586 - INFO - >>> {'loss': 2.1985, 'grad_norm': 1.077526569366455, 'learning_rate': 8.365998245902129e-07, 'epoch': 13.03781512605042}
>>> 2025-09-11 00:36:00,608 - INFO - >>> {'loss': 2.1845, 'grad_norm': 1.1384124755859375, 'learning_rate': 8.330788671421785e-07, 'epoch': 13.042016806722689}
>>> 2025-09-11 00:36:03,727 - INFO - >>> {'loss': 2.133, 'grad_norm': 1.1461466550827026, 'learning_rate': 8.295650125028188e-07, 'epoch': 13.046218487394958}
>>> 2025-09-11 00:36:07,020 - INFO - >>> {'loss': 2.2761, 'grad_norm': 1.2183079719543457, 'learning_rate': 8.2605826339477e-07, 'epoch': 13.050420168067227}
>>> 2025-09-11 00:36:10,418 - INFO - >>> {'loss': 2.1579, 'grad_norm': 1.1756818294525146, 'learning_rate': 8.225586225351745e-07, 'epoch': 13.054621848739496}
>>> 2025-09-11 00:36:13,405 - INFO - >>> {'loss': 2.0606, 'grad_norm': 1.271013855934143, 'learning_rate': 8.190660926356597e-07, 'epoch': 13.058823529411764}
>>> 2025-09-11 00:36:17,417 - INFO - >>> {'loss': 2.177, 'grad_norm': 0.9931694865226746, 'learning_rate': 8.155806764023388e-07, 'epoch': 13.063025210084033}
>>> 2025-09-11 00:36:20,464 - INFO - >>> {'loss': 2.1247, 'grad_norm': 1.2971484661102295, 'learning_rate': 8.121023765358205e-07, 'epoch': 13.067226890756302}
>>> 2025-09-11 00:36:23,813 - INFO - >>> {'loss': 2.2223, 'grad_norm': 1.1605224609375, 'learning_rate': 8.086311957311999e-07, 'epoch': 13.071428571428571}
>>> 2025-09-11 00:36:26,977 - INFO - >>> {'loss': 2.0562, 'grad_norm': 1.4030194282531738, 'learning_rate': 8.051671366780489e-07, 'epoch': 13.07563025210084}
>>> 2025-09-11 00:36:30,017 - INFO - >>> {'loss': 2.1959, 'grad_norm': 1.1771265268325806, 'learning_rate': 8.017102020604283e-07, 'epoch': 13.079831932773109}
>>> 2025-09-11 00:36:33,321 - INFO - >>> {'loss': 2.115, 'grad_norm': 1.1077888011932373, 'learning_rate': 7.982603945568746e-07, 'epoch': 13.084033613445378}
>>> 2025-09-11 00:36:37,079 - INFO - >>> {'loss': 2.3262, 'grad_norm': 1.1309369802474976, 'learning_rate': 7.948177168404036e-07, 'epoch': 13.088235294117647}
>>> 2025-09-11 00:36:40,137 - INFO - >>> {'loss': 2.2432, 'grad_norm': 1.1885151863098145, 'learning_rate': 7.913821715785064e-07, 'epoch': 13.092436974789916}
>>> 2025-09-11 00:36:43,801 - INFO - >>> {'loss': 2.0851, 'grad_norm': 1.1285593509674072, 'learning_rate': 7.879537614331489e-07, 'epoch': 13.096638655462185}
>>> 2025-09-11 00:36:47,532 - INFO - >>> {'loss': 2.2193, 'grad_norm': 1.0144380331039429, 'learning_rate': 7.845324890607687e-07, 'epoch': 13.100840336134453}
>>> 2025-09-11 00:36:50,933 - INFO - >>> {'loss': 2.2342, 'grad_norm': 1.0546213388442993, 'learning_rate': 7.811183571122649e-07, 'epoch': 13.105042016806722}
>>> 2025-09-11 00:36:54,194 - INFO - >>> {'loss': 2.0672, 'grad_norm': 1.2827222347259521, 'learning_rate': 7.777113682330161e-07, 'epoch': 13.109243697478991}
>>> 2025-09-11 00:36:58,157 - INFO - >>> {'loss': 2.232, 'grad_norm': 1.109606385231018, 'learning_rate': 7.743115250628618e-07, 'epoch': 13.11344537815126}
>>> 2025-09-11 00:37:01,808 - INFO - >>> {'loss': 2.241, 'grad_norm': 1.1327321529388428, 'learning_rate': 7.709188302360992e-07, 'epoch': 13.117647058823529}
>>> 2025-09-11 00:37:05,756 - INFO - >>> {'loss': 2.2519, 'grad_norm': 1.053709864616394, 'learning_rate': 7.675332863814944e-07, 'epoch': 13.121848739495798}
>>> 2025-09-11 00:37:09,805 - INFO - >>> {'loss': 2.2769, 'grad_norm': 1.1995368003845215, 'learning_rate': 7.641548961222667e-07, 'epoch': 13.126050420168067}
>>> 2025-09-11 00:37:13,481 - INFO - >>> {'loss': 2.1848, 'grad_norm': 1.159008264541626, 'learning_rate': 7.607836620760989e-07, 'epoch': 13.130252100840336}
>>> 2025-09-11 00:37:17,190 - INFO - >>> {'loss': 2.277, 'grad_norm': 1.1448639631271362, 'learning_rate': 7.574195868551226e-07, 'epoch': 13.134453781512605}
>>> 2025-09-11 00:37:20,608 - INFO - >>> {'loss': 2.1686, 'grad_norm': 1.1468576192855835, 'learning_rate': 7.540626730659284e-07, 'epoch': 13.138655462184873}
>>> 2025-09-11 00:37:23,665 - INFO - >>> {'loss': 2.1119, 'grad_norm': 1.2255746126174927, 'learning_rate': 7.50712923309549e-07, 'epoch': 13.142857142857142}
>>> 2025-09-11 00:37:27,604 - INFO - >>> {'loss': 2.2139, 'grad_norm': 1.075682520866394, 'learning_rate': 7.47370340181478e-07, 'epoch': 13.147058823529411}
>>> 2025-09-11 00:37:31,331 - INFO - >>> {'loss': 2.1366, 'grad_norm': 1.1667543649673462, 'learning_rate': 7.440349262716506e-07, 'epoch': 13.15126050420168}
>>> 2025-09-11 00:37:34,755 - INFO - >>> {'loss': 2.1032, 'grad_norm': 1.06790292263031, 'learning_rate': 7.407066841644428e-07, 'epoch': 13.155462184873949}
>>> 2025-09-11 00:37:38,247 - INFO - >>> {'loss': 2.2369, 'grad_norm': 1.122948169708252, 'learning_rate': 7.373856164386795e-07, 'epoch': 13.159663865546218}
>>> 2025-09-11 00:37:41,953 - INFO - >>> {'loss': 2.153, 'grad_norm': 1.1280077695846558, 'learning_rate': 7.340717256676244e-07, 'epoch': 13.163865546218487}
>>> 2025-09-11 00:37:45,399 - INFO - >>> {'loss': 2.1917, 'grad_norm': 1.103832483291626, 'learning_rate': 7.307650144189804e-07, 'epoch': 13.168067226890756}
>>> 2025-09-11 00:37:49,208 - INFO - >>> {'loss': 2.3149, 'grad_norm': 1.2662363052368164, 'learning_rate': 7.27465485254888e-07, 'epoch': 13.172268907563025}
>>> 2025-09-11 00:37:52,509 - INFO - >>> {'loss': 2.213, 'grad_norm': 1.178673505783081, 'learning_rate': 7.24173140731923e-07, 'epoch': 13.176470588235293}
>>> 2025-09-11 00:37:55,538 - INFO - >>> {'loss': 2.3398, 'grad_norm': 1.2287954092025757, 'learning_rate': 7.208879834010928e-07, 'epoch': 13.180672268907562}
>>> 2025-09-11 00:37:59,346 - INFO - >>> {'loss': 2.2255, 'grad_norm': 1.0345619916915894, 'learning_rate': 7.176100158078392e-07, 'epoch': 13.184873949579831}
>>> 2025-09-11 00:38:03,095 - INFO - >>> {'loss': 2.1636, 'grad_norm': 1.2777999639511108, 'learning_rate': 7.143392404920279e-07, 'epoch': 13.1890756302521}
>>> 2025-09-11 00:38:07,078 - INFO - >>> {'loss': 2.2497, 'grad_norm': 1.1700760126113892, 'learning_rate': 7.110756599879587e-07, 'epoch': 13.193277310924369}
>>> 2025-09-11 00:38:10,487 - INFO - >>> {'loss': 2.2015, 'grad_norm': 1.1354284286499023, 'learning_rate': 7.078192768243486e-07, 'epoch': 13.197478991596638}
>>> 2025-09-11 00:38:13,976 - INFO - >>> {'loss': 2.2717, 'grad_norm': 1.1941479444503784, 'learning_rate': 7.045700935243449e-07, 'epoch': 13.201680672268907}
>>> 2025-09-11 00:38:17,672 - INFO - >>> {'loss': 2.263, 'grad_norm': 1.078087568283081, 'learning_rate': 7.013281126055116e-07, 'epoch': 13.205882352941176}
>>> 2025-09-11 00:38:20,859 - INFO - >>> {'loss': 2.2191, 'grad_norm': 1.1176698207855225, 'learning_rate': 6.980933365798348e-07, 'epoch': 13.210084033613445}
>>> 2025-09-11 00:38:24,303 - INFO - >>> {'loss': 2.1681, 'grad_norm': 1.0529028177261353, 'learning_rate': 6.948657679537174e-07, 'epoch': 13.214285714285714}
>>> 2025-09-11 00:38:28,327 - INFO - >>> {'loss': 2.2186, 'grad_norm': 1.1770820617675781, 'learning_rate': 6.916454092279778e-07, 'epoch': 13.218487394957982}
>>> 2025-09-11 00:38:31,476 - INFO - >>> {'loss': 2.2397, 'grad_norm': 1.1496620178222656, 'learning_rate': 6.884322628978457e-07, 'epoch': 13.222689075630251}
>>> 2025-09-11 00:38:34,919 - INFO - >>> {'loss': 2.064, 'grad_norm': 1.0379739999771118, 'learning_rate': 6.852263314529672e-07, 'epoch': 13.22689075630252}
>>> 2025-09-11 00:38:37,978 - INFO - >>> {'loss': 2.1029, 'grad_norm': 1.2057671546936035, 'learning_rate': 6.820276173773954e-07, 'epoch': 13.231092436974789}
>>> 2025-09-11 00:38:41,855 - INFO - >>> {'loss': 2.2063, 'grad_norm': 1.2403067350387573, 'learning_rate': 6.788361231495855e-07, 'epoch': 13.235294117647058}
>>> 2025-09-11 00:38:45,558 - INFO - >>> {'loss': 2.1252, 'grad_norm': 1.2374083995819092, 'learning_rate': 6.756518512424104e-07, 'epoch': 13.239495798319327}
>>> 2025-09-11 00:38:48,975 - INFO - >>> {'loss': 2.0811, 'grad_norm': 1.1339257955551147, 'learning_rate': 6.724748041231388e-07, 'epoch': 13.243697478991596}
>>> 2025-09-11 00:38:52,887 - INFO - >>> {'loss': 2.1207, 'grad_norm': 1.137259602546692, 'learning_rate': 6.693049842534427e-07, 'epoch': 13.247899159663866}
>>> 2025-09-11 00:38:56,302 - INFO - >>> {'loss': 2.2043, 'grad_norm': 1.2355998754501343, 'learning_rate': 6.661423940893941e-07, 'epoch': 13.252100840336134}
>>> 2025-09-11 00:39:00,038 - INFO - >>> {'loss': 2.2507, 'grad_norm': 1.1115522384643555, 'learning_rate': 6.629870360814638e-07, 'epoch': 13.256302521008404}
>>> 2025-09-11 00:39:03,606 - INFO - >>> {'loss': 2.1668, 'grad_norm': 1.140609622001648, 'learning_rate': 6.598389126745209e-07, 'epoch': 13.260504201680673}
>>> 2025-09-11 00:39:06,876 - INFO - >>> {'loss': 2.1944, 'grad_norm': 1.1385390758514404, 'learning_rate': 6.566980263078248e-07, 'epoch': 13.264705882352942}
>>> 2025-09-11 00:39:10,704 - INFO - >>> {'loss': 2.2484, 'grad_norm': 1.1256799697875977, 'learning_rate': 6.535643794150304e-07, 'epoch': 13.268907563025211}
>>> 2025-09-11 00:39:14,692 - INFO - >>> {'loss': 2.1019, 'grad_norm': 1.0876080989837646, 'learning_rate': 6.504379744241829e-07, 'epoch': 13.27310924369748}
>>> 2025-09-11 00:39:18,231 - INFO - >>> {'loss': 2.1547, 'grad_norm': 1.1581147909164429, 'learning_rate': 6.473188137577147e-07, 'epoch': 13.277310924369749}
>>> 2025-09-11 00:39:21,155 - INFO - >>> {'loss': 2.2113, 'grad_norm': 1.5220133066177368, 'learning_rate': 6.44206899832448e-07, 'epoch': 13.281512605042018}
>>> 2025-09-11 00:39:25,118 - INFO - >>> {'loss': 2.27, 'grad_norm': 1.0566314458847046, 'learning_rate': 6.411022350595864e-07, 'epoch': 13.285714285714286}
>>> 2025-09-11 00:39:28,213 - INFO - >>> {'loss': 2.1557, 'grad_norm': 1.1732228994369507, 'learning_rate': 6.380048218447188e-07, 'epoch': 13.289915966386555}
>>> 2025-09-11 00:39:31,701 - INFO - >>> {'loss': 2.0812, 'grad_norm': 1.4137089252471924, 'learning_rate': 6.349146625878144e-07, 'epoch': 13.294117647058824}
>>> 2025-09-11 00:39:35,046 - INFO - >>> {'loss': 2.2758, 'grad_norm': 1.2646702527999878, 'learning_rate': 6.318317596832235e-07, 'epoch': 13.298319327731093}
>>> 2025-09-11 00:39:38,293 - INFO - >>> {'loss': 2.1041, 'grad_norm': 1.1581324338912964, 'learning_rate': 6.287561155196719e-07, 'epoch': 13.302521008403362}
>>> 2025-09-11 00:39:41,701 - INFO - >>> {'loss': 2.1418, 'grad_norm': 1.2833107709884644, 'learning_rate': 6.256877324802624e-07, 'epoch': 13.306722689075631}
>>> 2025-09-11 00:39:45,359 - INFO - >>> {'loss': 2.1617, 'grad_norm': 1.195109248161316, 'learning_rate': 6.226266129424718e-07, 'epoch': 13.3109243697479}
>>> 2025-09-11 00:39:48,951 - INFO - >>> {'loss': 2.2072, 'grad_norm': 1.138687252998352, 'learning_rate': 6.195727592781476e-07, 'epoch': 13.315126050420169}
>>> 2025-09-11 00:39:52,189 - INFO - >>> {'loss': 2.181, 'grad_norm': 1.100978970527649, 'learning_rate': 6.165261738535078e-07, 'epoch': 13.319327731092438}
>>> 2025-09-11 00:39:55,649 - INFO - >>> {'loss': 2.1429, 'grad_norm': 1.3408358097076416, 'learning_rate': 6.134868590291431e-07, 'epoch': 13.323529411764707}
>>> 2025-09-11 00:39:58,657 - INFO - >>> {'loss': 2.1456, 'grad_norm': 1.1294281482696533, 'learning_rate': 6.10454817160001e-07, 'epoch': 13.327731092436975}
>>> 2025-09-11 00:40:02,507 - INFO - >>> {'loss': 2.1758, 'grad_norm': 1.062103271484375, 'learning_rate': 6.074300505954023e-07, 'epoch': 13.331932773109244}
>>> 2025-09-11 00:40:06,273 - INFO - >>> {'loss': 2.2013, 'grad_norm': 1.0884910821914673, 'learning_rate': 6.044125616790297e-07, 'epoch': 13.336134453781513}
>>> 2025-09-11 00:40:09,084 - INFO - >>> {'loss': 2.1668, 'grad_norm': 1.3484752178192139, 'learning_rate': 6.014023527489233e-07, 'epoch': 13.340336134453782}
>>> 2025-09-11 00:40:12,473 - INFO - >>> {'loss': 2.1545, 'grad_norm': 1.1130542755126953, 'learning_rate': 5.983994261374837e-07, 'epoch': 13.344537815126051}
>>> 2025-09-11 00:40:16,259 - INFO - >>> {'loss': 2.1831, 'grad_norm': 1.2008209228515625, 'learning_rate': 5.9540378417147e-07, 'epoch': 13.34873949579832}
>>> 2025-09-11 00:40:19,664 - INFO - >>> {'loss': 2.1413, 'grad_norm': 1.1761693954467773, 'learning_rate': 5.924154291719974e-07, 'epoch': 13.352941176470589}
>>> 2025-09-11 00:40:22,416 - INFO - >>> {'loss': 2.2929, 'grad_norm': 1.2955036163330078, 'learning_rate': 5.894343634545341e-07, 'epoch': 13.357142857142858}
>>> 2025-09-11 00:40:25,910 - INFO - >>> {'loss': 2.0768, 'grad_norm': 1.0803707838058472, 'learning_rate': 5.864605893288989e-07, 'epoch': 13.361344537815127}
>>> 2025-09-11 00:40:29,537 - INFO - >>> {'loss': 2.1019, 'grad_norm': 1.0884748697280884, 'learning_rate': 5.83494109099264e-07, 'epoch': 13.365546218487395}
>>> 2025-09-11 00:40:32,732 - INFO - >>> {'loss': 2.2666, 'grad_norm': 1.1875979900360107, 'learning_rate': 5.805349250641456e-07, 'epoch': 13.369747899159664}
>>> 2025-09-11 00:40:36,665 - INFO - >>> {'loss': 2.1462, 'grad_norm': 1.1249092817306519, 'learning_rate': 5.775830395164128e-07, 'epoch': 13.373949579831933}
>>> 2025-09-11 00:40:40,746 - INFO - >>> {'loss': 2.1677, 'grad_norm': 1.0621871948242188, 'learning_rate': 5.746384547432738e-07, 'epoch': 13.378151260504202}
>>> 2025-09-11 00:40:43,924 - INFO - >>> {'loss': 2.2395, 'grad_norm': 1.190305233001709, 'learning_rate': 5.717011730262823e-07, 'epoch': 13.382352941176471}
>>> 2025-09-11 00:40:47,851 - INFO - >>> {'loss': 2.2821, 'grad_norm': 1.1485787630081177, 'learning_rate': 5.687711966413345e-07, 'epoch': 13.38655462184874}
>>> 2025-09-11 00:40:51,006 - INFO - >>> {'loss': 2.2133, 'grad_norm': 1.175345778465271, 'learning_rate': 5.658485278586633e-07, 'epoch': 13.390756302521009}
>>> 2025-09-11 00:40:54,359 - INFO - >>> {'loss': 2.2025, 'grad_norm': 1.0722438097000122, 'learning_rate': 5.62933168942843e-07, 'epoch': 13.394957983193278}
>>> 2025-09-11 00:40:57,686 - INFO - >>> {'loss': 2.2106, 'grad_norm': 1.2268470525741577, 'learning_rate': 5.600251221527808e-07, 'epoch': 13.399159663865547}
>>> 2025-09-11 00:41:01,043 - INFO - >>> {'loss': 2.2142, 'grad_norm': 1.0328936576843262, 'learning_rate': 5.571243897417211e-07, 'epoch': 13.403361344537815}
>>> 2025-09-11 00:41:05,135 - INFO - >>> {'loss': 2.2855, 'grad_norm': 1.0964990854263306, 'learning_rate': 5.54230973957236e-07, 'epoch': 13.407563025210084}
>>> 2025-09-11 00:41:09,041 - INFO - >>> {'loss': 2.2366, 'grad_norm': 1.0746464729309082, 'learning_rate': 5.513448770412366e-07, 'epoch': 13.411764705882353}
>>> 2025-09-11 00:41:12,103 - INFO - >>> {'loss': 2.2053, 'grad_norm': 1.225347876548767, 'learning_rate': 5.484661012299564e-07, 'epoch': 13.415966386554622}
>>> 2025-09-11 00:41:15,817 - INFO - >>> {'loss': 2.1888, 'grad_norm': 1.004812240600586, 'learning_rate': 5.455946487539565e-07, 'epoch': 13.420168067226891}
>>> 2025-09-11 00:41:19,845 - INFO - >>> {'loss': 2.1521, 'grad_norm': 1.0830235481262207, 'learning_rate': 5.427305218381263e-07, 'epoch': 13.42436974789916}
>>> 2025-09-11 00:41:23,846 - INFO - >>> {'loss': 2.2232, 'grad_norm': 1.1914291381835938, 'learning_rate': 5.398737227016804e-07, 'epoch': 13.428571428571429}
>>> 2025-09-11 00:41:27,264 - INFO - >>> {'loss': 2.2256, 'grad_norm': 1.0456831455230713, 'learning_rate': 5.370242535581516e-07, 'epoch': 13.432773109243698}
>>> 2025-09-11 00:41:31,063 - INFO - >>> {'loss': 2.229, 'grad_norm': 1.0688340663909912, 'learning_rate': 5.341821166153937e-07, 'epoch': 13.436974789915967}
>>> 2025-09-11 00:41:34,264 - INFO - >>> {'loss': 2.2148, 'grad_norm': 1.15690279006958, 'learning_rate': 5.313473140755832e-07, 'epoch': 13.441176470588236}
>>> 2025-09-11 00:41:37,209 - INFO - >>> {'loss': 2.1938, 'grad_norm': 1.1742714643478394, 'learning_rate': 5.285198481352105e-07, 'epoch': 13.445378151260504}
>>> 2025-09-11 00:41:40,856 - INFO - >>> {'loss': 2.3058, 'grad_norm': 1.2480199337005615, 'learning_rate': 5.256997209850812e-07, 'epoch': 13.449579831932773}
>>> 2025-09-11 00:41:44,839 - INFO - >>> {'loss': 2.2658, 'grad_norm': 1.1157196760177612, 'learning_rate': 5.228869348103172e-07, 'epoch': 13.453781512605042}
>>> 2025-09-11 00:41:48,007 - INFO - >>> {'loss': 2.1859, 'grad_norm': 1.2142202854156494, 'learning_rate': 5.200814917903485e-07, 'epoch': 13.457983193277311}
>>> 2025-09-11 00:41:51,001 - INFO - >>> {'loss': 2.106, 'grad_norm': 1.2317771911621094, 'learning_rate': 5.172833940989164e-07, 'epoch': 13.46218487394958}
>>> 2025-09-11 00:41:54,398 - INFO - >>> {'loss': 2.0418, 'grad_norm': 1.1182160377502441, 'learning_rate': 5.144926439040765e-07, 'epoch': 13.466386554621849}
>>> 2025-09-11 00:41:57,762 - INFO - >>> {'loss': 2.3338, 'grad_norm': 1.2566214799880981, 'learning_rate': 5.117092433681825e-07, 'epoch': 13.470588235294118}
>>> 2025-09-11 00:42:00,860 - INFO - >>> {'loss': 2.1759, 'grad_norm': 1.1907315254211426, 'learning_rate': 5.089331946478992e-07, 'epoch': 13.474789915966387}
>>> 2025-09-11 00:42:04,230 - INFO - >>> {'loss': 2.1104, 'grad_norm': 1.2178466320037842, 'learning_rate': 5.061644998941939e-07, 'epoch': 13.478991596638656}
>>> 2025-09-11 00:42:07,843 - INFO - >>> {'loss': 2.1725, 'grad_norm': 1.0863693952560425, 'learning_rate': 5.034031612523338e-07, 'epoch': 13.483193277310924}
>>> 2025-09-11 00:42:11,349 - INFO - >>> {'loss': 2.1473, 'grad_norm': 1.1265807151794434, 'learning_rate': 5.006491808618896e-07, 'epoch': 13.487394957983193}
>>> 2025-09-11 00:42:14,568 - INFO - >>> {'loss': 2.2475, 'grad_norm': 1.1996841430664062, 'learning_rate': 4.979025608567279e-07, 'epoch': 13.491596638655462}
>>> 2025-09-11 00:42:17,594 - INFO - >>> {'loss': 2.13, 'grad_norm': 1.1708014011383057, 'learning_rate': 4.951633033650138e-07, 'epoch': 13.495798319327731}
>>> 2025-09-11 00:42:20,616 - INFO - >>> {'loss': 2.1565, 'grad_norm': 1.2492953538894653, 'learning_rate': 4.924314105092053e-07, 'epoch': 13.5}
>>> 2025-09-11 00:42:24,495 - INFO - >>> {'loss': 2.1664, 'grad_norm': 1.0726280212402344, 'learning_rate': 4.897068844060582e-07, 'epoch': 13.504201680672269}
>>> 2025-09-11 00:42:27,840 - INFO - >>> {'loss': 2.1162, 'grad_norm': 1.1310698986053467, 'learning_rate': 4.86989727166618e-07, 'epoch': 13.508403361344538}
>>> 2025-09-11 00:42:31,379 - INFO - >>> {'loss': 2.3413, 'grad_norm': 1.0831458568572998, 'learning_rate': 4.842799408962184e-07, 'epoch': 13.512605042016807}
>>> 2025-09-11 00:42:34,997 - INFO - >>> {'loss': 2.2434, 'grad_norm': 1.0479739904403687, 'learning_rate': 4.815775276944845e-07, 'epoch': 13.516806722689076}
>>> 2025-09-11 00:42:38,265 - INFO - >>> {'loss': 2.1372, 'grad_norm': 1.1053681373596191, 'learning_rate': 4.788824896553313e-07, 'epoch': 13.521008403361344}
>>> 2025-09-11 00:42:41,321 - INFO - >>> {'loss': 2.1774, 'grad_norm': 1.1641958951950073, 'learning_rate': 4.761948288669516e-07, 'epoch': 13.525210084033613}
>>> 2025-09-11 00:42:44,365 - INFO - >>> {'loss': 2.1507, 'grad_norm': 1.2236535549163818, 'learning_rate': 4.735145474118297e-07, 'epoch': 13.529411764705882}
>>> 2025-09-11 00:42:48,031 - INFO - >>> {'loss': 2.2645, 'grad_norm': 1.0002248287200928, 'learning_rate': 4.7084164736672767e-07, 'epoch': 13.533613445378151}
>>> 2025-09-11 00:42:51,655 - INFO - >>> {'loss': 2.0766, 'grad_norm': 1.1443836688995361, 'learning_rate': 4.6817613080268886e-07, 'epoch': 13.53781512605042}
>>> 2025-09-11 00:42:54,682 - INFO - >>> {'loss': 2.2038, 'grad_norm': 1.206081509590149, 'learning_rate': 4.6551799978503786e-07, 'epoch': 13.542016806722689}
>>> 2025-09-11 00:42:58,445 - INFO - >>> {'loss': 2.1021, 'grad_norm': 1.0258548259735107, 'learning_rate': 4.6286725637337605e-07, 'epoch': 13.546218487394958}
>>> 2025-09-11 00:43:02,227 - INFO - >>> {'loss': 2.187, 'grad_norm': 1.1601183414459229, 'learning_rate': 4.6022390262157823e-07, 'epoch': 13.550420168067227}
>>> 2025-09-11 00:43:05,510 - INFO - >>> {'loss': 2.1888, 'grad_norm': 1.0953845977783203, 'learning_rate': 4.575879405777928e-07, 'epoch': 13.554621848739496}
>>> 2025-09-11 00:43:08,707 - INFO - >>> {'loss': 2.2699, 'grad_norm': 1.0791163444519043, 'learning_rate': 4.549593722844492e-07, 'epoch': 13.558823529411764}
>>> 2025-09-11 00:43:12,121 - INFO - >>> {'loss': 2.1508, 'grad_norm': 1.1484829187393188, 'learning_rate': 4.523381997782361e-07, 'epoch': 13.563025210084033}
>>> 2025-09-11 00:43:16,014 - INFO - >>> {'loss': 2.1861, 'grad_norm': 1.1334253549575806, 'learning_rate': 4.4972442509012006e-07, 'epoch': 13.567226890756302}
>>> 2025-09-11 00:43:19,882 - INFO - >>> {'loss': 2.2231, 'grad_norm': 1.1253451108932495, 'learning_rate': 4.4711805024533316e-07, 'epoch': 13.571428571428571}
>>> 2025-09-11 00:43:24,028 - INFO - >>> {'loss': 2.2357, 'grad_norm': 1.09483802318573, 'learning_rate': 4.445190772633734e-07, 'epoch': 13.57563025210084}
>>> 2025-09-11 00:43:28,086 - INFO - >>> {'loss': 2.175, 'grad_norm': 1.0522377490997314, 'learning_rate': 4.4192750815800436e-07, 'epoch': 13.579831932773109}
>>> 2025-09-11 00:43:32,220 - INFO - >>> {'loss': 2.1823, 'grad_norm': 1.2083227634429932, 'learning_rate': 4.3934334493725193e-07, 'epoch': 13.584033613445378}
>>> 2025-09-11 00:43:36,285 - INFO - >>> {'loss': 2.2394, 'grad_norm': 1.1809126138687134, 'learning_rate': 4.367665896034046e-07, 'epoch': 13.588235294117647}
>>> 2025-09-11 00:43:39,581 - INFO - >>> {'loss': 2.1507, 'grad_norm': 1.2742351293563843, 'learning_rate': 4.3419724415300847e-07, 'epoch': 13.592436974789916}
>>> 2025-09-11 00:43:42,881 - INFO - >>> {'loss': 2.1909, 'grad_norm': 1.1713383197784424, 'learning_rate': 4.3163531057687226e-07, 'epoch': 13.596638655462185}
>>> 2025-09-11 00:43:46,488 - INFO - >>> {'loss': 2.1594, 'grad_norm': 1.182079553604126, 'learning_rate': 4.2908079086006247e-07, 'epoch': 13.600840336134453}
>>> 2025-09-11 00:43:49,922 - INFO - >>> {'loss': 2.2779, 'grad_norm': 1.0012120008468628, 'learning_rate': 4.265336869818937e-07, 'epoch': 13.605042016806722}
>>> 2025-09-11 00:43:53,779 - INFO - >>> {'loss': 2.2105, 'grad_norm': 1.1034044027328491, 'learning_rate': 4.2399400091594154e-07, 'epoch': 13.609243697478991}
>>> 2025-09-11 00:43:57,173 - INFO - >>> {'loss': 2.278, 'grad_norm': 1.1518267393112183, 'learning_rate': 4.2146173463003095e-07, 'epoch': 13.61344537815126}
>>> 2025-09-11 00:44:01,177 - INFO - >>> {'loss': 2.1593, 'grad_norm': 1.13961923122406, 'learning_rate': 4.1893689008624004e-07, 'epoch': 13.617647058823529}
>>> 2025-09-11 00:44:04,156 - INFO - >>> {'loss': 2.1325, 'grad_norm': 1.2294132709503174, 'learning_rate': 4.1641946924089405e-07, 'epoch': 13.621848739495798}
>>> 2025-09-11 00:44:08,007 - INFO - >>> {'loss': 2.1426, 'grad_norm': 1.3032469749450684, 'learning_rate': 4.1390947404456926e-07, 'epoch': 13.626050420168067}
>>> 2025-09-11 00:44:11,064 - INFO - >>> {'loss': 2.2282, 'grad_norm': 1.088505506515503, 'learning_rate': 4.1140690644208116e-07, 'epoch': 13.630252100840336}
>>> 2025-09-11 00:44:14,802 - INFO - >>> {'loss': 2.0978, 'grad_norm': 1.1367040872573853, 'learning_rate': 4.0891176837249855e-07, 'epoch': 13.634453781512605}
>>> 2025-09-11 00:44:18,110 - INFO - >>> {'loss': 2.1472, 'grad_norm': 1.1705362796783447, 'learning_rate': 4.064240617691317e-07, 'epoch': 13.638655462184873}
>>> 2025-09-11 00:44:21,681 - INFO - >>> {'loss': 2.1506, 'grad_norm': 1.1230149269104004, 'learning_rate': 4.0394378855952766e-07, 'epoch': 13.642857142857142}
>>> 2025-09-11 00:44:24,680 - INFO - >>> {'loss': 2.2429, 'grad_norm': 1.1702460050582886, 'learning_rate': 4.014709506654779e-07, 'epoch': 13.647058823529411}
>>> 2025-09-11 00:44:28,674 - INFO - >>> {'loss': 2.2213, 'grad_norm': 1.210923433303833, 'learning_rate': 3.990055500030165e-07, 'epoch': 13.65126050420168}
>>> 2025-09-11 00:44:31,853 - INFO - >>> {'loss': 2.1623, 'grad_norm': 1.177286982536316, 'learning_rate': 3.965475884824055e-07, 'epoch': 13.655462184873949}
>>> 2025-09-11 00:44:35,579 - INFO - >>> {'loss': 2.2009, 'grad_norm': 1.360633134841919, 'learning_rate': 3.9409706800814906e-07, 'epoch': 13.659663865546218}
>>> 2025-09-11 00:44:39,221 - INFO - >>> {'loss': 2.2157, 'grad_norm': 1.1496258974075317, 'learning_rate': 3.9165399047898624e-07, 'epoch': 13.663865546218487}
>>> 2025-09-11 00:44:42,350 - INFO - >>> {'loss': 2.234, 'grad_norm': 1.1696410179138184, 'learning_rate': 3.892183577878883e-07, 'epoch': 13.668067226890756}
>>> 2025-09-11 00:44:46,218 - INFO - >>> {'loss': 2.1509, 'grad_norm': 1.1844727993011475, 'learning_rate': 3.867901718220546e-07, 'epoch': 13.672268907563025}
>>> 2025-09-11 00:44:49,612 - INFO - >>> {'loss': 2.0303, 'grad_norm': 1.2662633657455444, 'learning_rate': 3.843694344629201e-07, 'epoch': 13.676470588235293}
>>> 2025-09-11 00:44:52,963 - INFO - >>> {'loss': 2.1572, 'grad_norm': 1.0653167963027954, 'learning_rate': 3.8195614758614554e-07, 'epoch': 13.680672268907562}
>>> 2025-09-11 00:44:56,086 - INFO - >>> {'loss': 2.2775, 'grad_norm': 1.432997465133667, 'learning_rate': 3.7955031306161514e-07, 'epoch': 13.684873949579831}
>>> 2025-09-11 00:44:59,172 - INFO - >>> {'loss': 2.1234, 'grad_norm': 1.062657117843628, 'learning_rate': 3.7715193275344563e-07, 'epoch': 13.6890756302521}
>>> 2025-09-11 00:45:02,481 - INFO - >>> {'loss': 2.1724, 'grad_norm': 1.1854522228240967, 'learning_rate': 3.74761008519976e-07, 'epoch': 13.693277310924369}
>>> 2025-09-11 00:45:06,338 - INFO - >>> {'loss': 2.158, 'grad_norm': 1.1107524633407593, 'learning_rate': 3.723775422137632e-07, 'epoch': 13.697478991596638}
>>> 2025-09-11 00:45:10,178 - INFO - >>> {'loss': 2.1345, 'grad_norm': 1.0630303621292114, 'learning_rate': 3.7000153568159223e-07, 'epoch': 13.701680672268907}
>>> 2025-09-11 00:45:13,271 - INFO - >>> {'loss': 2.1472, 'grad_norm': 1.1836047172546387, 'learning_rate': 3.676329907644638e-07, 'epoch': 13.705882352941176}
>>> 2025-09-11 00:45:16,911 - INFO - >>> {'loss': 2.2499, 'grad_norm': 1.1708543300628662, 'learning_rate': 3.652719092975987e-07, 'epoch': 13.710084033613445}
>>> 2025-09-11 00:45:19,617 - INFO - >>> {'loss': 2.2508, 'grad_norm': 1.3672337532043457, 'learning_rate': 3.629182931104347e-07, 'epoch': 13.714285714285714}
>>> 2025-09-11 00:45:23,259 - INFO - >>> {'loss': 2.1849, 'grad_norm': 1.1847656965255737, 'learning_rate': 3.605721440266252e-07, 'epoch': 13.718487394957982}
>>> 2025-09-11 00:45:27,202 - INFO - >>> {'loss': 2.2391, 'grad_norm': 1.0709075927734375, 'learning_rate': 3.5823346386403503e-07, 'epoch': 13.722689075630251}
>>> 2025-09-11 00:45:31,119 - INFO - >>> {'loss': 2.2103, 'grad_norm': 1.117742657661438, 'learning_rate': 3.559022544347479e-07, 'epoch': 13.72689075630252}
>>> 2025-09-11 00:45:34,921 - INFO - >>> {'loss': 2.2546, 'grad_norm': 1.177660584449768, 'learning_rate': 3.535785175450568e-07, 'epoch': 13.731092436974789}
>>> 2025-09-11 00:45:38,458 - INFO - >>> {'loss': 2.1713, 'grad_norm': 1.3048783540725708, 'learning_rate': 3.5126225499545917e-07, 'epoch': 13.735294117647058}
>>> 2025-09-11 00:45:41,533 - INFO - >>> {'loss': 2.2466, 'grad_norm': 1.212488055229187, 'learning_rate': 3.4895346858066723e-07, 'epoch': 13.739495798319329}
>>> 2025-09-11 00:45:44,716 - INFO - >>> {'loss': 2.2402, 'grad_norm': 1.2964684963226318, 'learning_rate': 3.466521600896e-07, 'epoch': 13.743697478991596}
>>> 2025-09-11 00:45:48,006 - INFO - >>> {'loss': 2.2693, 'grad_norm': 1.1634628772735596, 'learning_rate': 3.4435833130537997e-07, 'epoch': 13.747899159663866}
>>> 2025-09-11 00:45:51,391 - INFO - >>> {'loss': 2.2626, 'grad_norm': 1.1899245977401733, 'learning_rate': 3.420719840053355e-07, 'epoch': 13.752100840336134}
>>> 2025-09-11 00:45:54,859 - INFO - >>> {'loss': 2.1543, 'grad_norm': 1.0981680154800415, 'learning_rate': 3.397931199609983e-07, 'epoch': 13.756302521008404}
>>> 2025-09-11 00:45:58,628 - INFO - >>> {'loss': 2.0992, 'grad_norm': 1.1251780986785889, 'learning_rate': 3.375217409381015e-07, 'epoch': 13.760504201680671}
>>> 2025-09-11 00:46:02,526 - INFO - >>> {'loss': 2.1681, 'grad_norm': 1.275866985321045, 'learning_rate': 3.352578486965774e-07, 'epoch': 13.764705882352942}
>>> 2025-09-11 00:46:05,911 - INFO - >>> {'loss': 2.1274, 'grad_norm': 1.1816320419311523, 'learning_rate': 3.330014449905594e-07, 'epoch': 13.768907563025211}
>>> 2025-09-11 00:46:09,064 - INFO - >>> {'loss': 2.2068, 'grad_norm': 1.1503995656967163, 'learning_rate': 3.3075253156837906e-07, 'epoch': 13.77310924369748}
>>> 2025-09-11 00:46:11,988 - INFO - >>> {'loss': 2.1546, 'grad_norm': 1.2172560691833496, 'learning_rate': 3.285111101725591e-07, 'epoch': 13.777310924369749}
>>> 2025-09-11 00:46:16,070 - INFO - >>> {'loss': 2.1544, 'grad_norm': 1.089713454246521, 'learning_rate': 3.2627718253982143e-07, 'epoch': 13.781512605042018}
>>> 2025-09-11 00:46:19,458 - INFO - >>> {'loss': 2.2169, 'grad_norm': 1.251042366027832, 'learning_rate': 3.2405075040108367e-07, 'epoch': 13.785714285714286}
>>> 2025-09-11 00:46:22,781 - INFO - >>> {'loss': 2.0423, 'grad_norm': 1.1869441270828247, 'learning_rate': 3.218318154814515e-07, 'epoch': 13.789915966386555}
>>> 2025-09-11 00:46:26,086 - INFO - >>> {'loss': 2.1871, 'grad_norm': 1.217517375946045, 'learning_rate': 3.196203795002228e-07, 'epoch': 13.794117647058824}
>>> 2025-09-11 00:46:29,414 - INFO - >>> {'loss': 2.1851, 'grad_norm': 1.2380263805389404, 'learning_rate': 3.17416444170886e-07, 'epoch': 13.798319327731093}
>>> 2025-09-11 00:46:33,174 - INFO - >>> {'loss': 2.3129, 'grad_norm': 1.1956616640090942, 'learning_rate': 3.1522001120111614e-07, 'epoch': 13.802521008403362}
>>> 2025-09-11 00:46:36,568 - INFO - >>> {'loss': 2.1499, 'grad_norm': 1.1329340934753418, 'learning_rate': 3.1303108229277736e-07, 'epoch': 13.806722689075631}
>>> 2025-09-11 00:46:40,586 - INFO - >>> {'loss': 2.2651, 'grad_norm': 0.9893066883087158, 'learning_rate': 3.1084965914191856e-07, 'epoch': 13.8109243697479}
>>> 2025-09-11 00:46:44,309 - INFO - >>> {'loss': 2.1418, 'grad_norm': 1.105790615081787, 'learning_rate': 3.0867574343876995e-07, 'epoch': 13.815126050420169}
>>> 2025-09-11 00:46:47,670 - INFO - >>> {'loss': 2.293, 'grad_norm': 1.2795130014419556, 'learning_rate': 3.0650933686774744e-07, 'epoch': 13.819327731092438}
>>> 2025-09-11 00:46:51,016 - INFO - >>> {'loss': 2.1132, 'grad_norm': 1.1834473609924316, 'learning_rate': 3.043504411074527e-07, 'epoch': 13.823529411764707}
>>> 2025-09-11 00:46:54,758 - INFO - >>> {'loss': 2.2138, 'grad_norm': 1.1712164878845215, 'learning_rate': 3.0219905783065997e-07, 'epoch': 13.827731092436975}
>>> 2025-09-11 00:46:58,149 - INFO - >>> {'loss': 2.165, 'grad_norm': 1.00514554977417, 'learning_rate': 3.000551887043268e-07, 'epoch': 13.831932773109244}
>>> 2025-09-11 00:47:01,457 - INFO - >>> {'loss': 2.1738, 'grad_norm': 1.1849864721298218, 'learning_rate': 2.979188353895901e-07, 'epoch': 13.836134453781513}
>>> 2025-09-11 00:47:05,203 - INFO - >>> {'loss': 2.2021, 'grad_norm': 1.157796025276184, 'learning_rate': 2.957899995417579e-07, 'epoch': 13.840336134453782}
>>> 2025-09-11 00:47:08,561 - INFO - >>> {'loss': 2.1076, 'grad_norm': 1.0866680145263672, 'learning_rate': 2.936686828103208e-07, 'epoch': 13.844537815126051}
>>> 2025-09-11 00:47:12,717 - INFO - >>> {'loss': 2.1645, 'grad_norm': 1.1148607730865479, 'learning_rate': 2.915548868389362e-07, 'epoch': 13.84873949579832}
>>> 2025-09-11 00:47:16,753 - INFO - >>> {'loss': 2.2229, 'grad_norm': 1.0860769748687744, 'learning_rate': 2.894486132654384e-07, 'epoch': 13.852941176470589}
>>> 2025-09-11 00:47:20,625 - INFO - >>> {'loss': 2.2558, 'grad_norm': 1.1003811359405518, 'learning_rate': 2.873498637218319e-07, 'epoch': 13.857142857142858}
>>> 2025-09-11 00:47:24,408 - INFO - >>> {'loss': 2.1572, 'grad_norm': 1.1088201999664307, 'learning_rate': 2.852586398342916e-07, 'epoch': 13.861344537815127}
>>> 2025-09-11 00:47:27,502 - INFO - >>> {'loss': 2.2006, 'grad_norm': 1.3084440231323242, 'learning_rate': 2.8317494322316343e-07, 'epoch': 13.865546218487395}
>>> 2025-09-11 00:47:31,774 - INFO - >>> {'loss': 2.2226, 'grad_norm': 1.1740895509719849, 'learning_rate': 2.810987755029548e-07, 'epoch': 13.869747899159664}
>>> 2025-09-11 00:47:35,230 - INFO - >>> {'loss': 2.2367, 'grad_norm': 1.095407485961914, 'learning_rate': 2.790301382823446e-07, 'epoch': 13.873949579831933}
>>> 2025-09-11 00:47:38,500 - INFO - >>> {'loss': 2.2868, 'grad_norm': 1.1316380500793457, 'learning_rate': 2.7696903316417945e-07, 'epoch': 13.878151260504202}
>>> 2025-09-11 00:47:41,548 - INFO - >>> {'loss': 2.123, 'grad_norm': 1.2043613195419312, 'learning_rate': 2.749154617454608e-07, 'epoch': 13.882352941176471}
>>> 2025-09-11 00:47:44,922 - INFO - >>> {'loss': 2.246, 'grad_norm': 1.1043705940246582, 'learning_rate': 2.728694256173614e-07, 'epoch': 13.88655462184874}
>>> 2025-09-11 00:47:48,580 - INFO - >>> {'loss': 2.2226, 'grad_norm': 1.155440092086792, 'learning_rate': 2.7083092636521093e-07, 'epoch': 13.890756302521009}
>>> 2025-09-11 00:47:51,733 - INFO - >>> {'loss': 2.2173, 'grad_norm': 1.1523089408874512, 'learning_rate': 2.687999655685014e-07, 'epoch': 13.894957983193278}
>>> 2025-09-11 00:47:55,274 - INFO - >>> {'loss': 2.1268, 'grad_norm': 1.0829600095748901, 'learning_rate': 2.667765448008819e-07, 'epoch': 13.899159663865547}
>>> 2025-09-11 00:47:58,571 - INFO - >>> {'loss': 2.3439, 'grad_norm': 1.1996231079101562, 'learning_rate': 2.6476066563016046e-07, 'epoch': 13.903361344537815}
>>> 2025-09-11 00:48:01,638 - INFO - >>> {'loss': 2.2927, 'grad_norm': 1.1872116327285767, 'learning_rate': 2.627523296183021e-07, 'epoch': 13.907563025210084}
>>> 2025-09-11 00:48:05,299 - INFO - >>> {'loss': 2.2256, 'grad_norm': 1.124009132385254, 'learning_rate': 2.607515383214243e-07, 'epoch': 13.911764705882353}
>>> 2025-09-11 00:48:08,799 - INFO - >>> {'loss': 2.0783, 'grad_norm': 1.0788147449493408, 'learning_rate': 2.587582932898047e-07, 'epoch': 13.915966386554622}
>>> 2025-09-11 00:48:12,629 - INFO - >>> {'loss': 2.2207, 'grad_norm': 1.1351593732833862, 'learning_rate': 2.5677259606786686e-07, 'epoch': 13.920168067226891}
>>> 2025-09-11 00:48:15,599 - INFO - >>> {'loss': 2.3086, 'grad_norm': 1.5492602586746216, 'learning_rate': 2.5479444819419e-07, 'epoch': 13.92436974789916}
>>> 2025-09-11 00:48:19,308 - INFO - >>> {'loss': 2.2448, 'grad_norm': 1.1311787366867065, 'learning_rate': 2.5282385120150353e-07, 'epoch': 13.928571428571429}
>>> 2025-09-11 00:48:22,758 - INFO - >>> {'loss': 2.1549, 'grad_norm': 1.108336329460144, 'learning_rate': 2.508608066166862e-07, 'epoch': 13.932773109243698}
>>> 2025-09-11 00:48:26,271 - INFO - >>> {'loss': 2.2841, 'grad_norm': 1.0145244598388672, 'learning_rate': 2.489053159607635e-07, 'epoch': 13.936974789915967}
>>> 2025-09-11 00:48:29,653 - INFO - >>> {'loss': 2.1563, 'grad_norm': 1.1602773666381836, 'learning_rate': 2.4695738074890895e-07, 'epoch': 13.941176470588236}
>>> 2025-09-11 00:48:33,748 - INFO - >>> {'loss': 2.3668, 'grad_norm': 1.0662791728973389, 'learning_rate': 2.4501700249044413e-07, 'epoch': 13.945378151260504}
>>> 2025-09-11 00:48:36,983 - INFO - >>> {'loss': 2.1955, 'grad_norm': 1.3393452167510986, 'learning_rate': 2.430841826888286e-07, 'epoch': 13.949579831932773}
>>> 2025-09-11 00:48:40,534 - INFO - >>> {'loss': 2.2474, 'grad_norm': 1.1788450479507446, 'learning_rate': 2.4115892284167444e-07, 'epoch': 13.953781512605042}
>>> 2025-09-11 00:48:44,659 - INFO - >>> {'loss': 2.1611, 'grad_norm': 1.1037635803222656, 'learning_rate': 2.392412244407294e-07, 'epoch': 13.957983193277311}
>>> 2025-09-11 00:48:48,699 - INFO - >>> {'loss': 2.1837, 'grad_norm': 1.1122242212295532, 'learning_rate': 2.3733108897188272e-07, 'epoch': 13.96218487394958}
>>> 2025-09-11 00:48:52,168 - INFO - >>> {'loss': 2.1788, 'grad_norm': 1.1190887689590454, 'learning_rate': 2.354285179151672e-07, 'epoch': 13.966386554621849}
>>> 2025-09-11 00:48:55,679 - INFO - >>> {'loss': 2.1248, 'grad_norm': 1.074892282485962, 'learning_rate': 2.3353351274475465e-07, 'epoch': 13.970588235294118}
>>> 2025-09-11 00:48:59,744 - INFO - >>> {'loss': 2.2328, 'grad_norm': 1.0209789276123047, 'learning_rate': 2.316460749289484e-07, 'epoch': 13.974789915966387}
>>> 2025-09-11 00:49:02,758 - INFO - >>> {'loss': 2.1112, 'grad_norm': 1.214691400527954, 'learning_rate': 2.2976620593019416e-07, 'epoch': 13.978991596638656}
>>> 2025-09-11 00:49:06,646 - INFO - >>> {'loss': 2.1586, 'grad_norm': 1.1347081661224365, 'learning_rate': 2.2789390720507342e-07, 'epoch': 13.983193277310924}
>>> 2025-09-11 00:49:10,506 - INFO - >>> {'loss': 2.0671, 'grad_norm': 1.1208820343017578, 'learning_rate': 2.2602918020429687e-07, 'epoch': 13.987394957983193}
>>> 2025-09-11 00:49:14,705 - INFO - >>> {'loss': 2.1615, 'grad_norm': 1.115694522857666, 'learning_rate': 2.2417202637271318e-07, 'epoch': 13.991596638655462}
>>> 2025-09-11 00:49:18,172 - INFO - >>> {'loss': 2.2927, 'grad_norm': 1.085498571395874, 'learning_rate': 2.223224471493035e-07, 'epoch': 13.995798319327731}
>>> 2025-09-11 00:49:21,938 - INFO - >>> {'loss': 2.1766, 'grad_norm': 1.0433937311172485, 'learning_rate': 2.204804439671737e-07, 'epoch': 14.0}
>>> 2025-09-11 00:49:25,508 - INFO - >>> {'loss': 2.1255, 'grad_norm': 1.2753193378448486, 'learning_rate': 2.1864601825356545e-07, 'epoch': 14.004201680672269}
>>> 2025-09-11 00:49:29,284 - INFO - >>> {'loss': 2.1287, 'grad_norm': 1.1268953084945679, 'learning_rate': 2.1681917142985065e-07, 'epoch': 14.008403361344538}
>>> 2025-09-11 00:49:33,400 - INFO - >>> {'loss': 2.2397, 'grad_norm': 1.105979561805725, 'learning_rate': 2.1499990491152035e-07, 'epoch': 14.012605042016807}
>>> 2025-09-11 00:49:36,972 - INFO - >>> {'loss': 2.1943, 'grad_norm': 1.1108344793319702, 'learning_rate': 2.1318822010820139e-07, 'epoch': 14.016806722689076}
>>> 2025-09-11 00:49:41,036 - INFO - >>> {'loss': 2.3184, 'grad_norm': 1.2180558443069458, 'learning_rate': 2.1138411842364092e-07, 'epoch': 14.021008403361344}
>>> 2025-09-11 00:49:44,672 - INFO - >>> {'loss': 2.2926, 'grad_norm': 1.2176121473312378, 'learning_rate': 2.0958760125571077e-07, 'epoch': 14.025210084033613}
>>> 2025-09-11 00:49:48,652 - INFO - >>> {'loss': 2.1906, 'grad_norm': 1.0041115283966064, 'learning_rate': 2.077986699964074e-07, 'epoch': 14.029411764705882}
>>> 2025-09-11 00:49:51,548 - INFO - >>> {'loss': 2.1569, 'grad_norm': 1.2080755233764648, 'learning_rate': 2.0601732603184987e-07, 'epoch': 14.033613445378151}
>>> 2025-09-11 00:49:55,035 - INFO - >>> {'loss': 2.2222, 'grad_norm': 1.0216621160507202, 'learning_rate': 2.0424357074227852e-07, 'epoch': 14.03781512605042}
>>> 2025-09-11 00:49:59,012 - INFO - >>> {'loss': 2.2194, 'grad_norm': 1.0109004974365234, 'learning_rate': 2.0247740550204843e-07, 'epoch': 14.042016806722689}
>>> 2025-09-11 00:50:02,584 - INFO - >>> {'loss': 2.1476, 'grad_norm': 1.0599336624145508, 'learning_rate': 2.0071883167964267e-07, 'epoch': 14.046218487394958}
>>> 2025-09-11 00:50:05,573 - INFO - >>> {'loss': 2.0989, 'grad_norm': 1.1429919004440308, 'learning_rate': 1.9896785063765577e-07, 'epoch': 14.050420168067227}
>>> 2025-09-11 00:50:09,636 - INFO - >>> {'loss': 2.2774, 'grad_norm': 1.0833979845046997, 'learning_rate': 1.9722446373280135e-07, 'epoch': 14.054621848739496}
>>> 2025-09-11 00:50:13,414 - INFO - >>> {'loss': 2.2546, 'grad_norm': 1.1157947778701782, 'learning_rate': 1.9548867231590661e-07, 'epoch': 14.058823529411764}
>>> 2025-09-11 00:50:16,516 - INFO - >>> {'loss': 2.1717, 'grad_norm': 1.2706650495529175, 'learning_rate': 1.9376047773191907e-07, 'epoch': 14.063025210084033}
>>> 2025-09-11 00:50:19,820 - INFO - >>> {'loss': 2.191, 'grad_norm': 1.1372487545013428, 'learning_rate': 1.9203988131989315e-07, 'epoch': 14.067226890756302}
>>> 2025-09-11 00:50:23,239 - INFO - >>> {'loss': 2.198, 'grad_norm': 1.170106053352356, 'learning_rate': 1.9032688441300017e-07, 'epoch': 14.071428571428571}
>>> 2025-09-11 00:50:26,328 - INFO - >>> {'loss': 2.1608, 'grad_norm': 1.1159812211990356, 'learning_rate': 1.886214883385229e-07, 'epoch': 14.07563025210084}
>>> 2025-09-11 00:50:30,039 - INFO - >>> {'loss': 2.2004, 'grad_norm': 1.0609139204025269, 'learning_rate': 1.8692369441785098e-07, 'epoch': 14.079831932773109}
>>> 2025-09-11 00:50:33,566 - INFO - >>> {'loss': 2.197, 'grad_norm': 1.036408543586731, 'learning_rate': 1.8523350396649098e-07, 'epoch': 14.084033613445378}
>>> 2025-09-11 00:50:36,953 - INFO - >>> {'loss': 2.1404, 'grad_norm': 1.0900567770004272, 'learning_rate': 1.8355091829405203e-07, 'epoch': 14.088235294117647}
>>> 2025-09-11 00:50:41,033 - INFO - >>> {'loss': 2.1622, 'grad_norm': 1.0577665567398071, 'learning_rate': 1.8187593870425124e-07, 'epoch': 14.092436974789916}
>>> 2025-09-11 00:50:44,805 - INFO - >>> {'loss': 2.1851, 'grad_norm': 1.041615605354309, 'learning_rate': 1.8020856649491382e-07, 'epoch': 14.096638655462185}
>>> 2025-09-11 00:50:47,708 - INFO - >>> {'loss': 2.1125, 'grad_norm': 1.1988428831100464, 'learning_rate': 1.7854880295797406e-07, 'epoch': 14.100840336134453}
>>> 2025-09-11 00:50:51,771 - INFO - >>> {'loss': 2.2613, 'grad_norm': 1.0642033815383911, 'learning_rate': 1.7689664937946215e-07, 'epoch': 14.105042016806722}
>>> 2025-09-11 00:50:55,448 - INFO - >>> {'loss': 2.1984, 'grad_norm': 1.184234619140625, 'learning_rate': 1.7525210703951856e-07, 'epoch': 14.109243697478991}
>>> 2025-09-11 00:50:59,190 - INFO - >>> {'loss': 2.1852, 'grad_norm': 1.0859882831573486, 'learning_rate': 1.736151772123862e-07, 'epoch': 14.11344537815126}
>>> 2025-09-11 00:51:03,249 - INFO - >>> {'loss': 2.2744, 'grad_norm': 1.0896025896072388, 'learning_rate': 1.7198586116640604e-07, 'epoch': 14.117647058823529}
>>> 2025-09-11 00:51:07,132 - INFO - >>> {'loss': 2.1345, 'grad_norm': 1.038199782371521, 'learning_rate': 1.703641601640227e-07, 'epoch': 14.121848739495798}
>>> 2025-09-11 00:51:10,503 - INFO - >>> {'loss': 2.128, 'grad_norm': 1.3113813400268555, 'learning_rate': 1.6875007546177767e-07, 'epoch': 14.126050420168067}
>>> 2025-09-11 00:51:14,110 - INFO - >>> {'loss': 2.1348, 'grad_norm': 1.2244982719421387, 'learning_rate': 1.6714360831031617e-07, 'epoch': 14.130252100840336}
>>> 2025-09-11 00:51:18,016 - INFO - >>> {'loss': 2.1265, 'grad_norm': 1.0799508094787598, 'learning_rate': 1.6554475995437358e-07, 'epoch': 14.134453781512605}
>>> 2025-09-11 00:51:21,124 - INFO - >>> {'loss': 2.2492, 'grad_norm': 1.1396586894989014, 'learning_rate': 1.6395353163278894e-07, 'epoch': 14.138655462184873}
>>> 2025-09-11 00:51:24,628 - INFO - >>> {'loss': 2.1872, 'grad_norm': 1.1527793407440186, 'learning_rate': 1.6236992457849376e-07, 'epoch': 14.142857142857142}
>>> 2025-09-11 00:51:28,049 - INFO - >>> {'loss': 2.1771, 'grad_norm': 1.16360342502594, 'learning_rate': 1.6079394001851323e-07, 'epoch': 14.147058823529411}
>>> 2025-09-11 00:51:30,878 - INFO - >>> {'loss': 2.2148, 'grad_norm': 1.3362823724746704, 'learning_rate': 1.5922557917397052e-07, 'epoch': 14.15126050420168}
>>> 2025-09-11 00:51:34,155 - INFO - >>> {'loss': 2.1539, 'grad_norm': 1.1339657306671143, 'learning_rate': 1.5766484326007693e-07, 'epoch': 14.155462184873949}
>>> 2025-09-11 00:51:37,860 - INFO - >>> {'loss': 2.1747, 'grad_norm': 1.1170903444290161, 'learning_rate': 1.561117334861395e-07, 'epoch': 14.159663865546218}
>>> 2025-09-11 00:51:41,207 - INFO - >>> {'loss': 2.1684, 'grad_norm': 1.098329782485962, 'learning_rate': 1.5456625105555344e-07, 'epoch': 14.163865546218487}
>>> 2025-09-11 00:51:45,217 - INFO - >>> {'loss': 2.1922, 'grad_norm': 1.1630939245224, 'learning_rate': 1.5302839716580863e-07, 'epoch': 14.168067226890756}
>>> 2025-09-11 00:51:49,176 - INFO - >>> {'loss': 2.0686, 'grad_norm': 1.2351784706115723, 'learning_rate': 1.5149817300847524e-07, 'epoch': 14.172268907563025}
>>> 2025-09-11 00:51:53,084 - INFO - >>> {'loss': 2.2697, 'grad_norm': 1.3826574087142944, 'learning_rate': 1.4997557976922262e-07, 'epoch': 14.176470588235293}
>>> 2025-09-11 00:51:56,533 - INFO - >>> {'loss': 2.2396, 'grad_norm': 1.1561640501022339, 'learning_rate': 1.4846061862780148e-07, 'epoch': 14.180672268907562}
>>> 2025-09-11 00:51:59,998 - INFO - >>> {'loss': 2.1795, 'grad_norm': 1.3225352764129639, 'learning_rate': 1.4695329075804732e-07, 'epoch': 14.184873949579831}
>>> 2025-09-11 00:52:03,068 - INFO - >>> {'loss': 2.1437, 'grad_norm': 1.1181832551956177, 'learning_rate': 1.4545359732788478e-07, 'epoch': 14.1890756302521}
>>> 2025-09-11 00:52:05,808 - INFO - >>> {'loss': 2.172, 'grad_norm': 1.1923900842666626, 'learning_rate': 1.4396153949932102e-07, 'epoch': 14.193277310924369}
>>> 2025-09-11 00:52:09,455 - INFO - >>> {'loss': 2.3047, 'grad_norm': 1.1382944583892822, 'learning_rate': 1.4247711842845014e-07, 'epoch': 14.197478991596638}
>>> 2025-09-11 00:52:12,398 - INFO - >>> {'loss': 2.1389, 'grad_norm': 1.3679534196853638, 'learning_rate': 1.4100033526544321e-07, 'epoch': 14.201680672268907}
>>> 2025-09-11 00:52:15,457 - INFO - >>> {'loss': 2.3392, 'grad_norm': 1.2572015523910522, 'learning_rate': 1.3953119115455938e-07, 'epoch': 14.205882352941176}
>>> 2025-09-11 00:52:19,425 - INFO - >>> {'loss': 2.1883, 'grad_norm': 1.1410517692565918, 'learning_rate': 1.3806968723413584e-07, 'epoch': 14.210084033613445}
>>> 2025-09-11 00:52:23,139 - INFO - >>> {'loss': 2.2128, 'grad_norm': 1.0679641962051392, 'learning_rate': 1.3661582463658786e-07, 'epoch': 14.214285714285714}
>>> 2025-09-11 00:52:26,255 - INFO - >>> {'loss': 2.1556, 'grad_norm': 1.2449654340744019, 'learning_rate': 1.3516960448841543e-07, 'epoch': 14.218487394957982}
>>> 2025-09-11 00:52:29,753 - INFO - >>> {'loss': 2.1724, 'grad_norm': 1.0521007776260376, 'learning_rate': 1.337310279101922e-07, 'epoch': 14.222689075630251}
>>> 2025-09-11 00:52:32,871 - INFO - >>> {'loss': 2.1811, 'grad_norm': 1.1348668336868286, 'learning_rate': 1.32300096016571e-07, 'epoch': 14.22689075630252}
>>> 2025-09-11 00:52:35,939 - INFO - >>> {'loss': 2.0217, 'grad_norm': 1.1983368396759033, 'learning_rate': 1.3087680991627938e-07, 'epoch': 14.231092436974789}
>>> 2025-09-11 00:52:39,910 - INFO - >>> {'loss': 2.2611, 'grad_norm': 1.2317255735397339, 'learning_rate': 1.294611707121274e-07, 'epoch': 14.235294117647058}
>>> 2025-09-11 00:52:43,207 - INFO - >>> {'loss': 2.1569, 'grad_norm': 1.0653742551803589, 'learning_rate': 1.2805317950099095e-07, 'epoch': 14.239495798319327}
>>> 2025-09-11 00:52:46,799 - INFO - >>> {'loss': 2.1763, 'grad_norm': 1.165048599243164, 'learning_rate': 1.2665283737382516e-07, 'epoch': 14.243697478991596}
>>> 2025-09-11 00:52:50,796 - INFO - >>> {'loss': 2.256, 'grad_norm': 1.0158170461654663, 'learning_rate': 1.252601454156599e-07, 'epoch': 14.247899159663866}
>>> 2025-09-11 00:52:54,078 - INFO - >>> {'loss': 2.1191, 'grad_norm': 1.2128243446350098, 'learning_rate': 1.2387510470559194e-07, 'epoch': 14.252100840336134}
>>> 2025-09-11 00:52:57,683 - INFO - >>> {'loss': 2.1712, 'grad_norm': 1.1055299043655396, 'learning_rate': 1.224977163167962e-07, 'epoch': 14.256302521008404}
>>> 2025-09-11 00:53:00,890 - INFO - >>> {'loss': 2.2287, 'grad_norm': 1.09665846824646, 'learning_rate': 1.2112798131651227e-07, 'epoch': 14.260504201680673}
>>> 2025-09-11 00:53:04,187 - INFO - >>> {'loss': 2.2067, 'grad_norm': 1.0696018934249878, 'learning_rate': 1.1976590076605344e-07, 'epoch': 14.264705882352942}
>>> 2025-09-11 00:53:07,347 - INFO - >>> {'loss': 2.1639, 'grad_norm': 1.2721660137176514, 'learning_rate': 1.1841147572079992e-07, 'epoch': 14.268907563025211}
>>> 2025-09-11 00:53:10,886 - INFO - >>> {'loss': 2.264, 'grad_norm': 1.182284951210022, 'learning_rate': 1.1706470723020446e-07, 'epoch': 14.27310924369748}
>>> 2025-09-11 00:53:14,206 - INFO - >>> {'loss': 2.2903, 'grad_norm': 1.1235780715942383, 'learning_rate': 1.1572559633778124e-07, 'epoch': 14.277310924369749}
>>> 2025-09-11 00:53:18,185 - INFO - >>> {'loss': 2.204, 'grad_norm': 1.2228718996047974, 'learning_rate': 1.1439414408111471e-07, 'epoch': 14.281512605042018}
>>> 2025-09-11 00:53:21,484 - INFO - >>> {'loss': 2.1555, 'grad_norm': 1.1064926385879517, 'learning_rate': 1.1307035149185519e-07, 'epoch': 14.285714285714286}
>>> 2025-09-11 00:53:25,857 - INFO - >>> {'loss': 2.2686, 'grad_norm': 1.161906361579895, 'learning_rate': 1.1175421959571553e-07, 'epoch': 14.289915966386555}
>>> 2025-09-11 00:53:29,841 - INFO - >>> {'loss': 2.2664, 'grad_norm': 1.1433426141738892, 'learning_rate': 1.1044574941247665e-07, 'epoch': 14.294117647058824}
>>> 2025-09-11 00:53:33,126 - INFO - >>> {'loss': 2.2957, 'grad_norm': 1.2141977548599243, 'learning_rate': 1.091449419559809e-07, 'epoch': 14.298319327731093}
>>> 2025-09-11 00:53:36,399 - INFO - >>> {'loss': 2.148, 'grad_norm': 1.160502552986145, 'learning_rate': 1.0785179823413206e-07, 'epoch': 14.302521008403362}
>>> 2025-09-11 00:53:40,098 - INFO - >>> {'loss': 2.227, 'grad_norm': 0.9945843815803528, 'learning_rate': 1.0656631924889749e-07, 'epoch': 14.306722689075631}
>>> 2025-09-11 00:53:43,110 - INFO - >>> {'loss': 2.123, 'grad_norm': 1.280802845954895, 'learning_rate': 1.0528850599630603e-07, 'epoch': 14.3109243697479}
>>> 2025-09-11 00:53:46,134 - INFO - >>> {'loss': 2.2433, 'grad_norm': 1.2741708755493164, 'learning_rate': 1.0401835946644678e-07, 'epoch': 14.315126050420169}
>>> 2025-09-11 00:53:49,903 - INFO - >>> {'loss': 2.1077, 'grad_norm': 1.2249646186828613, 'learning_rate': 1.0275588064346586e-07, 'epoch': 14.319327731092438}
>>> 2025-09-11 00:53:53,509 - INFO - >>> {'loss': 2.177, 'grad_norm': 1.1148779392242432, 'learning_rate': 1.0150107050557078e-07, 'epoch': 14.323529411764707}
>>> 2025-09-11 00:53:56,538 - INFO - >>> {'loss': 2.0944, 'grad_norm': 1.3036315441131592, 'learning_rate': 1.0025393002502714e-07, 'epoch': 14.327731092436975}
>>> 2025-09-11 00:53:59,558 - INFO - >>> {'loss': 2.1875, 'grad_norm': 1.1699585914611816, 'learning_rate': 9.901446016815752e-08, 'epoch': 14.331932773109244}
>>> 2025-09-11 00:54:02,653 - INFO - >>> {'loss': 2.3208, 'grad_norm': 1.218055248260498, 'learning_rate': 9.778266189533925e-08, 'epoch': 14.336134453781513}
>>> 2025-09-11 00:54:05,688 - INFO - >>> {'loss': 2.1625, 'grad_norm': 1.1248576641082764, 'learning_rate': 9.655853616100774e-08, 'epoch': 14.340336134453782}
>>> 2025-09-11 00:54:09,418 - INFO - >>> {'loss': 2.1499, 'grad_norm': 0.9948568940162659, 'learning_rate': 9.534208391365318e-08, 'epoch': 14.344537815126051}
>>> 2025-09-11 00:54:12,808 - INFO - >>> {'loss': 2.1518, 'grad_norm': 1.1112804412841797, 'learning_rate': 9.41333060958205e-08, 'epoch': 14.34873949579832}
>>> 2025-09-11 00:54:16,114 - INFO - >>> {'loss': 2.2798, 'grad_norm': 1.2292965650558472, 'learning_rate': 9.293220364410604e-08, 'epoch': 14.352941176470589}
>>> 2025-09-11 00:54:19,509 - INFO - >>> {'loss': 2.2372, 'grad_norm': 1.1570994853973389, 'learning_rate': 9.173877748915983e-08, 'epoch': 14.357142857142858}
>>> 2025-09-11 00:54:23,064 - INFO - >>> {'loss': 2.1462, 'grad_norm': 1.0403430461883545, 'learning_rate': 9.05530285556866e-08, 'epoch': 14.361344537815127}
>>> 2025-09-11 00:54:26,311 - INFO - >>> {'loss': 2.2566, 'grad_norm': 1.1356582641601562, 'learning_rate': 8.93749577624392e-08, 'epoch': 14.365546218487395}
>>> 2025-09-11 00:54:29,705 - INFO - >>> {'loss': 2.1935, 'grad_norm': 1.0972962379455566, 'learning_rate': 8.820456602222305e-08, 'epoch': 14.369747899159664}
>>> 2025-09-11 00:54:33,732 - INFO - >>> {'loss': 2.2181, 'grad_norm': 1.1222100257873535, 'learning_rate': 8.704185424189382e-08, 'epoch': 14.373949579831933}
>>> 2025-09-11 00:54:37,007 - INFO - >>> {'loss': 2.334, 'grad_norm': 1.234292984008789, 'learning_rate': 8.58868233223542e-08, 'epoch': 14.378151260504202}
>>> 2025-09-11 00:54:41,013 - INFO - >>> {'loss': 2.161, 'grad_norm': 1.1284223794937134, 'learning_rate': 8.473947415855943e-08, 'epoch': 14.382352941176471}
>>> 2025-09-11 00:54:44,353 - INFO - >>> {'loss': 2.2245, 'grad_norm': 1.2919834852218628, 'learning_rate': 8.359980763950836e-08, 'epoch': 14.38655462184874}
>>> 2025-09-11 00:54:47,301 - INFO - >>> {'loss': 2.1794, 'grad_norm': 1.1601676940917969, 'learning_rate': 8.24678246482502e-08, 'epoch': 14.390756302521009}
>>> 2025-09-11 00:54:51,183 - INFO - >>> {'loss': 2.1829, 'grad_norm': 1.037444829940796, 'learning_rate': 8.134352606188001e-08, 'epoch': 14.394957983193278}
>>> 2025-09-11 00:54:54,179 - INFO - >>> {'loss': 2.2451, 'grad_norm': 1.1670043468475342, 'learning_rate': 8.02269127515376e-08, 'epoch': 14.399159663865547}
>>> 2025-09-11 00:54:56,989 - INFO - >>> {'loss': 2.1817, 'grad_norm': 1.2975817918777466, 'learning_rate': 7.91179855824109e-08, 'epoch': 14.403361344537815}
>>> 2025-09-11 00:55:00,537 - INFO - >>> {'loss': 2.2261, 'grad_norm': 1.119399070739746, 'learning_rate': 7.801674541372705e-08, 'epoch': 14.407563025210084}
>>> 2025-09-11 00:55:04,645 - INFO - >>> {'loss': 2.1426, 'grad_norm': 1.0980054140090942, 'learning_rate': 7.692319309876239e-08, 'epoch': 14.411764705882353}
>>> 2025-09-11 00:55:08,410 - INFO - >>> {'loss': 2.2415, 'grad_norm': 1.1450923681259155, 'learning_rate': 7.583732948483469e-08, 'epoch': 14.415966386554622}
>>> 2025-09-11 00:55:11,517 - INFO - >>> {'loss': 2.1732, 'grad_norm': 1.1321324110031128, 'learning_rate': 7.475915541330314e-08, 'epoch': 14.420168067226891}
>>> 2025-09-11 00:55:15,053 - INFO - >>> {'loss': 2.2466, 'grad_norm': 1.1157170534133911, 'learning_rate': 7.36886717195695e-08, 'epoch': 14.42436974789916}
>>> 2025-09-11 00:55:18,415 - INFO - >>> {'loss': 2.2133, 'grad_norm': 1.1653062105178833, 'learning_rate': 7.262587923307807e-08, 'epoch': 14.428571428571429}
>>> 2025-09-11 00:55:22,090 - INFO - >>> {'loss': 2.1981, 'grad_norm': 1.120383381843567, 'learning_rate': 7.157077877731344e-08, 'epoch': 14.432773109243698}
>>> 2025-09-11 00:55:25,541 - INFO - >>> {'loss': 2.1624, 'grad_norm': 1.2137730121612549, 'learning_rate': 7.052337116979613e-08, 'epoch': 14.436974789915967}
>>> 2025-09-11 00:55:29,268 - INFO - >>> {'loss': 2.3088, 'grad_norm': 1.1171787977218628, 'learning_rate': 6.948365722209249e-08, 'epoch': 14.441176470588236}
>>> 2025-09-11 00:55:33,139 - INFO - >>> {'loss': 2.2364, 'grad_norm': 0.9948891997337341, 'learning_rate': 6.845163773980367e-08, 'epoch': 14.445378151260504}
>>> 2025-09-11 00:55:36,445 - INFO - >>> {'loss': 2.2517, 'grad_norm': 1.1532864570617676, 'learning_rate': 6.742731352256893e-08, 'epoch': 14.449579831932773}
>>> 2025-09-11 00:55:39,846 - INFO - >>> {'loss': 2.214, 'grad_norm': 1.151568055152893, 'learning_rate': 6.641068536406558e-08, 'epoch': 14.453781512605042}
>>> 2025-09-11 00:55:42,908 - INFO - >>> {'loss': 2.106, 'grad_norm': 1.2172765731811523, 'learning_rate': 6.540175405201022e-08, 'epoch': 14.457983193277311}
>>> 2025-09-11 00:55:46,002 - INFO - >>> {'loss': 2.104, 'grad_norm': 1.121019721031189, 'learning_rate': 6.440052036815081e-08, 'epoch': 14.46218487394958}
>>> 2025-09-11 00:55:50,054 - INFO - >>> {'loss': 2.1831, 'grad_norm': 1.0094834566116333, 'learning_rate': 6.34069850882757e-08, 'epoch': 14.466386554621849}
>>> 2025-09-11 00:55:53,647 - INFO - >>> {'loss': 2.1868, 'grad_norm': 1.1097434759140015, 'learning_rate': 6.242114898220353e-08, 'epoch': 14.470588235294118}
>>> 2025-09-11 00:55:56,787 - INFO - >>> {'loss': 2.2051, 'grad_norm': 1.2482086420059204, 'learning_rate': 6.144301281379217e-08, 'epoch': 14.474789915966387}
>>> 2025-09-11 00:56:00,610 - INFO - >>> {'loss': 2.226, 'grad_norm': 1.0486582517623901, 'learning_rate': 6.047257734093092e-08, 'epoch': 14.478991596638656}
>>> 2025-09-11 00:56:04,022 - INFO - >>> {'loss': 2.2164, 'grad_norm': 1.1303646564483643, 'learning_rate': 5.950984331554277e-08, 'epoch': 14.483193277310924}
>>> 2025-09-11 00:56:07,842 - INFO - >>> {'loss': 2.2062, 'grad_norm': 1.1797330379486084, 'learning_rate': 5.855481148358322e-08, 'epoch': 14.487394957983193}
>>> 2025-09-11 00:56:10,831 - INFO - >>> {'loss': 2.1787, 'grad_norm': 1.1427735090255737, 'learning_rate': 5.760748258503812e-08, 'epoch': 14.491596638655462}
>>> 2025-09-11 00:56:14,118 - INFO - >>> {'loss': 2.1714, 'grad_norm': 1.1318968534469604, 'learning_rate': 5.666785735392921e-08, 'epoch': 14.495798319327731}
>>> 2025-09-11 00:56:17,773 - INFO - >>> {'loss': 2.1207, 'grad_norm': 1.119743824005127, 'learning_rate': 5.5735936518304116e-08, 'epoch': 14.5}
>>> 2025-09-11 00:56:20,811 - INFO - >>> {'loss': 2.1433, 'grad_norm': 1.2035261392593384, 'learning_rate': 5.4811720800246326e-08, 'epoch': 14.504201680672269}
>>> 2025-09-11 00:56:24,713 - INFO - >>> {'loss': 2.2166, 'grad_norm': 1.138348937034607, 'learning_rate': 5.3895210915863025e-08, 'epoch': 14.508403361344538}
>>> 2025-09-11 00:56:27,763 - INFO - >>> {'loss': 2.1659, 'grad_norm': 1.136480450630188, 'learning_rate': 5.298640757529727e-08, 'epoch': 14.512605042016807}
>>> 2025-09-11 00:56:31,109 - INFO - >>> {'loss': 2.0397, 'grad_norm': 1.2047202587127686, 'learning_rate': 5.2085311482714676e-08, 'epoch': 14.516806722689076}
>>> 2025-09-11 00:56:33,933 - INFO - >>> {'loss': 2.1672, 'grad_norm': 1.2122832536697388, 'learning_rate': 5.11919233363134e-08, 'epoch': 14.521008403361344}
>>> 2025-09-11 00:56:37,050 - INFO - >>> {'loss': 2.1686, 'grad_norm': 1.1458923816680908, 'learning_rate': 5.030624382831639e-08, 'epoch': 14.525210084033613}
>>> 2025-09-11 00:56:40,415 - INFO - >>> {'loss': 2.1533, 'grad_norm': 1.3321533203125, 'learning_rate': 4.942827364497582e-08, 'epoch': 14.529411764705882}
>>> 2025-09-11 00:56:44,178 - INFO - >>> {'loss': 2.1978, 'grad_norm': 1.1492546796798706, 'learning_rate': 4.855801346657085e-08, 'epoch': 14.533613445378151}
>>> 2025-09-11 00:56:47,597 - INFO - >>> {'loss': 2.2159, 'grad_norm': 1.3227871656417847, 'learning_rate': 4.769546396740321e-08, 'epoch': 14.53781512605042}
>>> 2025-09-11 00:56:51,597 - INFO - >>> {'loss': 2.1665, 'grad_norm': 1.2248553037643433, 'learning_rate': 4.684062581580384e-08, 'epoch': 14.542016806722689}
>>> 2025-09-11 00:56:55,284 - INFO - >>> {'loss': 2.1847, 'grad_norm': 1.043308138847351, 'learning_rate': 4.599349967412625e-08, 'epoch': 14.546218487394958}
>>> 2025-09-11 00:56:58,418 - INFO - >>> {'loss': 2.2294, 'grad_norm': 1.218908429145813, 'learning_rate': 4.5154086198750944e-08, 'epoch': 14.550420168067227}
>>> 2025-09-11 00:57:01,434 - INFO - >>> {'loss': 2.1527, 'grad_norm': 1.1742116212844849, 'learning_rate': 4.432238604007988e-08, 'epoch': 14.554621848739496}
>>> 2025-09-11 00:57:04,638 - INFO - >>> {'loss': 2.3995, 'grad_norm': 1.3924124240875244, 'learning_rate': 4.349839984253978e-08, 'epoch': 14.558823529411764}
>>> 2025-09-11 00:57:08,508 - INFO - >>> {'loss': 2.1029, 'grad_norm': 1.0705926418304443, 'learning_rate': 4.2682128244579955e-08, 'epoch': 14.563025210084033}
>>> 2025-09-11 00:57:11,797 - INFO - >>> {'loss': 2.2827, 'grad_norm': 1.2497464418411255, 'learning_rate': 4.187357187867336e-08, 'epoch': 14.567226890756302}
>>> 2025-09-11 00:57:14,886 - INFO - >>> {'loss': 2.2304, 'grad_norm': 1.206687569618225, 'learning_rate': 4.107273137131218e-08, 'epoch': 14.571428571428571}
>>> 2025-09-11 00:57:18,362 - INFO - >>> {'loss': 2.2402, 'grad_norm': 1.1425763368606567, 'learning_rate': 4.0279607343014506e-08, 'epoch': 14.57563025210084}
>>> 2025-09-11 00:57:22,053 - INFO - >>> {'loss': 2.1364, 'grad_norm': 1.1338164806365967, 'learning_rate': 3.949420040831431e-08, 'epoch': 14.579831932773109}
>>> 2025-09-11 00:57:25,541 - INFO - >>> {'loss': 2.1245, 'grad_norm': 1.1852210760116577, 'learning_rate': 3.8716511175769244e-08, 'epoch': 14.584033613445378}
>>> 2025-09-11 00:57:29,380 - INFO - >>> {'loss': 2.1566, 'grad_norm': 1.1053282022476196, 'learning_rate': 3.794654024795841e-08, 'epoch': 14.588235294117647}
>>> 2025-09-11 00:57:32,912 - INFO - >>> {'loss': 2.1518, 'grad_norm': 1.0937262773513794, 'learning_rate': 3.718428822147679e-08, 'epoch': 14.592436974789916}
>>> 2025-09-11 00:57:36,753 - INFO - >>> {'loss': 2.2363, 'grad_norm': 1.1803525686264038, 'learning_rate': 3.642975568694196e-08, 'epoch': 14.596638655462185}
>>> 2025-09-11 00:57:40,185 - INFO - >>> {'loss': 2.1867, 'grad_norm': 1.127992868423462, 'learning_rate': 3.5682943228986246e-08, 'epoch': 14.600840336134453}
>>> 2025-09-11 00:57:44,028 - INFO - >>> {'loss': 2.1589, 'grad_norm': 1.3087235689163208, 'learning_rate': 3.4943851426264553e-08, 'epoch': 14.605042016806722}
>>> 2025-09-11 00:57:47,159 - INFO - >>> {'loss': 2.0334, 'grad_norm': 1.1259585618972778, 'learning_rate': 3.421248085144768e-08, 'epoch': 14.609243697478991}
>>> 2025-09-11 00:57:50,132 - INFO - >>> {'loss': 2.075, 'grad_norm': 1.1531673669815063, 'learning_rate': 3.348883207122233e-08, 'epoch': 14.61344537815126}
>>> 2025-09-11 00:57:53,325 - INFO - >>> {'loss': 2.232, 'grad_norm': 1.2197569608688354, 'learning_rate': 3.277290564629554e-08, 'epoch': 14.617647058823529}
>>> 2025-09-11 00:57:56,628 - INFO - >>> {'loss': 2.1556, 'grad_norm': 1.2919021844863892, 'learning_rate': 3.2064702131385796e-08, 'epoch': 14.621848739495798}
>>> 2025-09-11 00:58:00,038 - INFO - >>> {'loss': 2.1211, 'grad_norm': 1.1242387294769287, 'learning_rate': 3.1364222075234155e-08, 'epoch': 14.626050420168067}
>>> 2025-09-11 00:58:03,030 - INFO - >>> {'loss': 2.1338, 'grad_norm': 1.4323142766952515, 'learning_rate': 3.067146602059201e-08, 'epoch': 14.630252100840336}
>>> 2025-09-11 00:58:07,100 - INFO - >>> {'loss': 2.2482, 'grad_norm': 1.2101202011108398, 'learning_rate': 2.998643450422778e-08, 'epoch': 14.634453781512605}
>>> 2025-09-11 00:58:10,724 - INFO - >>> {'loss': 2.1526, 'grad_norm': 1.2072235345840454, 'learning_rate': 2.930912805692354e-08, 'epoch': 14.638655462184873}
>>> 2025-09-11 00:58:14,147 - INFO - >>> {'loss': 2.2071, 'grad_norm': 1.16183602809906, 'learning_rate': 2.86395472034795e-08, 'epoch': 14.642857142857142}
>>> 2025-09-11 00:58:17,768 - INFO - >>> {'loss': 2.1255, 'grad_norm': 1.2692281007766724, 'learning_rate': 2.797769246270621e-08, 'epoch': 14.647058823529411}
>>> 2025-09-11 00:58:21,005 - INFO - >>> {'loss': 2.1286, 'grad_norm': 1.1412886381149292, 'learning_rate': 2.7323564347427888e-08, 'epoch': 14.65126050420168}
>>> 2025-09-11 00:58:24,398 - INFO - >>> {'loss': 2.1991, 'grad_norm': 1.1756764650344849, 'learning_rate': 2.667716336448356e-08, 'epoch': 14.655462184873949}
>>> 2025-09-11 00:58:27,468 - INFO - >>> {'loss': 2.2098, 'grad_norm': 1.2136421203613281, 'learning_rate': 2.6038490014723694e-08, 'epoch': 14.659663865546218}
>>> 2025-09-11 00:58:31,567 - INFO - >>> {'loss': 2.1615, 'grad_norm': 1.0858265161514282, 'learning_rate': 2.540754479301355e-08, 'epoch': 14.663865546218487}
>>> 2025-09-11 00:58:34,636 - INFO - >>> {'loss': 2.2216, 'grad_norm': 1.1070537567138672, 'learning_rate': 2.4784328188226514e-08, 'epoch': 14.668067226890756}
>>> 2025-09-11 00:58:38,754 - INFO - >>> {'loss': 2.2439, 'grad_norm': 1.1298480033874512, 'learning_rate': 2.416884068325076e-08, 'epoch': 14.672268907563025}
>>> 2025-09-11 00:58:42,012 - INFO - >>> {'loss': 2.1647, 'grad_norm': 1.1025605201721191, 'learning_rate': 2.356108275498481e-08, 'epoch': 14.676470588235293}
>>> 2025-09-11 00:58:45,332 - INFO - >>> {'loss': 2.1049, 'grad_norm': 1.049784779548645, 'learning_rate': 2.296105487433753e-08, 'epoch': 14.680672268907562}
>>> 2025-09-11 00:58:49,216 - INFO - >>> {'loss': 2.2291, 'grad_norm': 1.170111060142517, 'learning_rate': 2.2368757506228134e-08, 'epoch': 14.684873949579831}
>>> 2025-09-11 00:58:52,744 - INFO - >>> {'loss': 2.2187, 'grad_norm': 1.1043368577957153, 'learning_rate': 2.1784191109588404e-08, 'epoch': 14.6890756302521}
>>> 2025-09-11 00:58:56,498 - INFO - >>> {'loss': 2.1622, 'grad_norm': 1.106646180152893, 'learning_rate': 2.120735613735714e-08, 'epoch': 14.693277310924369}
>>> 2025-09-11 00:58:59,437 - INFO - >>> {'loss': 2.156, 'grad_norm': 1.2937918901443481, 'learning_rate': 2.063825303648237e-08, 'epoch': 14.697478991596638}
>>> 2025-09-11 00:59:02,943 - INFO - >>> {'loss': 2.1382, 'grad_norm': 1.1708859205245972, 'learning_rate': 2.00768822479247e-08, 'epoch': 14.701680672268907}
>>> 2025-09-11 00:59:06,239 - INFO - >>> {'loss': 2.1328, 'grad_norm': 1.110459566116333, 'learning_rate': 1.9523244206649527e-08, 'epoch': 14.705882352941176}
>>> 2025-09-11 00:59:10,805 - INFO - >>> {'loss': 2.2912, 'grad_norm': 1.090184211730957, 'learning_rate': 1.8977339341633703e-08, 'epoch': 14.710084033613445}
>>> 2025-09-11 00:59:14,426 - INFO - >>> {'loss': 2.1273, 'grad_norm': 1.1626214981079102, 'learning_rate': 1.8439168075858882e-08, 'epoch': 14.714285714285714}
>>> 2025-09-11 00:59:17,654 - INFO - >>> {'loss': 2.1762, 'grad_norm': 1.124617576599121, 'learning_rate': 1.7908730826318166e-08, 'epoch': 14.718487394957982}
>>> 2025-09-11 00:59:20,897 - INFO - >>> {'loss': 2.2006, 'grad_norm': 1.1233267784118652, 'learning_rate': 1.738602800401057e-08, 'epoch': 14.722689075630251}
>>> 2025-09-11 00:59:24,016 - INFO - >>> {'loss': 2.2179, 'grad_norm': 1.2715355157852173, 'learning_rate': 1.687106001394212e-08, 'epoch': 14.72689075630252}
>>> 2025-09-11 00:59:27,370 - INFO - >>> {'loss': 2.2495, 'grad_norm': 1.0960029363632202, 'learning_rate': 1.6363827255124753e-08, 'epoch': 14.731092436974789}
>>> 2025-09-11 00:59:30,474 - INFO - >>> {'loss': 2.1952, 'grad_norm': 1.2111095190048218, 'learning_rate': 1.586433012057742e-08, 'epoch': 14.735294117647058}
>>> 2025-09-11 00:59:33,817 - INFO - >>> {'loss': 2.2546, 'grad_norm': 1.1812981367111206, 'learning_rate': 1.5372568997327197e-08, 'epoch': 14.739495798319329}
>>> 2025-09-11 00:59:37,235 - INFO - >>> {'loss': 2.1738, 'grad_norm': 1.0247441530227661, 'learning_rate': 1.4888544266404847e-08, 'epoch': 14.743697478991596}
>>> 2025-09-11 00:59:41,278 - INFO - >>> {'loss': 2.094, 'grad_norm': 1.10942542552948, 'learning_rate': 1.4412256302847039e-08, 'epoch': 14.747899159663866}
>>> 2025-09-11 00:59:45,010 - INFO - >>> {'loss': 2.2676, 'grad_norm': 1.0044060945510864, 'learning_rate': 1.3943705475695234e-08, 'epoch': 14.752100840336134}
>>> 2025-09-11 00:59:48,862 - INFO - >>> {'loss': 2.1776, 'grad_norm': 1.0263981819152832, 'learning_rate': 1.3482892147999027e-08, 'epoch': 14.756302521008404}
>>> 2025-09-11 00:59:52,542 - INFO - >>> {'loss': 2.2117, 'grad_norm': 1.1799850463867188, 'learning_rate': 1.302981667681058e-08, 'epoch': 14.760504201680671}
>>> 2025-09-11 00:59:56,249 - INFO - >>> {'loss': 2.1276, 'grad_norm': 1.0670201778411865, 'learning_rate': 1.2584479413184636e-08, 'epoch': 14.764705882352942}
>>> 2025-09-11 01:00:00,079 - INFO - >>> {'loss': 2.2141, 'grad_norm': 1.17247474193573, 'learning_rate': 1.214688070218295e-08, 'epoch': 14.768907563025211}
>>> 2025-09-11 01:00:03,766 - INFO - >>> {'loss': 2.149, 'grad_norm': 1.1527001857757568, 'learning_rate': 1.1717020882869855e-08, 'epoch': 14.77310924369748}
>>> 2025-09-11 01:00:07,264 - INFO - >>> {'loss': 2.2048, 'grad_norm': 1.0559449195861816, 'learning_rate': 1.1294900288314482e-08, 'epoch': 14.777310924369749}
>>> 2025-09-11 01:00:10,460 - INFO - >>> {'loss': 2.2617, 'grad_norm': 1.1920499801635742, 'learning_rate': 1.0880519245587418e-08, 'epoch': 14.781512605042018}
>>> 2025-09-11 01:00:14,279 - INFO - >>> {'loss': 2.1741, 'grad_norm': 0.9802107214927673, 'learning_rate': 1.0473878075765164e-08, 'epoch': 14.785714285714286}
>>> 2025-09-11 01:00:17,328 - INFO - >>> {'loss': 2.2322, 'grad_norm': 1.2286630868911743, 'learning_rate': 1.0074977093925687e-08, 'epoch': 14.789915966386555}
>>> 2025-09-11 01:00:21,201 - INFO - >>> {'loss': 2.071, 'grad_norm': 1.1378761529922485, 'learning_rate': 9.683816609148411e-09, 'epoch': 14.794117647058824}
>>> 2025-09-11 01:00:24,202 - INFO - >>> {'loss': 2.1895, 'grad_norm': 1.2951016426086426, 'learning_rate': 9.300396924516453e-09, 'epoch': 14.798319327731093}
>>> 2025-09-11 01:00:27,569 - INFO - >>> {'loss': 2.2354, 'grad_norm': 1.2477549314498901, 'learning_rate': 8.924718337116612e-09, 'epoch': 14.802521008403362}
>>> 2025-09-11 01:00:30,931 - INFO - >>> {'loss': 2.0508, 'grad_norm': 1.2489073276519775, 'learning_rate': 8.55678113803382e-09, 'epoch': 14.806722689075631}
>>> 2025-09-11 01:00:34,467 - INFO - >>> {'loss': 2.1933, 'grad_norm': 1.1816637516021729, 'learning_rate': 8.196585612358921e-09, 'epoch': 14.8109243697479}
>>> 2025-09-11 01:00:38,088 - INFO - >>> {'loss': 2.227, 'grad_norm': 1.288213849067688, 'learning_rate': 7.844132039180885e-09, 'epoch': 14.815126050420169}
>>> 2025-09-11 01:00:41,339 - INFO - >>> {'loss': 2.2296, 'grad_norm': 1.1890591382980347, 'learning_rate': 7.499420691592374e-09, 'epoch': 14.819327731092438}
>>> 2025-09-11 01:00:45,371 - INFO - >>> {'loss': 2.2034, 'grad_norm': 1.0976957082748413, 'learning_rate': 7.162451836685291e-09, 'epoch': 14.823529411764707}
>>> 2025-09-11 01:00:49,258 - INFO - >>> {'loss': 2.1422, 'grad_norm': 1.1502553224563599, 'learning_rate': 6.833225735554117e-09, 'epoch': 14.827731092436975}
>>> 2025-09-11 01:00:52,486 - INFO - >>> {'loss': 2.2785, 'grad_norm': 1.2227178812026978, 'learning_rate': 6.511742643293683e-09, 'epoch': 14.831932773109244}
>>> 2025-09-11 01:00:55,841 - INFO - >>> {'loss': 2.1165, 'grad_norm': 1.1803995370864868, 'learning_rate': 6.198002808998072e-09, 'epoch': 14.836134453781513}
>>> 2025-09-11 01:00:59,105 - INFO - >>> {'loss': 2.1281, 'grad_norm': 1.2397525310516357, 'learning_rate': 5.892006475761713e-09, 'epoch': 14.840336134453782}
>>> 2025-09-11 01:01:03,076 - INFO - >>> {'loss': 2.2079, 'grad_norm': 1.1196683645248413, 'learning_rate': 5.593753880681618e-09, 'epoch': 14.844537815126051}
>>> 2025-09-11 01:01:06,683 - INFO - >>> {'loss': 2.2731, 'grad_norm': 1.1436618566513062, 'learning_rate': 5.303245254850709e-09, 'epoch': 14.84873949579832}
>>> 2025-09-11 01:01:10,127 - INFO - >>> {'loss': 2.1891, 'grad_norm': 1.058336853981018, 'learning_rate': 5.020480823365592e-09, 'epoch': 14.852941176470589}
>>> 2025-09-11 01:01:13,896 - INFO - >>> {'loss': 2.1841, 'grad_norm': 1.0743212699890137, 'learning_rate': 4.7454608053210115e-09, 'epoch': 14.857142857142858}
>>> 2025-09-11 01:01:17,914 - INFO - >>> {'loss': 2.2773, 'grad_norm': 1.0351122617721558, 'learning_rate': 4.478185413809843e-09, 'epoch': 14.861344537815127}
>>> 2025-09-11 01:01:21,871 - INFO - >>> {'loss': 2.3074, 'grad_norm': 1.1563187837600708, 'learning_rate': 4.218654855925319e-09, 'epoch': 14.865546218487395}
>>> 2025-09-11 01:01:25,613 - INFO - >>> {'loss': 2.1527, 'grad_norm': 1.1611157655715942, 'learning_rate': 3.966869332759915e-09, 'epoch': 14.869747899159664}
>>> 2025-09-11 01:01:28,513 - INFO - >>> {'loss': 2.2356, 'grad_norm': 1.3006671667099, 'learning_rate': 3.722829039404241e-09, 'epoch': 14.873949579831933}
>>> 2025-09-11 01:01:32,296 - INFO - >>> {'loss': 2.0977, 'grad_norm': 1.1398836374282837, 'learning_rate': 3.486534164948152e-09, 'epoch': 14.878151260504202}
>>> 2025-09-11 01:01:36,177 - INFO - >>> {'loss': 2.2722, 'grad_norm': 1.215146541595459, 'learning_rate': 3.2579848924807478e-09, 'epoch': 14.882352941176471}
>>> 2025-09-11 01:01:40,308 - INFO - >>> {'loss': 2.2495, 'grad_norm': 1.0750186443328857, 'learning_rate': 3.0371813990881517e-09, 'epoch': 14.88655462184874}
>>> 2025-09-11 01:01:44,152 - INFO - >>> {'loss': 2.2081, 'grad_norm': 1.127997636795044, 'learning_rate': 2.8241238558557314e-09, 'epoch': 14.890756302521009}
>>> 2025-09-11 01:01:47,386 - INFO - >>> {'loss': 2.2347, 'grad_norm': 1.1782454252243042, 'learning_rate': 2.6188124278681002e-09, 'epoch': 14.894957983193278}
>>> 2025-09-11 01:01:50,375 - INFO - >>> {'loss': 2.1892, 'grad_norm': 1.2252469062805176, 'learning_rate': 2.4212472742057847e-09, 'epoch': 14.899159663865547}
>>> 2025-09-11 01:01:54,357 - INFO - >>> {'loss': 2.1611, 'grad_norm': 1.0535739660263062, 'learning_rate': 2.231428547947445e-09, 'epoch': 14.903361344537815}
>>> 2025-09-11 01:01:58,069 - INFO - >>> {'loss': 2.1737, 'grad_norm': 1.0949925184249878, 'learning_rate': 2.0493563961720974e-09, 'epoch': 14.907563025210084}
>>> 2025-09-11 01:02:01,560 - INFO - >>> {'loss': 2.3253, 'grad_norm': 1.088663935661316, 'learning_rate': 1.875030959953561e-09, 'epoch': 14.911764705882353}
>>> 2025-09-11 01:02:05,507 - INFO - >>> {'loss': 2.2312, 'grad_norm': 1.0409404039382935, 'learning_rate': 1.7084523743648996e-09, 'epoch': 14.915966386554622}
>>> 2025-09-11 01:02:09,374 - INFO - >>> {'loss': 2.2103, 'grad_norm': 1.003913402557373, 'learning_rate': 1.549620768476201e-09, 'epoch': 14.920168067226891}
>>> 2025-09-11 01:02:12,433 - INFO - >>> {'loss': 2.0837, 'grad_norm': 1.323906660079956, 'learning_rate': 1.3985362653545776e-09, 'epoch': 14.92436974789916}
>>> 2025-09-11 01:02:15,696 - INFO - >>> {'loss': 2.2167, 'grad_norm': 1.2153102159500122, 'learning_rate': 1.2551989820652754e-09, 'epoch': 14.928571428571429}
>>> 2025-09-11 01:02:18,705 - INFO - >>> {'loss': 2.2092, 'grad_norm': 1.1504570245742798, 'learning_rate': 1.1196090296716755e-09, 'epoch': 14.932773109243698}
>>> 2025-09-11 01:02:22,397 - INFO - >>> {'loss': 2.2505, 'grad_norm': 1.1670445203781128, 'learning_rate': 9.917665132308518e-10, 'epoch': 14.936974789915967}
>>> 2025-09-11 01:02:25,695 - INFO - >>> {'loss': 2.181, 'grad_norm': 1.1555818319320679, 'learning_rate': 8.716715318002333e-10, 'epoch': 14.941176470588236}
>>> 2025-09-11 01:02:29,444 - INFO - >>> {'loss': 2.2285, 'grad_norm': 1.2034763097763062, 'learning_rate': 7.593241784320526e-10, 'epoch': 14.945378151260504}
>>> 2025-09-11 01:02:32,720 - INFO - >>> {'loss': 2.1232, 'grad_norm': 1.3536213636398315, 'learning_rate': 6.547245401777869e-10, 'epoch': 14.949579831932773}
>>> 2025-09-11 01:02:36,094 - INFO - >>> {'loss': 2.2568, 'grad_norm': 1.221977949142456, 'learning_rate': 5.578726980837168e-10, 'epoch': 14.953781512605042}
>>> 2025-09-11 01:02:39,492 - INFO - >>> {'loss': 2.0973, 'grad_norm': 1.1372573375701904, 'learning_rate': 4.687687271942576e-10, 'epoch': 14.957983193277311}
>>> 2025-09-11 01:02:43,220 - INFO - >>> {'loss': 2.2186, 'grad_norm': 1.0179855823516846, 'learning_rate': 3.87412696548628e-10, 'epoch': 14.96218487394958}
>>> 2025-09-11 01:02:46,754 - INFO - >>> {'loss': 2.1794, 'grad_norm': 1.1562366485595703, 'learning_rate': 3.138046691852914e-10, 'epoch': 14.966386554621849}
>>> 2025-09-11 01:02:50,157 - INFO - >>> {'loss': 2.1096, 'grad_norm': 1.191004753112793, 'learning_rate': 2.479447021364045e-10, 'epoch': 14.970588235294118}
>>> 2025-09-11 01:02:53,174 - INFO - >>> {'loss': 2.1674, 'grad_norm': 1.1728993654251099, 'learning_rate': 1.8983284643336874e-10, 'epoch': 14.974789915966387}
>>> 2025-09-11 01:02:56,546 - INFO - >>> {'loss': 2.2935, 'grad_norm': 1.1430976390838623, 'learning_rate': 1.3946914710349923e-10, 'epoch': 14.978991596638656}
>>> 2025-09-11 01:02:59,899 - INFO - >>> {'loss': 2.1192, 'grad_norm': 1.10493004322052, 'learning_rate': 9.685364316891488e-11, 'epoch': 14.983193277310924}
>>> 2025-09-11 01:03:03,435 - INFO - >>> {'loss': 2.2896, 'grad_norm': 1.1681735515594482, 'learning_rate': 6.198636764986888e-11, 'epoch': 14.987394957983193}
>>> 2025-09-11 01:03:06,779 - INFO - >>> {'loss': 2.2295, 'grad_norm': 1.1812827587127686, 'learning_rate': 3.48673475625283e-11, 'epoch': 14.991596638655462}
>>> 2025-09-11 01:03:10,435 - INFO - >>> {'loss': 2.0997, 'grad_norm': 1.0410186052322388, 'learning_rate': 1.5496603920084342e-11, 'epoch': 14.995798319327731}
>>> 2025-09-11 01:03:13,521 - INFO - >>> {'loss': 2.2852, 'grad_norm': 1.1926896572113037, 'learning_rate': 3.87415173053185e-12, 'epoch': 15.0}
>>> 2025-09-11 01:03:14,161 - INFO - >>> {'train_runtime': 12496.6392, 'train_samples_per_second': 1.143, 'train_steps_per_second': 0.286, 'train_loss': 2.2268425814577846, 'epoch': 15.0}
>>> 2025-09-11 01:03:14,163 - INFO - 训练成功！
>>> 2025-09-11 01:03:14,163 - INFO - 模型存放位置：./output/qwen2-0.5b202509102134
>>> 2025-09-11 08:03:23,428 - INFO - ========__main__  202509110803========
>>> 2025-09-11 08:03:23,428 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 08:03:23,429 - INFO - 开始进行模型测试
>>> 2025-09-11 08:03:26,064 - INFO - 已选择模型文件夹: qwen2-0.5b202509102134
>>> 2025-09-11 08:03:26,067 - INFO - 最新的 LoRA checkpoint 路径:output/qwen2-0.5b202509102134/checkpoint-3570
>>> 2025-09-11 08:06:19,053 - INFO - ========__main__  202509110806========
>>> 2025-09-11 08:06:19,054 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 08:06:19,054 - INFO - 开始进行模型测试
>>> 2025-09-11 08:06:26,156 - INFO - 已选择模型文件夹: qwen2-0.5b202509102134
>>> 2025-09-11 08:06:26,160 - INFO - 最新的 LoRA checkpoint 路径:output/qwen2-0.5b202509102134/checkpoint-3570
>>> 2025-09-11 08:12:32,787 - INFO - ========__main__  202509110812========
>>> 2025-09-11 08:12:32,787 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 08:12:32,788 - INFO - 开始进行模型测试
>>> 2025-09-11 08:12:35,902 - INFO - 已选择模型文件夹: qwen2-0.5b202509102134
>>> 2025-09-11 08:12:35,905 - INFO - 最新的 LoRA checkpoint 路径:output/qwen2-0.5b202509102134/checkpoint-3570
>>> 2025-09-11 08:14:27,739 - INFO - 开始进行原始模型对话测试
>>> 2025-09-11 08:14:30,089 - INFO - 导入包完成
>>> 2025-09-11 08:14:30,096 - INFO - 配置文件读取完成
>>> 2025-09-11 08:15:32,040 - INFO - 开始进行原始模型对话测试
>>> 2025-09-11 08:15:34,398 - INFO - 导入包完成
>>> 2025-09-11 08:15:34,405 - INFO - 配置文件读取完成
>>> 2025-09-11 08:16:27,111 - INFO - ========__main__  202509110816========
>>> 2025-09-11 08:16:27,111 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 08:16:27,112 - INFO - 开始进行模型测试
>>> 2025-09-11 08:16:29,972 - INFO - 已选择模型文件夹: qwen2-0.5b202509102134
>>> 2025-09-11 08:16:29,975 - INFO - 最新的 LoRA checkpoint 路径:output/qwen2-0.5b202509102134/checkpoint-3570
>>> 2025-09-11 08:18:45,325 - INFO - 开始进行原始模型对话测试
>>> 2025-09-11 08:18:47,673 - INFO - 导入包完成
>>> 2025-09-11 08:18:47,680 - INFO - 配置文件读取完成
>>> 2025-09-11 08:20:16,530 - INFO - ========__main__  202509110820========
>>> 2025-09-11 08:20:16,531 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 08:20:16,532 - INFO - 开始进行模型测试
>>> 2025-09-11 08:20:18,815 - INFO - 已选择模型文件夹: qwen2-0.5b202509102134
>>> 2025-09-11 08:20:18,818 - INFO - 最新的 LoRA checkpoint 路径:output/qwen2-0.5b202509102134/checkpoint-3570
>>> 2025-09-11 08:33:52,619 - INFO - 导入包完成
>>> 2025-09-11 08:33:52,620 - INFO - ========train Qwen2ForCausalLM  202509110833========
>>> 2025-09-11 08:33:52,620 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 08:33:52,621 - INFO - 开始进行训练
>>> 2025-09-11 08:33:52,626 - INFO - 基础配置文件读取完成
>>> 2025-09-11 08:33:52,634 - INFO - 训练配置读取完成
>>> 2025-09-11 08:33:52,635 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-09-11 08:33:52,635 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-11 08:33:53,080 - INFO - tokenizer读取完成
>>> 2025-09-11 08:33:53,221 - INFO - model dtype:torch.bfloat16
>>> 2025-09-11 08:33:53,221 - INFO - 模型导入完成
>>> 2025-09-11 08:33:53,222 - INFO - 数据读取开始
>>> 2025-09-11 08:33:54,041 - INFO - 数据下载完成，训练集大小: 100
>>> 2025-09-11 08:34:36,657 - INFO - 导入包完成
>>> 2025-09-11 08:34:36,658 - INFO - ========train Qwen2ForCausalLM  202509110834========
>>> 2025-09-11 08:34:36,658 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 08:34:36,659 - INFO - 开始进行训练
>>> 2025-09-11 08:34:36,665 - INFO - 基础配置文件读取完成
>>> 2025-09-11 08:34:36,673 - INFO - 训练配置读取完成
>>> 2025-09-11 08:34:36,673 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-09-11 08:34:36,673 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-11 08:34:37,133 - INFO - tokenizer读取完成
>>> 2025-09-11 08:34:37,290 - INFO - model dtype:torch.bfloat16
>>> 2025-09-11 08:34:37,290 - INFO - 模型导入完成
>>> 2025-09-11 08:34:37,290 - INFO - 数据读取开始
>>> 2025-09-11 08:34:38,056 - INFO - 数据下载完成，训练集大小: 100
>>> 2025-09-11 08:36:25,936 - INFO - 导入包完成
>>> 2025-09-11 08:36:25,937 - INFO - ========train Qwen2ForCausalLM  202509110836========
>>> 2025-09-11 08:36:25,937 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 08:36:25,938 - INFO - 开始进行训练
>>> 2025-09-11 08:36:25,957 - INFO - 基础配置文件读取完成
>>> 2025-09-11 08:36:25,983 - INFO - 训练配置读取完成
>>> 2025-09-11 08:36:25,983 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-09-11 08:36:25,984 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-11 08:36:26,404 - INFO - tokenizer读取完成
>>> 2025-09-11 08:36:26,641 - INFO - model dtype:torch.bfloat16
>>> 2025-09-11 08:36:26,642 - INFO - 模型导入完成
>>> 2025-09-11 08:36:26,642 - INFO - 数据读取开始
>>> 2025-09-11 08:36:27,480 - INFO - 数据下载完成，训练集大小: 100
>>> 2025-09-11 08:37:57,635 - INFO - 导入包完成
>>> 2025-09-11 08:37:57,636 - INFO - ========train Qwen2ForCausalLM  202509110837========
>>> 2025-09-11 08:37:57,637 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 08:37:57,638 - INFO - 开始进行训练
>>> 2025-09-11 08:37:57,658 - INFO - 基础配置文件读取完成
>>> 2025-09-11 08:37:57,684 - INFO - 训练配置读取完成
>>> 2025-09-11 08:37:57,685 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-09-11 08:37:57,685 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-11 08:37:58,106 - INFO - tokenizer读取完成
>>> 2025-09-11 08:37:58,345 - INFO - model dtype:torch.bfloat16
>>> 2025-09-11 08:37:58,345 - INFO - 模型导入完成
>>> 2025-09-11 08:37:58,346 - INFO - 数据读取开始
>>> 2025-09-11 08:38:02,205 - INFO - 数据下载完成，训练集大小: 100
>>> 2025-09-11 08:38:20,079 - INFO - 数据映射完成
>>> 2025-09-11 08:38:20,080 - INFO - 打印训练参数如下
>>> 2025-09-11 08:38:20,080 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-11 08:38:20,081 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-11 08:38:20,081 - INFO -   load_in_4bit >>> True
>>> 2025-09-11 08:38:20,082 - INFO -   batch_size >>> 4
>>> 2025-09-11 08:38:20,082 - INFO -   gradient_accumulator_steps >>> 1
>>> 2025-09-11 08:38:20,082 - INFO -   warmup_steps >>> 1
>>> 2025-09-11 08:38:20,083 - INFO -   epoch >>> 15
>>> 2025-09-11 08:38:20,083 - INFO -   eval_steps >>> 5
>>> 2025-09-11 08:38:20,084 - INFO -   learning_rate >>> 2e-05
>>> 2025-09-11 08:38:20,084 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-11 08:38:20,085 - INFO -   max_seq_length >>> 2048
>>> 2025-09-11 08:38:20,085 - INFO -   r >>> 8
>>> 2025-09-11 08:38:20,086 - INFO -   interface_mode >>> False
>>> 2025-09-11 08:38:20,086 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-11 08:38:20,087 - INFO -   lora_alpha >>> 16
>>> 2025-09-11 08:38:20,087 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-11 08:38:20,088 - INFO -   bias >>> none
>>> 2025-09-11 08:38:20,088 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-11 08:38:20,089 - INFO -   random_state >>> 3407
>>> 2025-09-11 08:38:20,089 - INFO -   use_rslora >>> True
>>> 2025-09-11 08:38:20,090 - INFO -   loftq_config >>> None
>>> 2025-09-11 08:38:21,188 - INFO - 开始训练！
>>> 2025-09-11 08:38:22,750 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.04}
>>> 2025-09-11 08:38:23,504 - INFO - >>> {'loss': 3.1195, 'grad_norm': 4.457130432128906, 'learning_rate': 2e-05, 'epoch': 0.08}
>>> 2025-09-11 08:38:24,215 - INFO - >>> {'loss': 3.5344, 'grad_norm': 6.029007434844971, 'learning_rate': 1.9999647203724434e-05, 'epoch': 0.12}
>>> 2025-09-11 08:38:24,927 - INFO - >>> {'loss': 3.3716, 'grad_norm': 4.4539408683776855, 'learning_rate': 1.9998588839790777e-05, 'epoch': 0.16}
>>> 2025-09-11 08:38:25,638 - INFO - >>> {'loss': 2.9887, 'grad_norm': 5.2415337562561035, 'learning_rate': 1.9996824982876402e-05, 'epoch': 0.2}
>>> 2025-09-11 08:38:26,350 - INFO - >>> {'loss': 3.2314, 'grad_norm': 4.685360431671143, 'learning_rate': 1.999435575743774e-05, 'epoch': 0.24}
>>> 2025-09-11 08:38:27,062 - INFO - >>> {'loss': 2.9875, 'grad_norm': 3.5941920280456543, 'learning_rate': 1.999118133770149e-05, 'epoch': 0.28}
>>> 2025-09-11 08:38:27,774 - INFO - >>> {'loss': 3.1125, 'grad_norm': 5.9013824462890625, 'learning_rate': 1.9987301947652354e-05, 'epoch': 0.32}
>>> 2025-09-11 08:38:28,477 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.9982717861017198e-05, 'epoch': 0.36}
>>> 2025-09-11 08:38:29,189 - INFO - >>> {'loss': 2.9717, 'grad_norm': 6.085402488708496, 'learning_rate': 1.9977429401245764e-05, 'epoch': 0.4}
>>> 2025-09-11 08:38:29,891 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.9971436941487833e-05, 'epoch': 0.44}
>>> 2025-09-11 08:38:30,603 - INFO - >>> {'loss': 3.3577, 'grad_norm': 4.115562915802002, 'learning_rate': 1.9964740904566903e-05, 'epoch': 0.48}
>>> 2025-09-11 08:38:31,315 - INFO - >>> {'loss': 2.605, 'grad_norm': 4.904231071472168, 'learning_rate': 1.9957341762950346e-05, 'epoch': 0.52}
>>> 2025-09-11 08:38:31,787 - INFO - >>> {'loss': 3.4028, 'grad_norm': 2.996025323867798, 'learning_rate': 1.9949240038716092e-05, 'epoch': 0.56}
>>> 2025-09-11 08:38:32,498 - INFO - >>> {'loss': 3.6436, 'grad_norm': 2.9161226749420166, 'learning_rate': 1.994043630351576e-05, 'epoch': 0.6}
>>> 2025-09-11 08:38:33,208 - INFO - >>> {'loss': 3.3989, 'grad_norm': 3.1208114624023438, 'learning_rate': 1.9930931178534353e-05, 'epoch': 0.64}
>>> 2025-09-11 08:38:33,918 - INFO - >>> {'loss': 2.9202, 'grad_norm': 3.7241642475128174, 'learning_rate': 1.9920725334446404e-05, 'epoch': 0.68}
>>> 2025-09-11 08:38:34,629 - INFO - >>> {'loss': 3.1391, 'grad_norm': 1.9827481508255005, 'learning_rate': 1.9909819491368677e-05, 'epoch': 0.72}
>>> 2025-09-11 08:38:35,341 - INFO - >>> {'loss': 3.5057, 'grad_norm': 3.814225196838379, 'learning_rate': 1.989821441880933e-05, 'epoch': 0.76}
>>> 2025-09-11 08:38:36,052 - INFO - >>> {'loss': 3.0553, 'grad_norm': 1.9160759449005127, 'learning_rate': 1.988591093561364e-05, 'epoch': 0.8}
>>> 2025-09-11 08:38:36,762 - INFO - >>> {'loss': 3.1209, 'grad_norm': 3.9502921104431152, 'learning_rate': 1.9872909909906216e-05, 'epoch': 0.84}
>>> 2025-09-11 08:38:37,473 - INFO - >>> {'loss': 3.0601, 'grad_norm': 3.4976577758789062, 'learning_rate': 1.985921225902975e-05, 'epoch': 0.88}
>>> 2025-09-11 08:38:38,195 - INFO - >>> {'loss': 3.0211, 'grad_norm': 2.8113553524017334, 'learning_rate': 1.9844818949480284e-05, 'epoch': 0.92}
>>> 2025-09-11 08:38:38,906 - INFO - >>> {'loss': 3.2369, 'grad_norm': 3.371155023574829, 'learning_rate': 1.982973099683902e-05, 'epoch': 0.96}
>>> 2025-09-11 08:38:39,615 - INFO - >>> {'loss': 3.3795, 'grad_norm': 2.6180548667907715, 'learning_rate': 1.9813949465700653e-05, 'epoch': 1.0}
>>> 2025-09-11 08:38:40,331 - INFO - >>> {'loss': 3.4064, 'grad_norm': 2.5591602325439453, 'learning_rate': 1.9797475469598267e-05, 'epoch': 1.04}
>>> 2025-09-11 08:38:41,042 - INFO - >>> {'loss': 3.4003, 'grad_norm': 2.489109992980957, 'learning_rate': 1.9780310170924752e-05, 'epoch': 1.08}
>>> 2025-09-11 08:38:41,754 - INFO - >>> {'loss': 3.1561, 'grad_norm': 4.216402053833008, 'learning_rate': 1.9762454780850807e-05, 'epoch': 1.12}
>>> 2025-09-11 08:38:42,456 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.9743910559239443e-05, 'epoch': 1.16}
>>> 2025-09-11 08:38:43,167 - INFO - >>> {'loss': 3.7498, 'grad_norm': 3.8742265701293945, 'learning_rate': 1.972467881455713e-05, 'epoch': 1.2}
>>> 2025-09-11 08:38:43,877 - INFO - >>> {'loss': 3.1243, 'grad_norm': 2.870339870452881, 'learning_rate': 1.9704760903781446e-05, 'epoch': 1.24}
>>> 2025-09-11 08:38:44,586 - INFO - >>> {'loss': 3.0872, 'grad_norm': 1.9312266111373901, 'learning_rate': 1.968415823230534e-05, 'epoch': 1.28}
>>> 2025-09-11 08:38:45,296 - INFO - >>> {'loss': 2.6549, 'grad_norm': 3.728259563446045, 'learning_rate': 1.966287225383796e-05, 'epoch': 1.32}
>>> 2025-09-11 08:38:46,006 - INFO - >>> {'loss': 3.3365, 'grad_norm': 3.384354829788208, 'learning_rate': 1.96409044703021e-05, 'epoch': 1.3599999999999999}
>>> 2025-09-11 08:38:46,621 - INFO - >>> {'loss': 3.1226, 'grad_norm': 2.2034428119659424, 'learning_rate': 1.961825643172819e-05, 'epoch': 1.4}
>>> 2025-09-11 08:38:47,332 - INFO - >>> {'loss': 2.7411, 'grad_norm': 2.7042250633239746, 'learning_rate': 1.9594929736144978e-05, 'epoch': 1.44}
>>> 2025-09-11 08:38:48,042 - INFO - >>> {'loss': 2.9603, 'grad_norm': 2.76400089263916, 'learning_rate': 1.957092602946671e-05, 'epoch': 1.48}
>>> 2025-09-11 08:38:48,752 - INFO - >>> {'loss': 2.2464, 'grad_norm': 3.9914774894714355, 'learning_rate': 1.9546247005377065e-05, 'epoch': 1.52}
>>> 2025-09-11 08:38:49,472 - INFO - >>> {'loss': 2.9905, 'grad_norm': 2.0968661308288574, 'learning_rate': 1.9520894405209593e-05, 'epoch': 1.56}
>>> 2025-09-11 08:38:50,182 - INFO - >>> {'loss': 3.1131, 'grad_norm': 2.5134763717651367, 'learning_rate': 1.9494870017824877e-05, 'epoch': 1.6}
>>> 2025-09-11 08:38:50,893 - INFO - >>> {'loss': 2.7265, 'grad_norm': 2.2065021991729736, 'learning_rate': 1.9468175679484304e-05, 'epoch': 1.6400000000000001}
>>> 2025-09-11 08:38:51,609 - INFO - >>> {'loss': 3.5981, 'grad_norm': 2.6018311977386475, 'learning_rate': 1.9440813273720504e-05, 'epoch': 1.6800000000000002}
>>> 2025-09-11 08:38:52,321 - INFO - >>> {'loss': 2.7936, 'grad_norm': 1.9061949253082275, 'learning_rate': 1.941278473120445e-05, 'epoch': 1.72}
>>> 2025-09-11 08:38:53,032 - INFO - >>> {'loss': 3.0617, 'grad_norm': 3.4235262870788574, 'learning_rate': 1.938409202960922e-05, 'epoch': 1.76}
>>> 2025-09-11 08:38:53,744 - INFO - >>> {'loss': 2.826, 'grad_norm': 1.8137882947921753, 'learning_rate': 1.9354737193470464e-05, 'epoch': 1.8}
>>> 2025-09-11 08:38:54,445 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.932472229404356e-05, 'epoch': 1.8399999999999999}
>>> 2025-09-11 08:38:55,158 - INFO - >>> {'loss': 2.8798, 'grad_norm': 1.8390711545944214, 'learning_rate': 1.9294049449157448e-05, 'epoch': 1.88}
>>> 2025-09-11 08:38:55,869 - INFO - >>> {'loss': 2.721, 'grad_norm': 2.3981244564056396, 'learning_rate': 1.9262720823065217e-05, 'epoch': 1.92}
>>> 2025-09-11 08:38:56,600 - INFO - >>> {'loss': 2.9236, 'grad_norm': 2.445833683013916, 'learning_rate': 1.923073862629139e-05, 'epoch': 1.96}
>>> 2025-09-11 08:38:57,308 - INFO - >>> {'loss': 3.2183, 'grad_norm': 1.8830491304397583, 'learning_rate': 1.9198105115475946e-05, 'epoch': 2.0}
>>> 2025-09-11 08:38:58,025 - INFO - >>> {'loss': 2.7688, 'grad_norm': 2.6436257362365723, 'learning_rate': 1.91648225932151e-05, 'epoch': 2.04}
>>> 2025-09-11 08:38:58,736 - INFO - >>> {'loss': 3.1771, 'grad_norm': 2.06980299949646, 'learning_rate': 1.9130893407898834e-05, 'epoch': 2.08}
>>> 2025-09-11 08:38:59,448 - INFO - >>> {'loss': 2.5765, 'grad_norm': 1.9114570617675781, 'learning_rate': 1.9096319953545186e-05, 'epoch': 2.12}
>>> 2025-09-11 08:39:00,159 - INFO - >>> {'loss': 3.3702, 'grad_norm': 1.8975023031234741, 'learning_rate': 1.9061104669631343e-05, 'epoch': 2.16}
>>> 2025-09-11 08:39:00,872 - INFO - >>> {'loss': 3.6129, 'grad_norm': 3.567922353744507, 'learning_rate': 1.902525004092151e-05, 'epoch': 2.2}
>>> 2025-09-11 08:39:01,583 - INFO - >>> {'loss': 2.6618, 'grad_norm': 2.521266222000122, 'learning_rate': 1.8988758597291577e-05, 'epoch': 2.24}
>>> 2025-09-11 08:39:02,295 - INFO - >>> {'loss': 3.084, 'grad_norm': 1.7472742795944214, 'learning_rate': 1.8951632913550625e-05, 'epoch': 2.2800000000000002}
>>> 2025-09-11 08:39:03,005 - INFO - >>> {'loss': 3.0194, 'grad_norm': 2.033371925354004, 'learning_rate': 1.8913875609259246e-05, 'epoch': 2.32}
>>> 2025-09-11 08:39:03,707 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.8875489348544706e-05, 'epoch': 2.36}
>>> 2025-09-11 08:39:04,420 - INFO - >>> {'loss': 3.1475, 'grad_norm': 2.209843873977661, 'learning_rate': 1.8836476839912967e-05, 'epoch': 2.4}
>>> 2025-09-11 08:39:05,130 - INFO - >>> {'loss': 2.8948, 'grad_norm': 1.912182092666626, 'learning_rate': 1.8796840836057578e-05, 'epoch': 2.44}
>>> 2025-09-11 08:39:05,866 - INFO - >>> {'loss': 2.6729, 'grad_norm': 1.884416103363037, 'learning_rate': 1.8756584133665447e-05, 'epoch': 2.48}
>>> 2025-09-11 08:39:06,597 - INFO - >>> {'loss': 2.9069, 'grad_norm': 2.3164477348327637, 'learning_rate': 1.8715709573219507e-05, 'epoch': 2.52}
>>> 2025-09-11 08:39:07,309 - INFO - >>> {'loss': 2.947, 'grad_norm': 1.8516273498535156, 'learning_rate': 1.86742200387983e-05, 'epoch': 2.56}
>>> 2025-09-11 08:39:08,019 - INFO - >>> {'loss': 3.0258, 'grad_norm': 2.146592617034912, 'learning_rate': 1.8632118457872462e-05, 'epoch': 2.6}
>>> 2025-09-11 08:39:08,731 - INFO - >>> {'loss': 2.128, 'grad_norm': 3.6031055450439453, 'learning_rate': 1.8589407801098192e-05, 'epoch': 2.64}
>>> 2025-09-11 08:39:09,441 - INFO - >>> {'loss': 2.9374, 'grad_norm': 2.8305602073669434, 'learning_rate': 1.854609108210761e-05, 'epoch': 2.68}
>>> 2025-09-11 08:39:10,153 - INFO - >>> {'loss': 2.9455, 'grad_norm': 2.076988458633423, 'learning_rate': 1.8502171357296144e-05, 'epoch': 2.7199999999999998}
>>> 2025-09-11 08:39:10,865 - INFO - >>> {'loss': 2.7484, 'grad_norm': 1.7667094469070435, 'learning_rate': 1.845765172560686e-05, 'epoch': 2.76}
>>> 2025-09-11 08:39:11,567 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.8412535328311813e-05, 'epoch': 2.8}
>>> 2025-09-11 08:39:12,280 - INFO - >>> {'loss': 2.0255, 'grad_norm': 3.1419641971588135, 'learning_rate': 1.8366825348790387e-05, 'epoch': 2.84}
>>> 2025-09-11 08:39:12,990 - INFO - >>> {'loss': 2.7182, 'grad_norm': 1.7949411869049072, 'learning_rate': 1.8320525012304685e-05, 'epoch': 2.88}
>>> 2025-09-11 08:39:13,691 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.8273637585771962e-05, 'epoch': 2.92}
>>> 2025-09-11 08:39:14,412 - INFO - >>> {'loss': 2.8211, 'grad_norm': 1.774865746498108, 'learning_rate': 1.8226166377534113e-05, 'epoch': 2.96}
>>> 2025-09-11 08:39:15,124 - INFO - >>> {'loss': 2.6207, 'grad_norm': 2.244654655456543, 'learning_rate': 1.8178114737124225e-05, 'epoch': 3.0}
>>> 2025-09-11 08:39:15,838 - INFO - >>> {'loss': 3.0712, 'grad_norm': 1.5326980352401733, 'learning_rate': 1.8129486055030255e-05, 'epoch': 3.04}
>>> 2025-09-11 08:39:16,561 - INFO - >>> {'loss': 2.5497, 'grad_norm': 1.8561290502548218, 'learning_rate': 1.8080283762455793e-05, 'epoch': 3.08}
>>> 2025-09-11 08:39:17,273 - INFO - >>> {'loss': 3.4987, 'grad_norm': 2.586223840713501, 'learning_rate': 1.8030511331077945e-05, 'epoch': 3.12}
>>> 2025-09-11 08:39:17,975 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.7980172272802398e-05, 'epoch': 3.16}
>>> 2025-09-11 08:39:18,688 - INFO - >>> {'loss': 3.1616, 'grad_norm': 2.196079969406128, 'learning_rate': 1.7929270139515606e-05, 'epoch': 3.2}
>>> 2025-09-11 08:39:19,401 - INFO - >>> {'loss': 2.9457, 'grad_norm': 2.506702184677124, 'learning_rate': 1.7877808522834175e-05, 'epoch': 3.24}
>>> 2025-09-11 08:39:20,113 - INFO - >>> {'loss': 2.7861, 'grad_norm': 2.817781686782837, 'learning_rate': 1.782579105385145e-05, 'epoch': 3.2800000000000002}
>>> 2025-09-11 08:39:20,824 - INFO - >>> {'loss': 2.9783, 'grad_norm': 1.750414252281189, 'learning_rate': 1.7773221402881296e-05, 'epoch': 3.32}
>>> 2025-09-11 08:39:21,536 - INFO - >>> {'loss': 3.0208, 'grad_norm': 1.6331477165222168, 'learning_rate': 1.772010327919912e-05, 'epoch': 3.36}
>>> 2025-09-11 08:39:22,247 - INFO - >>> {'loss': 2.7414, 'grad_norm': 2.1493618488311768, 'learning_rate': 1.766644043078017e-05, 'epoch': 3.4}
>>> 2025-09-11 08:39:22,959 - INFO - >>> {'loss': 2.6954, 'grad_norm': 2.4395992755889893, 'learning_rate': 1.761223664403505e-05, 'epoch': 3.44}
>>> 2025-09-11 08:39:23,670 - INFO - >>> {'loss': 2.576, 'grad_norm': 2.5486721992492676, 'learning_rate': 1.7557495743542586e-05, 'epoch': 3.48}
>>> 2025-09-11 08:39:24,382 - INFO - >>> {'loss': 3.0004, 'grad_norm': 1.8456697463989258, 'learning_rate': 1.7502221591779932e-05, 'epoch': 3.52}
>>> 2025-09-11 08:39:25,094 - INFO - >>> {'loss': 2.0713, 'grad_norm': 3.1184585094451904, 'learning_rate': 1.744641808885007e-05, 'epoch': 3.56}
>>> 2025-09-11 08:39:25,807 - INFO - >>> {'loss': 2.7017, 'grad_norm': 1.540462851524353, 'learning_rate': 1.7390089172206594e-05, 'epoch': 3.6}
>>> 2025-09-11 08:39:26,520 - INFO - >>> {'loss': 2.6768, 'grad_norm': 1.4151839017868042, 'learning_rate': 1.7333238816375907e-05, 'epoch': 3.64}
>>> 2025-09-11 08:39:27,232 - INFO - >>> {'loss': 2.8666, 'grad_norm': 2.4661078453063965, 'learning_rate': 1.727587103267677e-05, 'epoch': 3.68}
>>> 2025-09-11 08:39:27,946 - INFO - >>> {'loss': 2.5968, 'grad_norm': 2.1369171142578125, 'learning_rate': 1.7217989868937267e-05, 'epoch': 3.7199999999999998}
>>> 2025-09-11 08:39:28,659 - INFO - >>> {'loss': 2.7653, 'grad_norm': 2.6177127361297607, 'learning_rate': 1.7159599409209194e-05, 'epoch': 3.76}
>>> 2025-09-11 08:39:29,372 - INFO - >>> {'loss': 2.3323, 'grad_norm': 3.4709973335266113, 'learning_rate': 1.7100703773479898e-05, 'epoch': 3.8}
>>> 2025-09-11 08:39:30,084 - INFO - >>> {'loss': 2.7983, 'grad_norm': 2.836141586303711, 'learning_rate': 1.704130711738157e-05, 'epoch': 3.84}
>>> 2025-09-11 08:39:30,787 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.6981413631898012e-05, 'epoch': 3.88}
>>> 2025-09-11 08:39:31,491 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.692102754306895e-05, 'epoch': 3.92}
>>> 2025-09-11 08:39:32,205 - INFO - >>> {'loss': 2.8829, 'grad_norm': 1.8505969047546387, 'learning_rate': 1.6860153111691834e-05, 'epoch': 3.96}
>>> 2025-09-11 08:39:32,912 - INFO - >>> {'loss': 2.4903, 'grad_norm': 2.4923360347747803, 'learning_rate': 1.6798794633021192e-05, 'epoch': 4.0}
>>> 2025-09-11 08:39:35,345 - INFO - >>> {'loss': 2.8547, 'grad_norm': 2.1915738582611084, 'learning_rate': 1.6736956436465573e-05, 'epoch': 4.04}
>>> 2025-09-11 08:39:36,057 - INFO - >>> {'loss': 2.4389, 'grad_norm': 2.161588191986084, 'learning_rate': 1.667464288528207e-05, 'epoch': 4.08}
>>> 2025-09-11 08:39:36,761 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.661185837626843e-05, 'epoch': 4.12}
>>> 2025-09-11 08:39:37,472 - INFO - >>> {'loss': 3.3965, 'grad_norm': 2.371933698654175, 'learning_rate': 1.6548607339452853e-05, 'epoch': 4.16}
>>> 2025-09-11 08:39:38,185 - INFO - >>> {'loss': 2.7222, 'grad_norm': 1.843483567237854, 'learning_rate': 1.648489423778137e-05, 'epoch': 4.2}
>>> 2025-09-11 08:39:38,899 - INFO - >>> {'loss': 2.7588, 'grad_norm': 2.949387311935425, 'learning_rate': 1.6420723566802982e-05, 'epoch': 4.24}
>>> 2025-09-11 08:39:39,611 - INFO - >>> {'loss': 2.7375, 'grad_norm': 1.9960923194885254, 'learning_rate': 1.6356099854352435e-05, 'epoch': 4.28}
>>> 2025-09-11 08:39:40,323 - INFO - >>> {'loss': 2.8153, 'grad_norm': 1.7384424209594727, 'learning_rate': 1.6291027660230735e-05, 'epoch': 4.32}
>>> 2025-09-11 08:39:41,035 - INFO - >>> {'loss': 2.7035, 'grad_norm': 2.8434622287750244, 'learning_rate': 1.6225511575883436e-05, 'epoch': 4.36}
>>> 2025-09-11 08:39:41,756 - INFO - >>> {'loss': 2.8005, 'grad_norm': 1.6152116060256958, 'learning_rate': 1.6159556224076637e-05, 'epoch': 4.4}
>>> 2025-09-11 08:39:42,468 - INFO - >>> {'loss': 2.4863, 'grad_norm': 2.1808395385742188, 'learning_rate': 1.6093166258570845e-05, 'epoch': 4.44}
>>> 2025-09-11 08:39:43,180 - INFO - >>> {'loss': 3.1426, 'grad_norm': 2.4072177410125732, 'learning_rate': 1.6026346363792565e-05, 'epoch': 4.48}
>>> 2025-09-11 08:39:43,892 - INFO - >>> {'loss': 2.6083, 'grad_norm': 1.6504968404769897, 'learning_rate': 1.5959101254503802e-05, 'epoch': 4.52}
>>> 2025-09-11 08:39:44,605 - INFO - >>> {'loss': 2.0487, 'grad_norm': 2.4583780765533447, 'learning_rate': 1.5891435675469376e-05, 'epoch': 4.5600000000000005}
>>> 2025-09-11 08:39:45,318 - INFO - >>> {'loss': 2.9468, 'grad_norm': 2.363485813140869, 'learning_rate': 1.582335440112214e-05, 'epoch': 4.6}
>>> 2025-09-11 08:39:46,021 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.57548622352261e-05, 'epoch': 4.64}
>>> 2025-09-11 08:39:46,735 - INFO - >>> {'loss': 2.8793, 'grad_norm': 1.8346123695373535, 'learning_rate': 1.5685964010537466e-05, 'epoch': 4.68}
>>> 2025-09-11 08:39:47,441 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.561666458846365e-05, 'epoch': 4.72}
>>> 2025-09-11 08:39:48,153 - INFO - >>> {'loss': 2.8214, 'grad_norm': 3.4732699394226074, 'learning_rate': 1.5546968858720245e-05, 'epoch': 4.76}
>>> 2025-09-11 08:39:48,866 - INFO - >>> {'loss': 2.8265, 'grad_norm': 1.9728604555130005, 'learning_rate': 1.5476881738986037e-05, 'epoch': 4.8}
>>> 2025-09-11 08:39:49,579 - INFO - >>> {'loss': 2.9365, 'grad_norm': 1.741029143333435, 'learning_rate': 1.5406408174555978e-05, 'epoch': 4.84}
>>> 2025-09-11 08:39:50,291 - INFO - >>> {'loss': 2.7924, 'grad_norm': 3.2157511711120605, 'learning_rate': 1.5335553137992286e-05, 'epoch': 4.88}
>>> 2025-09-11 08:39:51,003 - INFO - >>> {'loss': 2.6291, 'grad_norm': 1.8585755825042725, 'learning_rate': 1.526432162877356e-05, 'epoch': 4.92}
>>> 2025-09-11 08:39:51,716 - INFO - >>> {'loss': 2.7496, 'grad_norm': 2.0395607948303223, 'learning_rate': 1.519271867294203e-05, 'epoch': 4.96}
>>> 2025-09-11 08:39:52,417 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.5120749322748926e-05, 'epoch': 5.0}
>>> 2025-09-11 08:39:53,134 - INFO - >>> {'loss': 2.6892, 'grad_norm': 1.7707933187484741, 'learning_rate': 1.504841865629799e-05, 'epoch': 5.04}
>>> 2025-09-11 08:39:53,846 - INFO - >>> {'loss': 2.9709, 'grad_norm': 2.3012404441833496, 'learning_rate': 1.4975731777187159e-05, 'epoch': 5.08}
>>> 2025-09-11 08:39:54,559 - INFO - >>> {'loss': 2.6252, 'grad_norm': 2.909552574157715, 'learning_rate': 1.490269381414849e-05, 'epoch': 5.12}
>>> 2025-09-11 08:39:55,272 - INFO - >>> {'loss': 2.9931, 'grad_norm': 1.780259609222412, 'learning_rate': 1.4829309920686245e-05, 'epoch': 5.16}
>>> 2025-09-11 08:39:55,987 - INFO - >>> {'loss': 3.0758, 'grad_norm': 2.326460361480713, 'learning_rate': 1.4755585274713289e-05, 'epoch': 5.2}
>>> 2025-09-11 08:39:56,699 - INFO - >>> {'loss': 2.4645, 'grad_norm': 2.6287596225738525, 'learning_rate': 1.4681525078185717e-05, 'epoch': 5.24}
>>> 2025-09-11 08:39:57,411 - INFO - >>> {'loss': 2.9177, 'grad_norm': 2.1470584869384766, 'learning_rate': 1.4607134556735836e-05, 'epoch': 5.28}
>>> 2025-09-11 08:39:58,126 - INFO - >>> {'loss': 2.8135, 'grad_norm': 2.608851909637451, 'learning_rate': 1.4532418959303425e-05, 'epoch': 5.32}
>>> 2025-09-11 08:39:58,829 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.4457383557765385e-05, 'epoch': 5.36}
>>> 2025-09-11 08:39:59,579 - INFO - >>> {'loss': 2.5294, 'grad_norm': 1.7497117519378662, 'learning_rate': 1.4382033646563753e-05, 'epoch': 5.4}
>>> 2025-09-11 08:40:00,295 - INFO - >>> {'loss': 2.4746, 'grad_norm': 1.6875522136688232, 'learning_rate': 1.4306374542332141e-05, 'epoch': 5.44}
>>> 2025-09-11 08:40:01,008 - INFO - >>> {'loss': 1.4598, 'grad_norm': 3.691601514816284, 'learning_rate': 1.4230411583520581e-05, 'epoch': 5.48}
>>> 2025-09-11 08:40:01,723 - INFO - >>> {'loss': 2.6444, 'grad_norm': 1.8923861980438232, 'learning_rate': 1.4154150130018867e-05, 'epoch': 5.52}
>>> 2025-09-11 08:40:02,428 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.4077595562778348e-05, 'epoch': 5.5600000000000005}
>>> 2025-09-11 08:40:03,134 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.4000753283432267e-05, 'epoch': 5.6}
>>> 2025-09-11 08:40:03,848 - INFO - >>> {'loss': 2.4939, 'grad_norm': 3.4992847442626953, 'learning_rate': 1.3923628713914616e-05, 'epoch': 5.64}
>>> 2025-09-11 08:40:04,563 - INFO - >>> {'loss': 2.4768, 'grad_norm': 2.9474847316741943, 'learning_rate': 1.3846227296077568e-05, 'epoch': 5.68}
>>> 2025-09-11 08:40:05,278 - INFO - >>> {'loss': 2.1864, 'grad_norm': 3.4094197750091553, 'learning_rate': 1.3768554491307515e-05, 'epoch': 5.72}
>>> 2025-09-11 08:40:05,992 - INFO - >>> {'loss': 2.7438, 'grad_norm': 1.9916467666625977, 'learning_rate': 1.3690615780139703e-05, 'epoch': 5.76}
>>> 2025-09-11 08:40:06,706 - INFO - >>> {'loss': 2.6267, 'grad_norm': 1.7094638347625732, 'learning_rate': 1.3612416661871532e-05, 'epoch': 5.8}
>>> 2025-09-11 08:40:07,412 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.3533962654174542e-05, 'epoch': 5.84}
>>> 2025-09-11 08:40:08,128 - INFO - >>> {'loss': 2.6139, 'grad_norm': 2.9640684127807617, 'learning_rate': 1.3455259292705072e-05, 'epoch': 5.88}
>>> 2025-09-11 08:40:08,842 - INFO - >>> {'loss': 3.0274, 'grad_norm': 1.684770941734314, 'learning_rate': 1.337631213071369e-05, 'epoch': 5.92}
>>> 2025-09-11 08:40:09,565 - INFO - >>> {'loss': 2.5765, 'grad_norm': 1.8000004291534424, 'learning_rate': 1.3297126738653331e-05, 'epoch': 5.96}
>>> 2025-09-11 08:40:10,277 - INFO - >>> {'loss': 3.3909, 'grad_norm': 3.8661484718322754, 'learning_rate': 1.321770870378628e-05, 'epoch': 6.0}
>>> 2025-09-11 08:40:10,995 - INFO - >>> {'loss': 2.7026, 'grad_norm': 3.309596300125122, 'learning_rate': 1.3138063629789924e-05, 'epoch': 6.04}
>>> 2025-09-11 08:40:11,699 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.3058197136361344e-05, 'epoch': 6.08}
>>> 2025-09-11 08:40:12,414 - INFO - >>> {'loss': 2.8241, 'grad_norm': 1.7204762697219849, 'learning_rate': 1.2978114858820836e-05, 'epoch': 6.12}
>>> 2025-09-11 08:40:13,128 - INFO - >>> {'loss': 2.4453, 'grad_norm': 2.254615306854248, 'learning_rate': 1.2897822447714247e-05, 'epoch': 6.16}
>>> 2025-09-11 08:40:13,843 - INFO - >>> {'loss': 3.0485, 'grad_norm': 2.3340961933135986, 'learning_rate': 1.2817325568414299e-05, 'epoch': 6.2}
>>> 2025-09-11 08:40:14,557 - INFO - >>> {'loss': 2.6278, 'grad_norm': 1.8347795009613037, 'learning_rate': 1.2736629900720832e-05, 'epoch': 6.24}
>>> 2025-09-11 08:40:15,271 - INFO - >>> {'loss': 2.7712, 'grad_norm': 1.8184982538223267, 'learning_rate': 1.2655741138460045e-05, 'epoch': 6.28}
>>> 2025-09-11 08:40:15,975 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.257466498908276e-05, 'epoch': 6.32}
>>> 2025-09-11 08:40:16,690 - INFO - >>> {'loss': 2.3878, 'grad_norm': 4.071121692657471, 'learning_rate': 1.2493407173261676e-05, 'epoch': 6.36}
>>> 2025-09-11 08:40:17,395 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.2411973424487751e-05, 'epoch': 6.4}
>>> 2025-09-11 08:40:18,099 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.2330369488665648e-05, 'epoch': 6.44}
>>> 2025-09-11 08:40:18,813 - INFO - >>> {'loss': 2.1091, 'grad_norm': 3.620255470275879, 'learning_rate': 1.2248601123708279e-05, 'epoch': 6.48}
>>> 2025-09-11 08:40:19,526 - INFO - >>> {'loss': 2.6773, 'grad_norm': 1.7932759523391724, 'learning_rate': 1.2166674099130577e-05, 'epoch': 6.52}
>>> 2025-09-11 08:40:20,240 - INFO - >>> {'loss': 2.673, 'grad_norm': 1.8653488159179688, 'learning_rate': 1.2084594195642367e-05, 'epoch': 6.5600000000000005}
>>> 2025-09-11 08:40:20,954 - INFO - >>> {'loss': 2.9849, 'grad_norm': 3.809307098388672, 'learning_rate': 1.2002367204740498e-05, 'epoch': 6.6}
>>> 2025-09-11 08:40:21,670 - INFO - >>> {'loss': 3.022, 'grad_norm': 2.264085531234741, 'learning_rate': 1.1919998928300203e-05, 'epoch': 6.64}
>>> 2025-09-11 08:40:22,385 - INFO - >>> {'loss': 3.0826, 'grad_norm': 2.10901141166687, 'learning_rate': 1.1837495178165706e-05, 'epoch': 6.68}
>>> 2025-09-11 08:40:23,098 - INFO - >>> {'loss': 1.7266, 'grad_norm': 2.914386749267578, 'learning_rate': 1.1754861775740163e-05, 'epoch': 6.72}
>>> 2025-09-11 08:40:23,812 - INFO - >>> {'loss': 2.1432, 'grad_norm': 2.682044744491577, 'learning_rate': 1.1672104551574896e-05, 'epoch': 6.76}
>>> 2025-09-11 08:40:24,526 - INFO - >>> {'loss': 2.5765, 'grad_norm': 2.9517056941986084, 'learning_rate': 1.1589229344958e-05, 'epoch': 6.8}
>>> 2025-09-11 08:40:25,242 - INFO - >>> {'loss': 2.7859, 'grad_norm': 3.3021275997161865, 'learning_rate': 1.1506242003502322e-05, 'epoch': 6.84}
>>> 2025-09-11 08:40:25,969 - INFO - >>> {'loss': 2.6079, 'grad_norm': 1.5936100482940674, 'learning_rate': 1.1423148382732854e-05, 'epoch': 6.88}
>>> 2025-09-11 08:40:26,683 - INFO - >>> {'loss': 2.4384, 'grad_norm': 2.9918882846832275, 'learning_rate': 1.1339954345673582e-05, 'epoch': 6.92}
>>> 2025-09-11 08:40:27,396 - INFO - >>> {'loss': 2.5179, 'grad_norm': 1.6123054027557373, 'learning_rate': 1.1256665762433798e-05, 'epoch': 6.96}
>>> 2025-09-11 08:40:28,108 - INFO - >>> {'loss': 2.9118, 'grad_norm': 1.9426778554916382, 'learning_rate': 1.117328850979389e-05, 'epoch': 7.0}
>>> 2025-09-11 08:40:28,829 - INFO - >>> {'loss': 2.7562, 'grad_norm': 2.177427053451538, 'learning_rate': 1.1089828470790694e-05, 'epoch': 7.04}
>>> 2025-09-11 08:40:29,536 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.1006291534302402e-05, 'epoch': 7.08}
>>> 2025-09-11 08:40:30,250 - INFO - >>> {'loss': 2.3145, 'grad_norm': 2.2817554473876953, 'learning_rate': 1.092268359463302e-05, 'epoch': 7.12}
>>> 2025-09-11 08:40:30,966 - INFO - >>> {'loss': 2.0955, 'grad_norm': 2.5106399059295654, 'learning_rate': 1.0839010551096498e-05, 'epoch': 7.16}
>>> 2025-09-11 08:40:31,680 - INFO - >>> {'loss': 2.7954, 'grad_norm': 2.491279125213623, 'learning_rate': 1.0755278307600459e-05, 'epoch': 7.2}
>>> 2025-09-11 08:40:32,395 - INFO - >>> {'loss': 2.8526, 'grad_norm': 2.356060743331909, 'learning_rate': 1.0671492772229629e-05, 'epoch': 7.24}
>>> 2025-09-11 08:40:33,110 - INFO - >>> {'loss': 1.6943, 'grad_norm': 2.912184715270996, 'learning_rate': 1.058765985682898e-05, 'epoch': 7.28}
>>> 2025-09-11 08:40:33,827 - INFO - >>> {'loss': 3.2905, 'grad_norm': 2.735557794570923, 'learning_rate': 1.050378547658657e-05, 'epoch': 7.32}
>>> 2025-09-11 08:40:34,543 - INFO - >>> {'loss': 2.4612, 'grad_norm': 1.80570650100708, 'learning_rate': 1.0419875549616196e-05, 'epoch': 7.36}
>>> 2025-09-11 08:40:35,250 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.0335935996539803e-05, 'epoch': 7.4}
>>> 2025-09-11 08:40:35,955 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.0251972740069724e-05, 'epoch': 7.44}
>>> 2025-09-11 08:40:36,681 - INFO - >>> {'loss': 2.6401, 'grad_norm': 2.5397729873657227, 'learning_rate': 1.0167991704590803e-05, 'epoch': 7.48}
>>> 2025-09-11 08:40:37,397 - INFO - >>> {'loss': 2.7601, 'grad_norm': 1.694071888923645, 'learning_rate': 1.0083998815742335e-05, 'epoch': 7.52}
>>> 2025-09-11 08:40:38,103 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1e-05, 'epoch': 7.5600000000000005}
>>> 2025-09-11 08:40:38,818 - INFO - >>> {'loss': 2.4857, 'grad_norm': 2.1205930709838867, 'learning_rate': 9.916001184257668e-06, 'epoch': 7.6}
>>> 2025-09-11 08:40:39,535 - INFO - >>> {'loss': 3.3075, 'grad_norm': 3.217769145965576, 'learning_rate': 9.8320082954092e-06, 'epoch': 7.64}
>>> 2025-09-11 08:40:40,250 - INFO - >>> {'loss': 2.5227, 'grad_norm': 2.917438507080078, 'learning_rate': 9.748027259930276e-06, 'epoch': 7.68}
>>> 2025-09-11 08:40:40,814 - INFO - >>> {'loss': 2.5561, 'grad_norm': 1.7121261358261108, 'learning_rate': 9.6640640034602e-06, 'epoch': 7.72}
>>> 2025-09-11 08:40:41,529 - INFO - >>> {'loss': 3.0589, 'grad_norm': 1.7049212455749512, 'learning_rate': 9.580124450383804e-06, 'epoch': 7.76}
>>> 2025-09-11 08:40:42,244 - INFO - >>> {'loss': 2.8158, 'grad_norm': 2.0437867641448975, 'learning_rate': 9.496214523413433e-06, 'epoch': 7.8}
>>> 2025-09-11 08:40:42,958 - INFO - >>> {'loss': 2.4231, 'grad_norm': 1.9561989307403564, 'learning_rate': 9.412340143171025e-06, 'epoch': 7.84}
>>> 2025-09-11 08:40:43,671 - INFO - >>> {'loss': 2.5961, 'grad_norm': 1.9584863185882568, 'learning_rate': 9.328507227770375e-06, 'epoch': 7.88}
>>> 2025-09-11 08:40:44,387 - INFO - >>> {'loss': 1.7632, 'grad_norm': 4.305991172790527, 'learning_rate': 9.244721692399545e-06, 'epoch': 7.92}
>>> 2025-09-11 08:40:45,093 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.160989448903504e-06, 'epoch': 7.96}
>>> 2025-09-11 08:40:45,805 - INFO - >>> {'loss': 2.5633, 'grad_norm': 2.9945778846740723, 'learning_rate': 9.07731640536698e-06, 'epoch': 8.0}
>>> 2025-09-11 08:40:48,215 - INFO - >>> {'loss': 2.634, 'grad_norm': 1.9881471395492554, 'learning_rate': 8.9937084656976e-06, 'epoch': 8.04}
>>> 2025-09-11 08:40:48,929 - INFO - >>> {'loss': 3.1258, 'grad_norm': 1.9542518854141235, 'learning_rate': 8.910171529209306e-06, 'epoch': 8.08}
>>> 2025-09-11 08:40:49,645 - INFO - >>> {'loss': 2.4846, 'grad_norm': 2.0835816860198975, 'learning_rate': 8.826711490206113e-06, 'epoch': 8.12}
>>> 2025-09-11 08:40:50,350 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.743334237566202e-06, 'epoch': 8.16}
>>> 2025-09-11 08:40:51,064 - INFO - >>> {'loss': 2.3745, 'grad_norm': 2.273139715194702, 'learning_rate': 8.660045654326421e-06, 'epoch': 8.2}
>>> 2025-09-11 08:40:51,779 - INFO - >>> {'loss': 2.7145, 'grad_norm': 1.7751691341400146, 'learning_rate': 8.576851617267151e-06, 'epoch': 8.24}
>>> 2025-09-11 08:40:52,493 - INFO - >>> {'loss': 2.7912, 'grad_norm': 2.7453439235687256, 'learning_rate': 8.493757996497683e-06, 'epoch': 8.28}
>>> 2025-09-11 08:40:53,208 - INFO - >>> {'loss': 3.2846, 'grad_norm': 3.324972629547119, 'learning_rate': 8.410770655042003e-06, 'epoch': 8.32}
>>> 2025-09-11 08:40:53,924 - INFO - >>> {'loss': 2.9262, 'grad_norm': 3.998997926712036, 'learning_rate': 8.327895448425105e-06, 'epoch': 8.36}
>>> 2025-09-11 08:40:54,638 - INFO - >>> {'loss': 2.7019, 'grad_norm': 2.5207252502441406, 'learning_rate': 8.24513822425984e-06, 'epoch': 8.4}
>>> 2025-09-11 08:40:55,353 - INFO - >>> {'loss': 2.6823, 'grad_norm': 2.1745126247406006, 'learning_rate': 8.162504821834296e-06, 'epoch': 8.44}
>>> 2025-09-11 08:40:56,067 - INFO - >>> {'loss': 2.33, 'grad_norm': 2.6135318279266357, 'learning_rate': 8.0800010716998e-06, 'epoch': 8.48}
>>> 2025-09-11 08:40:56,783 - INFO - >>> {'loss': 2.3793, 'grad_norm': 3.081331491470337, 'learning_rate': 7.997632795259504e-06, 'epoch': 8.52}
>>> 2025-09-11 08:40:57,496 - INFO - >>> {'loss': 2.477, 'grad_norm': 1.8579832315444946, 'learning_rate': 7.915405804357632e-06, 'epoch': 8.56}
>>> 2025-09-11 08:40:58,210 - INFO - >>> {'loss': 2.5402, 'grad_norm': 2.1874783039093018, 'learning_rate': 7.833325900869428e-06, 'epoch': 8.6}
>>> 2025-09-11 08:40:58,924 - INFO - >>> {'loss': 2.5928, 'grad_norm': 3.7730140686035156, 'learning_rate': 7.751398876291725e-06, 'epoch': 8.64}
>>> 2025-09-11 08:40:59,648 - INFO - >>> {'loss': 2.4223, 'grad_norm': 2.11783766746521, 'learning_rate': 7.669630511334358e-06, 'epoch': 8.68}
>>> 2025-09-11 08:41:00,361 - INFO - >>> {'loss': 2.3149, 'grad_norm': 2.5909829139709473, 'learning_rate': 7.58802657551225e-06, 'epoch': 8.72}
>>> 2025-09-11 08:41:01,075 - INFO - >>> {'loss': 2.5817, 'grad_norm': 3.4438564777374268, 'learning_rate': 7.506592826738327e-06, 'epoch': 8.76}
>>> 2025-09-11 08:41:01,790 - INFO - >>> {'loss': 2.4723, 'grad_norm': 2.7260279655456543, 'learning_rate': 7.425335010917244e-06, 'epoch': 8.8}
>>> 2025-09-11 08:41:02,504 - INFO - >>> {'loss': 2.3217, 'grad_norm': 4.4079718589782715, 'learning_rate': 7.344258861539957e-06, 'epoch': 8.84}
>>> 2025-09-11 08:41:03,219 - INFO - >>> {'loss': 2.8752, 'grad_norm': 2.2776994705200195, 'learning_rate': 7.263370099279173e-06, 'epoch': 8.88}
>>> 2025-09-11 08:41:03,932 - INFO - >>> {'loss': 2.816, 'grad_norm': 2.0592563152313232, 'learning_rate': 7.182674431585703e-06, 'epoch': 8.92}
>>> 2025-09-11 08:41:04,647 - INFO - >>> {'loss': 2.0768, 'grad_norm': 4.044602394104004, 'learning_rate': 7.102177552285753e-06, 'epoch': 8.96}
>>> 2025-09-11 08:41:05,351 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.021885141179166e-06, 'epoch': 9.0}
>>> 2025-09-11 08:41:06,066 - INFO - >>> {'loss': 3.0198, 'grad_norm': 2.2270236015319824, 'learning_rate': 6.9418028636386595e-06, 'epoch': 9.04}
>>> 2025-09-11 08:41:06,778 - INFO - >>> {'loss': 2.5138, 'grad_norm': 3.1227800846099854, 'learning_rate': 6.861936370210083e-06, 'epoch': 9.08}
>>> 2025-09-11 08:41:07,491 - INFO - >>> {'loss': 2.457, 'grad_norm': 2.1453804969787598, 'learning_rate': 6.7822912962137225e-06, 'epoch': 9.12}
>>> 2025-09-11 08:41:08,204 - INFO - >>> {'loss': 2.568, 'grad_norm': 3.7198450565338135, 'learning_rate': 6.7028732613466695e-06, 'epoch': 9.16}
>>> 2025-09-11 08:41:08,916 - INFO - >>> {'loss': 2.5645, 'grad_norm': 3.51526141166687, 'learning_rate': 6.623687869286314e-06, 'epoch': 9.2}
>>> 2025-09-11 08:41:09,622 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.54474070729493e-06, 'epoch': 9.24}
>>> 2025-09-11 08:41:10,335 - INFO - >>> {'loss': 2.5463, 'grad_norm': 2.173339605331421, 'learning_rate': 6.466037345825462e-06, 'epoch': 9.28}
>>> 2025-09-11 08:41:11,049 - INFO - >>> {'loss': 2.3677, 'grad_norm': 3.2274820804595947, 'learning_rate': 6.387583338128471e-06, 'epoch': 9.32}
>>> 2025-09-11 08:41:11,752 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.3093842198603014e-06, 'epoch': 9.36}
>>> 2025-09-11 08:41:12,465 - INFO - >>> {'loss': 2.7462, 'grad_norm': 2.532724142074585, 'learning_rate': 6.2314455086924855e-06, 'epoch': 9.4}
>>> 2025-09-11 08:41:13,178 - INFO - >>> {'loss': 2.4102, 'grad_norm': 1.6551737785339355, 'learning_rate': 6.153772703922434e-06, 'epoch': 9.44}
>>> 2025-09-11 08:41:13,891 - INFO - >>> {'loss': 3.0749, 'grad_norm': 2.023045539855957, 'learning_rate': 6.076371286085387e-06, 'epoch': 9.48}
>>> 2025-09-11 08:41:14,604 - INFO - >>> {'loss': 2.5571, 'grad_norm': 3.357135057449341, 'learning_rate': 5.999246716567737e-06, 'epoch': 9.52}
>>> 2025-09-11 08:41:15,309 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.922404437221653e-06, 'epoch': 9.56}
>>> 2025-09-11 08:41:16,032 - INFO - >>> {'loss': 2.4851, 'grad_norm': 1.5939851999282837, 'learning_rate': 5.845849869981137e-06, 'epoch': 9.6}
>>> 2025-09-11 08:41:16,745 - INFO - >>> {'loss': 2.6576, 'grad_norm': 2.29506516456604, 'learning_rate': 5.7695884164794225e-06, 'epoch': 9.64}
>>> 2025-09-11 08:41:17,457 - INFO - >>> {'loss': 2.9043, 'grad_norm': 4.077685832977295, 'learning_rate': 5.693625457667862e-06, 'epoch': 9.68}
>>> 2025-09-11 08:41:18,170 - INFO - >>> {'loss': 2.0583, 'grad_norm': 3.9276981353759766, 'learning_rate': 5.61796635343625e-06, 'epoch': 9.72}
>>> 2025-09-11 08:41:18,884 - INFO - >>> {'loss': 2.4003, 'grad_norm': 1.8529441356658936, 'learning_rate': 5.542616442234618e-06, 'epoch': 9.76}
>>> 2025-09-11 08:41:19,596 - INFO - >>> {'loss': 2.6125, 'grad_norm': 2.0791893005371094, 'learning_rate': 5.467581040696577e-06, 'epoch': 9.8}
>>> 2025-09-11 08:41:20,300 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.392865443264164e-06, 'epoch': 9.84}
>>> 2025-09-11 08:41:21,004 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.318474921814289e-06, 'epoch': 9.88}
>>> 2025-09-11 08:41:21,718 - INFO - >>> {'loss': 2.3561, 'grad_norm': 1.9502074718475342, 'learning_rate': 5.244414725286717e-06, 'epoch': 9.92}
>>> 2025-09-11 08:41:22,431 - INFO - >>> {'loss': 2.5087, 'grad_norm': 3.1223747730255127, 'learning_rate': 5.170690079313756e-06, 'epoch': 9.96}
>>> 2025-09-11 08:41:23,142 - INFO - >>> {'loss': 2.9841, 'grad_norm': 1.8343225717544556, 'learning_rate': 5.097306185851515e-06, 'epoch': 10.0}
>>> 2025-09-11 08:41:23,860 - INFO - >>> {'loss': 2.7666, 'grad_norm': 2.831852674484253, 'learning_rate': 5.024268222812844e-06, 'epoch': 10.04}
>>> 2025-09-11 08:41:24,575 - INFO - >>> {'loss': 2.8737, 'grad_norm': 3.7634496688842773, 'learning_rate': 4.951581343702014e-06, 'epoch': 10.08}
>>> 2025-09-11 08:41:25,287 - INFO - >>> {'loss': 2.7064, 'grad_norm': 4.234188079833984, 'learning_rate': 4.879250677251077e-06, 'epoch': 10.12}
>>> 2025-09-11 08:41:26,002 - INFO - >>> {'loss': 2.665, 'grad_norm': 2.2247307300567627, 'learning_rate': 4.807281327057972e-06, 'epoch': 10.16}
>>> 2025-09-11 08:41:26,715 - INFO - >>> {'loss': 2.8456, 'grad_norm': 2.7728543281555176, 'learning_rate': 4.7356783712264405e-06, 'epoch': 10.2}
>>> 2025-09-11 08:41:27,428 - INFO - >>> {'loss': 2.0329, 'grad_norm': 4.079555988311768, 'learning_rate': 4.664446862007718e-06, 'epoch': 10.24}
>>> 2025-09-11 08:41:28,141 - INFO - >>> {'loss': 2.294, 'grad_norm': 2.638967990875244, 'learning_rate': 4.593591825444028e-06, 'epoch': 10.28}
>>> 2025-09-11 08:41:28,852 - INFO - >>> {'loss': 2.54, 'grad_norm': 1.8780213594436646, 'learning_rate': 4.523118261013969e-06, 'epoch': 10.32}
>>> 2025-09-11 08:41:29,566 - INFO - >>> {'loss': 2.2579, 'grad_norm': 2.0454089641571045, 'learning_rate': 4.453031141279758e-06, 'epoch': 10.36}
>>> 2025-09-11 08:41:30,104 - INFO - >>> {'loss': 2.8189, 'grad_norm': 1.5193262100219727, 'learning_rate': 4.383335411536357e-06, 'epoch': 10.4}
>>> 2025-09-11 08:41:30,816 - INFO - >>> {'loss': 2.5226, 'grad_norm': 2.1579694747924805, 'learning_rate': 4.314035989462535e-06, 'epoch': 10.44}
>>> 2025-09-11 08:41:31,529 - INFO - >>> {'loss': 2.4355, 'grad_norm': 3.275402545928955, 'learning_rate': 4.245137764773899e-06, 'epoch': 10.48}
>>> 2025-09-11 08:41:32,241 - INFO - >>> {'loss': 1.8621, 'grad_norm': 4.014593124389648, 'learning_rate': 4.176645598877862e-06, 'epoch': 10.52}
>>> 2025-09-11 08:41:32,952 - INFO - >>> {'loss': 2.5344, 'grad_norm': 2.1937735080718994, 'learning_rate': 4.108564324530626e-06, 'epoch': 10.56}
>>> 2025-09-11 08:41:33,665 - INFO - >>> {'loss': 2.7128, 'grad_norm': 3.60469913482666, 'learning_rate': 4.040898745496199e-06, 'epoch': 10.6}
>>> 2025-09-11 08:41:34,369 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 3.973653636207437e-06, 'epoch': 10.64}
>>> 2025-09-11 08:41:35,082 - INFO - >>> {'loss': 1.3245, 'grad_norm': 4.193896770477295, 'learning_rate': 3.90683374142916e-06, 'epoch': 10.68}
>>> 2025-09-11 08:41:35,796 - INFO - >>> {'loss': 2.5272, 'grad_norm': 3.5664591789245605, 'learning_rate': 3.840443775923365e-06, 'epoch': 10.72}
>>> 2025-09-11 08:41:36,497 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 3.774488424116569e-06, 'epoch': 10.76}
>>> 2025-09-11 08:41:37,211 - INFO - >>> {'loss': 2.7171, 'grad_norm': 2.363497495651245, 'learning_rate': 3.70897233976927e-06, 'epoch': 10.8}
>>> 2025-09-11 08:41:37,924 - INFO - >>> {'loss': 2.8229, 'grad_norm': 2.134340286254883, 'learning_rate': 3.6439001456475698e-06, 'epoch': 10.84}
>>> 2025-09-11 08:41:38,637 - INFO - >>> {'loss': 2.5632, 'grad_norm': 3.61037540435791, 'learning_rate': 3.5792764331970187e-06, 'epoch': 10.88}
>>> 2025-09-11 08:41:39,366 - INFO - >>> {'loss': 2.487, 'grad_norm': 2.2210400104522705, 'learning_rate': 3.5151057622186336e-06, 'epoch': 10.92}
>>> 2025-09-11 08:41:40,079 - INFO - >>> {'loss': 2.7887, 'grad_norm': 1.994485855102539, 'learning_rate': 3.4513926605471504e-06, 'epoch': 10.96}
>>> 2025-09-11 08:41:40,800 - INFO - >>> {'loss': 2.5899, 'grad_norm': 2.575460195541382, 'learning_rate': 3.3881416237315677e-06, 'epoch': 11.0}
>>> 2025-09-11 08:41:41,515 - INFO - >>> {'loss': 2.899, 'grad_norm': 4.139055252075195, 'learning_rate': 3.3253571147179333e-06, 'epoch': 11.04}
>>> 2025-09-11 08:41:42,227 - INFO - >>> {'loss': 2.6048, 'grad_norm': 3.087430953979492, 'learning_rate': 3.2630435635344283e-06, 'epoch': 11.08}
>>> 2025-09-11 08:41:42,958 - INFO - >>> {'loss': 2.7306, 'grad_norm': 2.356034517288208, 'learning_rate': 3.2012053669788136e-06, 'epoch': 11.12}
>>> 2025-09-11 08:41:43,662 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 3.139846888308169e-06, 'epoch': 11.16}
>>> 2025-09-11 08:41:44,374 - INFO - >>> {'loss': 2.6793, 'grad_norm': 2.306899070739746, 'learning_rate': 3.0789724569310532e-06, 'epoch': 11.2}
>>> 2025-09-11 08:41:45,096 - INFO - >>> {'loss': 2.4116, 'grad_norm': 1.71361243724823, 'learning_rate': 3.01858636810199e-06, 'epoch': 11.24}
>>> 2025-09-11 08:41:45,808 - INFO - >>> {'loss': 2.6642, 'grad_norm': 1.7991136312484741, 'learning_rate': 2.9586928826184323e-06, 'epoch': 11.28}
>>> 2025-09-11 08:41:46,519 - INFO - >>> {'loss': 2.8488, 'grad_norm': 3.871609687805176, 'learning_rate': 2.8992962265201032e-06, 'epoch': 11.32}
>>> 2025-09-11 08:41:47,232 - INFO - >>> {'loss': 1.8543, 'grad_norm': 4.065567493438721, 'learning_rate': 2.8404005907908083e-06, 'epoch': 11.36}
>>> 2025-09-11 08:41:47,945 - INFO - >>> {'loss': 2.8048, 'grad_norm': 2.31463885307312, 'learning_rate': 2.7820101310627357e-06, 'epoch': 11.4}
>>> 2025-09-11 08:41:48,648 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.724128967323234e-06, 'epoch': 11.44}
>>> 2025-09-11 08:41:49,363 - INFO - >>> {'loss': 2.6111, 'grad_norm': 2.2138402462005615, 'learning_rate': 2.6667611836240947e-06, 'epoch': 11.48}
>>> 2025-09-11 08:41:50,075 - INFO - >>> {'loss': 2.5057, 'grad_norm': 2.9295268058776855, 'learning_rate': 2.6099108277934105e-06, 'epoch': 11.52}
>>> 2025-09-11 08:41:50,788 - INFO - >>> {'loss': 1.9968, 'grad_norm': 3.1139235496520996, 'learning_rate': 2.553581911149935e-06, 'epoch': 11.56}
>>> 2025-09-11 08:41:51,500 - INFO - >>> {'loss': 2.7739, 'grad_norm': 2.072481870651245, 'learning_rate': 2.4977784082200728e-06, 'epoch': 11.6}
>>> 2025-09-11 08:41:52,211 - INFO - >>> {'loss': 3.2376, 'grad_norm': 3.3941471576690674, 'learning_rate': 2.4425042564574186e-06, 'epoch': 11.64}
>>> 2025-09-11 08:41:52,923 - INFO - >>> {'loss': 2.5703, 'grad_norm': 2.329967737197876, 'learning_rate': 2.3877633559649505e-06, 'epoch': 11.68}
>>> 2025-09-11 08:41:53,635 - INFO - >>> {'loss': 2.3688, 'grad_norm': 3.2647900581359863, 'learning_rate': 2.3335595692198344e-06, 'epoch': 11.72}
>>> 2025-09-11 08:41:54,349 - INFO - >>> {'loss': 2.9726, 'grad_norm': 2.6296756267547607, 'learning_rate': 2.2798967208008806e-06, 'epoch': 11.76}
>>> 2025-09-11 08:41:55,063 - INFO - >>> {'loss': 3.2291, 'grad_norm': 2.943082809448242, 'learning_rate': 2.2267785971187064e-06, 'epoch': 11.8}
>>> 2025-09-11 08:41:55,774 - INFO - >>> {'loss': 2.535, 'grad_norm': 2.4863154888153076, 'learning_rate': 2.1742089461485504e-06, 'epoch': 11.84}
>>> 2025-09-11 08:41:56,484 - INFO - >>> {'loss': 2.0818, 'grad_norm': 2.3166773319244385, 'learning_rate': 2.122191477165826e-06, 'epoch': 11.88}
>>> 2025-09-11 08:41:57,196 - INFO - >>> {'loss': 2.486, 'grad_norm': 1.8072128295898438, 'learning_rate': 2.0707298604843964e-06, 'epoch': 11.92}
>>> 2025-09-11 08:41:57,909 - INFO - >>> {'loss': 2.5138, 'grad_norm': 3.658769130706787, 'learning_rate': 2.019827727197605e-06, 'epoch': 11.96}
>>> 2025-09-11 08:41:58,619 - INFO - >>> {'loss': 2.0196, 'grad_norm': 3.268855094909668, 'learning_rate': 1.9694886689220592e-06, 'epoch': 12.0}
>>> 2025-09-11 08:42:01,050 - INFO - >>> {'loss': 2.6503, 'grad_norm': 2.427327871322632, 'learning_rate': 1.91971623754421e-06, 'epoch': 12.04}
>>> 2025-09-11 08:42:01,753 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.870513944969743e-06, 'epoch': 12.08}
>>> 2025-09-11 08:42:02,465 - INFO - >>> {'loss': 2.5562, 'grad_norm': 2.0302751064300537, 'learning_rate': 1.8218852628757755e-06, 'epoch': 12.12}
>>> 2025-09-11 08:42:03,179 - INFO - >>> {'loss': 2.241, 'grad_norm': 2.724702835083008, 'learning_rate': 1.773833622465888e-06, 'epoch': 12.16}
>>> 2025-09-11 08:42:03,903 - INFO - >>> {'loss': 2.5834, 'grad_norm': 2.578886032104492, 'learning_rate': 1.7263624142280377e-06, 'epoch': 12.2}
>>> 2025-09-11 08:42:04,615 - INFO - >>> {'loss': 2.4925, 'grad_norm': 3.2078614234924316, 'learning_rate': 1.6794749876953187e-06, 'epoch': 12.24}
>>> 2025-09-11 08:42:05,326 - INFO - >>> {'loss': 2.3454, 'grad_norm': 3.777596950531006, 'learning_rate': 1.6331746512096158e-06, 'epoch': 12.28}
>>> 2025-09-11 08:42:06,037 - INFO - >>> {'loss': 2.0877, 'grad_norm': 2.6843369007110596, 'learning_rate': 1.587464671688187e-06, 'epoch': 12.32}
>>> 2025-09-11 08:42:06,749 - INFO - >>> {'loss': 2.7063, 'grad_norm': 1.599868893623352, 'learning_rate': 1.5423482743931406e-06, 'epoch': 12.36}
>>> 2025-09-11 08:42:07,481 - INFO - >>> {'loss': 2.3971, 'grad_norm': 2.9083597660064697, 'learning_rate': 1.4978286427038602e-06, 'epoch': 12.4}
>>> 2025-09-11 08:42:08,196 - INFO - >>> {'loss': 2.8675, 'grad_norm': 3.2216877937316895, 'learning_rate': 1.4539089178923937e-06, 'epoch': 12.44}
>>> 2025-09-11 08:42:08,908 - INFO - >>> {'loss': 1.9372, 'grad_norm': 4.173287868499756, 'learning_rate': 1.4105921989018112e-06, 'epoch': 12.48}
>>> 2025-09-11 08:42:09,621 - INFO - >>> {'loss': 2.3672, 'grad_norm': 1.958723545074463, 'learning_rate': 1.3678815421275393e-06, 'epoch': 12.52}
>>> 2025-09-11 08:42:10,324 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.325779961201703e-06, 'epoch': 12.56}
>>> 2025-09-11 08:42:11,038 - INFO - >>> {'loss': 2.9676, 'grad_norm': 2.6249306201934814, 'learning_rate': 1.2842904267804934e-06, 'epoch': 12.6}
>>> 2025-09-11 08:42:11,750 - INFO - >>> {'loss': 2.5993, 'grad_norm': 3.1173102855682373, 'learning_rate': 1.2434158663345553e-06, 'epoch': 12.64}
>>> 2025-09-11 08:42:12,462 - INFO - >>> {'loss': 2.5051, 'grad_norm': 2.4947874546051025, 'learning_rate': 1.2031591639424234e-06, 'epoch': 12.68}
>>> 2025-09-11 08:42:13,174 - INFO - >>> {'loss': 2.5724, 'grad_norm': 1.8207776546478271, 'learning_rate': 1.1635231600870334e-06, 'epoch': 12.72}
>>> 2025-09-11 08:42:13,875 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.1245106514552973e-06, 'epoch': 12.76}
>>> 2025-09-11 08:42:14,588 - INFO - >>> {'loss': 2.9514, 'grad_norm': 2.99544358253479, 'learning_rate': 1.086124390740757e-06, 'epoch': 12.8}
>>> 2025-09-11 08:42:15,301 - INFO - >>> {'loss': 2.5869, 'grad_norm': 2.0780422687530518, 'learning_rate': 1.0483670864493777e-06, 'epoch': 12.84}
>>> 2025-09-11 08:42:16,014 - INFO - >>> {'loss': 3.085, 'grad_norm': 2.4822988510131836, 'learning_rate': 1.0112414027084262e-06, 'epoch': 12.88}
>>> 2025-09-11 08:42:16,725 - INFO - >>> {'loss': 2.428, 'grad_norm': 3.3343393802642822, 'learning_rate': 9.747499590784937e-07, 'epoch': 12.92}
>>> 2025-09-11 08:42:17,437 - INFO - >>> {'loss': 2.6418, 'grad_norm': 2.959280252456665, 'learning_rate': 9.388953303686587e-07, 'epoch': 12.96}
>>> 2025-09-11 08:42:18,147 - INFO - >>> {'loss': 2.7823, 'grad_norm': 2.562509775161743, 'learning_rate': 9.036800464548157e-07, 'epoch': 13.0}
>>> 2025-09-11 08:42:18,862 - INFO - >>> {'loss': 3.2307, 'grad_norm': 3.4493296146392822, 'learning_rate': 8.691065921011687e-07, 'epoch': 13.04}
>>> 2025-09-11 08:42:19,321 - INFO - >>> {'loss': 2.545, 'grad_norm': 1.9056214094161987, 'learning_rate': 8.351774067849006e-07, 'epoch': 13.08}
>>> 2025-09-11 08:42:20,033 - INFO - >>> {'loss': 2.1741, 'grad_norm': 2.2064497470855713, 'learning_rate': 8.018948845240538e-07, 'epoch': 13.12}
>>> 2025-09-11 08:42:20,746 - INFO - >>> {'loss': 2.2745, 'grad_norm': 4.566105842590332, 'learning_rate': 7.692613737086108e-07, 'epoch': 13.16}
>>> 2025-09-11 08:42:21,460 - INFO - >>> {'loss': 2.7005, 'grad_norm': 3.6501290798187256, 'learning_rate': 7.372791769347843e-07, 'epoch': 13.2}
>>> 2025-09-11 08:42:22,163 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.059505508425535e-07, 'epoch': 13.24}
>>> 2025-09-11 08:42:22,875 - INFO - >>> {'loss': 2.5935, 'grad_norm': 2.1477530002593994, 'learning_rate': 6.752777059564431e-07, 'epoch': 13.28}
>>> 2025-09-11 08:42:23,585 - INFO - >>> {'loss': 2.4923, 'grad_norm': 3.877878189086914, 'learning_rate': 6.452628065295374e-07, 'epoch': 13.32}
>>> 2025-09-11 08:42:24,308 - INFO - >>> {'loss': 2.8213, 'grad_norm': 2.3798091411590576, 'learning_rate': 6.159079703907823e-07, 'epoch': 13.36}
>>> 2025-09-11 08:42:25,021 - INFO - >>> {'loss': 2.4245, 'grad_norm': 3.308178424835205, 'learning_rate': 5.872152687955524e-07, 'epoch': 13.4}
>>> 2025-09-11 08:42:25,732 - INFO - >>> {'loss': 2.8361, 'grad_norm': 3.9924354553222656, 'learning_rate': 5.591867262794969e-07, 'epoch': 13.44}
>>> 2025-09-11 08:42:26,445 - INFO - >>> {'loss': 2.7999, 'grad_norm': 1.875464916229248, 'learning_rate': 5.318243205156981e-07, 'epoch': 13.48}
>>> 2025-09-11 08:42:27,147 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.051299821751254e-07, 'epoch': 13.52}
>>> 2025-09-11 08:42:27,859 - INFO - >>> {'loss': 2.5436, 'grad_norm': 2.0479440689086914, 'learning_rate': 4.791055947904099e-07, 'epoch': 13.56}
>>> 2025-09-11 08:42:28,573 - INFO - >>> {'loss': 2.3088, 'grad_norm': 2.684924364089966, 'learning_rate': 4.537529946229369e-07, 'epoch': 13.6}
>>> 2025-09-11 08:42:29,277 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.2907397053329027e-07, 'epoch': 13.64}
>>> 2025-09-11 08:42:29,990 - INFO - >>> {'loss': 2.4812, 'grad_norm': 3.215980052947998, 'learning_rate': 4.0507026385502747e-07, 'epoch': 13.68}
>>> 2025-09-11 08:42:30,703 - INFO - >>> {'loss': 1.936, 'grad_norm': 4.1912360191345215, 'learning_rate': 3.817435682718096e-07, 'epoch': 13.72}
>>> 2025-09-11 08:42:31,415 - INFO - >>> {'loss': 2.8296, 'grad_norm': 2.791703701019287, 'learning_rate': 3.5909552969790376e-07, 'epoch': 13.76}
>>> 2025-09-11 08:42:32,013 - INFO - >>> {'loss': 2.7387, 'grad_norm': 1.560065507888794, 'learning_rate': 3.3712774616204146e-07, 'epoch': 13.8}
>>> 2025-09-11 08:42:32,725 - INFO - >>> {'loss': 2.5771, 'grad_norm': 2.1404149532318115, 'learning_rate': 3.158417676946635e-07, 'epoch': 13.84}
>>> 2025-09-11 08:42:33,428 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.9523909621855583e-07, 'epoch': 13.88}
>>> 2025-09-11 08:42:34,141 - INFO - >>> {'loss': 2.4272, 'grad_norm': 1.753705382347107, 'learning_rate': 2.753211854428728e-07, 'epoch': 13.92}
>>> 2025-09-11 08:42:34,854 - INFO - >>> {'loss': 2.4521, 'grad_norm': 2.0693202018737793, 'learning_rate': 2.5608944076055966e-07, 'epoch': 13.96}
>>> 2025-09-11 08:42:35,564 - INFO - >>> {'loss': 2.5166, 'grad_norm': 3.197406053543091, 'learning_rate': 2.375452191491967e-07, 'epoch': 14.0}
>>> 2025-09-11 08:42:36,279 - INFO - >>> {'loss': 2.8478, 'grad_norm': 3.9230897426605225, 'learning_rate': 2.1968982907524804e-07, 'epoch': 14.04}
>>> 2025-09-11 08:42:36,990 - INFO - >>> {'loss': 2.5541, 'grad_norm': 2.1843268871307373, 'learning_rate': 2.0252453040173646e-07, 'epoch': 14.08}
>>> 2025-09-11 08:42:37,702 - INFO - >>> {'loss': 2.9659, 'grad_norm': 2.856801986694336, 'learning_rate': 1.8605053429935016e-07, 'epoch': 14.12}
>>> 2025-09-11 08:42:38,414 - INFO - >>> {'loss': 2.8042, 'grad_norm': 2.3345186710357666, 'learning_rate': 1.7026900316098217e-07, 'epoch': 14.16}
>>> 2025-09-11 08:42:39,126 - INFO - >>> {'loss': 2.9189, 'grad_norm': 2.499769926071167, 'learning_rate': 1.55181050519716e-07, 'epoch': 14.2}
>>> 2025-09-11 08:42:39,839 - INFO - >>> {'loss': 2.5938, 'grad_norm': 2.6759250164031982, 'learning_rate': 1.407877409702496e-07, 'epoch': 14.24}
>>> 2025-09-11 08:42:40,543 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.2709009009378438e-07, 'epoch': 14.28}
>>> 2025-09-11 08:42:41,255 - INFO - >>> {'loss': 2.7316, 'grad_norm': 1.9701935052871704, 'learning_rate': 1.1408906438636236e-07, 'epoch': 14.32}
>>> 2025-09-11 08:42:41,968 - INFO - >>> {'loss': 2.8061, 'grad_norm': 2.143355131149292, 'learning_rate': 1.0178558119067316e-07, 'epoch': 14.36}
>>> 2025-09-11 08:42:42,682 - INFO - >>> {'loss': 2.2931, 'grad_norm': 1.9086352586746216, 'learning_rate': 9.018050863132566e-08, 'epoch': 14.4}
>>> 2025-09-11 08:42:43,396 - INFO - >>> {'loss': 3.2561, 'grad_norm': 4.436717510223389, 'learning_rate': 7.927466555359808e-08, 'epoch': 14.44}
>>> 2025-09-11 08:42:44,100 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.906882146565097e-08, 'epoch': 14.48}
>>> 2025-09-11 08:42:44,812 - INFO - >>> {'loss': 2.3264, 'grad_norm': 2.532059669494629, 'learning_rate': 5.956369648424276e-08, 'epoch': 14.52}
>>> 2025-09-11 08:42:45,516 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.0759961283911584e-08, 'epoch': 14.56}
>>> 2025-09-11 08:42:46,230 - INFO - >>> {'loss': 2.4288, 'grad_norm': 2.0324461460113525, 'learning_rate': 4.2658237049655325e-08, 'epoch': 14.6}
>>> 2025-09-11 08:42:46,953 - INFO - >>> {'loss': 2.2884, 'grad_norm': 2.111767053604126, 'learning_rate': 3.525909543310002e-08, 'epoch': 14.64}
>>> 2025-09-11 08:42:47,664 - INFO - >>> {'loss': 2.3463, 'grad_norm': 3.7611334323883057, 'learning_rate': 2.8563058512168786e-08, 'epoch': 14.68}
>>> 2025-09-11 08:42:48,377 - INFO - >>> {'loss': 2.6006, 'grad_norm': 3.130676507949829, 'learning_rate': 2.257059875423795e-08, 'epoch': 14.72}
>>> 2025-09-11 08:42:49,089 - INFO - >>> {'loss': 2.549, 'grad_norm': 2.251955032348633, 'learning_rate': 1.7282138982803732e-08, 'epoch': 14.76}
>>> 2025-09-11 08:42:49,802 - INFO - >>> {'loss': 2.5025, 'grad_norm': 2.968254566192627, 'learning_rate': 1.2698052347649426e-08, 'epoch': 14.8}
>>> 2025-09-11 08:42:50,513 - INFO - >>> {'loss': 2.5186, 'grad_norm': 2.686805009841919, 'learning_rate': 8.818662298512025e-09, 'epoch': 14.84}
>>> 2025-09-11 08:42:51,227 - INFO - >>> {'loss': 1.984, 'grad_norm': 3.1325883865356445, 'learning_rate': 5.644242562264923e-09, 'epoch': 14.88}
>>> 2025-09-11 08:42:51,933 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 3.1750171235989115e-09, 'epoch': 14.92}
>>> 2025-09-11 08:42:52,646 - INFO - >>> {'loss': 2.5242, 'grad_norm': 1.7424070835113525, 'learning_rate': 1.4111602092226062e-09, 'epoch': 14.96}
>>> 2025-09-11 08:42:53,359 - INFO - >>> {'loss': 2.8332, 'grad_norm': 2.8175301551818848, 'learning_rate': 3.5279627556672466e-10, 'epoch': 15.0}
>>> 2025-09-11 08:42:55,092 - INFO - >>> {'train_runtime': 273.4444, 'train_samples_per_second': 5.486, 'train_steps_per_second': 1.371, 'train_loss': 2.342021027247111, 'epoch': 15.0}
>>> 2025-09-11 08:42:55,097 - INFO - 训练成功！
>>> 2025-09-11 08:42:55,098 - INFO - 模型存放位置：./output/qwen2-0.5b202509110838
>>> 2025-09-11 08:44:31,242 - INFO - 导入包完成
>>> 2025-09-11 08:44:31,243 - INFO - ========train Qwen2ForCausalLM  202509110844========
>>> 2025-09-11 08:44:31,243 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 08:44:31,244 - INFO - 开始进行训练
>>> 2025-09-11 08:44:31,250 - INFO - 基础配置文件读取完成
>>> 2025-09-11 08:44:31,257 - INFO - 训练配置读取完成
>>> 2025-09-11 08:44:31,258 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-09-11 08:44:31,258 - INFO - 模型路径:/home/liangshuqiao/models/qwen05
>>> 2025-09-11 08:44:31,698 - INFO - tokenizer读取完成
>>> 2025-09-11 08:44:31,854 - INFO - model dtype:torch.bfloat16
>>> 2025-09-11 08:44:31,855 - INFO - 模型导入完成
>>> 2025-09-11 08:44:31,855 - INFO - 数据读取开始
>>> 2025-09-11 08:44:32,648 - INFO - 数据下载完成，训练集大小: 100
>>> 2025-09-11 08:44:36,823 - INFO - 数据映射完成
>>> 2025-09-11 08:44:36,823 - INFO - 打印训练参数如下
>>> 2025-09-11 08:44:36,824 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-11 08:44:36,824 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-11 08:44:36,824 - INFO -   load_in_4bit >>> True
>>> 2025-09-11 08:44:36,825 - INFO -   batch_size >>> 8
>>> 2025-09-11 08:44:36,825 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-11 08:44:36,826 - INFO -   warmup_steps >>> 1
>>> 2025-09-11 08:44:36,826 - INFO -   epoch >>> 50
>>> 2025-09-11 08:44:36,826 - INFO -   eval_steps >>> 5
>>> 2025-09-11 08:44:36,827 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-11 08:44:36,827 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-11 08:44:36,827 - INFO -   max_seq_length >>> 2048
>>> 2025-09-11 08:44:36,828 - INFO -   r >>> 8
>>> 2025-09-11 08:44:36,828 - INFO -   interface_mode >>> False
>>> 2025-09-11 08:44:36,828 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-11 08:44:36,829 - INFO -   lora_alpha >>> 16
>>> 2025-09-11 08:44:36,829 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-11 08:44:36,829 - INFO -   bias >>> none
>>> 2025-09-11 08:44:36,830 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-11 08:44:36,830 - INFO -   random_state >>> 3407
>>> 2025-09-11 08:44:36,830 - INFO -   use_rslora >>> True
>>> 2025-09-11 08:44:36,831 - INFO -   loftq_config >>> None
>>> 2025-09-11 08:44:37,654 - INFO - 开始训练！
>>> 2025-09-11 08:44:43,036 - INFO - >>> {'loss': 3.1925, 'grad_norm': 3.943020820617676, 'learning_rate': 0.0, 'epoch': 0.3076923076923077}
>>> 2025-09-11 08:44:47,749 - INFO - >>> {'loss': 3.4763, 'grad_norm': 4.192591667175293, 'learning_rate': 0.0002, 'epoch': 0.6153846153846154}
>>> 2025-09-11 08:44:52,484 - INFO - >>> {'loss': 3.1524, 'grad_norm': 2.3339645862579346, 'learning_rate': 0.00019998753895176575, 'epoch': 0.9230769230769231}
>>> 2025-09-11 08:44:53,138 - INFO - >>> {'loss': 3.3793, 'grad_norm': 2.322328805923462, 'learning_rate': 0.0001999501589126174, 'epoch': 1.0}
>>> 2025-09-11 08:44:57,857 - INFO - >>> {'loss': 3.1884, 'grad_norm': 1.5004619359970093, 'learning_rate': 0.00019988786919844436, 'epoch': 1.3076923076923077}
>>> 2025-09-11 08:45:02,603 - INFO - >>> {'loss': 2.9295, 'grad_norm': 1.414176106452942, 'learning_rate': 0.00019980068533314934, 'epoch': 1.6153846153846154}
>>> 2025-09-11 08:45:07,321 - INFO - >>> {'loss': 2.8928, 'grad_norm': 1.195164442062378, 'learning_rate': 0.00019968862904477935, 'epoch': 1.9230769230769231}
>>> 2025-09-11 08:45:07,973 - INFO - >>> {'loss': 3.1792, 'grad_norm': 1.802088737487793, 'learning_rate': 0.00019955172826011062, 'epoch': 2.0}
>>> 2025-09-11 08:45:12,699 - INFO - >>> {'loss': 2.9875, 'grad_norm': 0.9929297566413879, 'learning_rate': 0.0001993900170976888, 'epoch': 2.3076923076923075}
>>> 2025-09-11 08:45:17,425 - INFO - >>> {'loss': 2.82, 'grad_norm': 0.9879565834999084, 'learning_rate': 0.00019920353585932578, 'epoch': 2.6153846153846154}
>>> 2025-09-11 08:45:22,176 - INFO - >>> {'loss': 2.7126, 'grad_norm': 0.9054719805717468, 'learning_rate': 0.00019899233102005573, 'epoch': 2.9230769230769234}
>>> 2025-09-11 08:45:22,830 - INFO - >>> {'loss': 2.5571, 'grad_norm': 2.062983751296997, 'learning_rate': 0.0001987564552165524, 'epoch': 3.0}
>>> 2025-09-11 08:45:27,581 - INFO - >>> {'loss': 2.9112, 'grad_norm': 0.9005677103996277, 'learning_rate': 0.00019849596723401107, 'epoch': 3.3076923076923075}
>>> 2025-09-11 08:45:32,312 - INFO - >>> {'loss': 2.6874, 'grad_norm': 0.9431849718093872, 'learning_rate': 0.00019821093199149804, 'epoch': 3.6153846153846154}
>>> 2025-09-11 08:45:37,089 - INFO - >>> {'loss': 2.6303, 'grad_norm': 1.113240361213684, 'learning_rate': 0.0001979014205257715, 'epoch': 3.9230769230769234}
>>> 2025-09-11 08:45:37,755 - INFO - >>> {'loss': 2.3208, 'grad_norm': 2.2723817825317383, 'learning_rate': 0.0001975675099735774, 'epoch': 4.0}
>>> 2025-09-11 08:45:42,653 - INFO - >>> {'loss': 2.6861, 'grad_norm': 0.9751418232917786, 'learning_rate': 0.00019720928355242568, 'epoch': 4.3076923076923075}
>>> 2025-09-11 08:45:47,862 - INFO - >>> {'loss': 2.5934, 'grad_norm': 1.0977709293365479, 'learning_rate': 0.00019682683053985072, 'epoch': 4.615384615384615}
>>> 2025-09-11 08:45:52,776 - INFO - >>> {'loss': 2.6749, 'grad_norm': 1.0215253829956055, 'learning_rate': 0.00019642024625116117, 'epoch': 4.923076923076923}
>>> 2025-09-11 08:45:53,428 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.00019598963201568573, 'epoch': 5.0}
>>> 2025-09-11 08:45:58,152 - INFO - >>> {'loss': 2.6702, 'grad_norm': 0.9307432174682617, 'learning_rate': 0.0001955350951515195, 'epoch': 5.3076923076923075}
>>> 2025-09-11 08:46:02,875 - INFO - >>> {'loss': 2.2945, 'grad_norm': 1.0421397686004639, 'learning_rate': 0.0001950567489387783, 'epoch': 5.615384615384615}
>>> 2025-09-11 08:46:07,630 - INFO - >>> {'loss': 2.5482, 'grad_norm': 0.857948899269104, 'learning_rate': 0.0001945547125913667, 'epoch': 5.923076923076923}
>>> 2025-09-11 08:46:08,286 - INFO - >>> {'loss': 3.1345, 'grad_norm': 4.174037933349609, 'learning_rate': 0.00019402911122726757, 'epoch': 6.0}
>>> 2025-09-11 08:46:13,021 - INFO - >>> {'loss': 2.5084, 'grad_norm': 0.948108971118927, 'learning_rate': 0.00019348007583735983, 'epoch': 6.3076923076923075}
>>> 2025-09-11 08:46:17,742 - INFO - >>> {'loss': 2.4808, 'grad_norm': 1.1260054111480713, 'learning_rate': 0.00019290774325277305, 'epoch': 6.615384615384615}
>>> 2025-09-11 08:46:22,492 - INFO - >>> {'loss': 2.359, 'grad_norm': 0.9549288153648376, 'learning_rate': 0.0001923122561107861, 'epoch': 6.923076923076923}
>>> 2025-09-11 08:46:23,146 - INFO - >>> {'loss': 2.562, 'grad_norm': 1.8262940645217896, 'learning_rate': 0.00019169376281927888, 'epoch': 7.0}
>>> 2025-09-11 08:46:27,875 - INFO - >>> {'loss': 2.3556, 'grad_norm': 1.1469659805297852, 'learning_rate': 0.00019105241751974622, 'epoch': 7.3076923076923075}
>>> 2025-09-11 08:46:32,625 - INFO - >>> {'loss': 2.4335, 'grad_norm': 1.0965254306793213, 'learning_rate': 0.0001903883800488824, 'epoch': 7.615384615384615}
>>> 2025-09-11 08:46:37,341 - INFO - >>> {'loss': 2.3457, 'grad_norm': 1.0638818740844727, 'learning_rate': 0.00018970181589874637, 'epoch': 7.923076923076923}
>>> 2025-09-11 08:46:37,995 - INFO - >>> {'loss': 2.2042, 'grad_norm': 3.464049816131592, 'learning_rate': 0.00018899289617551804, 'epoch': 8.0}
>>> 2025-09-11 08:46:42,724 - INFO - >>> {'loss': 2.4499, 'grad_norm': 1.0866074562072754, 'learning_rate': 0.0001882617975568547, 'epoch': 8.307692307692308}
>>> 2025-09-11 08:46:47,446 - INFO - >>> {'loss': 2.1746, 'grad_norm': 1.1253552436828613, 'learning_rate': 0.00018750870224785939, 'epoch': 8.615384615384615}
>>> 2025-09-11 08:46:52,194 - INFO - >>> {'loss': 2.2425, 'grad_norm': 1.3073837757110596, 'learning_rate': 0.00018673379793567146, 'epoch': 8.923076923076923}
>>> 2025-09-11 08:46:52,843 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0001859372777426912, 'epoch': 9.0}
>>> 2025-09-11 08:46:57,568 - INFO - >>> {'loss': 2.1737, 'grad_norm': 1.3369745016098022, 'learning_rate': 0.00018511934017844948, 'epoch': 9.307692307692308}
>>> 2025-09-11 08:47:02,315 - INFO - >>> {'loss': 2.2377, 'grad_norm': 0.9777262210845947, 'learning_rate': 0.00018428018909013506, 'epoch': 9.615384615384615}
>>> 2025-09-11 08:47:07,026 - INFO - >>> {'loss': 2.0911, 'grad_norm': 1.2379180192947388, 'learning_rate': 0.00018342003361179176, 'epoch': 9.923076923076923}
>>> 2025-09-11 08:47:07,680 - INFO - >>> {'loss': 2.6235, 'grad_norm': 2.4384379386901855, 'learning_rate': 0.00018253908811219764, 'epoch': 10.0}
>>> 2025-09-11 08:47:12,404 - INFO - >>> {'loss': 2.1539, 'grad_norm': 1.3090933561325073, 'learning_rate': 0.00018163757214143992, 'epoch': 10.307692307692308}
>>> 2025-09-11 08:47:17,122 - INFO - >>> {'loss': 2.1328, 'grad_norm': 1.0984586477279663, 'learning_rate': 0.00018071571037619853, 'epoch': 10.615384615384615}
>>> 2025-09-11 08:47:21,838 - INFO - >>> {'loss': 2.1271, 'grad_norm': 1.1641908884048462, 'learning_rate': 0.00017977373256375194, 'epoch': 10.923076923076923}
>>> 2025-09-11 08:47:22,504 - INFO - >>> {'loss': 2.1388, 'grad_norm': 3.5616753101348877, 'learning_rate': 0.00017881187346471925, 'epoch': 11.0}
>>> 2025-09-11 08:47:27,249 - INFO - >>> {'loss': 2.0879, 'grad_norm': 1.1851394176483154, 'learning_rate': 0.00017783037279455298, 'epoch': 11.307692307692308}
>>> 2025-09-11 08:47:31,969 - INFO - >>> {'loss': 2.1321, 'grad_norm': 1.3119834661483765, 'learning_rate': 0.00017682947516379707, 'epoch': 11.615384615384615}
>>> 2025-09-11 08:47:36,689 - INFO - >>> {'loss': 2.0242, 'grad_norm': 1.3081750869750977, 'learning_rate': 0.00017580943001712455, 'epoch': 11.923076923076923}
>>> 2025-09-11 08:47:37,342 - INFO - >>> {'loss': 1.4726, 'grad_norm': 4.740850448608398, 'learning_rate': 0.00017477049157117093, 'epoch': 12.0}
>>> 2025-09-11 08:47:42,092 - INFO - >>> {'loss': 1.8919, 'grad_norm': 1.388947606086731, 'learning_rate': 0.0001737129187511779, 'epoch': 12.307692307692308}
>>> 2025-09-11 08:47:46,810 - INFO - >>> {'loss': 2.0004, 'grad_norm': 1.5539792776107788, 'learning_rate': 0.00017263697512646394, 'epoch': 12.615384615384615}
>>> 2025-09-11 08:47:51,531 - INFO - >>> {'loss': 2.0017, 'grad_norm': 1.4158838987350464, 'learning_rate': 0.00017154292884473713, 'epoch': 12.923076923076923}
>>> 2025-09-11 08:47:52,185 - INFO - >>> {'loss': 2.1409, 'grad_norm': 4.037195682525635, 'learning_rate': 0.00017043105256526724, 'epoch': 13.0}
>>> 2025-09-11 08:47:56,913 - INFO - >>> {'loss': 1.8467, 'grad_norm': 1.5242011547088623, 'learning_rate': 0.00016930162339093318, 'epoch': 13.307692307692308}
>>> 2025-09-11 08:48:01,661 - INFO - >>> {'loss': 1.9276, 'grad_norm': 1.8543121814727783, 'learning_rate': 0.0001681549227991634, 'epoch': 13.615384615384615}
>>> 2025-09-11 08:48:06,377 - INFO - >>> {'loss': 1.8921, 'grad_norm': 1.3909683227539062, 'learning_rate': 0.00016699123657178553, 'epoch': 13.923076923076923}
>>> 2025-09-11 08:48:07,030 - INFO - >>> {'loss': 1.9821, 'grad_norm': 5.978368759155273, 'learning_rate': 0.00016581085472380376, 'epoch': 14.0}
>>> 2025-09-11 08:48:11,751 - INFO - >>> {'loss': 1.9724, 'grad_norm': 1.594329833984375, 'learning_rate': 0.00016461407143112097, 'epoch': 14.307692307692308}
>>> 2025-09-11 08:48:16,501 - INFO - >>> {'loss': 1.7224, 'grad_norm': 1.7854831218719482, 'learning_rate': 0.00016340118495722388, 'epoch': 14.615384615384615}
>>> 2025-09-11 08:48:21,219 - INFO - >>> {'loss': 1.6738, 'grad_norm': 2.1081221103668213, 'learning_rate': 0.00016217249757884955, 'epoch': 14.923076923076923}
>>> 2025-09-11 08:48:21,872 - INFO - >>> {'loss': 2.0974, 'grad_norm': 4.626577377319336, 'learning_rate': 0.0001609283155106517, 'epoch': 15.0}
>>> 2025-09-11 08:48:26,598 - INFO - >>> {'loss': 1.9402, 'grad_norm': 1.8566195964813232, 'learning_rate': 0.00015966894882888562, 'epoch': 15.307692307692308}
>>> 2025-09-11 08:48:31,346 - INFO - >>> {'loss': 1.5874, 'grad_norm': 2.339060068130493, 'learning_rate': 0.00015839471139413066, 'epoch': 15.615384615384615}
>>> 2025-09-11 08:48:36,069 - INFO - >>> {'loss': 1.6197, 'grad_norm': 1.8318307399749756, 'learning_rate': 0.0001571059207730695, 'epoch': 15.923076923076923}
>>> 2025-09-11 08:48:36,724 - INFO - >>> {'loss': 1.8312, 'grad_norm': 3.795274496078491, 'learning_rate': 0.00015580289815934401, 'epoch': 16.0}
>>> 2025-09-11 08:48:41,485 - INFO - >>> {'loss': 1.656, 'grad_norm': 1.6334999799728394, 'learning_rate': 0.00015448596829350706, 'epoch': 16.307692307692307}
>>> 2025-09-11 08:48:46,207 - INFO - >>> {'loss': 1.7377, 'grad_norm': 2.092902660369873, 'learning_rate': 0.00015315545938209015, 'epoch': 16.615384615384617}
>>> 2025-09-11 08:48:50,925 - INFO - >>> {'loss': 1.6345, 'grad_norm': 2.1598119735717773, 'learning_rate': 0.00015181170301580777, 'epoch': 16.923076923076923}
>>> 2025-09-11 08:48:51,579 - INFO - >>> {'loss': 1.0376, 'grad_norm': 5.385056972503662, 'learning_rate': 0.00015045503408691775, 'epoch': 17.0}
>>> 2025-09-11 08:48:56,302 - INFO - >>> {'loss': 1.5758, 'grad_norm': 1.9765673875808716, 'learning_rate': 0.00014908579070575936, 'epoch': 17.307692307692307}
>>> 2025-09-11 08:49:01,057 - INFO - >>> {'loss': 1.642, 'grad_norm': 1.9327237606048584, 'learning_rate': 0.00014770431411648897, 'epoch': 17.615384615384617}
>>> 2025-09-11 08:49:05,784 - INFO - >>> {'loss': 1.4585, 'grad_norm': 2.7630107402801514, 'learning_rate': 0.0001463109486120348, 'epoch': 17.923076923076923}
>>> 2025-09-11 08:49:06,438 - INFO - >>> {'loss': 1.6687, 'grad_norm': 7.781883239746094, 'learning_rate': 0.00014490604144829202, 'epoch': 18.0}
>>> 2025-09-11 08:49:11,185 - INFO - >>> {'loss': 1.4765, 'grad_norm': 1.935874104499817, 'learning_rate': 0.00014348994275757931, 'epoch': 18.307692307692307}
>>> 2025-09-11 08:49:15,911 - INFO - >>> {'loss': 1.5039, 'grad_norm': 3.177426815032959, 'learning_rate': 0.00014206300546137842, 'epoch': 18.615384615384617}
>>> 2025-09-11 08:49:20,629 - INFO - >>> {'loss': 1.5226, 'grad_norm': 2.2602157592773438, 'learning_rate': 0.00014062558518237892, 'epoch': 18.923076923076923}
>>> 2025-09-11 08:49:21,282 - INFO - >>> {'loss': 1.52, 'grad_norm': 6.479884624481201, 'learning_rate': 0.00013917804015584932, 'epoch': 19.0}
>>> 2025-09-11 08:49:26,008 - INFO - >>> {'loss': 1.5902, 'grad_norm': 2.180215835571289, 'learning_rate': 0.00013772073114035762, 'epoch': 19.307692307692307}
>>> 2025-09-11 08:49:30,751 - INFO - >>> {'loss': 1.3739, 'grad_norm': 2.443925380706787, 'learning_rate': 0.00013625402132786248, 'epoch': 19.615384615384617}
>>> 2025-09-11 08:49:35,471 - INFO - >>> {'loss': 1.3491, 'grad_norm': 2.634409189224243, 'learning_rate': 0.00013477827625319824, 'epoch': 19.923076923076923}
>>> 2025-09-11 08:49:36,124 - INFO - >>> {'loss': 0.8922, 'grad_norm': 6.411314010620117, 'learning_rate': 0.00013329386370297615, 'epoch': 20.0}
>>> 2025-09-11 08:49:40,848 - INFO - >>> {'loss': 1.4103, 'grad_norm': 2.6586050987243652, 'learning_rate': 0.00013180115362392382, 'epoch': 20.307692307692307}
>>> 2025-09-11 08:49:45,594 - INFO - >>> {'loss': 1.2861, 'grad_norm': 2.5527760982513428, 'learning_rate': 0.00013030051803068727, 'epoch': 20.615384615384617}
>>> 2025-09-11 08:49:50,307 - INFO - >>> {'loss': 1.3523, 'grad_norm': 2.937264919281006, 'learning_rate': 0.00012879233091311667, 'epoch': 20.923076923076923}
>>> 2025-09-11 08:49:50,959 - INFO - >>> {'loss': 1.4512, 'grad_norm': 5.331464767456055, 'learning_rate': 0.00012727696814306033, 'epoch': 21.0}
>>> 2025-09-11 08:49:55,676 - INFO - >>> {'loss': 1.3441, 'grad_norm': 2.5842158794403076, 'learning_rate': 0.0001257548073806897, 'epoch': 21.307692307692307}
>>> 2025-09-11 08:50:00,421 - INFO - >>> {'loss': 1.1968, 'grad_norm': 2.8229925632476807, 'learning_rate': 0.00012422622798037832, 'epoch': 21.615384615384617}
>>> 2025-09-11 08:50:05,136 - INFO - >>> {'loss': 1.3508, 'grad_norm': 2.749267578125, 'learning_rate': 0.000122691610896159, 'epoch': 21.923076923076923}
>>> 2025-09-11 08:50:05,789 - INFO - >>> {'loss': 1.1413, 'grad_norm': 6.3474884033203125, 'learning_rate': 0.00012115133858678191, 'epoch': 22.0}
>>> 2025-09-11 08:50:10,542 - INFO - >>> {'loss': 1.2802, 'grad_norm': 2.9829561710357666, 'learning_rate': 0.00011960579492039783, 'epoch': 22.307692307692307}
>>> 2025-09-11 08:50:15,257 - INFO - >>> {'loss': 1.2878, 'grad_norm': 3.719533920288086, 'learning_rate': 0.00011805536507889021, 'epoch': 22.615384615384617}
>>> 2025-09-11 08:50:19,968 - INFO - >>> {'loss': 1.1346, 'grad_norm': 2.907465934753418, 'learning_rate': 0.00011650043546187995, 'epoch': 22.923076923076923}
>>> 2025-09-11 08:50:20,621 - INFO - >>> {'loss': 1.1465, 'grad_norm': 7.154767036437988, 'learning_rate': 0.0001149413935904261, 'epoch': 23.0}
>>> 2025-09-11 08:50:25,345 - INFO - >>> {'loss': 1.1659, 'grad_norm': 3.4878621101379395, 'learning_rate': 0.00011337862801044792, 'epoch': 23.307692307692307}
>>> 2025-09-11 08:50:30,088 - INFO - >>> {'loss': 1.2064, 'grad_norm': 2.6253662109375, 'learning_rate': 0.00011181252819589081, 'epoch': 23.615384615384617}
>>> 2025-09-11 08:50:34,799 - INFO - >>> {'loss': 1.0932, 'grad_norm': 3.4791040420532227, 'learning_rate': 0.00011024348445166133, 'epoch': 23.923076923076923}
>>> 2025-09-11 08:50:35,450 - INFO - >>> {'loss': 1.1185, 'grad_norm': 6.038565158843994, 'learning_rate': 0.00010867188781635512, 'epoch': 24.0}
>>> 2025-09-11 08:50:40,171 - INFO - >>> {'loss': 1.0348, 'grad_norm': 2.8532567024230957, 'learning_rate': 0.0001070981299648016, 'epoch': 24.307692307692307}
>>> 2025-09-11 08:50:44,911 - INFO - >>> {'loss': 1.164, 'grad_norm': 3.247933864593506, 'learning_rate': 0.00010552260311045082, 'epoch': 24.615384615384617}
>>> 2025-09-11 08:50:49,628 - INFO - >>> {'loss': 1.0863, 'grad_norm': 3.249180793762207, 'learning_rate': 0.00010394569990762529, 'epoch': 24.923076923076923}
>>> 2025-09-11 08:50:50,276 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.00010236781335366239, 'epoch': 25.0}
>>> 2025-09-11 08:50:55,565 - INFO - >>> {'loss': 1.0244, 'grad_norm': 3.1244521141052246, 'learning_rate': 0.00010078933669097135, 'epoch': 25.307692307692307}
>>> 2025-09-11 08:51:00,281 - INFO - >>> {'loss': 0.9408, 'grad_norm': 3.54032039642334, 'learning_rate': 9.92106633090287e-05, 'epoch': 25.615384615384617}
>>> 2025-09-11 08:51:05,028 - INFO - >>> {'loss': 1.1437, 'grad_norm': 3.015613079071045, 'learning_rate': 9.763218664633763e-05, 'epoch': 25.923076923076923}
>>> 2025-09-11 08:51:05,676 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.605430009237474e-05, 'epoch': 26.0}
>>> 2025-09-11 08:51:10,398 - INFO - >>> {'loss': 0.6883, 'grad_norm': 4.681488513946533, 'learning_rate': 9.447739688954919e-05, 'epoch': 26.307692307692307}
>>> 2025-09-11 08:51:15,116 - INFO - >>> {'loss': 1.052, 'grad_norm': 3.7443931102752686, 'learning_rate': 9.29018700351984e-05, 'epoch': 26.615384615384617}
>>> 2025-09-11 08:51:19,867 - INFO - >>> {'loss': 1.0496, 'grad_norm': 2.9155616760253906, 'learning_rate': 9.132811218364495e-05, 'epoch': 26.923076923076923}
>>> 2025-09-11 08:51:20,520 - INFO - >>> {'loss': 0.3905, 'grad_norm': 12.879115104675293, 'learning_rate': 8.975651554833869e-05, 'epoch': 27.0}
>>> 2025-09-11 08:51:25,241 - INFO - >>> {'loss': 1.0009, 'grad_norm': 3.5619471073150635, 'learning_rate': 8.818747180410921e-05, 'epoch': 27.307692307692307}
>>> 2025-09-11 08:51:29,989 - INFO - >>> {'loss': 0.9662, 'grad_norm': 3.6185286045074463, 'learning_rate': 8.66213719895521e-05, 'epoch': 27.615384615384617}
>>> 2025-09-11 08:51:34,710 - INFO - >>> {'loss': 0.846, 'grad_norm': 4.375083923339844, 'learning_rate': 8.505860640957391e-05, 'epoch': 27.923076923076923}
>>> 2025-09-11 08:51:35,364 - INFO - >>> {'loss': 1.1084, 'grad_norm': 9.489462852478027, 'learning_rate': 8.349956453812009e-05, 'epoch': 28.0}
>>> 2025-09-11 08:51:40,092 - INFO - >>> {'loss': 0.8885, 'grad_norm': 3.6103708744049072, 'learning_rate': 8.194463492110981e-05, 'epoch': 28.307692307692307}
>>> 2025-09-11 08:51:44,848 - INFO - >>> {'loss': 0.9343, 'grad_norm': 4.246890544891357, 'learning_rate': 8.03942050796022e-05, 'epoch': 28.615384615384617}
>>> 2025-09-11 08:51:49,577 - INFO - >>> {'loss': 0.8916, 'grad_norm': 3.884312868118286, 'learning_rate': 7.88486614132181e-05, 'epoch': 28.923076923076923}
>>> 2025-09-11 08:51:50,231 - INFO - >>> {'loss': 1.0447, 'grad_norm': 9.020028114318848, 'learning_rate': 7.730838910384097e-05, 'epoch': 29.0}
>>> 2025-09-11 08:51:54,969 - INFO - >>> {'loss': 0.8756, 'grad_norm': 3.2345633506774902, 'learning_rate': 7.57737720196217e-05, 'epoch': 29.307692307692307}
>>> 2025-09-11 08:51:59,720 - INFO - >>> {'loss': 0.8991, 'grad_norm': 3.6942977905273438, 'learning_rate': 7.424519261931036e-05, 'epoch': 29.615384615384617}
>>> 2025-09-11 08:52:04,439 - INFO - >>> {'loss': 0.907, 'grad_norm': 4.1925482749938965, 'learning_rate': 7.27230318569397e-05, 'epoch': 29.923076923076923}
>>> 2025-09-11 08:52:05,095 - INFO - >>> {'loss': 0.289, 'grad_norm': 10.52873706817627, 'learning_rate': 7.120766908688336e-05, 'epoch': 30.0}
>>> 2025-09-11 08:52:09,866 - INFO - >>> {'loss': 0.7778, 'grad_norm': 4.628189563751221, 'learning_rate': 6.969948196931272e-05, 'epoch': 30.307692307692307}
>>> 2025-09-11 08:52:14,593 - INFO - >>> {'loss': 0.9212, 'grad_norm': 3.023886203765869, 'learning_rate': 6.819884637607619e-05, 'epoch': 30.615384615384617}
>>> 2025-09-11 08:52:19,312 - INFO - >>> {'loss': 0.7493, 'grad_norm': 3.490464687347412, 'learning_rate': 6.670613629702391e-05, 'epoch': 30.923076923076923}
>>> 2025-09-11 08:52:19,967 - INFO - >>> {'loss': 1.0164, 'grad_norm': 8.976737976074219, 'learning_rate': 6.522172374680177e-05, 'epoch': 31.0}
>>> 2025-09-11 08:52:24,728 - INFO - >>> {'loss': 0.6726, 'grad_norm': 2.915865182876587, 'learning_rate': 6.374597867213756e-05, 'epoch': 31.307692307692307}
>>> 2025-09-11 08:52:29,462 - INFO - >>> {'loss': 0.848, 'grad_norm': 4.824243068695068, 'learning_rate': 6.22792688596424e-05, 'epoch': 31.615384615384617}
>>> 2025-09-11 08:52:34,189 - INFO - >>> {'loss': 0.8922, 'grad_norm': 4.112224102020264, 'learning_rate': 6.0821959844150687e-05, 'epoch': 31.923076923076923}
>>> 2025-09-11 08:52:34,844 - INFO - >>> {'loss': 0.9907, 'grad_norm': 6.624147891998291, 'learning_rate': 5.9374414817621114e-05, 'epoch': 32.0}
>>> 2025-09-11 08:52:39,562 - INFO - >>> {'loss': 0.7128, 'grad_norm': 4.658869743347168, 'learning_rate': 5.7936994538621605e-05, 'epoch': 32.30769230769231}
>>> 2025-09-11 08:52:44,314 - INFO - >>> {'loss': 0.7757, 'grad_norm': 3.2239317893981934, 'learning_rate': 5.651005724242071e-05, 'epoch': 32.61538461538461}
>>> 2025-09-11 08:52:49,042 - INFO - >>> {'loss': 0.8149, 'grad_norm': 3.270697593688965, 'learning_rate': 5.509395855170798e-05, 'epoch': 32.92307692307692}
>>> 2025-09-11 08:52:49,693 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.368905138796523e-05, 'epoch': 33.0}
>>> 2025-09-11 08:52:54,429 - INFO - >>> {'loss': 0.685, 'grad_norm': 3.3546133041381836, 'learning_rate': 5.229568588351108e-05, 'epoch': 33.30769230769231}
>>> 2025-09-11 08:52:59,177 - INFO - >>> {'loss': 0.766, 'grad_norm': 3.6555001735687256, 'learning_rate': 5.0914209294240644e-05, 'epoch': 33.61538461538461}
>>> 2025-09-11 08:53:03,908 - INFO - >>> {'loss': 0.8074, 'grad_norm': 4.554618835449219, 'learning_rate': 4.9544965913082264e-05, 'epoch': 33.92307692307692}
>>> 2025-09-11 08:53:04,563 - INFO - >>> {'loss': 0.7876, 'grad_norm': 8.743650436401367, 'learning_rate': 4.818829698419225e-05, 'epoch': 34.0}
>>> 2025-09-11 08:53:09,318 - INFO - >>> {'loss': 0.7218, 'grad_norm': 3.389493942260742, 'learning_rate': 4.684454061790987e-05, 'epoch': 34.30769230769231}
>>> 2025-09-11 08:53:14,041 - INFO - >>> {'loss': 0.7289, 'grad_norm': 3.301889181137085, 'learning_rate': 4.5514031706492986e-05, 'epoch': 34.61538461538461}
>>> 2025-09-11 08:53:18,761 - INFO - >>> {'loss': 0.7223, 'grad_norm': 4.722321510314941, 'learning_rate': 4.4197101840655995e-05, 'epoch': 34.92307692307692}
>>> 2025-09-11 08:53:19,416 - INFO - >>> {'loss': 0.348, 'grad_norm': 14.051719665527344, 'learning_rate': 4.289407922693053e-05, 'epoch': 35.0}
>>> 2025-09-11 08:53:24,174 - INFO - >>> {'loss': 0.7397, 'grad_norm': 3.3834822177886963, 'learning_rate': 4.1605288605869365e-05, 'epoch': 35.30769230769231}
>>> 2025-09-11 08:53:28,898 - INFO - >>> {'loss': 0.7564, 'grad_norm': 3.5869195461273193, 'learning_rate': 4.033105117111441e-05, 'epoch': 35.61538461538461}
>>> 2025-09-11 08:53:33,618 - INFO - >>> {'loss': 0.5902, 'grad_norm': 3.725224256515503, 'learning_rate': 3.907168448934836e-05, 'epoch': 35.92307692307692}
>>> 2025-09-11 08:53:34,270 - INFO - >>> {'loss': 0.4178, 'grad_norm': 8.791812896728516, 'learning_rate': 3.7827502421150496e-05, 'epoch': 36.0}
>>> 2025-09-11 08:53:38,994 - INFO - >>> {'loss': 0.682, 'grad_norm': 3.3918204307556152, 'learning_rate': 3.659881504277613e-05, 'epoch': 36.30769230769231}
>>> 2025-09-11 08:53:43,712 - INFO - >>> {'loss': 0.6558, 'grad_norm': 4.165049076080322, 'learning_rate': 3.538592856887901e-05, 'epoch': 36.61538461538461}
>>> 2025-09-11 08:53:48,461 - INFO - >>> {'loss': 0.7245, 'grad_norm': 3.5195577144622803, 'learning_rate': 3.4189145276196245e-05, 'epoch': 36.92307692307692}
>>> 2025-09-11 08:53:49,115 - INFO - >>> {'loss': 0.6162, 'grad_norm': 9.465517044067383, 'learning_rate': 3.3008763428214505e-05, 'epoch': 37.0}
>>> 2025-09-11 08:53:53,869 - INFO - >>> {'loss': 0.7222, 'grad_norm': 3.3271749019622803, 'learning_rate': 3.1845077200836636e-05, 'epoch': 37.30769230769231}
>>> 2025-09-11 08:53:58,589 - INFO - >>> {'loss': 0.608, 'grad_norm': 3.234137535095215, 'learning_rate': 3.0698376609066825e-05, 'epoch': 37.61538461538461}
>>> 2025-09-11 08:54:03,308 - INFO - >>> {'loss': 0.652, 'grad_norm': 4.389739513397217, 'learning_rate': 2.9568947434732775e-05, 'epoch': 37.92307692307692}
>>> 2025-09-11 08:54:03,957 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.8457071155262884e-05, 'epoch': 38.0}
>>> 2025-09-11 08:54:08,679 - INFO - >>> {'loss': 0.6182, 'grad_norm': 3.242295265197754, 'learning_rate': 2.736302487353609e-05, 'epoch': 38.30769230769231}
>>> 2025-09-11 08:54:13,408 - INFO - >>> {'loss': 0.5603, 'grad_norm': 5.3428053855896, 'learning_rate': 2.628708124882212e-05, 'epoch': 38.61538461538461}
>>> 2025-09-11 08:54:18,156 - INFO - >>> {'loss': 0.7277, 'grad_norm': 3.2383322715759277, 'learning_rate': 2.5229508428829096e-05, 'epoch': 38.92307692307692}
>>> 2025-09-11 08:54:18,811 - INFO - >>> {'loss': 0.4942, 'grad_norm': 9.698275566101074, 'learning_rate': 2.4190569982875467e-05, 'epoch': 39.0}
>>> 2025-09-11 08:54:23,537 - INFO - >>> {'loss': 0.6286, 'grad_norm': 3.2989916801452637, 'learning_rate': 2.3170524836202933e-05, 'epoch': 39.30769230769231}
>>> 2025-09-11 08:54:28,248 - INFO - >>> {'loss': 0.568, 'grad_norm': 4.1013312339782715, 'learning_rate': 2.216962720544703e-05, 'epoch': 39.61538461538461}
>>> 2025-09-11 08:54:32,992 - INFO - >>> {'loss': 0.6931, 'grad_norm': 3.7330446243286133, 'learning_rate': 2.1188126535280773e-05, 'epoch': 39.92307692307692}
>>> 2025-09-11 08:54:33,647 - INFO - >>> {'loss': 0.406, 'grad_norm': 8.646604537963867, 'learning_rate': 2.022626743624807e-05, 'epoch': 40.0}
>>> 2025-09-11 08:54:38,397 - INFO - >>> {'loss': 0.6928, 'grad_norm': 3.487610101699829, 'learning_rate': 1.9284289623801477e-05, 'epoch': 40.30769230769231}
>>> 2025-09-11 08:54:43,119 - INFO - >>> {'loss': 0.6273, 'grad_norm': 3.1167991161346436, 'learning_rate': 1.8362427858560093e-05, 'epoch': 40.61538461538461}
>>> 2025-09-11 08:54:47,836 - INFO - >>> {'loss': 0.5236, 'grad_norm': 4.566977500915527, 'learning_rate': 1.74609118878024e-05, 'epoch': 40.92307692307692}
>>> 2025-09-11 08:54:48,490 - INFO - >>> {'loss': 0.0635, 'grad_norm': 17.9862117767334, 'learning_rate': 1.657996638820826e-05, 'epoch': 41.0}
>>> 2025-09-11 08:54:53,215 - INFO - >>> {'loss': 0.6126, 'grad_norm': 3.867245674133301, 'learning_rate': 1.5719810909864942e-05, 'epoch': 41.30769230769231}
>>> 2025-09-11 08:54:57,940 - INFO - >>> {'loss': 0.5498, 'grad_norm': 4.301716327667236, 'learning_rate': 1.4880659821550546e-05, 'epoch': 41.61538461538461}
>>> 2025-09-11 08:55:02,689 - INFO - >>> {'loss': 0.683, 'grad_norm': 3.490241765975952, 'learning_rate': 1.4062722257308803e-05, 'epoch': 41.92307692307692}
>>> 2025-09-11 08:55:03,342 - INFO - >>> {'loss': 0.3925, 'grad_norm': 6.849584102630615, 'learning_rate': 1.3266202064328548e-05, 'epoch': 42.0}
>>> 2025-09-11 08:55:08,071 - INFO - >>> {'loss': 0.6696, 'grad_norm': 4.893959999084473, 'learning_rate': 1.2491297752140641e-05, 'epoch': 42.30769230769231}
>>> 2025-09-11 08:55:12,790 - INFO - >>> {'loss': 0.566, 'grad_norm': 3.0985147953033447, 'learning_rate': 1.1738202443145308e-05, 'epoch': 42.61538461538461}
>>> 2025-09-11 08:55:17,537 - INFO - >>> {'loss': 0.6002, 'grad_norm': 3.362821578979492, 'learning_rate': 1.1007103824481979e-05, 'epoch': 42.92307692307692}
>>> 2025-09-11 08:55:18,194 - INFO - >>> {'loss': 0.6175, 'grad_norm': 9.271607398986816, 'learning_rate': 1.029818410125365e-05, 'epoch': 43.0}
>>> 2025-09-11 08:55:22,929 - INFO - >>> {'loss': 0.6451, 'grad_norm': 3.631755828857422, 'learning_rate': 9.611619951117657e-06, 'epoch': 43.30769230769231}
>>> 2025-09-11 08:55:27,650 - INFO - >>> {'loss': 0.4843, 'grad_norm': 3.6129908561706543, 'learning_rate': 8.94758248025378e-06, 'epoch': 43.61538461538461}
>>> 2025-09-11 08:55:32,397 - INFO - >>> {'loss': 0.6219, 'grad_norm': 2.8985462188720703, 'learning_rate': 8.306237180721121e-06, 'epoch': 43.92307692307692}
>>> 2025-09-11 08:55:33,051 - INFO - >>> {'loss': 0.4009, 'grad_norm': 20.45380401611328, 'learning_rate': 7.687743889213938e-06, 'epoch': 44.0}
>>> 2025-09-11 08:55:37,780 - INFO - >>> {'loss': 0.6209, 'grad_norm': 3.2190983295440674, 'learning_rate': 7.0922567472269444e-06, 'epoch': 44.30769230769231}
>>> 2025-09-11 08:55:42,537 - INFO - >>> {'loss': 0.5938, 'grad_norm': 3.5388600826263428, 'learning_rate': 6.519924162640167e-06, 'epoch': 44.61538461538461}
>>> 2025-09-11 08:55:47,260 - INFO - >>> {'loss': 0.5936, 'grad_norm': 3.5115389823913574, 'learning_rate': 5.9708887727324525e-06, 'epoch': 44.92307692307692}
>>> 2025-09-11 08:55:47,915 - INFO - >>> {'loss': 0.3745, 'grad_norm': 13.497248649597168, 'learning_rate': 5.445287408633304e-06, 'epoch': 45.0}
>>> 2025-09-11 08:55:52,640 - INFO - >>> {'loss': 0.5355, 'grad_norm': 4.142277717590332, 'learning_rate': 4.943251061221721e-06, 'epoch': 45.30769230769231}
>>> 2025-09-11 08:55:57,360 - INFO - >>> {'loss': 0.6042, 'grad_norm': 3.7733099460601807, 'learning_rate': 4.464904848480523e-06, 'epoch': 45.61538461538461}
>>> 2025-09-11 08:56:02,078 - INFO - >>> {'loss': 0.6268, 'grad_norm': 3.350104570388794, 'learning_rate': 4.0103679843142895e-06, 'epoch': 45.92307692307692}
>>> 2025-09-11 08:56:02,742 - INFO - >>> {'loss': 0.5537, 'grad_norm': 5.457403659820557, 'learning_rate': 3.5797537488388323e-06, 'epoch': 46.0}
>>> 2025-09-11 08:56:07,497 - INFO - >>> {'loss': 0.5696, 'grad_norm': 3.2736704349517822, 'learning_rate': 3.1731694601492833e-06, 'epoch': 46.30769230769231}
>>> 2025-09-11 08:56:12,214 - INFO - >>> {'loss': 0.6123, 'grad_norm': 3.12197208404541, 'learning_rate': 2.7907164475743043e-06, 'epoch': 46.61538461538461}
>>> 2025-09-11 08:56:16,929 - INFO - >>> {'loss': 0.5758, 'grad_norm': 3.2723703384399414, 'learning_rate': 2.4324900264226403e-06, 'epoch': 46.92307692307692}
>>> 2025-09-11 08:56:17,577 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.098579474228546e-06, 'epoch': 47.0}
>>> 2025-09-11 08:56:22,302 - INFO - >>> {'loss': 0.5384, 'grad_norm': 4.581834316253662, 'learning_rate': 1.7890680085019595e-06, 'epoch': 47.30769230769231}
>>> 2025-09-11 08:56:27,024 - INFO - >>> {'loss': 0.5836, 'grad_norm': 3.1133618354797363, 'learning_rate': 1.5040327659889608e-06, 'epoch': 47.61538461538461}
>>> 2025-09-11 08:56:31,770 - INFO - >>> {'loss': 0.6148, 'grad_norm': 2.8677945137023926, 'learning_rate': 1.2435447834476255e-06, 'epoch': 47.92307692307692}
>>> 2025-09-11 08:56:32,417 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.0076689799442873e-06, 'epoch': 48.0}
>>> 2025-09-11 08:56:37,141 - INFO - >>> {'loss': 0.5534, 'grad_norm': 2.8819220066070557, 'learning_rate': 7.964641406742135e-07, 'epoch': 48.30769230769231}
>>> 2025-09-11 08:56:41,859 - INFO - >>> {'loss': 0.5628, 'grad_norm': 3.8807625770568848, 'learning_rate': 6.099829023112235e-07, 'epoch': 48.61538461538461}
>>> 2025-09-11 08:56:46,603 - INFO - >>> {'loss': 0.6437, 'grad_norm': 3.4319093227386475, 'learning_rate': 4.482717398894165e-07, 'epoch': 48.92307692307692}
>>> 2025-09-11 08:56:47,256 - INFO - >>> {'loss': 0.7956, 'grad_norm': 12.715228080749512, 'learning_rate': 3.1137095522068007e-07, 'epoch': 49.0}
>>> 2025-09-11 08:56:51,984 - INFO - >>> {'loss': 0.6346, 'grad_norm': 3.379674196243286, 'learning_rate': 1.9931466685065847e-07, 'epoch': 49.30769230769231}
>>> 2025-09-11 08:56:56,704 - INFO - >>> {'loss': 0.5887, 'grad_norm': 2.859358549118042, 'learning_rate': 1.1213080155564326e-07, 'epoch': 49.61538461538461}
>>> 2025-09-11 08:57:01,446 - INFO - >>> {'loss': 0.5503, 'grad_norm': 3.377197504043579, 'learning_rate': 4.9841087382618276e-08, 'epoch': 49.92307692307692}
>>> 2025-09-11 08:57:02,100 - INFO - >>> {'loss': 0.1504, 'grad_norm': 9.623717308044434, 'learning_rate': 1.2461048234269079e-08, 'epoch': 50.0}
>>> 2025-09-11 08:57:02,661 - INFO - >>> {'train_runtime': 744.7281, 'train_samples_per_second': 6.714, 'train_steps_per_second': 0.269, 'train_loss': 1.2945821527391672, 'epoch': 50.0}
>>> 2025-09-11 08:57:02,663 - INFO - 训练成功！
>>> 2025-09-11 08:57:02,663 - INFO - 模型存放位置：./output/qwen2-0.5b202509110844
>>> 2025-09-11 09:00:42,778 - INFO - 开始进行原始模型对话测试
>>> 2025-09-11 09:00:45,129 - INFO - 导入包完成
>>> 2025-09-11 09:00:45,136 - INFO - 配置文件读取完成
>>> 2025-09-11 09:01:24,290 - INFO - ========__main__  202509110901========
>>> 2025-09-11 09:01:24,291 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 09:01:24,291 - INFO - 开始进行模型测试
>>> 2025-09-11 09:01:29,236 - INFO - 已选择模型文件夹: qwen2-0.5b202509110844
>>> 2025-09-11 09:01:29,239 - INFO - 最新的 LoRA checkpoint 路径:output/qwen2-0.5b202509110844/checkpoint-200
>>> 2025-09-11 09:05:47,186 - INFO - 导入包完成
>>> 2025-09-11 09:05:47,187 - INFO - ========train Qwen2ForCausalLM  202509110905========
>>> 2025-09-11 09:05:47,187 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 09:05:47,188 - INFO - 开始进行训练
>>> 2025-09-11 09:05:47,194 - INFO - 基础配置文件读取完成
>>> 2025-09-11 09:05:47,201 - INFO - 训练配置读取完成
>>> 2025-09-11 09:05:47,202 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-09-11 09:05:47,202 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-09-11 09:05:47,709 - INFO - tokenizer读取完成
>>> 2025-09-11 09:05:48,205 - INFO - model dtype:torch.bfloat16
>>> 2025-09-11 09:05:48,205 - INFO - 模型导入完成
>>> 2025-09-11 09:05:48,206 - INFO - 数据读取开始
>>> 2025-09-11 09:05:48,990 - INFO - 数据下载完成，训练集大小: 100
>>> 2025-09-11 09:05:53,688 - INFO - 数据映射完成
>>> 2025-09-11 09:05:53,689 - INFO - 打印训练参数如下
>>> 2025-09-11 09:05:53,689 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-11 09:05:53,690 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-11 09:05:53,690 - INFO -   load_in_4bit >>> True
>>> 2025-09-11 09:05:53,690 - INFO -   batch_size >>> 8
>>> 2025-09-11 09:05:53,691 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-11 09:05:53,691 - INFO -   warmup_steps >>> 1
>>> 2025-09-11 09:05:53,691 - INFO -   epoch >>> 50
>>> 2025-09-11 09:05:53,692 - INFO -   eval_steps >>> 5
>>> 2025-09-11 09:05:53,692 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-11 09:05:53,692 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-11 09:05:53,693 - INFO -   max_seq_length >>> 2048
>>> 2025-09-11 09:05:53,693 - INFO -   r >>> 8
>>> 2025-09-11 09:05:53,693 - INFO -   interface_mode >>> False
>>> 2025-09-11 09:05:53,694 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-11 09:05:53,694 - INFO -   lora_alpha >>> 16
>>> 2025-09-11 09:05:53,694 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-11 09:05:53,695 - INFO -   bias >>> none
>>> 2025-09-11 09:05:53,695 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-11 09:05:53,695 - INFO -   random_state >>> 3407
>>> 2025-09-11 09:05:53,696 - INFO -   use_rslora >>> True
>>> 2025-09-11 09:05:53,696 - INFO -   loftq_config >>> None
>>> 2025-09-11 09:06:02,926 - INFO - 开始训练！
>>> 2025-09-11 09:07:18,471 - INFO - 导入包完成
>>> 2025-09-11 09:07:18,471 - INFO - ========train Qwen2ForCausalLM  202509110907========
>>> 2025-09-11 09:07:18,472 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 09:07:18,472 - INFO - 开始进行训练
>>> 2025-09-11 09:07:18,478 - INFO - 基础配置文件读取完成
>>> 2025-09-11 09:07:18,486 - INFO - 训练配置读取完成
>>> 2025-09-11 09:07:18,486 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-09-11 09:07:18,487 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-09-11 09:07:18,934 - INFO - tokenizer读取完成
>>> 2025-09-11 09:07:19,089 - INFO - model dtype:torch.bfloat16
>>> 2025-09-11 09:07:19,090 - INFO - 模型导入完成
>>> 2025-09-11 09:07:19,090 - INFO - 数据读取开始
>>> 2025-09-11 09:07:20,102 - INFO - 数据下载完成，训练集大小: 100
>>> 2025-09-11 09:07:24,323 - INFO - 数据映射完成
>>> 2025-09-11 09:07:24,324 - INFO - 打印训练参数如下
>>> 2025-09-11 09:07:24,324 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-11 09:07:24,325 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-11 09:07:24,325 - INFO -   load_in_4bit >>> True
>>> 2025-09-11 09:07:24,326 - INFO -   batch_size >>> 4
>>> 2025-09-11 09:07:24,326 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-11 09:07:24,326 - INFO -   warmup_steps >>> 1
>>> 2025-09-11 09:07:24,327 - INFO -   epoch >>> 50
>>> 2025-09-11 09:07:24,327 - INFO -   eval_steps >>> 5
>>> 2025-09-11 09:07:24,327 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-11 09:07:24,328 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-11 09:07:24,328 - INFO -   max_seq_length >>> 2048
>>> 2025-09-11 09:07:24,328 - INFO -   r >>> 8
>>> 2025-09-11 09:07:24,329 - INFO -   interface_mode >>> False
>>> 2025-09-11 09:07:24,329 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-11 09:07:24,330 - INFO -   lora_alpha >>> 16
>>> 2025-09-11 09:07:24,330 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-11 09:07:24,330 - INFO -   bias >>> none
>>> 2025-09-11 09:07:24,330 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-11 09:07:24,331 - INFO -   random_state >>> 3407
>>> 2025-09-11 09:07:24,331 - INFO -   use_rslora >>> True
>>> 2025-09-11 09:07:24,332 - INFO -   loftq_config >>> None
>>> 2025-09-11 09:07:30,480 - INFO - 开始训练！
>>> 2025-09-11 09:08:01,337 - INFO - >>> {'loss': 3.5267, 'grad_norm': 1.9737170934677124, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-09-11 09:08:32,550 - INFO - >>> {'loss': 3.6737, 'grad_norm': 2.220628261566162, 'learning_rate': 0.0002, 'epoch': 0.32}
>>> 2025-09-11 09:09:02,997 - INFO - >>> {'loss': 3.6792, 'grad_norm': 2.268340826034546, 'learning_rate': 0.00019999594849888085, 'epoch': 0.48}
>>> 2025-09-11 09:09:32,525 - INFO - >>> {'loss': 3.5939, 'grad_norm': 2.1249146461486816, 'learning_rate': 0.0001999837943238166, 'epoch': 0.64}
>>> 2025-09-11 09:10:07,152 - INFO - >>> {'loss': 3.0714, 'grad_norm': 1.3450411558151245, 'learning_rate': 0.00019996353845966032, 'epoch': 0.8}
>>> 2025-09-11 09:10:38,750 - INFO - >>> {'loss': 2.8942, 'grad_norm': 1.605076551437378, 'learning_rate': 0.00019993518254774517, 'epoch': 0.96}
>>> 2025-09-11 09:10:46,251 - INFO - >>> {'loss': 3.0227, 'grad_norm': 1.5241148471832275, 'learning_rate': 0.00019989872888575126, 'epoch': 1.0}
>>> 2025-09-11 09:11:16,216 - INFO - >>> {'loss': 2.7006, 'grad_norm': 0.9179468154907227, 'learning_rate': 0.00019985418042751975, 'epoch': 1.16}
>>> 2025-09-11 09:11:49,617 - INFO - >>> {'loss': 2.4471, 'grad_norm': 0.7003553509712219, 'learning_rate': 0.0001998015407828131, 'epoch': 1.32}
>>> 2025-09-11 09:12:20,159 - INFO - >>> {'loss': 2.4113, 'grad_norm': 0.8157334327697754, 'learning_rate': 0.00019974081421702294, 'epoch': 1.48}
>>> 2025-09-11 09:12:51,676 - INFO - >>> {'loss': 2.2126, 'grad_norm': 0.6663153171539307, 'learning_rate': 0.00019967200565082426, 'epoch': 1.6400000000000001}
>>> 2025-09-11 09:13:22,668 - INFO - >>> {'loss': 2.4152, 'grad_norm': 0.5701825022697449, 'learning_rate': 0.00019959512065977671, 'epoch': 1.8}
>>> 2025-09-11 09:13:52,631 - INFO - >>> {'loss': 2.2268, 'grad_norm': 0.760826826095581, 'learning_rate': 0.00019951016547387288, 'epoch': 1.96}
>>> 2025-09-11 09:14:00,142 - INFO - >>> {'loss': 2.4409, 'grad_norm': 1.037477970123291, 'learning_rate': 0.00019941714697703332, 'epoch': 2.0}
>>> 2025-09-11 09:14:30,775 - INFO - >>> {'loss': 2.317, 'grad_norm': 0.7648516893386841, 'learning_rate': 0.0001993160727065489, 'epoch': 2.16}
>>> 2025-09-11 09:15:00,866 - INFO - >>> {'loss': 2.2416, 'grad_norm': 0.8134384155273438, 'learning_rate': 0.0001992069508524701, 'epoch': 2.32}
>>> 2025-09-11 09:15:34,341 - INFO - >>> {'loss': 2.1331, 'grad_norm': 0.7544810175895691, 'learning_rate': 0.0001990897902569431, 'epoch': 2.48}
>>> 2025-09-11 09:16:05,354 - INFO - >>> {'loss': 2.1607, 'grad_norm': 0.8262713551521301, 'learning_rate': 0.0001989646004134937, 'epoch': 2.64}
>>> 2025-09-11 09:16:35,324 - INFO - >>> {'loss': 2.0828, 'grad_norm': 0.7916695475578308, 'learning_rate': 0.0001988313914662576, 'epoch': 2.8}
>>> 2025-09-11 09:17:06,860 - INFO - >>> {'loss': 1.9279, 'grad_norm': 0.6696217656135559, 'learning_rate': 0.00019869017420915888, 'epoch': 2.96}
>>> 2025-09-11 09:17:14,345 - INFO - >>> {'loss': 2.1622, 'grad_norm': 1.4314647912979126, 'learning_rate': 0.00019854096008503494, 'epoch': 3.0}
>>> 2025-09-11 09:17:49,412 - INFO - >>> {'loss': 2.1287, 'grad_norm': 0.5376343727111816, 'learning_rate': 0.00019838376118470964, 'epoch': 3.16}
>>> 2025-09-11 09:18:21,004 - INFO - >>> {'loss': 2.1267, 'grad_norm': 0.6521815061569214, 'learning_rate': 0.00019821859024601345, 'epoch': 3.32}
>>> 2025-09-11 09:18:50,972 - INFO - >>> {'loss': 1.9232, 'grad_norm': 0.5370975732803345, 'learning_rate': 0.00019804546065275112, 'epoch': 3.48}
>>> 2025-09-11 09:19:20,985 - INFO - >>> {'loss': 1.9114, 'grad_norm': 0.5113623142242432, 'learning_rate': 0.00019786438643361757, 'epoch': 3.64}
>>> 2025-09-11 09:19:50,917 - INFO - >>> {'loss': 1.704, 'grad_norm': 0.8582519888877869, 'learning_rate': 0.00019767538226106077, 'epoch': 3.8}
>>> 2025-09-11 09:20:20,857 - INFO - >>> {'loss': 1.8652, 'grad_norm': 0.8500133156776428, 'learning_rate': 0.00019747846345009306, 'epoch': 3.96}
>>> 2025-09-11 09:20:28,345 - INFO - >>> {'loss': 1.6494, 'grad_norm': 1.3763014078140259, 'learning_rate': 0.00019727364595705012, 'epoch': 4.0}
>>> 2025-09-11 09:20:58,310 - INFO - >>> {'loss': 1.9877, 'grad_norm': 0.7740324139595032, 'learning_rate': 0.00019706094637829798, 'epoch': 4.16}
>>> 2025-09-11 09:21:29,380 - INFO - >>> {'loss': 1.7114, 'grad_norm': 0.6060006022453308, 'learning_rate': 0.00019684038194888828, 'epoch': 4.32}
>>> 2025-09-11 09:22:00,961 - INFO - >>> {'loss': 1.7792, 'grad_norm': 0.62657630443573, 'learning_rate': 0.00019661197054116164, 'epoch': 4.48}
>>> 2025-09-11 09:22:34,932 - INFO - >>> {'loss': 1.6865, 'grad_norm': 0.7368484735488892, 'learning_rate': 0.0001963757306632996, 'epoch': 4.64}
>>> 2025-09-11 09:23:04,865 - INFO - >>> {'loss': 1.9006, 'grad_norm': 0.7863322496414185, 'learning_rate': 0.00019613168145782468, 'epoch': 4.8}
>>> 2025-09-11 09:23:34,796 - INFO - >>> {'loss': 1.8451, 'grad_norm': 0.7729089260101318, 'learning_rate': 0.0001958798427000495, 'epoch': 4.96}
>>> 2025-09-11 09:23:42,268 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.00019562023479647426, 'epoch': 5.0}
>>> 2025-09-11 09:24:13,270 - INFO - >>> {'loss': 1.8883, 'grad_norm': 0.8041192293167114, 'learning_rate': 0.00019535287878313316, 'epoch': 5.16}
>>> 2025-09-11 09:24:43,791 - INFO - >>> {'loss': 1.5727, 'grad_norm': 0.8485186696052551, 'learning_rate': 0.00019507779632388996, 'epoch': 5.32}
>>> 2025-09-11 09:25:13,795 - INFO - >>> {'loss': 1.4068, 'grad_norm': 0.839230477809906, 'learning_rate': 0.0001947950097086825, 'epoch': 5.48}
>>> 2025-09-11 09:25:43,808 - INFO - >>> {'loss': 1.4431, 'grad_norm': 1.2334423065185547, 'learning_rate': 0.00019450454185171648, 'epoch': 5.64}
>>> 2025-09-11 09:26:13,850 - INFO - >>> {'loss': 1.6, 'grad_norm': 0.8549513816833496, 'learning_rate': 0.00019420641628960895, 'epoch': 5.8}
>>> 2025-09-11 09:26:48,974 - INFO - >>> {'loss': 1.7078, 'grad_norm': 0.8020724654197693, 'learning_rate': 0.00019390065717948083, 'epoch': 5.96}
>>> 2025-09-11 09:26:56,481 - INFO - >>> {'loss': 1.9392, 'grad_norm': 2.8156020641326904, 'learning_rate': 0.00019358728929699966, 'epoch': 6.0}
>>> 2025-09-11 09:27:29,980 - INFO - >>> {'loss': 1.4268, 'grad_norm': 0.9722881317138672, 'learning_rate': 0.00019326633803437194, 'epoch': 6.16}
>>> 2025-09-11 09:28:00,574 - INFO - >>> {'loss': 1.5375, 'grad_norm': 0.8490774035453796, 'learning_rate': 0.00019293782939828571, 'epoch': 6.32}
>>> 2025-09-11 09:28:30,499 - INFO - >>> {'loss': 1.0801, 'grad_norm': 2.1115825176239014, 'learning_rate': 0.0001926017900078031, 'epoch': 6.48}
>>> 2025-09-11 09:29:00,485 - INFO - >>> {'loss': 1.5714, 'grad_norm': 0.9448990225791931, 'learning_rate': 0.00019225824709220342, 'epoch': 6.64}
>>> 2025-09-11 09:29:30,418 - INFO - >>> {'loss': 1.3642, 'grad_norm': 1.161260724067688, 'learning_rate': 0.0001919072284887768, 'epoch': 6.8}
>>> 2025-09-11 09:30:03,100 - INFO - >>> {'loss': 1.4795, 'grad_norm': 1.0685031414031982, 'learning_rate': 0.00019154876264056863, 'epoch': 6.96}
>>> 2025-09-11 09:30:10,632 - INFO - >>> {'loss': 1.3449, 'grad_norm': 1.5230990648269653, 'learning_rate': 0.0001911828785940745, 'epoch': 7.0}
>>> 2025-09-11 09:30:41,627 - INFO - >>> {'loss': 1.304, 'grad_norm': 1.315841555595398, 'learning_rate': 0.0001908096059968869, 'epoch': 7.16}
>>> 2025-09-11 09:31:11,588 - INFO - >>> {'loss': 1.3841, 'grad_norm': 1.3380416631698608, 'learning_rate': 0.00019042897509529279, 'epoch': 7.32}
>>> 2025-09-11 09:31:43,163 - INFO - >>> {'loss': 1.4102, 'grad_norm': 1.5096708536148071, 'learning_rate': 0.00019004101673182258, 'epoch': 7.48}
>>> 2025-09-11 09:32:13,157 - INFO - >>> {'loss': 1.3745, 'grad_norm': 1.578823447227478, 'learning_rate': 0.0001896457623427512, 'epoch': 7.64}
>>> 2025-09-11 09:32:46,740 - INFO - >>> {'loss': 1.2235, 'grad_norm': 1.2006391286849976, 'learning_rate': 0.00018924324395555066, 'epoch': 7.8}
>>> 2025-09-11 09:33:16,687 - INFO - >>> {'loss': 1.0844, 'grad_norm': 1.5260299444198608, 'learning_rate': 0.00018883349418629484, 'epoch': 7.96}
>>> 2025-09-11 09:33:24,183 - INFO - >>> {'loss': 0.8713, 'grad_norm': 3.1883931159973145, 'learning_rate': 0.00018841654623701673, 'epoch': 8.0}
>>> 2025-09-11 09:33:54,714 - INFO - >>> {'loss': 1.2655, 'grad_norm': 1.3193330764770508, 'learning_rate': 0.00018799243389301798, 'epoch': 8.16}
>>> 2025-09-11 09:34:25,735 - INFO - >>> {'loss': 1.1322, 'grad_norm': 1.3409886360168457, 'learning_rate': 0.0001875611915201313, 'epoch': 8.32}
>>> 2025-09-11 09:34:55,639 - INFO - >>> {'loss': 0.9656, 'grad_norm': 1.5135724544525146, 'learning_rate': 0.00018712285406193585, 'epoch': 8.48}
>>> 2025-09-11 09:35:25,566 - INFO - >>> {'loss': 0.895, 'grad_norm': 1.5800377130508423, 'learning_rate': 0.00018667745703692572, 'epoch': 8.64}
>>> 2025-09-11 09:35:57,098 - INFO - >>> {'loss': 1.0173, 'grad_norm': 2.2600769996643066, 'learning_rate': 0.00018622503653563174, 'epoch': 8.8}
>>> 2025-09-11 09:36:30,553 - INFO - >>> {'loss': 1.0353, 'grad_norm': 1.930572271347046, 'learning_rate': 0.00018576562921769727, 'epoch': 8.96}
>>> 2025-09-11 09:36:38,040 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0001852992723089076, 'epoch': 9.0}
>>> 2025-09-11 09:37:08,017 - INFO - >>> {'loss': 0.8625, 'grad_norm': 2.230581760406494, 'learning_rate': 0.00018482600359817343, 'epoch': 9.16}
>>> 2025-09-11 09:37:37,993 - INFO - >>> {'loss': 0.7486, 'grad_norm': 2.5943291187286377, 'learning_rate': 0.00018434586143446907, 'epoch': 9.32}
>>> 2025-09-11 09:38:08,069 - INFO - >>> {'loss': 0.9033, 'grad_norm': 1.8862749338150024, 'learning_rate': 0.00018385888472372472, 'epoch': 9.48}
>>> 2025-09-11 09:38:43,237 - INFO - >>> {'loss': 0.7923, 'grad_norm': 1.9013385772705078, 'learning_rate': 0.00018336511292567419, 'epoch': 9.64}
>>> 2025-09-11 09:39:14,346 - INFO - >>> {'loss': 0.822, 'grad_norm': 2.2980246543884277, 'learning_rate': 0.0001828645860506573, 'epoch': 9.8}
>>> 2025-09-11 09:39:44,339 - INFO - >>> {'loss': 0.4646, 'grad_norm': 2.171365737915039, 'learning_rate': 0.00018235734465637794, 'epoch': 9.96}
>>> 2025-09-11 09:39:52,434 - INFO - >>> {'loss': 1.0917, 'grad_norm': 3.698805809020996, 'learning_rate': 0.00018184342984461766, 'epoch': 10.0}
>>> 2025-09-11 09:40:22,446 - INFO - >>> {'loss': 0.706, 'grad_norm': 2.2504777908325195, 'learning_rate': 0.00018132288325790517, 'epoch': 10.16}
>>> 2025-09-11 09:40:55,931 - INFO - >>> {'loss': 0.5973, 'grad_norm': 2.019148111343384, 'learning_rate': 0.00018079574707614203, 'epoch': 10.32}
>>> 2025-09-11 09:41:25,780 - INFO - >>> {'loss': 0.5577, 'grad_norm': 1.655604362487793, 'learning_rate': 0.00018026206401318482, 'epoch': 10.48}
>>> 2025-09-11 09:41:55,779 - INFO - >>> {'loss': 0.5906, 'grad_norm': 2.954878091812134, 'learning_rate': 0.0001797218773133841, 'epoch': 10.64}
>>> 2025-09-11 09:42:25,771 - INFO - >>> {'loss': 0.5244, 'grad_norm': 4.686089038848877, 'learning_rate': 0.00017917523074808023, 'epoch': 10.8}
>>> 2025-09-11 09:42:56,408 - INFO - >>> {'loss': 0.6529, 'grad_norm': 2.9415230751037598, 'learning_rate': 0.0001786221686120567, 'epoch': 10.96}
>>> 2025-09-11 09:43:05,529 - INFO - >>> {'loss': 0.7626, 'grad_norm': 5.952953815460205, 'learning_rate': 0.00017806273571995066, 'epoch': 11.0}
>>> 2025-09-11 09:43:35,515 - INFO - >>> {'loss': 0.4992, 'grad_norm': 3.548419952392578, 'learning_rate': 0.00017749697740262197, 'epoch': 11.16}
>>> 2025-09-11 09:44:08,209 - INFO - >>> {'loss': 0.404, 'grad_norm': 2.9259066581726074, 'learning_rate': 0.0001769249395034797, 'epoch': 11.32}
>>> 2025-09-11 09:44:41,647 - INFO - >>> {'loss': 0.494, 'grad_norm': 3.1691110134124756, 'learning_rate': 0.00017634666837476766, 'epoch': 11.48}
>>> 2025-09-11 09:45:11,632 - INFO - >>> {'loss': 0.3886, 'grad_norm': 2.5526647567749023, 'learning_rate': 0.0001757622108738083, 'epoch': 11.64}
>>> 2025-09-11 09:45:42,140 - INFO - >>> {'loss': 0.5745, 'grad_norm': 2.7317700386047363, 'learning_rate': 0.00017517161435920607, 'epoch': 11.8}
>>> 2025-09-11 09:46:12,088 - INFO - >>> {'loss': 0.3624, 'grad_norm': 2.3558032512664795, 'learning_rate': 0.00017457492668700967, 'epoch': 11.96}
>>> 2025-09-11 09:46:19,570 - INFO - >>> {'loss': 0.3637, 'grad_norm': 6.485208034515381, 'learning_rate': 0.00017397219620683465, 'epoch': 12.0}
>>> 2025-09-11 09:46:52,921 - INFO - >>> {'loss': 0.3354, 'grad_norm': 2.2387497425079346, 'learning_rate': 0.00017336347175794524, 'epoch': 12.16}
>>> 2025-09-11 09:47:24,425 - INFO - >>> {'loss': 0.2756, 'grad_norm': 2.4640800952911377, 'learning_rate': 0.00017274880266529715, 'epoch': 12.32}
>>> 2025-09-11 09:47:55,447 - INFO - >>> {'loss': 0.2318, 'grad_norm': 2.024219036102295, 'learning_rate': 0.0001721282387355408, 'epoch': 12.48}
>>> 2025-09-11 09:48:25,972 - INFO - >>> {'loss': 0.2891, 'grad_norm': 2.590061902999878, 'learning_rate': 0.0001715018302529852, 'epoch': 12.64}
>>> 2025-09-11 09:48:55,982 - INFO - >>> {'loss': 0.2644, 'grad_norm': 2.1514296531677246, 'learning_rate': 0.00017086962797552375, 'epoch': 12.8}
>>> 2025-09-11 09:49:26,010 - INFO - >>> {'loss': 0.3695, 'grad_norm': 3.2594239711761475, 'learning_rate': 0.0001702316831305212, 'epoch': 12.96}
>>> 2025-09-11 09:49:33,529 - INFO - >>> {'loss': 0.3242, 'grad_norm': 5.170313358306885, 'learning_rate': 0.00016958804741066253, 'epoch': 13.0}
>>> 2025-09-11 09:50:01,463 - INFO - >>> {'loss': 0.1787, 'grad_norm': 1.693886637687683, 'learning_rate': 0.0001689387729697646, 'epoch': 13.16}
>>> 2025-09-11 09:50:31,468 - INFO - >>> {'loss': 0.1156, 'grad_norm': 2.9333393573760986, 'learning_rate': 0.00016828391241854984, 'epoch': 13.32}
>>> 2025-09-11 09:51:03,090 - INFO - >>> {'loss': 0.2245, 'grad_norm': 1.9074381589889526, 'learning_rate': 0.0001676235188203834, 'epoch': 13.48}
>>> 2025-09-11 09:51:33,038 - INFO - >>> {'loss': 0.2076, 'grad_norm': 2.158228635787964, 'learning_rate': 0.00016695764568697328, 'epoch': 13.64}
>>> 2025-09-11 09:52:07,069 - INFO - >>> {'loss': 0.1994, 'grad_norm': 2.499680519104004, 'learning_rate': 0.00016628634697403447, 'epoch': 13.8}
>>> 2025-09-11 09:52:38,118 - INFO - >>> {'loss': 0.1767, 'grad_norm': 3.2009379863739014, 'learning_rate': 0.00016560967707691663, 'epoch': 13.96}
>>> 2025-09-11 09:52:45,615 - INFO - >>> {'loss': 0.1607, 'grad_norm': 4.242190361022949, 'learning_rate': 0.0001649276908261967, 'epoch': 14.0}
>>> 2025-09-11 09:53:15,587 - INFO - >>> {'loss': 0.1226, 'grad_norm': 1.372530221939087, 'learning_rate': 0.00016424044348323582, 'epoch': 14.16}
>>> 2025-09-11 09:53:46,133 - INFO - >>> {'loss': 0.188, 'grad_norm': 2.9259896278381348, 'learning_rate': 0.0001635479907357016, 'epoch': 14.32}
>>> 2025-09-11 09:54:16,789 - INFO - >>> {'loss': 0.1163, 'grad_norm': 2.0354535579681396, 'learning_rate': 0.00016285038869305563, 'epoch': 14.48}
>>> 2025-09-11 09:54:48,386 - INFO - >>> {'loss': 0.1224, 'grad_norm': 3.2267353534698486, 'learning_rate': 0.0001621476938820071, 'epoch': 14.64}
>>> 2025-09-11 09:55:18,342 - INFO - >>> {'loss': 0.0978, 'grad_norm': 2.2279276847839355, 'learning_rate': 0.00016143996324193225, 'epoch': 14.8}
>>> 2025-09-11 09:55:49,385 - INFO - >>> {'loss': 0.1203, 'grad_norm': 3.082629919052124, 'learning_rate': 0.00016072725412026066, 'epoch': 14.96}
>>> 2025-09-11 09:56:00,341 - INFO - >>> {'loss': 0.156, 'grad_norm': 4.853183746337891, 'learning_rate': 0.00016000962426782845, 'epoch': 15.0}
>>> 2025-09-11 09:56:30,856 - INFO - >>> {'loss': 0.107, 'grad_norm': 1.893222451210022, 'learning_rate': 0.0001592871318341986, 'epoch': 15.16}
>>> 2025-09-11 09:57:01,914 - INFO - >>> {'loss': 0.0778, 'grad_norm': 1.4547300338745117, 'learning_rate': 0.0001585598353629492, 'epoch': 15.32}
>>> 2025-09-11 09:57:31,823 - INFO - >>> {'loss': 0.0389, 'grad_norm': 3.2846949100494385, 'learning_rate': 0.00015782779378692956, 'epoch': 15.48}
>>> 2025-09-11 09:58:03,351 - INFO - >>> {'loss': 0.0592, 'grad_norm': 1.5678430795669556, 'learning_rate': 0.000157091066423485, 'epoch': 15.64}
>>> 2025-09-11 09:58:33,300 - INFO - >>> {'loss': 0.0932, 'grad_norm': 2.17512583732605, 'learning_rate': 0.0001563497129696503, 'epoch': 15.8}
>>> 2025-09-11 09:59:03,240 - INFO - >>> {'loss': 0.0617, 'grad_norm': 1.6520909070968628, 'learning_rate': 0.00015560379349731233, 'epoch': 15.96}
>>> 2025-09-11 09:59:14,192 - INFO - >>> {'loss': 0.0748, 'grad_norm': 3.646514892578125, 'learning_rate': 0.00015485336844834273, 'epoch': 16.0}
>>> 2025-09-11 09:59:44,848 - INFO - >>> {'loss': 0.0466, 'grad_norm': 0.8158283233642578, 'learning_rate': 0.00015409849862969995, 'epoch': 16.16}
>>> 2025-09-11 10:00:16,448 - INFO - >>> {'loss': 0.0334, 'grad_norm': 1.4531525373458862, 'learning_rate': 0.00015333924520850227, 'epoch': 16.32}
>>> 2025-09-11 10:00:44,057 - INFO - 导入包完成
>>> 2025-09-11 10:00:44,057 - INFO - ========train Qwen2ForCausalLM  202509111000========
>>> 2025-09-11 10:00:44,058 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 10:00:44,058 - INFO - 开始进行训练
>>> 2025-09-11 10:00:44,065 - INFO - 基础配置文件读取完成
>>> 2025-09-11 10:00:44,072 - INFO - 训练配置读取完成
>>> 2025-09-11 10:00:44,073 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-09-11 10:00:44,073 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-09-11 10:00:44,527 - INFO - tokenizer读取完成
>>> 2025-09-11 10:00:44,706 - INFO - model dtype:torch.bfloat16
>>> 2025-09-11 10:00:44,706 - INFO - 模型导入完成
>>> 2025-09-11 10:00:44,706 - INFO - 数据读取开始
>>> 2025-09-11 10:00:45,471 - INFO - 数据下载完成，训练集大小: 100
>>> 2025-09-11 10:00:46,441 - INFO - >>> {'loss': 0.05, 'grad_norm': 1.030793309211731, 'learning_rate': 0.00015257566970707146, 'epoch': 16.48}
>>> 2025-09-11 10:00:49,758 - INFO - 数据映射完成
>>> 2025-09-11 10:00:49,759 - INFO - 打印训练参数如下
>>> 2025-09-11 10:00:49,760 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-11 10:00:49,760 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-11 10:00:49,760 - INFO -   load_in_4bit >>> True
>>> 2025-09-11 10:00:49,761 - INFO -   batch_size >>> 4
>>> 2025-09-11 10:00:49,761 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-11 10:00:49,761 - INFO -   warmup_steps >>> 1
>>> 2025-09-11 10:00:49,762 - INFO -   epoch >>> 50
>>> 2025-09-11 10:00:49,762 - INFO -   eval_steps >>> 5
>>> 2025-09-11 10:00:49,762 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-11 10:00:49,763 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-11 10:00:49,763 - INFO -   max_seq_length >>> 2048
>>> 2025-09-11 10:00:49,764 - INFO -   r >>> 8
>>> 2025-09-11 10:00:49,764 - INFO -   interface_mode >>> False
>>> 2025-09-11 10:00:49,764 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-11 10:00:49,765 - INFO -   lora_alpha >>> 16
>>> 2025-09-11 10:00:49,765 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-11 10:00:49,765 - INFO -   bias >>> none
>>> 2025-09-11 10:00:49,766 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-11 10:00:49,766 - INFO -   random_state >>> 3407
>>> 2025-09-11 10:00:49,766 - INFO -   use_rslora >>> True
>>> 2025-09-11 10:00:49,767 - INFO -   loftq_config >>> None
>>> 2025-09-11 10:01:17,604 - INFO - >>> {'loss': 0.0406, 'grad_norm': 1.2456014156341553, 'learning_rate': 0.0001518078339979475, 'epoch': 16.64}
>>> 2025-09-11 10:01:51,070 - INFO - >>> {'loss': 0.0428, 'grad_norm': 1.1053111553192139, 'learning_rate': 0.00015103580029887504, 'epoch': 16.8}
>>> 2025-09-11 10:02:21,047 - INFO - >>> {'loss': 0.0449, 'grad_norm': 1.980879783630371, 'learning_rate': 0.00015025963116776202, 'epoch': 16.96}
>>> 2025-09-11 10:02:28,549 - INFO - >>> {'loss': 0.0533, 'grad_norm': 1.9846614599227905, 'learning_rate': 0.00014947938949761054, 'epoch': 17.0}
>>> 2025-09-11 10:02:58,515 - INFO - >>> {'loss': 0.0217, 'grad_norm': 0.824654757976532, 'learning_rate': 0.0001486951385114205, 'epoch': 17.16}
>>> 2025-09-11 10:03:32,027 - INFO - >>> {'loss': 0.0254, 'grad_norm': 0.6038233041763306, 'learning_rate': 0.00014790694175706697, 'epoch': 17.32}
>>> 2025-09-11 10:04:04,223 - INFO - >>> {'loss': 0.0303, 'grad_norm': 1.5602306127548218, 'learning_rate': 0.00014711486310215052, 'epoch': 17.48}
>>> 2025-09-11 10:04:35,360 - INFO - >>> {'loss': 0.0388, 'grad_norm': 1.0208799839019775, 'learning_rate': 0.00014631896672882234, 'epoch': 17.64}
>>> 2025-09-11 10:05:05,375 - INFO - >>> {'loss': 0.0367, 'grad_norm': 1.193246841430664, 'learning_rate': 0.00014551931712858334, 'epoch': 17.8}
>>> 2025-09-11 10:05:35,332 - INFO - >>> {'loss': 0.0149, 'grad_norm': 0.6140216588973999, 'learning_rate': 0.00014471597909705857, 'epoch': 17.96}
>>> 2025-09-11 10:05:42,833 - INFO - >>> {'loss': 0.0278, 'grad_norm': 2.851853847503662, 'learning_rate': 0.00014390901772874667, 'epoch': 18.0}
>>> 2025-09-11 10:06:13,374 - INFO - >>> {'loss': 0.0132, 'grad_norm': 0.705117404460907, 'learning_rate': 0.00014309849841174537, 'epoch': 18.16}
>>> 2025-09-11 10:06:46,054 - INFO - >>> {'loss': 0.0181, 'grad_norm': 0.5044190883636475, 'learning_rate': 0.0001422844868224531, 'epoch': 18.32}
>>> 2025-09-11 10:07:16,003 - INFO - >>> {'loss': 0.0158, 'grad_norm': 0.7680056095123291, 'learning_rate': 0.00014146704892024713, 'epoch': 18.48}
>>> 2025-09-11 10:07:45,941 - INFO - >>> {'loss': 0.0145, 'grad_norm': 0.4692820608615875, 'learning_rate': 0.000140646250942139, 'epoch': 18.64}
>>> 2025-09-11 10:08:15,849 - INFO - >>> {'loss': 0.013, 'grad_norm': 0.9932224750518799, 'learning_rate': 0.00013982215939740725, 'epoch': 18.8}
>>> 2025-09-11 10:08:49,300 - INFO - >>> {'loss': 0.0242, 'grad_norm': 1.384952425956726, 'learning_rate': 0.00013899484106220814, 'epoch': 18.96}
>>> 2025-09-11 10:08:56,786 - INFO - >>> {'loss': 0.0382, 'grad_norm': 1.534299373626709, 'learning_rate': 0.00013816436297416494, 'epoch': 19.0}
>>> 2025-09-11 10:09:31,334 - INFO - >>> {'loss': 0.0178, 'grad_norm': 0.913887083530426, 'learning_rate': 0.00013733079242693572, 'epoch': 19.16}
>>> 2025-09-11 10:10:01,256 - INFO - >>> {'loss': 0.0076, 'grad_norm': 0.24210026860237122, 'learning_rate': 0.00013649419696476055, 'epoch': 19.32}
>>> 2025-09-11 10:10:29,769 - INFO - >>> {'loss': 0.0132, 'grad_norm': 0.5556080937385559, 'learning_rate': 0.00013565464437698848, 'epoch': 19.48}
>>> 2025-09-11 10:11:01,359 - INFO - >>> {'loss': 0.0108, 'grad_norm': 0.5218355655670166, 'learning_rate': 0.00013481220269258447, 'epoch': 19.64}
>>> 2025-09-11 10:11:31,985 - INFO - >>> {'loss': 0.0127, 'grad_norm': 0.6950980424880981, 'learning_rate': 0.00013396694017461707, 'epoch': 19.8}
>>> 2025-09-11 10:12:01,992 - INFO - >>> {'loss': 0.0352, 'grad_norm': 2.4268577098846436, 'learning_rate': 0.00013311892531472704, 'epoch': 19.96}
>>> 2025-09-11 10:12:09,506 - INFO - >>> {'loss': 0.0071, 'grad_norm': 0.7854084968566895, 'learning_rate': 0.00013226822682757745, 'epoch': 20.0}
>>> 2025-09-11 10:12:39,506 - INFO - >>> {'loss': 0.0114, 'grad_norm': 0.43497562408447266, 'learning_rate': 0.00013141491364528576, 'epoch': 20.16}
>>> 2025-09-11 10:13:13,000 - INFO - >>> {'loss': 0.0116, 'grad_norm': 0.3362779915332794, 'learning_rate': 0.00013055905491183821, 'epoch': 20.32}
>>> 2025-09-11 10:13:43,594 - INFO - >>> {'loss': 0.0092, 'grad_norm': 0.4406265914440155, 'learning_rate': 0.00012970071997748712, 'epoch': 20.48}
>>> 2025-09-11 10:14:15,147 - INFO - >>> {'loss': 0.0082, 'grad_norm': 0.39629319310188293, 'learning_rate': 0.00012883997839313152, 'epoch': 20.64}
>>> 2025-09-11 10:14:45,123 - INFO - >>> {'loss': 0.0168, 'grad_norm': 0.571441113948822, 'learning_rate': 0.00012797689990468112, 'epoch': 20.8}
>>> 2025-09-11 10:15:16,154 - INFO - >>> {'loss': 0.0072, 'grad_norm': 0.28879985213279724, 'learning_rate': 0.0001271115544474053, 'epoch': 20.96}
>>> 2025-09-11 10:15:23,682 - INFO - >>> {'loss': 0.0072, 'grad_norm': 0.7898661494255066, 'learning_rate': 0.00012624401214026573, 'epoch': 21.0}
>>> 2025-09-11 10:15:53,670 - INFO - >>> {'loss': 0.0074, 'grad_norm': 0.7409789562225342, 'learning_rate': 0.000125374343280235, 'epoch': 21.16}
>>> 2025-09-11 10:16:24,705 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.15038728713989258, 'learning_rate': 0.00012450261833660034, 'epoch': 21.32}
>>> 2025-09-11 10:16:54,644 - INFO - >>> {'loss': 0.0103, 'grad_norm': 0.340675950050354, 'learning_rate': 0.0001236289079452534, 'epoch': 21.48}
>>> 2025-09-11 10:17:26,237 - INFO - >>> {'loss': 0.0092, 'grad_norm': 0.292365700006485, 'learning_rate': 0.00012275328290296678, 'epoch': 21.64}
>>> 2025-09-11 10:17:56,224 - INFO - >>> {'loss': 0.0075, 'grad_norm': 0.497060090303421, 'learning_rate': 0.00012187581416165721, 'epoch': 21.8}
>>> 2025-09-11 10:18:30,233 - INFO - >>> {'loss': 0.0094, 'grad_norm': 1.3859182596206665, 'learning_rate': 0.0001209965728226365, 'epoch': 21.96}
>>> 2025-09-11 10:18:37,745 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.15240086615085602, 'learning_rate': 0.00012011563013084996, 'epoch': 22.0}
>>> 2025-09-11 10:19:09,300 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.24816638231277466, 'learning_rate': 0.00011923305746910371, 'epoch': 22.16}
>>> 2025-09-11 10:19:42,780 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.12204544991254807, 'learning_rate': 0.00011834892635228023, 'epoch': 22.32}
>>> 2025-09-11 10:20:12,686 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.2376129925251007, 'learning_rate': 0.00011746330842154371, 'epoch': 22.48}
>>> 2025-09-11 10:20:42,622 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.19414715468883514, 'learning_rate': 0.00011657627543853491, 'epoch': 22.64}
>>> 2025-09-11 10:21:12,583 - INFO - >>> {'loss': 0.0084, 'grad_norm': 0.6036275625228882, 'learning_rate': 0.0001156878992795563, 'epoch': 22.8}
>>> 2025-09-11 10:21:42,558 - INFO - >>> {'loss': 0.0095, 'grad_norm': 0.30105048418045044, 'learning_rate': 0.00011479825192974792, 'epoch': 22.96}
>>> 2025-09-11 10:21:50,057 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.15240442752838135, 'learning_rate': 0.00011390740547725443, 'epoch': 23.0}
>>> 2025-09-11 10:22:20,016 - INFO - >>> {'loss': 0.0061, 'grad_norm': 0.24426518380641937, 'learning_rate': 0.00011301543210738384, 'epoch': 23.16}
>>> 2025-09-11 10:22:49,967 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.129123255610466, 'learning_rate': 0.00011212240409675825, 'epoch': 23.32}
>>> 2025-09-11 10:23:22,165 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.12391947954893112, 'learning_rate': 0.00011122839380745737, 'epoch': 23.48}
>>> 2025-09-11 10:23:53,269 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.07635755091905594, 'learning_rate': 0.00011033347368115494, 'epoch': 23.64}
>>> 2025-09-11 10:24:23,274 - INFO - >>> {'loss': 0.0081, 'grad_norm': 0.24526278674602509, 'learning_rate': 0.00010943771623324883, 'epoch': 23.8}
>>> 2025-09-11 10:24:53,266 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.0656764879822731, 'learning_rate': 0.00010854119404698511, 'epoch': 23.96}
>>> 2025-09-11 10:25:04,302 - INFO - >>> {'loss': 0.0364, 'grad_norm': 3.1538443565368652, 'learning_rate': 0.00010764397976757656, 'epoch': 24.0}
>>> 2025-09-11 10:25:34,329 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.10731863230466843, 'learning_rate': 0.00010674614609631634, 'epoch': 24.16}
>>> 2025-09-11 10:26:04,352 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.06979356706142426, 'learning_rate': 0.00010584776578468698, 'epoch': 24.32}
>>> 2025-09-11 10:26:36,025 - INFO - >>> {'loss': 0.0059, 'grad_norm': 0.16281451284885406, 'learning_rate': 0.00010494891162846514, 'epoch': 24.48}
>>> 2025-09-11 10:27:09,488 - INFO - >>> {'loss': 0.0069, 'grad_norm': 0.525534451007843, 'learning_rate': 0.0001040496564618233, 'epoch': 24.64}
>>> 2025-09-11 10:27:41,188 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.0688605085015297, 'learning_rate': 0.0001031500731514277, 'epoch': 24.8}
>>> 2025-09-11 10:28:11,146 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.06929340958595276, 'learning_rate': 0.00010225023459053415, 'epoch': 24.96}
>>> 2025-09-11 10:28:18,648 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.00010135021369308137, 'epoch': 25.0}
>>> 2025-09-11 10:28:48,608 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.05829792842268944, 'learning_rate': 0.00010045008338778279, 'epoch': 25.16}
>>> 2025-09-11 10:29:18,595 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.12649355828762054, 'learning_rate': 9.954991661221723e-05, 'epoch': 25.32}
>>> 2025-09-11 10:29:48,541 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.06197704002261162, 'learning_rate': 9.864978630691866e-05, 'epoch': 25.48}
>>> 2025-09-11 10:30:19,026 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.16232995688915253, 'learning_rate': 9.774976540946587e-05, 'epoch': 25.64}
>>> 2025-09-11 10:30:50,578 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.13289634883403778, 'learning_rate': 9.684992684857232e-05, 'epoch': 25.8}
>>> 2025-09-11 10:31:25,028 - INFO - >>> {'loss': 0.0096, 'grad_norm': 1.1764260530471802, 'learning_rate': 9.595034353817672e-05, 'epoch': 25.96}
>>> 2025-09-11 10:31:32,499 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.50510883715349e-05, 'epoch': 26.0}
>>> 2025-09-11 10:32:02,398 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.2973175644874573, 'learning_rate': 9.415223421531307e-05, 'epoch': 26.16}
>>> 2025-09-11 10:32:32,343 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.08638571202754974, 'learning_rate': 9.325385390368368e-05, 'epoch': 26.32}
>>> 2025-09-11 10:33:02,425 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.04819205403327942, 'learning_rate': 9.235602023242349e-05, 'epoch': 26.48}
>>> 2025-09-11 10:33:33,580 - INFO - >>> {'loss': 0.006, 'grad_norm': 0.16571985185146332, 'learning_rate': 9.145880595301494e-05, 'epoch': 26.64}
>>> 2025-09-11 10:34:27,467 - INFO - 导入包完成
>>> 2025-09-11 10:34:27,468 - INFO - ========train Qwen2ForCausalLM  202509111034========
>>> 2025-09-11 10:34:27,468 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 10:34:27,469 - INFO - 开始进行训练
>>> 2025-09-11 10:34:27,475 - INFO - 基础配置文件读取完成
>>> 2025-09-11 10:34:27,483 - INFO - 训练配置读取完成
>>> 2025-09-11 10:34:27,483 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-09-11 10:34:27,484 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-09-11 10:34:27,953 - INFO - tokenizer读取完成
>>> 2025-09-11 10:34:28,109 - INFO - model dtype:torch.bfloat16
>>> 2025-09-11 10:34:28,110 - INFO - 模型导入完成
>>> 2025-09-11 10:34:28,110 - INFO - 数据读取开始
>>> 2025-09-11 10:34:29,313 - INFO - 数据下载完成，训练集大小: 100
>>> 2025-09-11 10:34:33,643 - INFO - 数据映射完成
>>> 2025-09-11 10:34:33,644 - INFO - 打印训练参数如下
>>> 2025-09-11 10:34:33,645 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-11 10:34:33,645 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-11 10:34:33,645 - INFO -   load_in_4bit >>> True
>>> 2025-09-11 10:34:33,646 - INFO -   batch_size >>> 4
>>> 2025-09-11 10:34:33,646 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-11 10:34:33,646 - INFO -   warmup_steps >>> 1
>>> 2025-09-11 10:34:33,647 - INFO -   epoch >>> 50
>>> 2025-09-11 10:34:33,647 - INFO -   eval_steps >>> 5
>>> 2025-09-11 10:34:33,647 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-11 10:34:33,648 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-11 10:34:33,648 - INFO -   max_seq_length >>> 2048
>>> 2025-09-11 10:34:33,648 - INFO -   r >>> 8
>>> 2025-09-11 10:34:33,649 - INFO -   interface_mode >>> False
>>> 2025-09-11 10:34:33,649 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-11 10:34:33,649 - INFO -   lora_alpha >>> 16
>>> 2025-09-11 10:34:33,650 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-11 10:34:33,650 - INFO -   bias >>> none
>>> 2025-09-11 10:34:33,650 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-11 10:34:33,651 - INFO -   random_state >>> 3407
>>> 2025-09-11 10:34:33,651 - INFO -   use_rslora >>> True
>>> 2025-09-11 10:34:33,651 - INFO -   loftq_config >>> None
>>> 2025-09-11 10:34:39,695 - INFO - 开始训练！
>>> 2025-09-11 10:35:10,164 - INFO - >>> {'loss': 3.5267, 'grad_norm': 1.9658596515655518, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-09-11 10:35:40,507 - INFO - >>> {'loss': 3.6737, 'grad_norm': 2.2726831436157227, 'learning_rate': 0.0002, 'epoch': 0.32}
>>> 2025-09-11 10:36:11,533 - INFO - >>> {'loss': 3.6748, 'grad_norm': 2.2940351963043213, 'learning_rate': 0.00019999594849888085, 'epoch': 0.48}
>>> 2025-09-11 10:36:41,080 - INFO - >>> {'loss': 3.594, 'grad_norm': 2.034668207168579, 'learning_rate': 0.0001999837943238166, 'epoch': 0.64}
>>> 2025-09-11 10:37:15,713 - INFO - >>> {'loss': 3.0726, 'grad_norm': 1.3554052114486694, 'learning_rate': 0.00019996353845966032, 'epoch': 0.8}
>>> 2025-09-11 10:37:47,303 - INFO - >>> {'loss': 2.8915, 'grad_norm': 1.606859803199768, 'learning_rate': 0.00019993518254774517, 'epoch': 0.96}
>>> 2025-09-11 10:37:54,806 - INFO - >>> {'loss': 3.019, 'grad_norm': 1.5141152143478394, 'learning_rate': 0.00019989872888575126, 'epoch': 1.0}
>>> 2025-09-11 10:38:24,778 - INFO - >>> {'loss': 2.6943, 'grad_norm': 0.9092156887054443, 'learning_rate': 0.00019985418042751975, 'epoch': 1.16}
>>> 2025-09-11 10:38:58,209 - INFO - >>> {'loss': 2.4444, 'grad_norm': 0.6310715675354004, 'learning_rate': 0.0001998015407828131, 'epoch': 1.32}
>>> 2025-09-11 10:39:28,799 - INFO - >>> {'loss': 2.4065, 'grad_norm': 0.8081472516059875, 'learning_rate': 0.00019974081421702294, 'epoch': 1.48}
>>> 2025-09-11 10:40:00,385 - INFO - >>> {'loss': 2.2126, 'grad_norm': 0.6666548252105713, 'learning_rate': 0.00019967200565082426, 'epoch': 1.6400000000000001}
>>> 2025-09-11 10:40:31,430 - INFO - >>> {'loss': 2.4124, 'grad_norm': 0.5671318769454956, 'learning_rate': 0.00019959512065977671, 'epoch': 1.8}
>>> 2025-09-11 10:41:01,387 - INFO - >>> {'loss': 2.2235, 'grad_norm': 0.7510217428207397, 'learning_rate': 0.00019951016547387288, 'epoch': 1.96}
>>> 2025-09-11 10:41:08,881 - INFO - >>> {'loss': 2.4387, 'grad_norm': 1.036710500717163, 'learning_rate': 0.00019941714697703332, 'epoch': 2.0}
>>> 2025-09-11 10:41:39,402 - INFO - >>> {'loss': 2.3184, 'grad_norm': 0.7614377737045288, 'learning_rate': 0.0001993160727065489, 'epoch': 2.16}
>>> 2025-09-11 10:42:09,345 - INFO - >>> {'loss': 2.2357, 'grad_norm': 0.8085991144180298, 'learning_rate': 0.0001992069508524701, 'epoch': 2.32}
>>> 2025-09-11 10:42:42,717 - INFO - >>> {'loss': 2.135, 'grad_norm': 0.7612737417221069, 'learning_rate': 0.0001990897902569431, 'epoch': 2.48}
>>> 2025-09-11 10:43:13,671 - INFO - >>> {'loss': 2.1524, 'grad_norm': 0.8248044848442078, 'learning_rate': 0.0001989646004134937, 'epoch': 2.64}
>>> 2025-09-11 10:43:43,606 - INFO - >>> {'loss': 2.0783, 'grad_norm': 0.7807849645614624, 'learning_rate': 0.0001988313914662576, 'epoch': 2.8}
>>> 2025-09-11 10:44:15,175 - INFO - >>> {'loss': 1.9295, 'grad_norm': 0.6611538529396057, 'learning_rate': 0.00019869017420915888, 'epoch': 2.96}
>>> 2025-09-11 10:44:22,666 - INFO - >>> {'loss': 2.16, 'grad_norm': 1.429587960243225, 'learning_rate': 0.00019854096008503494, 'epoch': 3.0}
>>> 2025-09-11 10:44:57,780 - INFO - >>> {'loss': 2.1258, 'grad_norm': 0.5416873097419739, 'learning_rate': 0.00019838376118470964, 'epoch': 3.16}
>>> 2025-09-11 10:45:29,450 - INFO - >>> {'loss': 2.1226, 'grad_norm': 0.6596843600273132, 'learning_rate': 0.00019821859024601345, 'epoch': 3.32}
>>> 2025-09-11 10:45:59,487 - INFO - >>> {'loss': 1.9227, 'grad_norm': 0.535672128200531, 'learning_rate': 0.00019804546065275112, 'epoch': 3.48}
>>> 2025-09-11 10:46:29,548 - INFO - >>> {'loss': 1.9067, 'grad_norm': 0.5068620443344116, 'learning_rate': 0.00019786438643361757, 'epoch': 3.64}
>>> 2025-09-11 10:46:59,511 - INFO - >>> {'loss': 1.7021, 'grad_norm': 0.8497135639190674, 'learning_rate': 0.00019767538226106077, 'epoch': 3.8}
>>> 2025-09-11 10:47:29,458 - INFO - >>> {'loss': 1.857, 'grad_norm': 0.8558863997459412, 'learning_rate': 0.00019747846345009306, 'epoch': 3.96}
>>> 2025-09-11 10:47:36,944 - INFO - >>> {'loss': 1.6417, 'grad_norm': 1.3822039365768433, 'learning_rate': 0.00019727364595705012, 'epoch': 4.0}
>>> 2025-09-11 10:48:06,897 - INFO - >>> {'loss': 1.9785, 'grad_norm': 0.7644791603088379, 'learning_rate': 0.00019706094637829798, 'epoch': 4.16}
>>> 2025-09-11 10:48:38,008 - INFO - >>> {'loss': 1.7092, 'grad_norm': 0.6023964881896973, 'learning_rate': 0.00019684038194888828, 'epoch': 4.32}
>>> 2025-09-11 10:49:09,615 - INFO - >>> {'loss': 1.7775, 'grad_norm': 0.6286163330078125, 'learning_rate': 0.00019661197054116164, 'epoch': 4.48}
>>> 2025-09-11 10:49:43,620 - INFO - >>> {'loss': 1.6855, 'grad_norm': 0.7387130260467529, 'learning_rate': 0.0001963757306632996, 'epoch': 4.64}
>>> 2025-09-11 10:50:13,631 - INFO - >>> {'loss': 1.8946, 'grad_norm': 0.7736541032791138, 'learning_rate': 0.00019613168145782468, 'epoch': 4.8}
>>> 2025-09-11 10:50:43,643 - INFO - >>> {'loss': 1.8363, 'grad_norm': 0.7545225620269775, 'learning_rate': 0.0001958798427000495, 'epoch': 4.96}
>>> 2025-09-11 10:50:51,137 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.00019562023479647426, 'epoch': 5.0}
>>> 2025-09-11 10:51:22,246 - INFO - >>> {'loss': 1.8818, 'grad_norm': 0.8059050440788269, 'learning_rate': 0.00019535287878313316, 'epoch': 5.16}
>>> 2025-09-11 10:51:52,830 - INFO - >>> {'loss': 1.5684, 'grad_norm': 0.8504809141159058, 'learning_rate': 0.00019507779632388996, 'epoch': 5.32}
>>> 2025-09-11 10:52:22,848 - INFO - >>> {'loss': 1.4029, 'grad_norm': 0.8398330211639404, 'learning_rate': 0.0001947950097086825, 'epoch': 5.48}
>>> 2025-09-11 10:52:52,834 - INFO - >>> {'loss': 1.4457, 'grad_norm': 1.2383679151535034, 'learning_rate': 0.00019450454185171648, 'epoch': 5.64}
>>> 2025-09-11 10:53:22,834 - INFO - >>> {'loss': 1.5913, 'grad_norm': 0.8671970367431641, 'learning_rate': 0.00019420641628960895, 'epoch': 5.8}
>>> 2025-09-11 10:53:57,897 - INFO - >>> {'loss': 1.704, 'grad_norm': 0.7888402938842773, 'learning_rate': 0.00019390065717948083, 'epoch': 5.96}
>>> 2025-09-11 10:54:05,385 - INFO - >>> {'loss': 1.9384, 'grad_norm': 2.83260440826416, 'learning_rate': 0.00019358728929699966, 'epoch': 6.0}
>>> 2025-09-11 10:54:38,814 - INFO - >>> {'loss': 1.4244, 'grad_norm': 0.9945796728134155, 'learning_rate': 0.00019326633803437194, 'epoch': 6.16}
>>> 2025-09-11 10:55:09,360 - INFO - >>> {'loss': 1.5332, 'grad_norm': 0.8621694445610046, 'learning_rate': 0.00019293782939828571, 'epoch': 6.32}
>>> 2025-09-11 10:55:39,258 - INFO - >>> {'loss': 1.0722, 'grad_norm': 2.254472255706787, 'learning_rate': 0.0001926017900078031, 'epoch': 6.48}
>>> 2025-09-11 10:56:09,260 - INFO - >>> {'loss': 1.5676, 'grad_norm': 0.9504251480102539, 'learning_rate': 0.00019225824709220342, 'epoch': 6.64}
>>> 2025-09-11 10:56:39,250 - INFO - >>> {'loss': 1.3661, 'grad_norm': 1.1892317533493042, 'learning_rate': 0.0001919072284887768, 'epoch': 6.8}
>>> 2025-09-11 10:57:11,989 - INFO - >>> {'loss': 1.47, 'grad_norm': 1.0780524015426636, 'learning_rate': 0.00019154876264056863, 'epoch': 6.96}
>>> 2025-09-11 10:57:19,531 - INFO - >>> {'loss': 1.3498, 'grad_norm': 1.5501073598861694, 'learning_rate': 0.0001911828785940745, 'epoch': 7.0}
>>> 2025-09-11 10:57:50,550 - INFO - >>> {'loss': 1.2964, 'grad_norm': 1.325596809387207, 'learning_rate': 0.0001908096059968869, 'epoch': 7.16}
>>> 2025-09-11 10:58:20,522 - INFO - >>> {'loss': 1.3919, 'grad_norm': 1.3569655418395996, 'learning_rate': 0.00019042897509529279, 'epoch': 7.32}
>>> 2025-09-11 10:58:52,080 - INFO - >>> {'loss': 1.3949, 'grad_norm': 1.462388277053833, 'learning_rate': 0.00019004101673182258, 'epoch': 7.48}
>>> 2025-09-11 10:59:22,068 - INFO - >>> {'loss': 1.3635, 'grad_norm': 1.5037425756454468, 'learning_rate': 0.0001896457623427512, 'epoch': 7.64}
>>> 2025-09-11 10:59:55,663 - INFO - >>> {'loss': 1.2219, 'grad_norm': 1.1843740940093994, 'learning_rate': 0.00018924324395555066, 'epoch': 7.8}
>>> 2025-09-11 11:00:25,624 - INFO - >>> {'loss': 1.0841, 'grad_norm': 1.5532667636871338, 'learning_rate': 0.00018883349418629484, 'epoch': 7.96}
>>> 2025-09-11 11:00:33,127 - INFO - >>> {'loss': 0.8708, 'grad_norm': 3.1011295318603516, 'learning_rate': 0.00018841654623701673, 'epoch': 8.0}
>>> 2025-09-11 11:01:03,687 - INFO - >>> {'loss': 1.2657, 'grad_norm': 1.3539565801620483, 'learning_rate': 0.00018799243389301798, 'epoch': 8.16}
>>> 2025-09-11 11:01:34,772 - INFO - >>> {'loss': 1.1289, 'grad_norm': 1.3587958812713623, 'learning_rate': 0.0001875611915201313, 'epoch': 8.32}
>>> 2025-09-11 11:02:04,762 - INFO - >>> {'loss': 0.963, 'grad_norm': 1.5627168416976929, 'learning_rate': 0.00018712285406193585, 'epoch': 8.48}
>>> 2025-09-11 11:02:34,762 - INFO - >>> {'loss': 0.8803, 'grad_norm': 1.6070375442504883, 'learning_rate': 0.00018667745703692572, 'epoch': 8.64}
>>> 2025-09-11 11:03:06,343 - INFO - >>> {'loss': 1.0104, 'grad_norm': 2.3818557262420654, 'learning_rate': 0.00018622503653563174, 'epoch': 8.8}
>>> 2025-09-11 11:03:39,806 - INFO - >>> {'loss': 1.0475, 'grad_norm': 1.9985781908035278, 'learning_rate': 0.00018576562921769727, 'epoch': 8.96}
>>> 2025-09-11 11:03:47,289 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0001852992723089076, 'epoch': 9.0}
>>> 2025-09-11 11:04:17,257 - INFO - >>> {'loss': 0.8578, 'grad_norm': 1.967407464981079, 'learning_rate': 0.00018482600359817343, 'epoch': 9.16}
>>> 2025-09-11 11:04:47,188 - INFO - >>> {'loss': 0.7562, 'grad_norm': 2.612180471420288, 'learning_rate': 0.00018434586143446907, 'epoch': 9.32}
>>> 2025-09-11 11:05:17,182 - INFO - >>> {'loss': 0.8983, 'grad_norm': 1.8380978107452393, 'learning_rate': 0.00018385888472372472, 'epoch': 9.48}
>>> 2025-09-11 11:05:52,232 - INFO - >>> {'loss': 0.81, 'grad_norm': 1.8819178342819214, 'learning_rate': 0.00018336511292567419, 'epoch': 9.64}
>>> 2025-09-11 11:06:23,254 - INFO - >>> {'loss': 0.825, 'grad_norm': 2.305511236190796, 'learning_rate': 0.0001828645860506573, 'epoch': 9.8}
>>> 2025-09-11 11:06:53,190 - INFO - >>> {'loss': 0.4593, 'grad_norm': 2.235903263092041, 'learning_rate': 0.00018235734465637794, 'epoch': 9.96}
>>> 2025-09-11 11:07:01,278 - INFO - >>> {'loss': 1.0749, 'grad_norm': 3.747859239578247, 'learning_rate': 0.00018184342984461766, 'epoch': 10.0}
>>> 2025-09-11 11:07:31,253 - INFO - >>> {'loss': 0.7047, 'grad_norm': 2.4763097763061523, 'learning_rate': 0.00018132288325790517, 'epoch': 10.16}
>>> 2025-09-11 11:08:04,689 - INFO - >>> {'loss': 0.6096, 'grad_norm': 2.1502649784088135, 'learning_rate': 0.00018079574707614203, 'epoch': 10.32}
>>> 2025-09-11 11:08:34,519 - INFO - >>> {'loss': 0.5659, 'grad_norm': 1.7767354249954224, 'learning_rate': 0.00018026206401318482, 'epoch': 10.48}
>>> 2025-09-11 11:09:04,497 - INFO - >>> {'loss': 0.6012, 'grad_norm': 3.5984323024749756, 'learning_rate': 0.0001797218773133841, 'epoch': 10.64}
>>> 2025-09-11 11:09:34,474 - INFO - >>> {'loss': 0.4872, 'grad_norm': 3.632216453552246, 'learning_rate': 0.00017917523074808023, 'epoch': 10.8}
>>> 2025-09-11 11:10:05,097 - INFO - >>> {'loss': 0.6432, 'grad_norm': 3.0667850971221924, 'learning_rate': 0.0001786221686120567, 'epoch': 10.96}
>>> 2025-09-11 11:10:14,217 - INFO - >>> {'loss': 0.7153, 'grad_norm': 5.2481865882873535, 'learning_rate': 0.00017806273571995066, 'epoch': 11.0}
>>> 2025-09-11 11:10:44,222 - INFO - >>> {'loss': 0.4598, 'grad_norm': 3.1106085777282715, 'learning_rate': 0.00017749697740262197, 'epoch': 11.16}
>>> 2025-09-11 11:11:16,975 - INFO - >>> {'loss': 0.4146, 'grad_norm': 2.3688180446624756, 'learning_rate': 0.0001769249395034797, 'epoch': 11.32}
>>> 2025-09-11 11:11:50,440 - INFO - >>> {'loss': 0.5064, 'grad_norm': 3.3239004611968994, 'learning_rate': 0.00017634666837476766, 'epoch': 11.48}
>>> 2025-09-11 11:12:20,470 - INFO - >>> {'loss': 0.3773, 'grad_norm': 2.3490474224090576, 'learning_rate': 0.0001757622108738083, 'epoch': 11.64}
>>> 2025-09-11 11:12:51,047 - INFO - >>> {'loss': 0.5544, 'grad_norm': 2.857971429824829, 'learning_rate': 0.00017517161435920607, 'epoch': 11.8}
>>> 2025-09-11 11:13:21,055 - INFO - >>> {'loss': 0.3512, 'grad_norm': 2.1702353954315186, 'learning_rate': 0.00017457492668700967, 'epoch': 11.96}
>>> 2025-09-11 11:13:28,554 - INFO - >>> {'loss': 0.337, 'grad_norm': 4.848863124847412, 'learning_rate': 0.00017397219620683465, 'epoch': 12.0}
>>> 2025-09-11 11:14:02,001 - INFO - >>> {'loss': 0.3256, 'grad_norm': 2.0589547157287598, 'learning_rate': 0.00017336347175794524, 'epoch': 12.16}
>>> 2025-09-11 11:14:33,591 - INFO - >>> {'loss': 0.2413, 'grad_norm': 2.4294300079345703, 'learning_rate': 0.00017274880266529715, 'epoch': 12.32}
>>> 2025-09-11 11:15:04,679 - INFO - >>> {'loss': 0.2299, 'grad_norm': 2.27378249168396, 'learning_rate': 0.0001721282387355408, 'epoch': 12.48}
>>> 2025-09-11 11:15:35,236 - INFO - >>> {'loss': 0.2775, 'grad_norm': 2.3852832317352295, 'learning_rate': 0.0001715018302529852, 'epoch': 12.64}
>>> 2025-09-11 11:16:05,206 - INFO - >>> {'loss': 0.2576, 'grad_norm': 2.7186405658721924, 'learning_rate': 0.00017086962797552375, 'epoch': 12.8}
>>> 2025-09-11 11:16:35,140 - INFO - >>> {'loss': 0.3514, 'grad_norm': 3.1779353618621826, 'learning_rate': 0.0001702316831305212, 'epoch': 12.96}
>>> 2025-09-11 11:16:42,628 - INFO - >>> {'loss': 0.326, 'grad_norm': 5.479020118713379, 'learning_rate': 0.00016958804741066253, 'epoch': 13.0}
>>> 2025-09-11 11:17:10,477 - INFO - >>> {'loss': 0.1972, 'grad_norm': 1.8425461053848267, 'learning_rate': 0.0001689387729697646, 'epoch': 13.16}
>>> 2025-09-11 11:17:40,408 - INFO - >>> {'loss': 0.1369, 'grad_norm': 3.382429599761963, 'learning_rate': 0.00016828391241854984, 'epoch': 13.32}
>>> 2025-09-11 11:18:12,003 - INFO - >>> {'loss': 0.2092, 'grad_norm': 1.6808475255966187, 'learning_rate': 0.0001676235188203834, 'epoch': 13.48}
>>> 2025-09-11 11:18:41,983 - INFO - >>> {'loss': 0.2013, 'grad_norm': 2.196190118789673, 'learning_rate': 0.00016695764568697328, 'epoch': 13.64}
>>> 2025-09-11 11:19:16,094 - INFO - >>> {'loss': 0.174, 'grad_norm': 1.912387490272522, 'learning_rate': 0.00016628634697403447, 'epoch': 13.8}
>>> 2025-09-11 11:19:47,217 - INFO - >>> {'loss': 0.1576, 'grad_norm': 2.168478488922119, 'learning_rate': 0.00016560967707691663, 'epoch': 13.96}
>>> 2025-09-11 11:19:54,734 - INFO - >>> {'loss': 0.1567, 'grad_norm': 8.14702320098877, 'learning_rate': 0.0001649276908261967, 'epoch': 14.0}
>>> 2025-09-11 11:20:24,767 - INFO - >>> {'loss': 0.1263, 'grad_norm': 1.6259676218032837, 'learning_rate': 0.00016424044348323582, 'epoch': 14.16}
>>> 2025-09-11 11:20:55,337 - INFO - >>> {'loss': 0.145, 'grad_norm': 2.0692977905273438, 'learning_rate': 0.0001635479907357016, 'epoch': 14.32}
>>> 2025-09-11 11:21:26,002 - INFO - >>> {'loss': 0.1226, 'grad_norm': 2.5156731605529785, 'learning_rate': 0.00016285038869305563, 'epoch': 14.48}
>>> 2025-09-11 11:21:57,529 - INFO - >>> {'loss': 0.1104, 'grad_norm': 2.137268543243408, 'learning_rate': 0.0001621476938820071, 'epoch': 14.64}
>>> 2025-09-11 11:22:27,430 - INFO - >>> {'loss': 0.0826, 'grad_norm': 1.6738786697387695, 'learning_rate': 0.00016143996324193225, 'epoch': 14.8}
>>> 2025-09-11 11:22:58,383 - INFO - >>> {'loss': 0.0691, 'grad_norm': 1.655430793762207, 'learning_rate': 0.00016072725412026066, 'epoch': 14.96}
>>> 2025-09-11 11:23:09,326 - INFO - >>> {'loss': 0.1489, 'grad_norm': 6.010675430297852, 'learning_rate': 0.00016000962426782845, 'epoch': 15.0}
>>> 2025-09-11 11:23:39,778 - INFO - >>> {'loss': 0.0791, 'grad_norm': 1.7132647037506104, 'learning_rate': 0.0001592871318341986, 'epoch': 15.16}
>>> 2025-09-11 11:24:10,773 - INFO - >>> {'loss': 0.0833, 'grad_norm': 1.6461827754974365, 'learning_rate': 0.0001585598353629492, 'epoch': 15.32}
>>> 2025-09-11 11:24:40,662 - INFO - >>> {'loss': 0.0271, 'grad_norm': 1.6675834655761719, 'learning_rate': 0.00015782779378692956, 'epoch': 15.48}
>>> 2025-09-11 11:25:12,220 - INFO - >>> {'loss': 0.0438, 'grad_norm': 1.2035503387451172, 'learning_rate': 0.000157091066423485, 'epoch': 15.64}
>>> 2025-09-11 11:25:42,194 - INFO - >>> {'loss': 0.0801, 'grad_norm': 1.9095494747161865, 'learning_rate': 0.0001563497129696503, 'epoch': 15.8}
>>> 2025-09-11 11:26:12,134 - INFO - >>> {'loss': 0.054, 'grad_norm': 1.3592337369918823, 'learning_rate': 0.00015560379349731233, 'epoch': 15.96}
>>> 2025-09-11 11:26:23,087 - INFO - >>> {'loss': 0.0821, 'grad_norm': 3.6217825412750244, 'learning_rate': 0.00015485336844834273, 'epoch': 16.0}
>>> 2025-09-11 11:26:53,689 - INFO - >>> {'loss': 0.0389, 'grad_norm': 0.8541739583015442, 'learning_rate': 0.00015409849862969995, 'epoch': 16.16}
>>> 2025-09-11 11:27:25,184 - INFO - >>> {'loss': 0.0254, 'grad_norm': 0.712313175201416, 'learning_rate': 0.00015333924520850227, 'epoch': 16.32}
>>> 2025-09-11 11:27:55,015 - INFO - >>> {'loss': 0.0487, 'grad_norm': 1.5718226432800293, 'learning_rate': 0.00015257566970707146, 'epoch': 16.48}
>>> 2025-09-11 11:28:25,948 - INFO - >>> {'loss': 0.0365, 'grad_norm': 1.0930871963500977, 'learning_rate': 0.0001518078339979475, 'epoch': 16.64}
>>> 2025-09-11 11:28:59,255 - INFO - >>> {'loss': 0.0399, 'grad_norm': 1.0880872011184692, 'learning_rate': 0.00015103580029887504, 'epoch': 16.8}
>>> 2025-09-11 11:29:29,070 - INFO - >>> {'loss': 0.0402, 'grad_norm': 1.7980459928512573, 'learning_rate': 0.00015025963116776202, 'epoch': 16.96}
>>> 2025-09-11 11:29:36,541 - INFO - >>> {'loss': 0.041, 'grad_norm': 1.7761400938034058, 'learning_rate': 0.00014947938949761054, 'epoch': 17.0}
>>> 2025-09-11 11:30:06,381 - INFO - >>> {'loss': 0.0163, 'grad_norm': 0.7042121291160583, 'learning_rate': 0.0001486951385114205, 'epoch': 17.16}
>>> 2025-09-11 11:30:39,842 - INFO - >>> {'loss': 0.0319, 'grad_norm': 1.5309256315231323, 'learning_rate': 0.00014790694175706697, 'epoch': 17.32}
>>> 2025-09-11 11:31:12,018 - INFO - >>> {'loss': 0.0252, 'grad_norm': 1.4597761631011963, 'learning_rate': 0.00014711486310215052, 'epoch': 17.48}
>>> 2025-09-11 11:31:43,133 - INFO - >>> {'loss': 0.0474, 'grad_norm': 1.6641697883605957, 'learning_rate': 0.00014631896672882234, 'epoch': 17.64}
>>> 2025-09-11 11:32:13,130 - INFO - >>> {'loss': 0.042, 'grad_norm': 1.8546522855758667, 'learning_rate': 0.00014551931712858334, 'epoch': 17.8}
>>> 2025-09-11 11:32:43,036 - INFO - >>> {'loss': 0.018, 'grad_norm': 2.8018925189971924, 'learning_rate': 0.00014471597909705857, 'epoch': 17.96}
>>> 2025-09-11 11:32:50,533 - INFO - >>> {'loss': 0.0267, 'grad_norm': 1.750018835067749, 'learning_rate': 0.00014390901772874667, 'epoch': 18.0}
>>> 2025-09-11 11:33:21,002 - INFO - >>> {'loss': 0.0178, 'grad_norm': 0.7378106713294983, 'learning_rate': 0.00014309849841174537, 'epoch': 18.16}
>>> 2025-09-11 11:33:53,654 - INFO - >>> {'loss': 0.0175, 'grad_norm': 0.5648723840713501, 'learning_rate': 0.0001422844868224531, 'epoch': 18.32}
>>> 2025-09-11 11:34:23,537 - INFO - >>> {'loss': 0.02, 'grad_norm': 1.047799825668335, 'learning_rate': 0.00014146704892024713, 'epoch': 18.48}
>>> 2025-09-11 11:34:53,430 - INFO - >>> {'loss': 0.0164, 'grad_norm': 1.0675241947174072, 'learning_rate': 0.000140646250942139, 'epoch': 18.64}
>>> 2025-09-11 11:35:23,283 - INFO - >>> {'loss': 0.0185, 'grad_norm': 1.5231674909591675, 'learning_rate': 0.00013982215939740725, 'epoch': 18.8}
>>> 2025-09-11 11:35:56,747 - INFO - >>> {'loss': 0.0381, 'grad_norm': 0.8654522895812988, 'learning_rate': 0.00013899484106220814, 'epoch': 18.96}
>>> 2025-09-11 11:36:04,239 - INFO - >>> {'loss': 0.032, 'grad_norm': 3.2375190258026123, 'learning_rate': 0.00013816436297416494, 'epoch': 19.0}
>>> 2025-09-11 11:36:38,814 - INFO - >>> {'loss': 0.0217, 'grad_norm': 0.7001802921295166, 'learning_rate': 0.00013733079242693572, 'epoch': 19.16}
>>> 2025-09-11 11:37:08,719 - INFO - >>> {'loss': 0.0087, 'grad_norm': 0.34722214937210083, 'learning_rate': 0.00013649419696476055, 'epoch': 19.32}
>>> 2025-09-11 11:37:37,156 - INFO - >>> {'loss': 0.0359, 'grad_norm': 1.740553855895996, 'learning_rate': 0.00013565464437698848, 'epoch': 19.48}
>>> 2025-09-11 11:38:08,633 - INFO - >>> {'loss': 0.0126, 'grad_norm': 1.2449798583984375, 'learning_rate': 0.00013481220269258447, 'epoch': 19.64}
>>> 2025-09-11 11:38:39,117 - INFO - >>> {'loss': 0.0175, 'grad_norm': 0.9950555562973022, 'learning_rate': 0.00013396694017461707, 'epoch': 19.8}
>>> 2025-09-11 11:39:08,926 - INFO - >>> {'loss': 0.0233, 'grad_norm': 1.402880311012268, 'learning_rate': 0.00013311892531472704, 'epoch': 19.96}
>>> 2025-09-11 11:39:16,391 - INFO - >>> {'loss': 0.0084, 'grad_norm': 0.4253495931625366, 'learning_rate': 0.00013226822682757745, 'epoch': 20.0}
>>> 2025-09-11 11:39:46,244 - INFO - >>> {'loss': 0.0151, 'grad_norm': 0.5356153249740601, 'learning_rate': 0.00013141491364528576, 'epoch': 20.16}
>>> 2025-09-11 11:40:19,712 - INFO - >>> {'loss': 0.012, 'grad_norm': 0.27873796224594116, 'learning_rate': 0.00013055905491183821, 'epoch': 20.32}
>>> 2025-09-11 11:40:50,339 - INFO - >>> {'loss': 0.015, 'grad_norm': 0.9641465544700623, 'learning_rate': 0.00012970071997748712, 'epoch': 20.48}
>>> 2025-09-11 11:41:21,931 - INFO - >>> {'loss': 0.009, 'grad_norm': 0.4948805272579193, 'learning_rate': 0.00012883997839313152, 'epoch': 20.64}
>>> 2025-09-11 11:41:51,941 - INFO - >>> {'loss': 0.0226, 'grad_norm': 1.5357372760772705, 'learning_rate': 0.00012797689990468112, 'epoch': 20.8}
>>> 2025-09-11 11:42:22,951 - INFO - >>> {'loss': 0.0109, 'grad_norm': 0.5700737833976746, 'learning_rate': 0.0001271115544474053, 'epoch': 20.96}
>>> 2025-09-11 11:42:30,476 - INFO - >>> {'loss': 0.0108, 'grad_norm': 0.5870547294616699, 'learning_rate': 0.00012624401214026573, 'epoch': 21.0}
>>> 2025-09-11 11:43:00,441 - INFO - >>> {'loss': 0.0094, 'grad_norm': 0.5077799558639526, 'learning_rate': 0.000125374343280235, 'epoch': 21.16}
>>> 2025-09-11 11:43:31,430 - INFO - >>> {'loss': 0.0067, 'grad_norm': 0.24074934422969818, 'learning_rate': 0.00012450261833660034, 'epoch': 21.32}
>>> 2025-09-11 11:44:01,335 - INFO - >>> {'loss': 0.0108, 'grad_norm': 0.45339155197143555, 'learning_rate': 0.0001236289079452534, 'epoch': 21.48}
>>> 2025-09-11 11:44:32,914 - INFO - >>> {'loss': 0.0098, 'grad_norm': 0.33470213413238525, 'learning_rate': 0.00012275328290296678, 'epoch': 21.64}
>>> 2025-09-11 11:45:02,880 - INFO - >>> {'loss': 0.0088, 'grad_norm': 0.2824455797672272, 'learning_rate': 0.00012187581416165721, 'epoch': 21.8}
>>> 2025-09-11 11:45:36,903 - INFO - >>> {'loss': 0.0136, 'grad_norm': 1.7900326251983643, 'learning_rate': 0.0001209965728226365, 'epoch': 21.96}
>>> 2025-09-11 11:45:44,414 - INFO - >>> {'loss': 0.0078, 'grad_norm': 0.5533862113952637, 'learning_rate': 0.00012011563013084996, 'epoch': 22.0}
>>> 2025-09-11 11:46:15,982 - INFO - >>> {'loss': 0.0073, 'grad_norm': 0.45967042446136475, 'learning_rate': 0.00011923305746910371, 'epoch': 22.16}
>>> 2025-09-11 11:46:49,476 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.34939324855804443, 'learning_rate': 0.00011834892635228023, 'epoch': 22.32}
>>> 2025-09-11 11:47:19,445 - INFO - >>> {'loss': 0.0057, 'grad_norm': 0.19885550439357758, 'learning_rate': 0.00011746330842154371, 'epoch': 22.48}
>>> 2025-09-11 11:47:49,410 - INFO - >>> {'loss': 0.0057, 'grad_norm': 0.28121641278266907, 'learning_rate': 0.00011657627543853491, 'epoch': 22.64}
>>> 2025-09-11 11:48:19,365 - INFO - >>> {'loss': 0.009, 'grad_norm': 0.2669544219970703, 'learning_rate': 0.0001156878992795563, 'epoch': 22.8}
>>> 2025-09-11 11:48:49,321 - INFO - >>> {'loss': 0.0114, 'grad_norm': 0.5278646945953369, 'learning_rate': 0.00011479825192974792, 'epoch': 22.96}
>>> 2025-09-11 11:48:56,811 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.35433030128479004, 'learning_rate': 0.00011390740547725443, 'epoch': 23.0}
>>> 2025-09-11 11:49:26,673 - INFO - >>> {'loss': 0.0076, 'grad_norm': 0.26228123903274536, 'learning_rate': 0.00011301543210738384, 'epoch': 23.16}
>>> 2025-09-11 11:49:56,506 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.14516717195510864, 'learning_rate': 0.00011212240409675825, 'epoch': 23.32}
>>> 2025-09-11 11:50:28,577 - INFO - >>> {'loss': 0.0058, 'grad_norm': 0.24280588328838348, 'learning_rate': 0.00011122839380745737, 'epoch': 23.48}
>>> 2025-09-11 11:50:59,548 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.11798213422298431, 'learning_rate': 0.00011033347368115494, 'epoch': 23.64}
>>> 2025-09-11 11:51:29,436 - INFO - >>> {'loss': 0.0063, 'grad_norm': 0.2182454764842987, 'learning_rate': 0.00010943771623324883, 'epoch': 23.8}
>>> 2025-09-11 11:51:59,300 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.11229334771633148, 'learning_rate': 0.00010854119404698511, 'epoch': 23.96}
>>> 2025-09-11 11:52:10,331 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.18311960995197296, 'learning_rate': 0.00010764397976757656, 'epoch': 24.0}
>>> 2025-09-11 11:52:40,284 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.10331477224826813, 'learning_rate': 0.00010674614609631634, 'epoch': 24.16}
>>> 2025-09-11 11:53:10,266 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.07722409069538116, 'learning_rate': 0.00010584776578468698, 'epoch': 24.32}
>>> 2025-09-11 11:53:41,920 - INFO - >>> {'loss': 0.006, 'grad_norm': 0.15943732857704163, 'learning_rate': 0.00010494891162846514, 'epoch': 24.48}
>>> 2025-09-11 11:54:15,327 - INFO - >>> {'loss': 0.0069, 'grad_norm': 0.2693643867969513, 'learning_rate': 0.0001040496564618233, 'epoch': 24.64}
>>> 2025-09-11 11:54:46,969 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.10213359445333481, 'learning_rate': 0.0001031500731514277, 'epoch': 24.8}
>>> 2025-09-11 11:55:16,873 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.06584019213914871, 'learning_rate': 0.00010225023459053415, 'epoch': 24.96}
>>> 2025-09-11 11:55:24,346 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.00010135021369308137, 'epoch': 25.0}
>>> 2025-09-11 11:55:54,226 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.04832179844379425, 'learning_rate': 0.00010045008338778279, 'epoch': 25.16}
>>> 2025-09-11 11:56:24,156 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.14261290431022644, 'learning_rate': 9.954991661221723e-05, 'epoch': 25.32}
>>> 2025-09-11 11:56:54,069 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.07498380541801453, 'learning_rate': 9.864978630691866e-05, 'epoch': 25.48}
>>> 2025-09-11 11:57:24,563 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.1537872701883316, 'learning_rate': 9.774976540946587e-05, 'epoch': 25.64}
>>> 2025-09-11 11:57:56,128 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.14275436103343964, 'learning_rate': 9.684992684857232e-05, 'epoch': 25.8}
>>> 2025-09-11 11:58:30,609 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.05830077826976776, 'learning_rate': 9.595034353817672e-05, 'epoch': 25.96}
>>> 2025-09-11 11:58:38,079 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.50510883715349e-05, 'epoch': 26.0}
>>> 2025-09-11 11:59:07,963 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.25066637992858887, 'learning_rate': 9.415223421531307e-05, 'epoch': 26.16}
>>> 2025-09-11 11:59:37,808 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.04770422354340553, 'learning_rate': 9.325385390368368e-05, 'epoch': 26.32}
>>> 2025-09-11 12:00:07,755 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.043186262249946594, 'learning_rate': 9.235602023242349e-05, 'epoch': 26.48}
>>> 2025-09-11 12:00:38,709 - INFO - >>> {'loss': 0.0057, 'grad_norm': 0.17636524140834808, 'learning_rate': 9.145880595301494e-05, 'epoch': 26.64}
>>> 2025-09-11 12:01:12,020 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.043118152767419815, 'learning_rate': 9.056228376675118e-05, 'epoch': 26.8}
>>> 2025-09-11 12:01:44,169 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.12235002964735031, 'learning_rate': 8.966652631884505e-05, 'epoch': 26.96}
>>> 2025-09-11 12:01:51,649 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.07698668539524078, 'learning_rate': 8.877160619254265e-05, 'epoch': 27.0}
>>> 2025-09-11 12:02:22,592 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.03672952204942703, 'learning_rate': 8.787759590324176e-05, 'epoch': 27.16}
>>> 2025-09-11 12:02:52,552 - INFO - >>> {'loss': 0.007, 'grad_norm': 0.2802730202674866, 'learning_rate': 8.698456789261616e-05, 'epoch': 27.32}
>>> 2025-09-11 12:03:22,606 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.02996201068162918, 'learning_rate': 8.609259452274558e-05, 'epoch': 27.48}
>>> 2025-09-11 12:03:58,354 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.12416058778762817, 'learning_rate': 8.520174807025209e-05, 'epoch': 27.64}
>>> 2025-09-11 12:04:28,352 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.04640360549092293, 'learning_rate': 8.43121007204437e-05, 'epoch': 27.8}
>>> 2025-09-11 12:04:58,241 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.190191388130188, 'learning_rate': 8.342372456146511e-05, 'epoch': 27.96}
>>> 2025-09-11 12:05:05,733 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.08354803174734116, 'learning_rate': 8.253669157845631e-05, 'epoch': 28.0}
>>> 2025-09-11 12:05:36,751 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.1462540626525879, 'learning_rate': 8.16510736477198e-05, 'epoch': 28.16}
>>> 2025-09-11 12:06:06,671 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.03843492269515991, 'learning_rate': 8.076694253089631e-05, 'epoch': 28.32}
>>> 2025-09-11 12:06:25,694 - INFO - 导入包完成
>>> 2025-09-11 12:06:25,695 - INFO - ========train Qwen2ForCausalLM  202509111206========
>>> 2025-09-11 12:06:25,695 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 12:06:25,696 - INFO - 开始进行训练
>>> 2025-09-11 12:06:25,702 - INFO - 基础配置文件读取完成
>>> 2025-09-11 12:06:25,709 - INFO - 训练配置读取完成
>>> 2025-09-11 12:06:25,710 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-09-11 12:06:25,710 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-09-11 12:06:26,158 - INFO - tokenizer读取完成
>>> 2025-09-11 12:06:26,312 - INFO - model dtype:torch.bfloat16
>>> 2025-09-11 12:06:26,313 - INFO - 模型导入完成
>>> 2025-09-11 12:06:26,313 - INFO - 数据读取开始
>>> 2025-09-11 12:06:27,107 - INFO - 数据下载完成，训练集大小: 100
>>> 2025-09-11 12:06:31,240 - INFO - 数据映射完成
>>> 2025-09-11 12:06:31,241 - INFO - 打印训练参数如下
>>> 2025-09-11 12:06:31,241 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-11 12:06:31,241 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-11 12:06:31,242 - INFO -   load_in_4bit >>> True
>>> 2025-09-11 12:06:31,242 - INFO -   batch_size >>> 4
>>> 2025-09-11 12:06:31,242 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-11 12:06:31,243 - INFO -   warmup_steps >>> 1
>>> 2025-09-11 12:06:31,243 - INFO -   epoch >>> 50
>>> 2025-09-11 12:06:31,243 - INFO -   eval_steps >>> 5
>>> 2025-09-11 12:06:31,244 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-11 12:06:31,244 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-11 12:06:31,245 - INFO -   max_seq_length >>> 2048
>>> 2025-09-11 12:06:31,245 - INFO -   r >>> 8
>>> 2025-09-11 12:06:31,245 - INFO -   interface_mode >>> False
>>> 2025-09-11 12:06:31,246 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-11 12:06:31,246 - INFO -   lora_alpha >>> 16
>>> 2025-09-11 12:06:31,246 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-11 12:06:31,247 - INFO -   bias >>> none
>>> 2025-09-11 12:06:31,247 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-11 12:06:31,247 - INFO -   random_state >>> 3407
>>> 2025-09-11 12:06:31,248 - INFO -   use_rslora >>> True
>>> 2025-09-11 12:06:31,248 - INFO -   loftq_config >>> None
>>> 2025-09-11 12:06:37,442 - INFO - 开始训练！
>>> 2025-09-11 12:07:07,933 - INFO - >>> {'loss': 3.5267, 'grad_norm': 2.054274320602417, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-09-11 12:07:38,464 - INFO - >>> {'loss': 3.6737, 'grad_norm': 2.331066846847534, 'learning_rate': 0.0002, 'epoch': 0.32}
>>> 2025-09-11 12:08:09,280 - INFO - >>> {'loss': 3.6764, 'grad_norm': 2.369567394256592, 'learning_rate': 0.00019999594849888085, 'epoch': 0.48}
>>> 2025-09-11 12:08:38,852 - INFO - >>> {'loss': 3.5879, 'grad_norm': 2.1660027503967285, 'learning_rate': 0.0001999837943238166, 'epoch': 0.64}
>>> 2025-09-11 12:09:13,439 - INFO - >>> {'loss': 3.0701, 'grad_norm': 1.3787002563476562, 'learning_rate': 0.00019996353845966032, 'epoch': 0.8}
>>> 2025-09-11 12:09:45,031 - INFO - >>> {'loss': 2.8887, 'grad_norm': 1.6304941177368164, 'learning_rate': 0.00019993518254774517, 'epoch': 0.96}
>>> 2025-09-11 12:09:52,531 - INFO - >>> {'loss': 3.0216, 'grad_norm': 1.5336902141571045, 'learning_rate': 0.00019989872888575126, 'epoch': 1.0}
>>> 2025-09-11 12:10:22,511 - INFO - >>> {'loss': 2.6907, 'grad_norm': 0.9210084676742554, 'learning_rate': 0.00019985418042751975, 'epoch': 1.16}
>>> 2025-09-11 12:10:55,858 - INFO - >>> {'loss': 2.4465, 'grad_norm': 0.6997708678245544, 'learning_rate': 0.0001998015407828131, 'epoch': 1.32}
>>> 2025-09-11 12:11:26,336 - INFO - >>> {'loss': 2.4042, 'grad_norm': 0.8098757863044739, 'learning_rate': 0.00019974081421702294, 'epoch': 1.48}
>>> 2025-09-11 12:11:57,782 - INFO - >>> {'loss': 2.2074, 'grad_norm': 0.6617834568023682, 'learning_rate': 0.00019967200565082426, 'epoch': 1.6400000000000001}
>>> 2025-09-11 12:12:28,692 - INFO - >>> {'loss': 2.4068, 'grad_norm': 0.5669006109237671, 'learning_rate': 0.00019959512065977671, 'epoch': 1.8}
>>> 2025-09-11 12:12:58,551 - INFO - >>> {'loss': 2.2278, 'grad_norm': 0.7690882086753845, 'learning_rate': 0.00019951016547387288, 'epoch': 1.96}
>>> 2025-09-11 12:13:06,019 - INFO - >>> {'loss': 2.444, 'grad_norm': 1.0498102903366089, 'learning_rate': 0.00019941714697703332, 'epoch': 2.0}
>>> 2025-09-11 12:13:36,516 - INFO - >>> {'loss': 2.3179, 'grad_norm': 0.7638331055641174, 'learning_rate': 0.0001993160727065489, 'epoch': 2.16}
>>> 2025-09-11 12:14:06,507 - INFO - >>> {'loss': 2.2348, 'grad_norm': 0.8032247424125671, 'learning_rate': 0.0001992069508524701, 'epoch': 2.32}
>>> 2025-09-11 12:14:39,957 - INFO - >>> {'loss': 2.1325, 'grad_norm': 0.7592586278915405, 'learning_rate': 0.0001990897902569431, 'epoch': 2.48}
>>> 2025-09-11 12:15:10,940 - INFO - >>> {'loss': 2.1516, 'grad_norm': 0.832562267780304, 'learning_rate': 0.0001989646004134937, 'epoch': 2.64}
>>> 2025-09-11 12:15:40,874 - INFO - >>> {'loss': 2.0754, 'grad_norm': 0.8004792332649231, 'learning_rate': 0.0001988313914662576, 'epoch': 2.8}
>>> 2025-09-11 12:16:12,369 - INFO - >>> {'loss': 1.926, 'grad_norm': 0.6680799722671509, 'learning_rate': 0.00019869017420915888, 'epoch': 2.96}
>>> 2025-09-11 12:16:19,825 - INFO - >>> {'loss': 2.1526, 'grad_norm': 1.4376411437988281, 'learning_rate': 0.00019854096008503494, 'epoch': 3.0}
>>> 2025-09-11 12:16:54,838 - INFO - >>> {'loss': 2.1232, 'grad_norm': 0.5399423241615295, 'learning_rate': 0.00019838376118470964, 'epoch': 3.16}
>>> 2025-09-11 12:17:26,370 - INFO - >>> {'loss': 2.119, 'grad_norm': 0.6566309332847595, 'learning_rate': 0.00019821859024601345, 'epoch': 3.32}
>>> 2025-09-11 12:17:56,273 - INFO - >>> {'loss': 1.9161, 'grad_norm': 0.5375581979751587, 'learning_rate': 0.00019804546065275112, 'epoch': 3.48}
>>> 2025-09-11 12:18:26,237 - INFO - >>> {'loss': 1.9064, 'grad_norm': 0.5259333252906799, 'learning_rate': 0.00019786438643361757, 'epoch': 3.64}
>>> 2025-09-11 12:18:56,063 - INFO - >>> {'loss': 1.7003, 'grad_norm': 0.8685925602912903, 'learning_rate': 0.00019767538226106077, 'epoch': 3.8}
>>> 2025-09-11 12:19:25,933 - INFO - >>> {'loss': 1.8603, 'grad_norm': 0.8677660226821899, 'learning_rate': 0.00019747846345009306, 'epoch': 3.96}
>>> 2025-09-11 12:19:33,395 - INFO - >>> {'loss': 1.6346, 'grad_norm': 1.3723894357681274, 'learning_rate': 0.00019727364595705012, 'epoch': 4.0}
>>> 2025-09-11 12:20:03,275 - INFO - >>> {'loss': 1.9802, 'grad_norm': 0.7835268378257751, 'learning_rate': 0.00019706094637829798, 'epoch': 4.16}
>>> 2025-09-11 12:20:34,341 - INFO - >>> {'loss': 1.7038, 'grad_norm': 0.6075997948646545, 'learning_rate': 0.00019684038194888828, 'epoch': 4.32}
>>> 2025-09-11 12:21:05,823 - INFO - >>> {'loss': 1.7703, 'grad_norm': 0.6267790198326111, 'learning_rate': 0.00019661197054116164, 'epoch': 4.48}
>>> 2025-09-11 12:21:39,728 - INFO - >>> {'loss': 1.685, 'grad_norm': 0.7510702013969421, 'learning_rate': 0.0001963757306632996, 'epoch': 4.64}
>>> 2025-09-11 12:22:09,566 - INFO - >>> {'loss': 1.8923, 'grad_norm': 0.7877790331840515, 'learning_rate': 0.00019613168145782468, 'epoch': 4.8}
>>> 2025-09-11 12:22:39,393 - INFO - >>> {'loss': 1.8334, 'grad_norm': 0.7791215777397156, 'learning_rate': 0.0001958798427000495, 'epoch': 4.96}
>>> 2025-09-11 12:22:46,849 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.00019562023479647426, 'epoch': 5.0}
>>> 2025-09-11 12:23:17,761 - INFO - >>> {'loss': 1.8774, 'grad_norm': 0.7973801493644714, 'learning_rate': 0.00019535287878313316, 'epoch': 5.16}
>>> 2025-09-11 12:23:48,150 - INFO - >>> {'loss': 1.5645, 'grad_norm': 0.8499245643615723, 'learning_rate': 0.00019507779632388996, 'epoch': 5.32}
>>> 2025-09-11 12:24:18,075 - INFO - >>> {'loss': 1.3987, 'grad_norm': 0.8431214094161987, 'learning_rate': 0.0001947950097086825, 'epoch': 5.48}
>>> 2025-09-11 12:24:48,041 - INFO - >>> {'loss': 1.4354, 'grad_norm': 1.2321933507919312, 'learning_rate': 0.00019450454185171648, 'epoch': 5.64}
>>> 2025-09-11 12:25:18,056 - INFO - >>> {'loss': 1.582, 'grad_norm': 0.8541808724403381, 'learning_rate': 0.00019420641628960895, 'epoch': 5.8}
>>> 2025-09-11 12:25:53,144 - INFO - >>> {'loss': 1.6993, 'grad_norm': 0.794236421585083, 'learning_rate': 0.00019390065717948083, 'epoch': 5.96}
>>> 2025-09-11 12:26:00,630 - INFO - >>> {'loss': 1.9119, 'grad_norm': 2.883087635040283, 'learning_rate': 0.00019358728929699966, 'epoch': 6.0}
>>> 2025-09-11 12:26:34,042 - INFO - >>> {'loss': 1.4141, 'grad_norm': 0.9856309294700623, 'learning_rate': 0.00019326633803437194, 'epoch': 6.16}
>>> 2025-09-11 12:27:04,593 - INFO - >>> {'loss': 1.5228, 'grad_norm': 0.8693742752075195, 'learning_rate': 0.00019293782939828571, 'epoch': 6.32}
>>> 2025-09-11 12:27:34,434 - INFO - >>> {'loss': 1.084, 'grad_norm': 2.250469207763672, 'learning_rate': 0.0001926017900078031, 'epoch': 6.48}
>>> 2025-09-11 12:28:04,380 - INFO - >>> {'loss': 1.5636, 'grad_norm': 0.9651551842689514, 'learning_rate': 0.00019225824709220342, 'epoch': 6.64}
>>> 2025-09-11 12:28:34,273 - INFO - >>> {'loss': 1.3556, 'grad_norm': 1.2028998136520386, 'learning_rate': 0.0001919072284887768, 'epoch': 6.8}
>>> 2025-09-11 12:29:06,905 - INFO - >>> {'loss': 1.4662, 'grad_norm': 1.093441128730774, 'learning_rate': 0.00019154876264056863, 'epoch': 6.96}
>>> 2025-09-11 12:29:14,442 - INFO - >>> {'loss': 1.3249, 'grad_norm': 1.5736403465270996, 'learning_rate': 0.0001911828785940745, 'epoch': 7.0}
>>> 2025-09-11 12:29:45,379 - INFO - >>> {'loss': 1.2883, 'grad_norm': 1.3488770723342896, 'learning_rate': 0.0001908096059968869, 'epoch': 7.16}
>>> 2025-09-11 12:30:15,332 - INFO - >>> {'loss': 1.3762, 'grad_norm': 1.3740907907485962, 'learning_rate': 0.00019042897509529279, 'epoch': 7.32}
>>> 2025-09-11 12:30:46,899 - INFO - >>> {'loss': 1.3886, 'grad_norm': 1.5020307302474976, 'learning_rate': 0.00019004101673182258, 'epoch': 7.48}
>>> 2025-09-11 12:31:16,865 - INFO - >>> {'loss': 1.3665, 'grad_norm': 1.5894073247909546, 'learning_rate': 0.0001896457623427512, 'epoch': 7.64}
>>> 2025-09-11 12:31:50,433 - INFO - >>> {'loss': 1.2051, 'grad_norm': 1.1889610290527344, 'learning_rate': 0.00018924324395555066, 'epoch': 7.8}
>>> 2025-09-11 12:32:20,345 - INFO - >>> {'loss': 1.0667, 'grad_norm': 1.6556535959243774, 'learning_rate': 0.00018883349418629484, 'epoch': 7.96}
>>> 2025-09-11 12:32:27,823 - INFO - >>> {'loss': 0.8759, 'grad_norm': 3.2412335872650146, 'learning_rate': 0.00018841654623701673, 'epoch': 8.0}
>>> 2025-09-11 12:32:58,307 - INFO - >>> {'loss': 1.2593, 'grad_norm': 1.3796274662017822, 'learning_rate': 0.00018799243389301798, 'epoch': 8.16}
>>> 2025-09-11 12:33:29,249 - INFO - >>> {'loss': 1.1196, 'grad_norm': 1.401969313621521, 'learning_rate': 0.0001875611915201313, 'epoch': 8.32}
>>> 2025-09-11 12:33:59,077 - INFO - >>> {'loss': 0.9496, 'grad_norm': 1.6043658256530762, 'learning_rate': 0.00018712285406193585, 'epoch': 8.48}
>>> 2025-09-11 12:34:28,934 - INFO - >>> {'loss': 0.8711, 'grad_norm': 1.6302578449249268, 'learning_rate': 0.00018667745703692572, 'epoch': 8.64}
>>> 2025-09-11 12:35:00,384 - INFO - >>> {'loss': 0.9996, 'grad_norm': 2.3347041606903076, 'learning_rate': 0.00018622503653563174, 'epoch': 8.8}
>>> 2025-09-11 12:35:33,781 - INFO - >>> {'loss': 1.0299, 'grad_norm': 2.0166332721710205, 'learning_rate': 0.00018576562921769727, 'epoch': 8.96}
>>> 2025-09-11 12:35:41,260 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0001852992723089076, 'epoch': 9.0}
>>> 2025-09-11 12:36:11,189 - INFO - >>> {'loss': 0.8422, 'grad_norm': 2.147286891937256, 'learning_rate': 0.00018482600359817343, 'epoch': 9.16}
>>> 2025-09-11 12:36:41,074 - INFO - >>> {'loss': 0.7296, 'grad_norm': 2.5557236671447754, 'learning_rate': 0.00018434586143446907, 'epoch': 9.32}
>>> 2025-09-11 12:37:11,122 - INFO - >>> {'loss': 0.8742, 'grad_norm': 1.9215683937072754, 'learning_rate': 0.00018385888472372472, 'epoch': 9.48}
>>> 2025-09-11 12:37:46,186 - INFO - >>> {'loss': 0.7937, 'grad_norm': 1.943729043006897, 'learning_rate': 0.00018336511292567419, 'epoch': 9.64}
>>> 2025-09-11 12:38:17,255 - INFO - >>> {'loss': 0.8006, 'grad_norm': 2.307382106781006, 'learning_rate': 0.0001828645860506573, 'epoch': 9.8}
>>> 2025-09-11 12:38:47,232 - INFO - >>> {'loss': 0.458, 'grad_norm': 2.3332793712615967, 'learning_rate': 0.00018235734465637794, 'epoch': 9.96}
>>> 2025-09-11 12:38:55,332 - INFO - >>> {'loss': 1.0721, 'grad_norm': 3.883129358291626, 'learning_rate': 0.00018184342984461766, 'epoch': 10.0}
>>> 2025-09-11 12:39:25,341 - INFO - >>> {'loss': 0.6704, 'grad_norm': 2.6694443225860596, 'learning_rate': 0.00018132288325790517, 'epoch': 10.16}
>>> 2025-09-11 12:39:58,827 - INFO - >>> {'loss': 0.5881, 'grad_norm': 2.1786062717437744, 'learning_rate': 0.00018079574707614203, 'epoch': 10.32}
>>> 2025-09-11 12:40:28,621 - INFO - >>> {'loss': 0.5488, 'grad_norm': 1.7714574337005615, 'learning_rate': 0.00018026206401318482, 'epoch': 10.48}
>>> 2025-09-11 12:40:58,536 - INFO - >>> {'loss': 0.5449, 'grad_norm': 3.0541768074035645, 'learning_rate': 0.0001797218773133841, 'epoch': 10.64}
>>> 2025-09-11 12:41:28,422 - INFO - >>> {'loss': 0.4756, 'grad_norm': 4.251870155334473, 'learning_rate': 0.00017917523074808023, 'epoch': 10.8}
>>> 2025-09-11 12:41:58,987 - INFO - >>> {'loss': 0.6212, 'grad_norm': 2.912803888320923, 'learning_rate': 0.0001786221686120567, 'epoch': 10.96}
>>> 2025-09-11 12:42:08,104 - INFO - >>> {'loss': 0.7419, 'grad_norm': 6.033134460449219, 'learning_rate': 0.00017806273571995066, 'epoch': 11.0}
>>> 2025-09-11 12:42:38,017 - INFO - >>> {'loss': 0.4364, 'grad_norm': 2.9909448623657227, 'learning_rate': 0.00017749697740262197, 'epoch': 11.16}
>>> 2025-09-11 12:43:10,693 - INFO - >>> {'loss': 0.3939, 'grad_norm': 3.1091861724853516, 'learning_rate': 0.0001769249395034797, 'epoch': 11.32}
>>> 2025-09-11 12:43:44,045 - INFO - >>> {'loss': 0.4712, 'grad_norm': 3.5601301193237305, 'learning_rate': 0.00017634666837476766, 'epoch': 11.48}
>>> 2025-09-11 12:44:13,947 - INFO - >>> {'loss': 0.358, 'grad_norm': 2.3383002281188965, 'learning_rate': 0.0001757622108738083, 'epoch': 11.64}
>>> 2025-09-11 12:44:44,353 - INFO - >>> {'loss': 0.5398, 'grad_norm': 3.095392942428589, 'learning_rate': 0.00017517161435920607, 'epoch': 11.8}
>>> 2025-09-11 12:45:14,198 - INFO - >>> {'loss': 0.3435, 'grad_norm': 2.5034356117248535, 'learning_rate': 0.00017457492668700967, 'epoch': 11.96}
>>> 2025-09-11 12:45:21,654 - INFO - >>> {'loss': 0.3702, 'grad_norm': 9.617463111877441, 'learning_rate': 0.00017397219620683465, 'epoch': 12.0}
>>> 2025-09-11 12:45:54,923 - INFO - >>> {'loss': 0.3298, 'grad_norm': 2.5159313678741455, 'learning_rate': 0.00017336347175794524, 'epoch': 12.16}
>>> 2025-09-11 12:46:26,377 - INFO - >>> {'loss': 0.2585, 'grad_norm': 2.922471761703491, 'learning_rate': 0.00017274880266529715, 'epoch': 12.32}
>>> 2025-09-11 12:46:57,344 - INFO - >>> {'loss': 0.2344, 'grad_norm': 2.8105759620666504, 'learning_rate': 0.0001721282387355408, 'epoch': 12.48}
>>> 2025-09-11 12:47:27,827 - INFO - >>> {'loss': 0.2563, 'grad_norm': 2.0277445316314697, 'learning_rate': 0.0001715018302529852, 'epoch': 12.64}
>>> 2025-09-11 12:47:57,751 - INFO - >>> {'loss': 0.2429, 'grad_norm': 2.2382164001464844, 'learning_rate': 0.00017086962797552375, 'epoch': 12.8}
>>> 2025-09-11 12:48:27,682 - INFO - >>> {'loss': 0.323, 'grad_norm': 3.6083507537841797, 'learning_rate': 0.0001702316831305212, 'epoch': 12.96}
>>> 2025-09-11 12:48:35,174 - INFO - >>> {'loss': 0.364, 'grad_norm': 7.008806228637695, 'learning_rate': 0.00016958804741066253, 'epoch': 13.0}
>>> 2025-09-11 12:49:03,026 - INFO - >>> {'loss': 0.1585, 'grad_norm': 1.5576478242874146, 'learning_rate': 0.0001689387729697646, 'epoch': 13.16}
>>> 2025-09-11 12:49:32,998 - INFO - >>> {'loss': 0.1266, 'grad_norm': 2.611786365509033, 'learning_rate': 0.00016828391241854984, 'epoch': 13.32}
>>> 2025-09-11 12:50:04,599 - INFO - >>> {'loss': 0.1996, 'grad_norm': 1.8192219734191895, 'learning_rate': 0.0001676235188203834, 'epoch': 13.48}
>>> 2025-09-11 12:50:34,558 - INFO - >>> {'loss': 0.1931, 'grad_norm': 2.1822123527526855, 'learning_rate': 0.00016695764568697328, 'epoch': 13.64}
>>> 2025-09-11 12:51:08,612 - INFO - >>> {'loss': 0.1811, 'grad_norm': 2.149651288986206, 'learning_rate': 0.00016628634697403447, 'epoch': 13.8}
>>> 2025-09-11 12:51:39,687 - INFO - >>> {'loss': 0.1726, 'grad_norm': 2.2050392627716064, 'learning_rate': 0.00016560967707691663, 'epoch': 13.96}
>>> 2025-09-11 12:51:47,193 - INFO - >>> {'loss': 0.2409, 'grad_norm': 7.359114646911621, 'learning_rate': 0.0001649276908261967, 'epoch': 14.0}
>>> 2025-09-11 12:52:17,159 - INFO - >>> {'loss': 0.1224, 'grad_norm': 1.564581036567688, 'learning_rate': 0.00016424044348323582, 'epoch': 14.16}
>>> 2025-09-11 12:52:47,703 - INFO - >>> {'loss': 0.1522, 'grad_norm': 2.4493167400360107, 'learning_rate': 0.0001635479907357016, 'epoch': 14.32}
>>> 2025-09-11 12:53:18,367 - INFO - >>> {'loss': 0.0954, 'grad_norm': 1.8221421241760254, 'learning_rate': 0.00016285038869305563, 'epoch': 14.48}
>>> 2025-09-11 12:53:49,969 - INFO - >>> {'loss': 0.0948, 'grad_norm': 1.7030154466629028, 'learning_rate': 0.0001621476938820071, 'epoch': 14.64}
>>> 2025-09-11 12:54:19,924 - INFO - >>> {'loss': 0.0773, 'grad_norm': 1.872058629989624, 'learning_rate': 0.00016143996324193225, 'epoch': 14.8}
>>> 2025-09-11 12:54:50,923 - INFO - >>> {'loss': 0.0779, 'grad_norm': 1.8579107522964478, 'learning_rate': 0.00016072725412026066, 'epoch': 14.96}
>>> 2025-09-11 12:55:01,879 - INFO - >>> {'loss': 0.1462, 'grad_norm': 4.775940895080566, 'learning_rate': 0.00016000962426782845, 'epoch': 15.0}
>>> 2025-09-11 12:55:32,400 - INFO - >>> {'loss': 0.0671, 'grad_norm': 2.1281800270080566, 'learning_rate': 0.0001592871318341986, 'epoch': 15.16}
>>> 2025-09-11 12:56:03,414 - INFO - >>> {'loss': 0.0714, 'grad_norm': 1.26529061794281, 'learning_rate': 0.0001585598353629492, 'epoch': 15.32}
>>> 2025-09-11 12:56:33,320 - INFO - >>> {'loss': 0.0286, 'grad_norm': 1.0534956455230713, 'learning_rate': 0.00015782779378692956, 'epoch': 15.48}
>>> 2025-09-11 12:57:04,843 - INFO - >>> {'loss': 0.0587, 'grad_norm': 1.9883657693862915, 'learning_rate': 0.000157091066423485, 'epoch': 15.64}
>>> 2025-09-11 12:57:34,801 - INFO - >>> {'loss': 0.0723, 'grad_norm': 2.100830078125, 'learning_rate': 0.0001563497129696503, 'epoch': 15.8}
>>> 2025-09-11 12:58:04,662 - INFO - >>> {'loss': 0.062, 'grad_norm': 1.5827175378799438, 'learning_rate': 0.00015560379349731233, 'epoch': 15.96}
>>> 2025-09-11 12:58:15,586 - INFO - >>> {'loss': 0.072, 'grad_norm': 2.968287944793701, 'learning_rate': 0.00015485336844834273, 'epoch': 16.0}
>>> 2025-09-11 12:58:46,128 - INFO - >>> {'loss': 0.0368, 'grad_norm': 0.7754208445549011, 'learning_rate': 0.00015409849862969995, 'epoch': 16.16}
>>> 2025-09-11 12:59:17,583 - INFO - >>> {'loss': 0.0306, 'grad_norm': 0.801277220249176, 'learning_rate': 0.00015333924520850227, 'epoch': 16.32}
>>> 2025-09-11 12:59:47,420 - INFO - >>> {'loss': 0.0492, 'grad_norm': 1.165579080581665, 'learning_rate': 0.00015257566970707146, 'epoch': 16.48}
>>> 2025-09-11 13:00:18,348 - INFO - >>> {'loss': 0.0432, 'grad_norm': 2.059218168258667, 'learning_rate': 0.0001518078339979475, 'epoch': 16.64}
>>> 2025-09-11 13:00:51,702 - INFO - >>> {'loss': 0.0355, 'grad_norm': 1.3500832319259644, 'learning_rate': 0.00015103580029887504, 'epoch': 16.8}
>>> 2025-09-11 13:01:21,562 - INFO - >>> {'loss': 0.0556, 'grad_norm': 3.302307367324829, 'learning_rate': 0.00015025963116776202, 'epoch': 16.96}
>>> 2025-09-11 13:01:29,024 - INFO - >>> {'loss': 0.0513, 'grad_norm': 3.1063334941864014, 'learning_rate': 0.00014947938949761054, 'epoch': 17.0}
>>> 2025-09-11 13:01:58,918 - INFO - >>> {'loss': 0.0235, 'grad_norm': 1.241755723953247, 'learning_rate': 0.0001486951385114205, 'epoch': 17.16}
>>> 2025-09-11 13:02:32,389 - INFO - >>> {'loss': 0.025, 'grad_norm': 0.9987509250640869, 'learning_rate': 0.00014790694175706697, 'epoch': 17.32}
>>> 2025-09-11 13:03:04,553 - INFO - >>> {'loss': 0.0206, 'grad_norm': 0.6498962640762329, 'learning_rate': 0.00014711486310215052, 'epoch': 17.48}
>>> 2025-09-11 13:03:35,669 - INFO - >>> {'loss': 0.0424, 'grad_norm': 2.085941791534424, 'learning_rate': 0.00014631896672882234, 'epoch': 17.64}
>>> 2025-09-11 13:04:05,687 - INFO - >>> {'loss': 0.034, 'grad_norm': 2.0450868606567383, 'learning_rate': 0.00014551931712858334, 'epoch': 17.8}
>>> 2025-09-11 13:04:35,659 - INFO - >>> {'loss': 0.0235, 'grad_norm': 1.830849289894104, 'learning_rate': 0.00014471597909705857, 'epoch': 17.96}
>>> 2025-09-11 13:04:43,157 - INFO - >>> {'loss': 0.0199, 'grad_norm': 1.3155795335769653, 'learning_rate': 0.00014390901772874667, 'epoch': 18.0}
>>> 2025-09-11 13:05:13,680 - INFO - >>> {'loss': 0.0152, 'grad_norm': 0.6849524974822998, 'learning_rate': 0.00014309849841174537, 'epoch': 18.16}
>>> 2025-09-11 13:05:46,369 - INFO - >>> {'loss': 0.0178, 'grad_norm': 0.7239314913749695, 'learning_rate': 0.0001422844868224531, 'epoch': 18.32}
>>> 2025-09-11 13:06:16,317 - INFO - >>> {'loss': 0.0271, 'grad_norm': 3.5275511741638184, 'learning_rate': 0.00014146704892024713, 'epoch': 18.48}
>>> 2025-09-11 13:06:46,271 - INFO - >>> {'loss': 0.0182, 'grad_norm': 1.4349814653396606, 'learning_rate': 0.000140646250942139, 'epoch': 18.64}
>>> 2025-09-11 13:07:16,214 - INFO - >>> {'loss': 0.0397, 'grad_norm': 3.4787533283233643, 'learning_rate': 0.00013982215939740725, 'epoch': 18.8}
>>> 2025-09-11 13:07:49,719 - INFO - >>> {'loss': 0.0399, 'grad_norm': 1.459596037864685, 'learning_rate': 0.00013899484106220814, 'epoch': 18.96}
>>> 2025-09-11 13:07:57,227 - INFO - >>> {'loss': 0.0364, 'grad_norm': 1.4893286228179932, 'learning_rate': 0.00013816436297416494, 'epoch': 19.0}
>>> 2025-09-11 13:08:31,788 - INFO - >>> {'loss': 0.0208, 'grad_norm': 0.7569593191146851, 'learning_rate': 0.00013733079242693572, 'epoch': 19.16}
>>> 2025-09-11 13:09:01,673 - INFO - >>> {'loss': 0.0166, 'grad_norm': 1.7042429447174072, 'learning_rate': 0.00013649419696476055, 'epoch': 19.32}
>>> 2025-09-11 13:09:30,081 - INFO - >>> {'loss': 0.016, 'grad_norm': 0.7492533922195435, 'learning_rate': 0.00013565464437698848, 'epoch': 19.48}
>>> 2025-09-11 13:10:01,533 - INFO - >>> {'loss': 0.0184, 'grad_norm': 1.0863893032073975, 'learning_rate': 0.00013481220269258447, 'epoch': 19.64}
>>> 2025-09-11 13:10:32,029 - INFO - >>> {'loss': 0.015, 'grad_norm': 0.6934711337089539, 'learning_rate': 0.00013396694017461707, 'epoch': 19.8}
>>> 2025-09-11 13:11:01,854 - INFO - >>> {'loss': 0.0168, 'grad_norm': 0.9995949864387512, 'learning_rate': 0.00013311892531472704, 'epoch': 19.96}
>>> 2025-09-11 13:11:09,313 - INFO - >>> {'loss': 0.0139, 'grad_norm': 2.2852604389190674, 'learning_rate': 0.00013226822682757745, 'epoch': 20.0}
>>> 2025-09-11 13:11:39,136 - INFO - >>> {'loss': 0.0147, 'grad_norm': 0.81910240650177, 'learning_rate': 0.00013141491364528576, 'epoch': 20.16}
>>> 2025-09-11 13:12:12,459 - INFO - >>> {'loss': 0.0124, 'grad_norm': 0.5110061168670654, 'learning_rate': 0.00013055905491183821, 'epoch': 20.32}
>>> 2025-09-11 13:12:42,972 - INFO - >>> {'loss': 0.0137, 'grad_norm': 0.7947089076042175, 'learning_rate': 0.00012970071997748712, 'epoch': 20.48}
>>> 2025-09-11 13:13:14,522 - INFO - >>> {'loss': 0.0077, 'grad_norm': 0.5104238390922546, 'learning_rate': 0.00012883997839313152, 'epoch': 20.64}
>>> 2025-09-11 13:13:44,490 - INFO - >>> {'loss': 0.0198, 'grad_norm': 0.9811960458755493, 'learning_rate': 0.00012797689990468112, 'epoch': 20.8}
>>> 2025-09-11 13:14:15,502 - INFO - >>> {'loss': 0.0106, 'grad_norm': 0.5398973226547241, 'learning_rate': 0.0001271115544474053, 'epoch': 20.96}
>>> 2025-09-11 13:14:23,037 - INFO - >>> {'loss': 0.0179, 'grad_norm': 1.0012208223342896, 'learning_rate': 0.00012624401214026573, 'epoch': 21.0}
>>> 2025-09-11 13:14:53,052 - INFO - >>> {'loss': 0.0087, 'grad_norm': 0.2527499198913574, 'learning_rate': 0.000125374343280235, 'epoch': 21.16}
>>> 2025-09-11 13:15:24,116 - INFO - >>> {'loss': 0.0075, 'grad_norm': 0.24303923547267914, 'learning_rate': 0.00012450261833660034, 'epoch': 21.32}
>>> 2025-09-11 13:15:54,105 - INFO - >>> {'loss': 0.0131, 'grad_norm': 0.4665089249610901, 'learning_rate': 0.0001236289079452534, 'epoch': 21.48}
>>> 2025-09-11 13:16:25,693 - INFO - >>> {'loss': 0.0205, 'grad_norm': 0.8497178554534912, 'learning_rate': 0.00012275328290296678, 'epoch': 21.64}
>>> 2025-09-11 13:16:55,664 - INFO - >>> {'loss': 0.0097, 'grad_norm': 0.4123755693435669, 'learning_rate': 0.00012187581416165721, 'epoch': 21.8}
>>> 2025-09-11 13:17:29,674 - INFO - >>> {'loss': 0.0075, 'grad_norm': 0.4044021964073181, 'learning_rate': 0.0001209965728226365, 'epoch': 21.96}
>>> 2025-09-11 13:17:37,182 - INFO - >>> {'loss': 0.0128, 'grad_norm': 0.7843579053878784, 'learning_rate': 0.00012011563013084996, 'epoch': 22.0}
>>> 2025-09-11 13:18:08,734 - INFO - >>> {'loss': 0.0075, 'grad_norm': 0.33349597454071045, 'learning_rate': 0.00011923305746910371, 'epoch': 22.16}
>>> 2025-09-11 13:18:42,190 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.14217723906040192, 'learning_rate': 0.00011834892635228023, 'epoch': 22.32}
>>> 2025-09-11 13:19:12,088 - INFO - >>> {'loss': 0.0058, 'grad_norm': 0.21094387769699097, 'learning_rate': 0.00011746330842154371, 'epoch': 22.48}
>>> 2025-09-11 13:19:41,981 - INFO - >>> {'loss': 0.0097, 'grad_norm': 0.9181451201438904, 'learning_rate': 0.00011657627543853491, 'epoch': 22.64}
>>> 2025-09-11 13:20:11,872 - INFO - >>> {'loss': 0.0073, 'grad_norm': 0.25488758087158203, 'learning_rate': 0.0001156878992795563, 'epoch': 22.8}
>>> 2025-09-11 13:20:41,745 - INFO - >>> {'loss': 0.0113, 'grad_norm': 0.3354901671409607, 'learning_rate': 0.00011479825192974792, 'epoch': 22.96}
>>> 2025-09-11 13:20:49,213 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.3946439325809479, 'learning_rate': 0.00011390740547725443, 'epoch': 23.0}
>>> 2025-09-11 13:21:19,043 - INFO - >>> {'loss': 0.0067, 'grad_norm': 0.23716606199741364, 'learning_rate': 0.00011301543210738384, 'epoch': 23.16}
>>> 2025-09-11 13:21:48,925 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.13982820510864258, 'learning_rate': 0.00011212240409675825, 'epoch': 23.32}
>>> 2025-09-11 13:22:21,117 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.17998994886875153, 'learning_rate': 0.00011122839380745737, 'epoch': 23.48}
>>> 2025-09-11 13:22:52,189 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.28586313128471375, 'learning_rate': 0.00011033347368115494, 'epoch': 23.64}
>>> 2025-09-11 13:23:22,186 - INFO - >>> {'loss': 0.0064, 'grad_norm': 0.2198857218027115, 'learning_rate': 0.00010943771623324883, 'epoch': 23.8}
>>> 2025-09-11 13:23:52,177 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.11784153431653976, 'learning_rate': 0.00010854119404698511, 'epoch': 23.96}
>>> 2025-09-11 13:24:03,283 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.13957750797271729, 'learning_rate': 0.00010764397976757656, 'epoch': 24.0}
>>> 2025-09-11 13:24:33,282 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.08789718151092529, 'learning_rate': 0.00010674614609631634, 'epoch': 24.16}
>>> 2025-09-11 13:25:03,275 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.07959341257810593, 'learning_rate': 0.00010584776578468698, 'epoch': 24.32}
>>> 2025-09-11 13:25:34,958 - INFO - >>> {'loss': 0.0057, 'grad_norm': 0.11852008104324341, 'learning_rate': 0.00010494891162846514, 'epoch': 24.48}
>>> 2025-09-11 13:26:08,365 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.1936465948820114, 'learning_rate': 0.0001040496564618233, 'epoch': 24.64}
>>> 2025-09-11 13:26:40,018 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.09745044261217117, 'learning_rate': 0.0001031500731514277, 'epoch': 24.8}
>>> 2025-09-11 13:27:09,930 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.08253318071365356, 'learning_rate': 0.00010225023459053415, 'epoch': 24.96}
>>> 2025-09-11 13:27:17,414 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.00010135021369308137, 'epoch': 25.0}
>>> 2025-09-11 13:27:47,327 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.0709790512919426, 'learning_rate': 0.00010045008338778279, 'epoch': 25.16}
>>> 2025-09-11 13:28:17,281 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.20559155941009521, 'learning_rate': 9.954991661221723e-05, 'epoch': 25.32}
>>> 2025-09-11 13:28:47,186 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.07790780067443848, 'learning_rate': 9.864978630691866e-05, 'epoch': 25.48}
>>> 2025-09-11 13:29:17,642 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.10790486633777618, 'learning_rate': 9.774976540946587e-05, 'epoch': 25.64}
>>> 2025-09-11 13:29:49,150 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.1436222940683365, 'learning_rate': 9.684992684857232e-05, 'epoch': 25.8}
>>> 2025-09-11 13:30:23,550 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.08310315012931824, 'learning_rate': 9.595034353817672e-05, 'epoch': 25.96}
>>> 2025-09-11 13:30:31,011 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.50510883715349e-05, 'epoch': 26.0}
>>> 2025-09-11 13:31:00,826 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.22141176462173462, 'learning_rate': 9.415223421531307e-05, 'epoch': 26.16}
>>> 2025-09-11 13:31:30,667 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.08415796607732773, 'learning_rate': 9.325385390368368e-05, 'epoch': 26.32}
>>> 2025-09-11 13:32:00,685 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.052382953464984894, 'learning_rate': 9.235602023242349e-05, 'epoch': 26.48}
>>> 2025-09-11 13:32:31,854 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.18993884325027466, 'learning_rate': 9.145880595301494e-05, 'epoch': 26.64}
>>> 2025-09-11 13:33:05,429 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.047491900622844696, 'learning_rate': 9.056228376675118e-05, 'epoch': 26.8}
>>> 2025-09-11 13:33:37,761 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.11037561297416687, 'learning_rate': 8.966652631884505e-05, 'epoch': 26.96}
>>> 2025-09-11 13:33:45,261 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.09465155005455017, 'learning_rate': 8.877160619254265e-05, 'epoch': 27.0}
>>> 2025-09-11 13:34:16,292 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.040749818086624146, 'learning_rate': 8.787759590324176e-05, 'epoch': 27.16}
>>> 2025-09-11 13:34:46,240 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.26745423674583435, 'learning_rate': 8.698456789261616e-05, 'epoch': 27.32}
>>> 2025-09-11 13:35:16,175 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.027903912588953972, 'learning_rate': 8.609259452274558e-05, 'epoch': 27.48}
>>> 2025-09-11 13:35:51,789 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.15954452753067017, 'learning_rate': 8.520174807025209e-05, 'epoch': 27.64}
>>> 2025-09-11 13:36:21,674 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.043783508241176605, 'learning_rate': 8.43121007204437e-05, 'epoch': 27.8}
>>> 2025-09-11 13:36:51,520 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.2573702335357666, 'learning_rate': 8.342372456146511e-05, 'epoch': 27.96}
>>> 2025-09-11 13:36:59,000 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.0718364268541336, 'learning_rate': 8.253669157845631e-05, 'epoch': 28.0}
>>> 2025-09-11 13:37:29,964 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.16254059970378876, 'learning_rate': 8.16510736477198e-05, 'epoch': 28.16}
>>> 2025-09-11 13:37:59,860 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.035359106957912445, 'learning_rate': 8.076694253089631e-05, 'epoch': 28.32}
>>> 2025-09-11 13:38:31,400 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.1323745995759964, 'learning_rate': 7.988436986915005e-05, 'epoch': 28.48}
>>> 2025-09-11 13:39:01,910 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.030882686376571655, 'learning_rate': 7.900342717736353e-05, 'epoch': 28.64}
>>> 2025-09-11 13:39:35,929 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.17979586124420166, 'learning_rate': 7.812418583834283e-05, 'epoch': 28.8}
>>> 2025-09-11 13:40:05,879 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.0456726998090744, 'learning_rate': 7.724671709703327e-05, 'epoch': 28.96}
>>> 2025-09-11 13:40:13,373 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.06647370755672455, 'learning_rate': 7.637109205474664e-05, 'epoch': 29.0}
>>> 2025-09-11 13:40:41,862 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.02346608228981495, 'learning_rate': 7.549738166339971e-05, 'epoch': 29.16}
>>> 2025-09-11 13:41:15,260 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.17831619083881378, 'learning_rate': 7.462565671976503e-05, 'epoch': 29.32}
>>> 2025-09-11 13:41:46,756 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.10353706777095795, 'learning_rate': 7.375598785973428e-05, 'epoch': 29.48}
>>> 2025-09-11 13:42:17,645 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.03689222410321236, 'learning_rate': 7.288844555259471e-05, 'epoch': 29.64}
>>> 2025-09-11 13:42:47,506 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.03145667538046837, 'learning_rate': 7.202310009531885e-05, 'epoch': 29.8}
>>> 2025-09-11 13:43:17,947 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.03202246129512787, 'learning_rate': 7.11600216068685e-05, 'epoch': 29.96}
>>> 2025-09-11 13:43:25,405 - INFO - >>> {'loss': 0.0146, 'grad_norm': 0.955510139465332, 'learning_rate': 7.029928002251287e-05, 'epoch': 30.0}
>>> 2025-09-11 13:43:55,228 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.029544759541749954, 'learning_rate': 6.944094508816181e-05, 'epoch': 30.16}
>>> 2025-09-11 13:44:26,790 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.15069714188575745, 'learning_rate': 6.858508635471428e-05, 'epoch': 30.32}
>>> 2025-09-11 13:45:00,307 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.02024545520544052, 'learning_rate': 6.773177317242256e-05, 'epoch': 30.48}
>>> 2025-09-11 13:45:30,870 - INFO - >>> {'loss': 0.007, 'grad_norm': 0.252782940864563, 'learning_rate': 6.688107468527297e-05, 'epoch': 30.64}
>>> 2025-09-11 13:46:00,905 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.10212626308202744, 'learning_rate': 6.603305982538295e-05, 'epoch': 30.8}
>>> 2025-09-11 13:46:30,921 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.03049970604479313, 'learning_rate': 6.518779730741555e-05, 'epoch': 30.96}
>>> 2025-09-11 13:46:38,419 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.05668419599533081, 'learning_rate': 6.434535562301153e-05, 'epoch': 31.0}
>>> 2025-09-11 13:47:08,366 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.08682935684919357, 'learning_rate': 6.350580303523947e-05, 'epoch': 31.16}
>>> 2025-09-11 13:47:39,960 - INFO - >>> {'loss': 0.007, 'grad_norm': 0.17341399192810059, 'learning_rate': 6.266920757306429e-05, 'epoch': 31.32}
>>> 2025-09-11 13:48:09,939 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.0225870031863451, 'learning_rate': 6.183563702583506e-05, 'epoch': 31.48}
>>> 2025-09-11 13:48:39,838 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.05278465151786804, 'learning_rate': 6.1005158937791886e-05, 'epoch': 31.64}
>>> 2025-09-11 13:49:09,780 - INFO - >>> {'loss': 0.0075, 'grad_norm': 0.2828850746154785, 'learning_rate': 6.017784060259279e-05, 'epoch': 31.8}
>>> 2025-09-11 13:49:40,271 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.03404557332396507, 'learning_rate': 5.935374905786102e-05, 'epoch': 31.96}
>>> 2025-09-11 13:49:51,295 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.027691887691617012, 'learning_rate': 5.85329510797529e-05, 'epoch': 32.0}
>>> 2025-09-11 13:50:21,804 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.24773995578289032, 'learning_rate': 5.771551317754691e-05, 'epoch': 32.16}
>>> 2025-09-11 13:50:51,744 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.07490594685077667, 'learning_rate': 5.6901501588254624e-05, 'epoch': 32.32}
>>> 2025-09-11 13:51:26,865 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.13305941224098206, 'learning_rate': 5.6090982271253336e-05, 'epoch': 32.48}
>>> 2025-09-11 13:51:56,839 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.024137990549206734, 'learning_rate': 5.528402090294143e-05, 'epoch': 32.64}
>>> 2025-09-11 13:52:27,875 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.02168048731982708, 'learning_rate': 5.448068287141663e-05, 'epoch': 32.8}
>>> 2025-09-11 13:52:57,599 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.021603018045425415, 'learning_rate': 5.3681033271177686e-05, 'epoch': 32.96}
>>> 2025-09-11 13:53:05,074 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.288513689784951e-05, 'epoch': 33.0}
>>> 2025-09-11 13:53:36,073 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.021667318418622017, 'learning_rate': 5.209305824293307e-05, 'epoch': 33.16}
>>> 2025-09-11 13:54:05,449 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.020233426243066788, 'learning_rate': 5.130486148857951e-05, 'epoch': 33.32}
>>> 2025-09-11 13:54:36,899 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.2214856892824173, 'learning_rate': 5.05206105023895e-05, 'epoch': 33.48}
>>> 2025-09-11 13:55:06,757 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.11273258179426193, 'learning_rate': 4.974036883223798e-05, 'epoch': 33.64}
>>> 2025-09-11 13:55:36,621 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.027977261692285538, 'learning_rate': 4.896419970112499e-05, 'epoch': 33.8}
>>> 2025-09-11 13:56:06,614 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.022830916568636894, 'learning_rate': 4.819216600205254e-05, 'epoch': 33.96}
>>> 2025-09-11 13:56:17,585 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.03437121585011482, 'learning_rate': 4.742433029292855e-05, 'epoch': 34.0}
>>> 2025-09-11 13:56:49,266 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.11804015934467316, 'learning_rate': 4.6660754791497754e-05, 'epoch': 34.16}
>>> 2025-09-11 13:57:19,303 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.11156956851482391, 'learning_rate': 4.5901501370300094e-05, 'epoch': 34.32}
>>> 2025-09-11 13:57:49,899 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.018970372155308723, 'learning_rate': 4.51466315516573e-05, 'epoch': 34.48}
>>> 2025-09-11 13:58:20,974 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.1511651873588562, 'learning_rate': 4.439620650268771e-05, 'epoch': 34.64}
>>> 2025-09-11 13:58:50,916 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.03178340941667557, 'learning_rate': 4.3650287030349755e-05, 'epoch': 34.8}
>>> 2025-09-11 13:59:24,332 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.023713715374469757, 'learning_rate': 4.290893357651502e-05, 'epoch': 34.96}
>>> 2025-09-11 13:59:31,822 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.07443379610776901, 'learning_rate': 4.217220621307043e-05, 'epoch': 35.0}
>>> 2025-09-11 14:00:03,385 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.1035078689455986, 'learning_rate': 4.14401646370508e-05, 'epoch': 35.16}
>>> 2025-09-11 14:00:36,865 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.0320344939827919, 'learning_rate': 4.071286816580142e-05, 'epoch': 35.32}
>>> 2025-09-11 14:01:06,845 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.16649793088436127, 'learning_rate': 3.999037573217157e-05, 'epoch': 35.48}
>>> 2025-09-11 14:01:37,452 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.14088617265224457, 'learning_rate': 3.9272745879739346e-05, 'epoch': 35.64}
>>> 2025-09-11 14:02:08,499 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.021730966866016388, 'learning_rate': 3.8560036758067764e-05, 'epoch': 35.8}
>>> 2025-09-11 14:02:38,477 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.021477647125720978, 'learning_rate': 3.7852306117992896e-05, 'epoch': 35.96}
>>> 2025-09-11 14:02:45,980 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.03715591877698898, 'learning_rate': 3.7149611306944355e-05, 'epoch': 36.0}
>>> 2025-09-11 14:03:15,969 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.12044478207826614, 'learning_rate': 3.645200926429844e-05, 'epoch': 36.16}
>>> 2025-09-11 14:03:45,952 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.19207309186458588, 'learning_rate': 3.5759556516764205e-05, 'epoch': 36.32}
>>> 2025-09-11 14:04:19,355 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.02610546350479126, 'learning_rate': 3.507230917380332e-05, 'epoch': 36.48}
>>> 2025-09-11 14:04:49,275 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.020660273730754852, 'learning_rate': 3.439032292308338e-05, 'epoch': 36.64}
>>> 2025-09-11 14:05:21,373 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.13125212490558624, 'learning_rate': 3.3713653025965544e-05, 'epoch': 36.8}
>>> 2025-09-11 14:05:52,339 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.019216973334550858, 'learning_rate': 3.30423543130267e-05, 'epoch': 36.96}
>>> 2025-09-11 14:05:59,821 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.0408174954354763, 'learning_rate': 3.237648117961665e-05, 'epoch': 37.0}
>>> 2025-09-11 14:06:34,832 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.09348389506340027, 'learning_rate': 3.1716087581450196e-05, 'epoch': 37.16}
>>> 2025-09-11 14:07:05,852 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.017887122929096222, 'learning_rate': 3.106122703023544e-05, 'epoch': 37.32}
>>> 2025-09-11 14:07:35,826 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.10577111691236496, 'learning_rate': 3.041195258933749e-05, 'epoch': 37.48}
>>> 2025-09-11 14:08:05,847 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.015687329694628716, 'learning_rate': 2.976831686947884e-05, 'epoch': 37.64}
>>> 2025-09-11 14:08:36,429 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.16433535516262054, 'learning_rate': 2.913037202447625e-05, 'epoch': 37.8}
>>> 2025-09-11 14:09:06,410 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.0236076470464468, 'learning_rate': 2.8498169747014825e-05, 'epoch': 37.96}
>>> 2025-09-11 14:09:13,904 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.7871761264459227e-05, 'epoch': 38.0}
>>> 2025-09-11 14:09:43,917 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.129651740193367, 'learning_rate': 2.725119733470284e-05, 'epoch': 38.16}
>>> 2025-09-11 14:10:13,924 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.01861722581088543, 'learning_rate': 2.663652824205476e-05, 'epoch': 38.32}
>>> 2025-09-11 14:10:43,892 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.023439662531018257, 'learning_rate': 2.602780379316535e-05, 'epoch': 38.48}
>>> 2025-09-11 14:11:17,350 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.02080725133419037, 'learning_rate': 2.5425073312990334e-05, 'epoch': 38.64}
>>> 2025-09-11 14:11:50,669 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.08683115243911743, 'learning_rate': 2.482838564079397e-05, 'epoch': 38.8}
>>> 2025-09-11 14:12:20,684 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.023682676255702972, 'learning_rate': 2.423778912619171e-05, 'epoch': 38.96}
>>> 2025-09-11 14:12:28,202 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.034342408180236816, 'learning_rate': 2.3653331625232367e-05, 'epoch': 39.0}
>>> 2025-09-11 14:12:58,810 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.01835784502327442, 'learning_rate': 2.3075060496520308e-05, 'epoch': 39.16}
>>> 2025-09-11 14:13:28,867 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.0918961688876152, 'learning_rate': 2.250302259737803e-05, 'epoch': 39.32}
>>> 2025-09-11 14:13:58,850 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.02628987841308117, 'learning_rate': 2.1937264280049363e-05, 'epoch': 39.48}
>>> 2025-09-11 14:14:28,831 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.16379816830158234, 'learning_rate': 2.137783138794335e-05, 'epoch': 39.64}
>>> 2025-09-11 14:14:58,806 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.021313348785042763, 'learning_rate': 2.0824769251919775e-05, 'epoch': 39.8}
>>> 2025-09-11 14:15:33,911 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.0979394018650055, 'learning_rate': 2.027812268661592e-05, 'epoch': 39.96}
>>> 2025-09-11 14:15:41,398 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.03197295963764191, 'learning_rate': 1.9737935986815205e-05, 'epoch': 40.0}
>>> 2025-09-11 14:16:13,049 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.1196802482008934, 'learning_rate': 1.9204252923858003e-05, 'epoch': 40.16}
>>> 2025-09-11 14:16:42,958 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.09125564992427826, 'learning_rate': 1.8677116742094857e-05, 'epoch': 40.32}
>>> 2025-09-11 14:17:16,426 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.014052614569664001, 'learning_rate': 1.8156570155382356e-05, 'epoch': 40.48}
>>> 2025-09-11 14:17:47,381 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.032762810587882996, 'learning_rate': 1.764265534362205e-05, 'epoch': 40.64}
>>> 2025-09-11 14:18:17,199 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.020616326481103897, 'learning_rate': 1.7135413949342704e-05, 'epoch': 40.8}
>>> 2025-09-11 14:18:47,111 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.02150014415383339, 'learning_rate': 1.6634887074325843e-05, 'epoch': 40.96}
>>> 2025-09-11 14:18:54,601 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.040835924446582794, 'learning_rate': 1.6141115276275298e-05, 'epoch': 41.0}
>>> 2025-09-11 14:19:24,558 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.022174296900629997, 'learning_rate': 1.565413856553095e-05, 'epoch': 41.16}
>>> 2025-09-11 14:19:58,621 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.018665393814444542, 'learning_rate': 1.5173996401826562e-05, 'epoch': 41.32}
>>> 2025-09-11 14:20:28,628 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.10679856687784195, 'learning_rate': 1.470072769109242e-05, 'epoch': 41.48}
>>> 2025-09-11 14:20:59,642 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.1441037356853485, 'learning_rate': 1.4234370782302742e-05, 'epoch': 41.64}
>>> 2025-09-11 14:21:31,283 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.10649028420448303, 'learning_rate': 1.3774963464368295e-05, 'epoch': 41.8}
>>> 2025-09-11 14:22:01,249 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.015396920032799244, 'learning_rate': 1.3322542963074314e-05, 'epoch': 41.96}
>>> 2025-09-11 14:22:08,761 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.025840818881988525, 'learning_rate': 1.287714593806415e-05, 'epoch': 42.0}
>>> 2025-09-11 14:22:38,706 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.019873788580298424, 'learning_rate': 1.2438808479868714e-05, 'epoch': 42.16}
>>> 2025-09-11 14:23:08,657 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.027407918125391006, 'learning_rate': 1.2007566106982049e-05, 'epoch': 42.32}
>>> 2025-09-11 14:23:39,704 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.01744607463479042, 'learning_rate': 1.1583453762983287e-05, 'epoch': 42.48}
>>> 2025-09-11 14:24:09,687 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.014553076587617397, 'learning_rate': 1.1166505813705187e-05, 'epoch': 42.64}
>>> 2025-09-11 14:24:44,733 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.1641835868358612, 'learning_rate': 1.0756756044449356e-05, 'epoch': 42.8}
>>> 2025-09-11 14:25:14,731 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.017267199233174324, 'learning_rate': 1.035423765724879e-05, 'epoch': 42.96}
>>> 2025-09-11 14:25:22,931 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.3675456941127777, 'learning_rate': 9.958983268177423e-06, 'epoch': 43.0}
>>> 2025-09-11 14:25:52,831 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.022007448598742485, 'learning_rate': 9.571024904707237e-06, 'epoch': 43.16}
>>> 2025-09-11 14:26:27,003 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.01747121848165989, 'learning_rate': 9.190394003113123e-06, 'epoch': 43.32}
>>> 2025-09-11 14:26:57,192 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.016005195677280426, 'learning_rate': 8.817121405925544e-06, 'epoch': 43.48}
>>> 2025-09-11 14:27:27,192 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.021155888214707375, 'learning_rate': 8.451237359431397e-06, 'epoch': 43.64}
>>> 2025-09-11 14:27:58,255 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.09654218703508377, 'learning_rate': 8.092771511223185e-06, 'epoch': 43.8}
>>> 2025-09-11 14:28:28,131 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.13211071491241455, 'learning_rate': 7.741752907796585e-06, 'epoch': 43.96}
>>> 2025-09-11 14:28:35,621 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.059224508702754974, 'learning_rate': 7.3982099921969135e-06, 'epoch': 44.0}
>>> 2025-09-11 14:29:09,049 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.015244076028466225, 'learning_rate': 7.0621706017143015e-06, 'epoch': 44.16}
>>> 2025-09-11 14:29:38,959 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.026046814396977425, 'learning_rate': 6.73366196562808e-06, 'epoch': 44.32}
>>> 2025-09-11 14:30:08,891 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.14923202991485596, 'learning_rate': 6.4127107030003685e-06, 'epoch': 44.48}
>>> 2025-09-11 14:30:40,473 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.15030521154403687, 'learning_rate': 6.099342820519183e-06, 'epoch': 44.64}
>>> 2025-09-11 14:31:10,470 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.12870849668979645, 'learning_rate': 5.793583710391059e-06, 'epoch': 44.8}
>>> 2025-09-11 14:31:41,568 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.016399476677179337, 'learning_rate': 5.495458148283506e-06, 'epoch': 44.96}
>>> 2025-09-11 14:31:49,078 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.05646195262670517, 'learning_rate': 5.2049902913175355e-06, 'epoch': 45.0}
>>> 2025-09-11 14:32:20,126 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.022085130214691162, 'learning_rate': 4.9222036761100595e-06, 'epoch': 45.16}
>>> 2025-09-11 14:32:50,134 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.13170450925827026, 'learning_rate': 4.647121216866857e-06, 'epoch': 45.32}
>>> 2025-09-11 14:33:20,148 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.024916542693972588, 'learning_rate': 4.379765203525754e-06, 'epoch': 45.48}
>>> 2025-09-11 14:33:50,203 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.014820984564721584, 'learning_rate': 4.1201572999505e-06, 'epoch': 45.64}
>>> 2025-09-11 14:34:20,862 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.013718106783926487, 'learning_rate': 3.868318542175331e-06, 'epoch': 45.8}
>>> 2025-09-11 14:34:54,306 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.17841292917728424, 'learning_rate': 3.6242693367004364e-06, 'epoch': 45.96}
>>> 2025-09-11 14:35:03,455 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.25003325939178467, 'learning_rate': 3.3880294588383597e-06, 'epoch': 46.0}
>>> 2025-09-11 14:35:35,037 - INFO - >>> {'loss': 0.0076, 'grad_norm': 0.36426809430122375, 'learning_rate': 3.1596180511117234e-06, 'epoch': 46.16}
>>> 2025-09-11 14:36:05,605 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.013649264350533485, 'learning_rate': 2.9390536217020147e-06, 'epoch': 46.32}
>>> 2025-09-11 14:36:36,706 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.01719510741531849, 'learning_rate': 2.726354042949875e-06, 'epoch': 46.48}
>>> 2025-09-11 14:37:06,659 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.01717614382505417, 'learning_rate': 2.5215365499069443e-06, 'epoch': 46.64}
>>> 2025-09-11 14:37:40,089 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.09959264099597931, 'learning_rate': 2.3246177389392388e-06, 'epoch': 46.8}
>>> 2025-09-11 14:38:10,010 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.02214094251394272, 'learning_rate': 2.1356135663824326e-06, 'epoch': 46.96}
>>> 2025-09-11 14:38:17,491 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.9545393472488737e-06, 'epoch': 47.0}
>>> 2025-09-11 14:38:48,488 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.02366660349071026, 'learning_rate': 1.7814097539865627e-06, 'epoch': 47.16}
>>> 2025-09-11 14:39:18,305 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.023223841562867165, 'learning_rate': 1.6162388152903495e-06, 'epoch': 47.32}
>>> 2025-09-11 14:39:51,469 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.013928682543337345, 'learning_rate': 1.4590399149650768e-06, 'epoch': 47.48}
>>> 2025-09-11 14:40:21,407 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.11103756725788116, 'learning_rate': 1.309825790841146e-06, 'epoch': 47.64}
>>> 2025-09-11 14:40:52,001 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.10149778425693512, 'learning_rate': 1.168608533742399e-06, 'epoch': 47.8}
>>> 2025-09-11 14:41:23,668 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.1235029548406601, 'learning_rate': 1.0353995865063138e-06, 'epoch': 47.96}
>>> 2025-09-11 14:41:31,169 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.10209743056889e-07, 'epoch': 48.0}
>>> 2025-09-11 14:42:04,633 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.01738571748137474, 'learning_rate': 7.930491475299229e-07, 'epoch': 48.16}
>>> 2025-09-11 14:42:35,233 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.013455779291689396, 'learning_rate': 6.839272934511143e-07, 'epoch': 48.32}
>>> 2025-09-11 14:43:05,160 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.018843526020646095, 'learning_rate': 5.828530229667228e-07, 'epoch': 48.48}
>>> 2025-09-11 14:43:35,755 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.13845890760421753, 'learning_rate': 4.89834526127153e-07, 'epoch': 48.64}
>>> 2025-09-11 14:44:05,669 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.14051911234855652, 'learning_rate': 4.0487934022328535e-07, 'epoch': 48.8}
>>> 2025-09-11 14:44:37,230 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.1343710720539093, 'learning_rate': 3.27994349175742e-07, 'epoch': 48.96}
>>> 2025-09-11 14:44:44,732 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.04422483965754509, 'learning_rate': 2.591857829770672e-07, 'epoch': 49.0}
>>> 2025-09-11 14:45:18,163 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.022749993950128555, 'learning_rate': 1.9845921718690818e-07, 'epoch': 49.16}
>>> 2025-09-11 14:45:49,778 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.0165809765458107, 'learning_rate': 1.4581957248026578e-07, 'epoch': 49.32}
>>> 2025-09-11 14:46:17,156 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.06840809434652328, 'learning_rate': 1.0127111424872438e-07, 'epoch': 49.48}
>>> 2025-09-11 14:46:47,126 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.24810270965099335, 'learning_rate': 6.481745225485059e-08, 'epoch': 49.64}
>>> 2025-09-11 14:47:18,692 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.13537858426570892, 'learning_rate': 3.6461540339682855e-08, 'epoch': 49.8}
>>> 2025-09-11 14:47:48,645 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.017940467223525047, 'learning_rate': 1.6205676183411732e-08, 'epoch': 49.96}
>>> 2025-09-11 14:47:56,147 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.0382380336523056, 'learning_rate': 4.051501119162193e-09, 'epoch': 50.0}
>>> 2025-09-11 14:47:56,840 - INFO - >>> {'train_runtime': 9679.0686, 'train_samples_per_second': 0.517, 'train_steps_per_second': 0.036, 'train_loss': 0.3828436227617619, 'epoch': 50.0}
>>> 2025-09-11 14:47:56,842 - INFO - 训练成功！
>>> 2025-09-11 14:47:56,843 - INFO - 模型存放位置：./output/qwen3-8b202509111206
>>> 2025-09-11 14:52:48,374 - INFO - 开始进行原始模型对话测试
>>> 2025-09-11 14:52:50,751 - INFO - 导入包完成
>>> 2025-09-11 14:52:50,758 - INFO - 配置文件读取完成
>>> 2025-09-11 14:55:22,413 - INFO - ========__main__  202509111455========
>>> 2025-09-11 14:55:22,413 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 14:55:22,414 - INFO - 开始进行模型测试
>>> 2025-09-11 14:55:28,097 - INFO - 已选择模型文件夹: qwen3-8b202509111206
>>> 2025-09-11 14:55:28,099 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509111206/checkpoint-350
>>> 2025-09-11 15:47:20,661 - INFO - 导入包完成
>>> 2025-09-11 15:47:20,662 - INFO - ========train Qwen2ForCausalLM  202509111547========
>>> 2025-09-11 15:47:20,662 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 15:47:20,663 - INFO - 开始进行训练
>>> 2025-09-11 15:47:20,669 - INFO - 基础配置文件读取完成
>>> 2025-09-11 15:47:20,676 - INFO - 训练配置读取完成
>>> 2025-09-11 15:47:20,677 - INFO - 数据集路径：dataset/own/Medical_20250816.json
>>> 2025-09-11 15:47:20,677 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-09-11 15:47:21,124 - INFO - tokenizer读取完成
>>> 2025-09-11 15:47:21,281 - INFO - model dtype:torch.bfloat16
>>> 2025-09-11 15:47:21,282 - INFO - 模型导入完成
>>> 2025-09-11 15:47:21,282 - INFO - 数据读取开始
>>> 2025-09-11 15:47:22,086 - INFO - 数据下载完成，训练集大小: 100
>>> 2025-09-11 15:47:26,292 - INFO - 数据映射完成
>>> 2025-09-11 15:47:26,293 - INFO - 打印训练参数如下
>>> 2025-09-11 15:47:26,293 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-11 15:47:26,294 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-11 15:47:26,294 - INFO -   load_in_4bit >>> True
>>> 2025-09-11 15:47:26,294 - INFO -   batch_size >>> 4
>>> 2025-09-11 15:47:26,295 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-11 15:47:26,295 - INFO -   warmup_steps >>> 1
>>> 2025-09-11 15:47:26,295 - INFO -   epoch >>> 50
>>> 2025-09-11 15:47:26,296 - INFO -   eval_steps >>> 5
>>> 2025-09-11 15:47:26,296 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-11 15:47:26,296 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-11 15:47:26,297 - INFO -   max_seq_length >>> 2048
>>> 2025-09-11 15:47:26,297 - INFO -   r >>> 8
>>> 2025-09-11 15:47:26,297 - INFO -   interface_mode >>> False
>>> 2025-09-11 15:47:26,298 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-11 15:47:26,298 - INFO -   lora_alpha >>> 16
>>> 2025-09-11 15:47:26,298 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-11 15:47:26,299 - INFO -   bias >>> none
>>> 2025-09-11 15:47:26,299 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-11 15:47:26,300 - INFO -   random_state >>> 3407
>>> 2025-09-11 15:47:26,300 - INFO -   use_rslora >>> True
>>> 2025-09-11 15:47:26,300 - INFO -   loftq_config >>> None
>>> 2025-09-11 15:47:32,361 - INFO - 开始训练！
>>> 2025-09-11 15:48:02,790 - INFO - >>> {'loss': 3.5267, 'grad_norm': 1.9307844638824463, 'learning_rate': 0.0, 'epoch': 0.16}
>>> 2025-09-11 15:48:32,636 - INFO - >>> {'loss': 3.6737, 'grad_norm': 2.181020975112915, 'learning_rate': 0.0002, 'epoch': 0.32}
>>> 2025-09-11 15:49:02,967 - INFO - >>> {'loss': 3.6711, 'grad_norm': 2.240957498550415, 'learning_rate': 0.00019999594849888085, 'epoch': 0.48}
>>> 2025-09-11 15:49:33,165 - INFO - >>> {'loss': 3.5911, 'grad_norm': 2.001052141189575, 'learning_rate': 0.0001999837943238166, 'epoch': 0.64}
>>> 2025-09-11 15:50:07,828 - INFO - >>> {'loss': 3.0644, 'grad_norm': 1.335556983947754, 'learning_rate': 0.00019996353845966032, 'epoch': 0.8}
>>> 2025-09-11 15:50:39,443 - INFO - >>> {'loss': 2.8868, 'grad_norm': 1.599292516708374, 'learning_rate': 0.00019993518254774517, 'epoch': 0.96}
>>> 2025-09-11 15:50:46,949 - INFO - >>> {'loss': 3.0188, 'grad_norm': 1.5158658027648926, 'learning_rate': 0.00019989872888575126, 'epoch': 1.0}
>>> 2025-09-11 15:51:16,886 - INFO - >>> {'loss': 2.6871, 'grad_norm': 0.8989437222480774, 'learning_rate': 0.00019985418042751975, 'epoch': 1.16}
>>> 2025-09-11 15:51:50,259 - INFO - >>> {'loss': 2.4379, 'grad_norm': 0.6210998892784119, 'learning_rate': 0.0001998015407828131, 'epoch': 1.32}
>>> 2025-09-11 15:52:20,755 - INFO - >>> {'loss': 2.3966, 'grad_norm': 0.7947554588317871, 'learning_rate': 0.00019974081421702294, 'epoch': 1.48}
>>> 2025-09-11 15:52:52,207 - INFO - >>> {'loss': 2.2064, 'grad_norm': 0.6577418446540833, 'learning_rate': 0.00019967200565082426, 'epoch': 1.6400000000000001}
>>> 2025-09-11 15:53:23,117 - INFO - >>> {'loss': 2.4096, 'grad_norm': 0.5666244626045227, 'learning_rate': 0.00019959512065977671, 'epoch': 1.8}
>>> 2025-09-11 15:53:52,947 - INFO - >>> {'loss': 2.2261, 'grad_norm': 0.7628770470619202, 'learning_rate': 0.00019951016547387288, 'epoch': 1.96}
>>> 2025-09-11 15:54:00,406 - INFO - >>> {'loss': 2.4448, 'grad_norm': 1.0591462850570679, 'learning_rate': 0.00019941714697703332, 'epoch': 2.0}
>>> 2025-09-11 15:54:30,811 - INFO - >>> {'loss': 2.3171, 'grad_norm': 0.7581175565719604, 'learning_rate': 0.0001993160727065489, 'epoch': 2.16}
>>> 2025-09-11 15:55:00,720 - INFO - >>> {'loss': 2.2331, 'grad_norm': 0.7962110042572021, 'learning_rate': 0.0001992069508524701, 'epoch': 2.32}
>>> 2025-09-11 15:55:34,088 - INFO - >>> {'loss': 2.1272, 'grad_norm': 0.7366707921028137, 'learning_rate': 0.0001990897902569431, 'epoch': 2.48}
>>> 2025-09-11 15:56:05,036 - INFO - >>> {'loss': 2.1501, 'grad_norm': 0.8273051977157593, 'learning_rate': 0.0001989646004134937, 'epoch': 2.64}
>>> 2025-09-11 15:56:34,977 - INFO - >>> {'loss': 2.0772, 'grad_norm': 0.802774965763092, 'learning_rate': 0.0001988313914662576, 'epoch': 2.8}
>>> 2025-09-11 15:57:06,513 - INFO - >>> {'loss': 1.9245, 'grad_norm': 0.6591737270355225, 'learning_rate': 0.00019869017420915888, 'epoch': 2.96}
>>> 2025-09-11 15:57:13,989 - INFO - >>> {'loss': 2.1567, 'grad_norm': 1.4297884702682495, 'learning_rate': 0.00019854096008503494, 'epoch': 3.0}
>>> 2025-09-11 15:57:49,058 - INFO - >>> {'loss': 2.121, 'grad_norm': 0.5273472666740417, 'learning_rate': 0.00019838376118470964, 'epoch': 3.16}
>>> 2025-09-11 15:58:20,690 - INFO - >>> {'loss': 2.1195, 'grad_norm': 0.6323837637901306, 'learning_rate': 0.00019821859024601345, 'epoch': 3.32}
>>> 2025-09-11 15:58:50,696 - INFO - >>> {'loss': 1.9171, 'grad_norm': 0.5294250845909119, 'learning_rate': 0.00019804546065275112, 'epoch': 3.48}
>>> 2025-09-11 15:59:20,755 - INFO - >>> {'loss': 1.9049, 'grad_norm': 0.5292693376541138, 'learning_rate': 0.00019786438643361757, 'epoch': 3.64}
>>> 2025-09-11 15:59:50,712 - INFO - >>> {'loss': 1.6988, 'grad_norm': 0.8568820357322693, 'learning_rate': 0.00019767538226106077, 'epoch': 3.8}
>>> 2025-09-11 16:00:20,632 - INFO - >>> {'loss': 1.8583, 'grad_norm': 0.8492069840431213, 'learning_rate': 0.00019747846345009306, 'epoch': 3.96}
>>> 2025-09-11 16:00:28,113 - INFO - >>> {'loss': 1.6442, 'grad_norm': 1.365403413772583, 'learning_rate': 0.00019727364595705012, 'epoch': 4.0}
>>> 2025-09-11 16:00:58,029 - INFO - >>> {'loss': 1.9832, 'grad_norm': 0.7845714688301086, 'learning_rate': 0.00019706094637829798, 'epoch': 4.16}
>>> 2025-09-11 16:01:29,089 - INFO - >>> {'loss': 1.7067, 'grad_norm': 0.6058638095855713, 'learning_rate': 0.00019684038194888828, 'epoch': 4.32}
>>> 2025-09-11 16:02:00,579 - INFO - >>> {'loss': 1.7769, 'grad_norm': 0.621805727481842, 'learning_rate': 0.00019661197054116164, 'epoch': 4.48}
>>> 2025-09-11 16:02:34,474 - INFO - >>> {'loss': 1.6835, 'grad_norm': 0.7264933586120605, 'learning_rate': 0.0001963757306632996, 'epoch': 4.64}
>>> 2025-09-11 16:03:04,306 - INFO - >>> {'loss': 1.8932, 'grad_norm': 0.771881103515625, 'learning_rate': 0.00019613168145782468, 'epoch': 4.8}
>>> 2025-09-11 16:03:34,120 - INFO - >>> {'loss': 1.8341, 'grad_norm': 0.7505115270614624, 'learning_rate': 0.0001958798427000495, 'epoch': 4.96}
>>> 2025-09-11 16:03:41,565 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.00019562023479647426, 'epoch': 5.0}
>>> 2025-09-11 16:04:12,478 - INFO - >>> {'loss': 1.8771, 'grad_norm': 0.7645353078842163, 'learning_rate': 0.00019535287878313316, 'epoch': 5.16}
>>> 2025-09-11 16:04:42,892 - INFO - >>> {'loss': 1.5648, 'grad_norm': 0.8292467594146729, 'learning_rate': 0.00019507779632388996, 'epoch': 5.32}
>>> 2025-09-11 16:05:12,763 - INFO - >>> {'loss': 1.3993, 'grad_norm': 0.8180845975875854, 'learning_rate': 0.0001947950097086825, 'epoch': 5.48}
>>> 2025-09-11 16:05:42,642 - INFO - >>> {'loss': 1.4333, 'grad_norm': 1.2317960262298584, 'learning_rate': 0.00019450454185171648, 'epoch': 5.64}
>>> 2025-09-11 16:06:12,593 - INFO - >>> {'loss': 1.5886, 'grad_norm': 0.87530517578125, 'learning_rate': 0.00019420641628960895, 'epoch': 5.8}
>>> 2025-09-11 16:06:47,637 - INFO - >>> {'loss': 1.706, 'grad_norm': 0.7671912312507629, 'learning_rate': 0.00019390065717948083, 'epoch': 5.96}
>>> 2025-09-11 16:06:55,129 - INFO - >>> {'loss': 1.9357, 'grad_norm': 2.8133442401885986, 'learning_rate': 0.00019358728929699966, 'epoch': 6.0}
>>> 2025-09-11 16:07:28,587 - INFO - >>> {'loss': 1.419, 'grad_norm': 0.9421026706695557, 'learning_rate': 0.00019326633803437194, 'epoch': 6.16}
>>> 2025-09-11 16:07:59,182 - INFO - >>> {'loss': 1.5261, 'grad_norm': 0.8469133973121643, 'learning_rate': 0.00019293782939828571, 'epoch': 6.32}
>>> 2025-09-11 16:08:29,093 - INFO - >>> {'loss': 1.0654, 'grad_norm': 2.1875548362731934, 'learning_rate': 0.0001926017900078031, 'epoch': 6.48}
>>> 2025-09-11 16:08:59,082 - INFO - >>> {'loss': 1.5663, 'grad_norm': 0.9374640583992004, 'learning_rate': 0.00019225824709220342, 'epoch': 6.64}
>>> 2025-09-11 16:09:29,017 - INFO - >>> {'loss': 1.3658, 'grad_norm': 1.2003611326217651, 'learning_rate': 0.0001919072284887768, 'epoch': 6.8}
>>> 2025-09-11 16:10:01,685 - INFO - >>> {'loss': 1.4695, 'grad_norm': 1.065861463546753, 'learning_rate': 0.00019154876264056863, 'epoch': 6.96}
>>> 2025-09-11 16:10:09,222 - INFO - >>> {'loss': 1.3444, 'grad_norm': 1.5505995750427246, 'learning_rate': 0.0001911828785940745, 'epoch': 7.0}
>>> 2025-09-11 16:10:40,212 - INFO - >>> {'loss': 1.2909, 'grad_norm': 1.3021105527877808, 'learning_rate': 0.0001908096059968869, 'epoch': 7.16}
>>> 2025-09-11 16:11:10,210 - INFO - >>> {'loss': 1.3856, 'grad_norm': 1.3234596252441406, 'learning_rate': 0.00019042897509529279, 'epoch': 7.32}
>>> 2025-09-11 16:11:41,804 - INFO - >>> {'loss': 1.3981, 'grad_norm': 1.4891670942306519, 'learning_rate': 0.00019004101673182258, 'epoch': 7.48}
>>> 2025-09-11 16:12:11,800 - INFO - >>> {'loss': 1.3762, 'grad_norm': 1.556607961654663, 'learning_rate': 0.0001896457623427512, 'epoch': 7.64}
>>> 2025-09-11 16:12:45,400 - INFO - >>> {'loss': 1.2102, 'grad_norm': 1.1613926887512207, 'learning_rate': 0.00018924324395555066, 'epoch': 7.8}
>>> 2025-09-11 16:13:15,328 - INFO - >>> {'loss': 1.0777, 'grad_norm': 1.5045936107635498, 'learning_rate': 0.00018883349418629484, 'epoch': 7.96}
>>> 2025-09-11 16:13:22,817 - INFO - >>> {'loss': 0.8817, 'grad_norm': 3.2086946964263916, 'learning_rate': 0.00018841654623701673, 'epoch': 8.0}
>>> 2025-09-11 16:13:53,314 - INFO - >>> {'loss': 1.2629, 'grad_norm': 1.345200538635254, 'learning_rate': 0.00018799243389301798, 'epoch': 8.16}
>>> 2025-09-11 16:14:24,266 - INFO - >>> {'loss': 1.1218, 'grad_norm': 1.3721622228622437, 'learning_rate': 0.0001875611915201313, 'epoch': 8.32}
>>> 2025-09-11 16:14:54,078 - INFO - >>> {'loss': 0.9595, 'grad_norm': 1.5285152196884155, 'learning_rate': 0.00018712285406193585, 'epoch': 8.48}
>>> 2025-09-11 16:15:23,891 - INFO - >>> {'loss': 0.8887, 'grad_norm': 1.5549198389053345, 'learning_rate': 0.00018667745703692572, 'epoch': 8.64}
>>> 2025-09-11 16:15:55,361 - INFO - >>> {'loss': 1.0089, 'grad_norm': 2.313412666320801, 'learning_rate': 0.00018622503653563174, 'epoch': 8.8}
>>> 2025-09-11 16:16:28,788 - INFO - >>> {'loss': 1.0375, 'grad_norm': 1.9100362062454224, 'learning_rate': 0.00018576562921769727, 'epoch': 8.96}
>>> 2025-09-11 16:16:36,260 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0001852992723089076, 'epoch': 9.0}
>>> 2025-09-11 16:17:06,256 - INFO - >>> {'loss': 0.8595, 'grad_norm': 2.193557024002075, 'learning_rate': 0.00018482600359817343, 'epoch': 9.16}
>>> 2025-09-11 16:17:36,232 - INFO - >>> {'loss': 0.7517, 'grad_norm': 2.695326566696167, 'learning_rate': 0.00018434586143446907, 'epoch': 9.32}
>>> 2025-09-11 16:18:06,283 - INFO - >>> {'loss': 0.8832, 'grad_norm': 1.8193528652191162, 'learning_rate': 0.00018385888472372472, 'epoch': 9.48}
>>> 2025-09-11 16:18:41,362 - INFO - >>> {'loss': 0.7932, 'grad_norm': 1.8194832801818848, 'learning_rate': 0.00018336511292567419, 'epoch': 9.64}
>>> 2025-09-11 16:19:12,338 - INFO - >>> {'loss': 0.8304, 'grad_norm': 2.355381488800049, 'learning_rate': 0.0001828645860506573, 'epoch': 9.8}
>>> 2025-09-11 16:19:42,232 - INFO - >>> {'loss': 0.4696, 'grad_norm': 2.3279855251312256, 'learning_rate': 0.00018235734465637794, 'epoch': 9.96}
>>> 2025-09-11 16:19:50,306 - INFO - >>> {'loss': 1.0738, 'grad_norm': 3.6325507164001465, 'learning_rate': 0.00018184342984461766, 'epoch': 10.0}
>>> 2025-09-11 16:20:20,194 - INFO - >>> {'loss': 0.6842, 'grad_norm': 2.170537233352661, 'learning_rate': 0.00018132288325790517, 'epoch': 10.16}
>>> 2025-09-11 16:20:53,578 - INFO - >>> {'loss': 0.5949, 'grad_norm': 2.048335552215576, 'learning_rate': 0.00018079574707614203, 'epoch': 10.32}
>>> 2025-09-11 16:21:23,276 - INFO - >>> {'loss': 0.5547, 'grad_norm': 1.6234729290008545, 'learning_rate': 0.00018026206401318482, 'epoch': 10.48}
>>> 2025-09-11 16:21:53,158 - INFO - >>> {'loss': 0.6007, 'grad_norm': 3.346487283706665, 'learning_rate': 0.0001797218773133841, 'epoch': 10.64}
>>> 2025-09-11 16:22:23,019 - INFO - >>> {'loss': 0.4962, 'grad_norm': 4.0240864753723145, 'learning_rate': 0.00017917523074808023, 'epoch': 10.8}
>>> 2025-09-11 16:22:53,589 - INFO - >>> {'loss': 0.641, 'grad_norm': 2.849245548248291, 'learning_rate': 0.0001786221686120567, 'epoch': 10.96}
>>> 2025-09-11 16:23:02,692 - INFO - >>> {'loss': 0.7184, 'grad_norm': 5.2486138343811035, 'learning_rate': 0.00017806273571995066, 'epoch': 11.0}
>>> 2025-09-11 16:23:32,605 - INFO - >>> {'loss': 0.4761, 'grad_norm': 3.3415493965148926, 'learning_rate': 0.00017749697740262197, 'epoch': 11.16}
>>> 2025-09-11 16:24:05,245 - INFO - >>> {'loss': 0.3958, 'grad_norm': 2.3888707160949707, 'learning_rate': 0.0001769249395034797, 'epoch': 11.32}
>>> 2025-09-11 16:24:38,615 - INFO - >>> {'loss': 0.4629, 'grad_norm': 2.790783166885376, 'learning_rate': 0.00017634666837476766, 'epoch': 11.48}
>>> 2025-09-11 16:25:08,559 - INFO - >>> {'loss': 0.3931, 'grad_norm': 2.4242396354675293, 'learning_rate': 0.0001757622108738083, 'epoch': 11.64}
>>> 2025-09-11 16:25:38,993 - INFO - >>> {'loss': 0.5533, 'grad_norm': 2.792494297027588, 'learning_rate': 0.00017517161435920607, 'epoch': 11.8}
>>> 2025-09-11 16:26:08,891 - INFO - >>> {'loss': 0.3554, 'grad_norm': 2.7154202461242676, 'learning_rate': 0.00017457492668700967, 'epoch': 11.96}
>>> 2025-09-11 16:26:16,352 - INFO - >>> {'loss': 0.327, 'grad_norm': 4.500041961669922, 'learning_rate': 0.00017397219620683465, 'epoch': 12.0}
>>> 2025-09-11 16:26:49,661 - INFO - >>> {'loss': 0.2998, 'grad_norm': 2.102487325668335, 'learning_rate': 0.00017336347175794524, 'epoch': 12.16}
>>> 2025-09-11 16:27:21,148 - INFO - >>> {'loss': 0.2574, 'grad_norm': 2.6695919036865234, 'learning_rate': 0.00017274880266529715, 'epoch': 12.32}
>>> 2025-09-11 16:27:52,192 - INFO - >>> {'loss': 0.2242, 'grad_norm': 2.0514495372772217, 'learning_rate': 0.0001721282387355408, 'epoch': 12.48}
>>> 2025-09-11 16:28:22,753 - INFO - >>> {'loss': 0.2593, 'grad_norm': 2.3434860706329346, 'learning_rate': 0.0001715018302529852, 'epoch': 12.64}
>>> 2025-09-11 16:28:52,766 - INFO - >>> {'loss': 0.279, 'grad_norm': 2.379472255706787, 'learning_rate': 0.00017086962797552375, 'epoch': 12.8}
>>> 2025-09-11 16:29:22,773 - INFO - >>> {'loss': 0.3248, 'grad_norm': 3.190830707550049, 'learning_rate': 0.0001702316831305212, 'epoch': 12.96}
>>> 2025-09-11 16:29:30,282 - INFO - >>> {'loss': 0.3489, 'grad_norm': 4.891545295715332, 'learning_rate': 0.00016958804741066253, 'epoch': 13.0}
>>> 2025-09-11 16:29:58,192 - INFO - >>> {'loss': 0.1812, 'grad_norm': 1.642616629600525, 'learning_rate': 0.0001689387729697646, 'epoch': 13.16}
>>> 2025-09-11 16:30:28,176 - INFO - >>> {'loss': 0.1038, 'grad_norm': 2.011226177215576, 'learning_rate': 0.00016828391241854984, 'epoch': 13.32}
>>> 2025-09-11 16:30:59,811 - INFO - >>> {'loss': 0.2056, 'grad_norm': 1.7210283279418945, 'learning_rate': 0.0001676235188203834, 'epoch': 13.48}
>>> 2025-09-11 16:31:29,791 - INFO - >>> {'loss': 0.2019, 'grad_norm': 2.2685186862945557, 'learning_rate': 0.00016695764568697328, 'epoch': 13.64}
>>> 2025-09-11 16:32:03,851 - INFO - >>> {'loss': 0.1803, 'grad_norm': 2.3495192527770996, 'learning_rate': 0.00016628634697403447, 'epoch': 13.8}
>>> 2025-09-11 16:32:34,915 - INFO - >>> {'loss': 0.1614, 'grad_norm': 2.759316921234131, 'learning_rate': 0.00016560967707691663, 'epoch': 13.96}
>>> 2025-09-11 16:32:42,420 - INFO - >>> {'loss': 0.1703, 'grad_norm': 4.656376361846924, 'learning_rate': 0.0001649276908261967, 'epoch': 14.0}
>>> 2025-09-11 16:33:12,376 - INFO - >>> {'loss': 0.1148, 'grad_norm': 1.2989484071731567, 'learning_rate': 0.00016424044348323582, 'epoch': 14.16}
>>> 2025-09-11 16:33:42,876 - INFO - >>> {'loss': 0.1929, 'grad_norm': 2.9264073371887207, 'learning_rate': 0.0001635479907357016, 'epoch': 14.32}
>>> 2025-09-11 16:34:13,508 - INFO - >>> {'loss': 0.102, 'grad_norm': 1.7755409479141235, 'learning_rate': 0.00016285038869305563, 'epoch': 14.48}
>>> 2025-09-11 16:34:45,074 - INFO - >>> {'loss': 0.0867, 'grad_norm': 1.183602213859558, 'learning_rate': 0.0001621476938820071, 'epoch': 14.64}
>>> 2025-09-11 16:35:14,983 - INFO - >>> {'loss': 0.0833, 'grad_norm': 1.8280961513519287, 'learning_rate': 0.00016143996324193225, 'epoch': 14.8}
>>> 2025-09-11 16:35:45,973 - INFO - >>> {'loss': 0.0726, 'grad_norm': 1.9095126390457153, 'learning_rate': 0.00016072725412026066, 'epoch': 14.96}
>>> 2025-09-11 16:35:56,924 - INFO - >>> {'loss': 0.1182, 'grad_norm': 3.568629264831543, 'learning_rate': 0.00016000962426782845, 'epoch': 15.0}
>>> 2025-09-11 16:36:27,422 - INFO - >>> {'loss': 0.0931, 'grad_norm': 2.187527656555176, 'learning_rate': 0.0001592871318341986, 'epoch': 15.16}
>>> 2025-09-11 16:36:58,442 - INFO - >>> {'loss': 0.0858, 'grad_norm': 1.9743058681488037, 'learning_rate': 0.0001585598353629492, 'epoch': 15.32}
>>> 2025-09-11 16:37:28,329 - INFO - >>> {'loss': 0.0191, 'grad_norm': 0.7805014848709106, 'learning_rate': 0.00015782779378692956, 'epoch': 15.48}
>>> 2025-09-11 16:37:59,859 - INFO - >>> {'loss': 0.0527, 'grad_norm': 1.4158767461776733, 'learning_rate': 0.000157091066423485, 'epoch': 15.64}
>>> 2025-09-11 16:38:29,763 - INFO - >>> {'loss': 0.0649, 'grad_norm': 1.1002542972564697, 'learning_rate': 0.0001563497129696503, 'epoch': 15.8}
>>> 2025-09-11 16:38:59,613 - INFO - >>> {'loss': 0.0586, 'grad_norm': 3.172679901123047, 'learning_rate': 0.00015560379349731233, 'epoch': 15.96}
>>> 2025-09-11 16:39:10,542 - INFO - >>> {'loss': 0.0791, 'grad_norm': 3.795658588409424, 'learning_rate': 0.00015485336844834273, 'epoch': 16.0}
>>> 2025-09-11 16:39:41,057 - INFO - >>> {'loss': 0.0383, 'grad_norm': 1.032526969909668, 'learning_rate': 0.00015409849862969995, 'epoch': 16.16}
>>> 2025-09-11 16:40:12,512 - INFO - >>> {'loss': 0.0263, 'grad_norm': 1.1907711029052734, 'learning_rate': 0.00015333924520850227, 'epoch': 16.32}
>>> 2025-09-11 16:40:42,371 - INFO - >>> {'loss': 0.0641, 'grad_norm': 2.1937272548675537, 'learning_rate': 0.00015257566970707146, 'epoch': 16.48}
>>> 2025-09-11 16:41:13,387 - INFO - >>> {'loss': 0.0412, 'grad_norm': 1.1230926513671875, 'learning_rate': 0.0001518078339979475, 'epoch': 16.64}
>>> 2025-09-11 16:41:46,808 - INFO - >>> {'loss': 0.0358, 'grad_norm': 0.9955900311470032, 'learning_rate': 0.00015103580029887504, 'epoch': 16.8}
>>> 2025-09-11 16:42:16,760 - INFO - >>> {'loss': 0.0351, 'grad_norm': 1.511924386024475, 'learning_rate': 0.00015025963116776202, 'epoch': 16.96}
>>> 2025-09-11 16:42:24,255 - INFO - >>> {'loss': 0.0339, 'grad_norm': 1.5550639629364014, 'learning_rate': 0.00014947938949761054, 'epoch': 17.0}
>>> 2025-09-11 16:42:54,202 - INFO - >>> {'loss': 0.0208, 'grad_norm': 1.3537908792495728, 'learning_rate': 0.0001486951385114205, 'epoch': 17.16}
>>> 2025-09-11 16:43:27,708 - INFO - >>> {'loss': 0.0174, 'grad_norm': 0.3631656765937805, 'learning_rate': 0.00014790694175706697, 'epoch': 17.32}
>>> 2025-09-11 16:43:59,907 - INFO - >>> {'loss': 0.0277, 'grad_norm': 1.1369240283966064, 'learning_rate': 0.00014711486310215052, 'epoch': 17.48}
>>> 2025-09-11 16:44:31,025 - INFO - >>> {'loss': 0.031, 'grad_norm': 0.802219808101654, 'learning_rate': 0.00014631896672882234, 'epoch': 17.64}
>>> 2025-09-11 16:45:00,992 - INFO - >>> {'loss': 0.0444, 'grad_norm': 2.0467922687530518, 'learning_rate': 0.00014551931712858334, 'epoch': 17.8}
>>> 2025-09-11 16:45:30,913 - INFO - >>> {'loss': 0.0289, 'grad_norm': 2.4274308681488037, 'learning_rate': 0.00014471597909705857, 'epoch': 17.96}
>>> 2025-09-11 16:45:38,410 - INFO - >>> {'loss': 0.0269, 'grad_norm': 1.3334929943084717, 'learning_rate': 0.00014390901772874667, 'epoch': 18.0}
>>> 2025-09-11 16:46:08,881 - INFO - >>> {'loss': 0.0154, 'grad_norm': 0.5979023575782776, 'learning_rate': 0.00014309849841174537, 'epoch': 18.16}
>>> 2025-09-11 16:46:41,516 - INFO - >>> {'loss': 0.0181, 'grad_norm': 0.9977516531944275, 'learning_rate': 0.0001422844868224531, 'epoch': 18.32}
>>> 2025-09-11 16:47:11,423 - INFO - >>> {'loss': 0.0175, 'grad_norm': 1.0848394632339478, 'learning_rate': 0.00014146704892024713, 'epoch': 18.48}
>>> 2025-09-11 16:47:41,326 - INFO - >>> {'loss': 0.0185, 'grad_norm': 1.0255166292190552, 'learning_rate': 0.000140646250942139, 'epoch': 18.64}
>>> 2025-09-11 16:48:11,196 - INFO - >>> {'loss': 0.0154, 'grad_norm': 1.201514482498169, 'learning_rate': 0.00013982215939740725, 'epoch': 18.8}
>>> 2025-09-11 16:48:44,661 - INFO - >>> {'loss': 0.0386, 'grad_norm': 2.1955320835113525, 'learning_rate': 0.00013899484106220814, 'epoch': 18.96}
>>> 2025-09-11 16:48:52,148 - INFO - >>> {'loss': 0.0204, 'grad_norm': 0.7277173399925232, 'learning_rate': 0.00013816436297416494, 'epoch': 19.0}
>>> 2025-09-11 16:49:26,670 - INFO - >>> {'loss': 0.0239, 'grad_norm': 1.186026930809021, 'learning_rate': 0.00013733079242693572, 'epoch': 19.16}
>>> 2025-09-11 16:49:56,498 - INFO - >>> {'loss': 0.0109, 'grad_norm': 0.5084662437438965, 'learning_rate': 0.00013649419696476055, 'epoch': 19.32}
>>> 2025-09-11 16:50:24,880 - INFO - >>> {'loss': 0.022, 'grad_norm': 1.0418224334716797, 'learning_rate': 0.00013565464437698848, 'epoch': 19.48}
>>> 2025-09-11 16:50:56,314 - INFO - >>> {'loss': 0.0119, 'grad_norm': 0.5420453548431396, 'learning_rate': 0.00013481220269258447, 'epoch': 19.64}
>>> 2025-09-11 16:51:26,839 - INFO - >>> {'loss': 0.018, 'grad_norm': 1.3514543771743774, 'learning_rate': 0.00013396694017461707, 'epoch': 19.8}
>>> 2025-09-11 16:51:56,783 - INFO - >>> {'loss': 0.016, 'grad_norm': 0.7427264451980591, 'learning_rate': 0.00013311892531472704, 'epoch': 19.96}
>>> 2025-09-11 16:52:04,273 - INFO - >>> {'loss': 0.0069, 'grad_norm': 1.461871862411499, 'learning_rate': 0.00013226822682757745, 'epoch': 20.0}
>>> 2025-09-11 16:52:34,248 - INFO - >>> {'loss': 0.0143, 'grad_norm': 0.41302481293678284, 'learning_rate': 0.00013141491364528576, 'epoch': 20.16}
>>> 2025-09-11 16:53:07,721 - INFO - >>> {'loss': 0.0095, 'grad_norm': 0.31269410252571106, 'learning_rate': 0.00013055905491183821, 'epoch': 20.32}
>>> 2025-09-11 16:53:38,327 - INFO - >>> {'loss': 0.0137, 'grad_norm': 0.9824133515357971, 'learning_rate': 0.00012970071997748712, 'epoch': 20.48}
>>> 2025-09-11 16:54:09,908 - INFO - >>> {'loss': 0.0101, 'grad_norm': 0.610589325428009, 'learning_rate': 0.00012883997839313152, 'epoch': 20.64}
>>> 2025-09-11 16:54:39,930 - INFO - >>> {'loss': 0.0332, 'grad_norm': 1.5832319259643555, 'learning_rate': 0.00012797689990468112, 'epoch': 20.8}
>>> 2025-09-11 16:55:10,994 - INFO - >>> {'loss': 0.0183, 'grad_norm': 1.403222680091858, 'learning_rate': 0.0001271115544474053, 'epoch': 20.96}
>>> 2025-09-11 16:55:18,530 - INFO - >>> {'loss': 0.0254, 'grad_norm': 4.141658782958984, 'learning_rate': 0.00012624401214026573, 'epoch': 21.0}
>>> 2025-09-11 16:55:48,541 - INFO - >>> {'loss': 0.0179, 'grad_norm': 1.3025516271591187, 'learning_rate': 0.000125374343280235, 'epoch': 21.16}
>>> 2025-09-11 16:56:19,597 - INFO - >>> {'loss': 0.0079, 'grad_norm': 0.29506129026412964, 'learning_rate': 0.00012450261833660034, 'epoch': 21.32}
>>> 2025-09-11 16:56:49,556 - INFO - >>> {'loss': 0.0208, 'grad_norm': 1.4852479696273804, 'learning_rate': 0.0001236289079452534, 'epoch': 21.48}
>>> 2025-09-11 16:57:21,164 - INFO - >>> {'loss': 0.0102, 'grad_norm': 0.630635678768158, 'learning_rate': 0.00012275328290296678, 'epoch': 21.64}
>>> 2025-09-11 16:57:51,160 - INFO - >>> {'loss': 0.0076, 'grad_norm': 0.48713529109954834, 'learning_rate': 0.00012187581416165721, 'epoch': 21.8}
>>> 2025-09-11 16:58:25,206 - INFO - >>> {'loss': 0.0068, 'grad_norm': 0.20920541882514954, 'learning_rate': 0.0001209965728226365, 'epoch': 21.96}
>>> 2025-09-11 16:58:32,724 - INFO - >>> {'loss': 0.0132, 'grad_norm': 1.2296222448349, 'learning_rate': 0.00012011563013084996, 'epoch': 22.0}
>>> 2025-09-11 16:59:04,347 - INFO - >>> {'loss': 0.0087, 'grad_norm': 0.47130340337753296, 'learning_rate': 0.00011923305746910371, 'epoch': 22.16}
>>> 2025-09-11 16:59:37,858 - INFO - >>> {'loss': 0.0062, 'grad_norm': 0.1664409041404724, 'learning_rate': 0.00011834892635228023, 'epoch': 22.32}
>>> 2025-09-11 17:00:07,783 - INFO - >>> {'loss': 0.0129, 'grad_norm': 1.2533282041549683, 'learning_rate': 0.00011746330842154371, 'epoch': 22.48}
>>> 2025-09-11 17:00:37,705 - INFO - >>> {'loss': 0.0084, 'grad_norm': 0.4427216649055481, 'learning_rate': 0.00011657627543853491, 'epoch': 22.64}
>>> 2025-09-11 17:01:07,620 - INFO - >>> {'loss': 0.0092, 'grad_norm': 0.45545828342437744, 'learning_rate': 0.0001156878992795563, 'epoch': 22.8}
>>> 2025-09-11 17:01:37,522 - INFO - >>> {'loss': 0.0406, 'grad_norm': 3.445655107498169, 'learning_rate': 0.00011479825192974792, 'epoch': 22.96}
>>> 2025-09-11 17:01:45,002 - INFO - >>> {'loss': 0.0062, 'grad_norm': 0.29427236318588257, 'learning_rate': 0.00011390740547725443, 'epoch': 23.0}
>>> 2025-09-11 17:02:14,855 - INFO - >>> {'loss': 0.0096, 'grad_norm': 0.37511780858039856, 'learning_rate': 0.00011301543210738384, 'epoch': 23.16}
>>> 2025-09-11 17:02:44,731 - INFO - >>> {'loss': 0.006, 'grad_norm': 0.19244855642318726, 'learning_rate': 0.00011212240409675825, 'epoch': 23.32}
>>> 2025-09-11 17:03:16,935 - INFO - >>> {'loss': 0.0093, 'grad_norm': 0.801551342010498, 'learning_rate': 0.00011122839380745737, 'epoch': 23.48}
>>> 2025-09-11 17:03:48,039 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.4118579030036926, 'learning_rate': 0.00011033347368115494, 'epoch': 23.64}
>>> 2025-09-11 17:04:18,053 - INFO - >>> {'loss': 0.012, 'grad_norm': 0.4750998020172119, 'learning_rate': 0.00010943771623324883, 'epoch': 23.8}
>>> 2025-09-11 17:04:48,080 - INFO - >>> {'loss': 0.0063, 'grad_norm': 0.4266706109046936, 'learning_rate': 0.00010854119404698511, 'epoch': 23.96}
>>> 2025-09-11 17:04:59,105 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.33964803814888, 'learning_rate': 0.00010764397976757656, 'epoch': 24.0}
>>> 2025-09-11 17:05:29,057 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.155917227268219, 'learning_rate': 0.00010674614609631634, 'epoch': 24.16}
>>> 2025-09-11 17:05:58,990 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.24622373282909393, 'learning_rate': 0.00010584776578468698, 'epoch': 24.32}
>>> 2025-09-11 17:06:30,571 - INFO - >>> {'loss': 0.0116, 'grad_norm': 0.8240596652030945, 'learning_rate': 0.00010494891162846514, 'epoch': 24.48}
>>> 2025-09-11 17:07:03,941 - INFO - >>> {'loss': 0.0045, 'grad_norm': 0.13875837624073029, 'learning_rate': 0.0001040496564618233, 'epoch': 24.64}
>>> 2025-09-11 17:07:35,580 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.1687004566192627, 'learning_rate': 0.0001031500731514277, 'epoch': 24.8}
>>> 2025-09-11 17:08:05,489 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.5408428907394409, 'learning_rate': 0.00010225023459053415, 'epoch': 24.96}
>>> 2025-09-11 17:08:12,976 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.00010135021369308137, 'epoch': 25.0}
>>> 2025-09-11 17:08:42,897 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.12614218890666962, 'learning_rate': 0.00010045008338778279, 'epoch': 25.16}
>>> 2025-09-11 17:09:12,868 - INFO - >>> {'loss': 0.0058, 'grad_norm': 0.6938681602478027, 'learning_rate': 9.954991661221723e-05, 'epoch': 25.32}
>>> 2025-09-11 17:09:42,815 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.17862972617149353, 'learning_rate': 9.864978630691866e-05, 'epoch': 25.48}
>>> 2025-09-11 17:10:13,341 - INFO - >>> {'loss': 0.0063, 'grad_norm': 0.26867493987083435, 'learning_rate': 9.774976540946587e-05, 'epoch': 25.64}
>>> 2025-09-11 17:10:44,903 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.1853647083044052, 'learning_rate': 9.684992684857232e-05, 'epoch': 25.8}
>>> 2025-09-11 17:11:19,357 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.09610742330551147, 'learning_rate': 9.595034353817672e-05, 'epoch': 25.96}
>>> 2025-09-11 17:11:26,823 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.50510883715349e-05, 'epoch': 26.0}
>>> 2025-09-11 17:11:56,658 - INFO - >>> {'loss': 0.0232, 'grad_norm': 2.414478302001953, 'learning_rate': 9.415223421531307e-05, 'epoch': 26.16}
>>> 2025-09-11 17:12:26,485 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.08577457815408707, 'learning_rate': 9.325385390368368e-05, 'epoch': 26.32}
>>> 2025-09-11 17:12:56,374 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.08876799792051315, 'learning_rate': 9.235602023242349e-05, 'epoch': 26.48}
>>> 2025-09-11 17:13:27,371 - INFO - >>> {'loss': 0.0059, 'grad_norm': 0.1774718165397644, 'learning_rate': 9.145880595301494e-05, 'epoch': 26.64}
>>> 2025-09-11 17:14:00,781 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.06116964668035507, 'learning_rate': 9.056228376675118e-05, 'epoch': 26.8}
>>> 2025-09-11 17:14:33,035 - INFO - >>> {'loss': 0.0043, 'grad_norm': 0.25213485956192017, 'learning_rate': 8.966652631884505e-05, 'epoch': 26.96}
>>> 2025-09-11 17:14:40,536 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.38357603549957275, 'learning_rate': 8.877160619254265e-05, 'epoch': 27.0}
>>> 2025-09-11 17:15:11,572 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.06409559398889542, 'learning_rate': 8.787759590324176e-05, 'epoch': 27.16}
>>> 2025-09-11 17:15:41,597 - INFO - >>> {'loss': 0.008, 'grad_norm': 0.30137622356414795, 'learning_rate': 8.698456789261616e-05, 'epoch': 27.32}
>>> 2025-09-11 17:16:11,655 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.04782439395785332, 'learning_rate': 8.609259452274558e-05, 'epoch': 27.48}
>>> 2025-09-11 17:16:47,338 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.11490821093320847, 'learning_rate': 8.520174807025209e-05, 'epoch': 27.64}
>>> 2025-09-11 17:17:17,280 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.1079399436712265, 'learning_rate': 8.43121007204437e-05, 'epoch': 27.8}
>>> 2025-09-11 17:17:47,160 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.7252976894378662, 'learning_rate': 8.342372456146511e-05, 'epoch': 27.96}
>>> 2025-09-11 17:17:54,636 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.15008793771266937, 'learning_rate': 8.253669157845631e-05, 'epoch': 28.0}
>>> 2025-09-11 17:18:25,643 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.1597946733236313, 'learning_rate': 8.16510736477198e-05, 'epoch': 28.16}
>>> 2025-09-11 17:18:55,573 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.05733702704310417, 'learning_rate': 8.076694253089631e-05, 'epoch': 28.32}
>>> 2025-09-11 17:19:27,138 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.14523565769195557, 'learning_rate': 7.988436986915005e-05, 'epoch': 28.48}
>>> 2025-09-11 17:19:57,652 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.046348877251148224, 'learning_rate': 7.900342717736353e-05, 'epoch': 28.64}
>>> 2025-09-11 17:20:31,726 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.14260992407798767, 'learning_rate': 7.812418583834283e-05, 'epoch': 28.8}
>>> 2025-09-11 17:21:01,666 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.05975231155753136, 'learning_rate': 7.724671709703327e-05, 'epoch': 28.96}
>>> 2025-09-11 17:21:09,162 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.0817967876791954, 'learning_rate': 7.637109205474664e-05, 'epoch': 29.0}
>>> 2025-09-11 17:21:37,691 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.039716556668281555, 'learning_rate': 7.549738166339971e-05, 'epoch': 29.16}
>>> 2025-09-11 17:22:11,098 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.1430945098400116, 'learning_rate': 7.462565671976503e-05, 'epoch': 29.32}
>>> 2025-09-11 17:22:42,661 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.15829454362392426, 'learning_rate': 7.375598785973428e-05, 'epoch': 29.48}
>>> 2025-09-11 17:23:13,618 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.05793265253305435, 'learning_rate': 7.288844555259471e-05, 'epoch': 29.64}
>>> 2025-09-11 17:23:43,537 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.04073552414774895, 'learning_rate': 7.202310009531885e-05, 'epoch': 29.8}
>>> 2025-09-11 17:24:13,989 - INFO - >>> {'loss': 0.0021, 'grad_norm': 0.044304780662059784, 'learning_rate': 7.11600216068685e-05, 'epoch': 29.96}
>>> 2025-09-11 17:24:21,450 - INFO - >>> {'loss': 0.0119, 'grad_norm': 0.9239816069602966, 'learning_rate': 7.029928002251287e-05, 'epoch': 30.0}
>>> 2025-09-11 17:24:51,271 - INFO - >>> {'loss': 0.0019, 'grad_norm': 0.07280372083187103, 'learning_rate': 6.944094508816181e-05, 'epoch': 30.16}
>>> 2025-09-11 17:25:22,733 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.18159545958042145, 'learning_rate': 6.858508635471428e-05, 'epoch': 30.32}
>>> 2025-09-11 17:25:56,152 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.031135259196162224, 'learning_rate': 6.773177317242256e-05, 'epoch': 30.48}
>>> 2025-09-11 17:26:26,662 - INFO - >>> {'loss': 0.0088, 'grad_norm': 0.2787705361843109, 'learning_rate': 6.688107468527297e-05, 'epoch': 30.64}
>>> 2025-09-11 17:26:56,655 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.22402337193489075, 'learning_rate': 6.603305982538295e-05, 'epoch': 30.8}
>>> 2025-09-11 17:27:26,704 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.05166596546769142, 'learning_rate': 6.518779730741555e-05, 'epoch': 30.96}
>>> 2025-09-11 17:27:34,215 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.08679904043674469, 'learning_rate': 6.434535562301153e-05, 'epoch': 31.0}
>>> 2025-09-11 17:28:04,252 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.08161275833845139, 'learning_rate': 6.350580303523947e-05, 'epoch': 31.16}
>>> 2025-09-11 17:28:35,911 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.165044367313385, 'learning_rate': 6.266920757306429e-05, 'epoch': 31.32}
>>> 2025-09-11 17:29:05,925 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.03463316708803177, 'learning_rate': 6.183563702583506e-05, 'epoch': 31.48}
>>> 2025-09-11 17:29:35,806 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.06360602378845215, 'learning_rate': 6.1005158937791886e-05, 'epoch': 31.64}
>>> 2025-09-11 17:30:05,738 - INFO - >>> {'loss': 0.009, 'grad_norm': 0.25394508242607117, 'learning_rate': 6.017784060259279e-05, 'epoch': 31.8}
>>> 2025-09-11 17:30:36,198 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.044455092400312424, 'learning_rate': 5.935374905786102e-05, 'epoch': 31.96}
>>> 2025-09-11 17:30:47,224 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.032863907516002655, 'learning_rate': 5.85329510797529e-05, 'epoch': 32.0}
>>> 2025-09-11 17:31:17,683 - INFO - >>> {'loss': 0.0071, 'grad_norm': 0.2593596577644348, 'learning_rate': 5.771551317754691e-05, 'epoch': 32.16}
>>> 2025-09-11 17:31:47,581 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.06502118706703186, 'learning_rate': 5.6901501588254624e-05, 'epoch': 32.32}
>>> 2025-09-11 17:32:22,705 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.10642380267381668, 'learning_rate': 5.6090982271253336e-05, 'epoch': 32.48}
>>> 2025-09-11 17:32:52,653 - INFO - >>> {'loss': 0.0017, 'grad_norm': 0.030168108642101288, 'learning_rate': 5.528402090294143e-05, 'epoch': 32.64}
>>> 2025-09-11 17:33:23,654 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.02719154767692089, 'learning_rate': 5.448068287141663e-05, 'epoch': 32.8}
>>> 2025-09-11 17:33:53,370 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.0274931862950325, 'learning_rate': 5.3681033271177686e-05, 'epoch': 32.96}
>>> 2025-09-11 17:34:00,843 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.288513689784951e-05, 'epoch': 33.0}
>>> 2025-09-11 17:34:31,827 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.02789956144988537, 'learning_rate': 5.209305824293307e-05, 'epoch': 33.16}
>>> 2025-09-11 17:35:01,239 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.03323959931731224, 'learning_rate': 5.130486148857951e-05, 'epoch': 33.32}
>>> 2025-09-11 17:35:32,774 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.17305849492549896, 'learning_rate': 5.05206105023895e-05, 'epoch': 33.48}
>>> 2025-09-11 17:36:02,607 - INFO - >>> {'loss': 0.0045, 'grad_norm': 0.16230444610118866, 'learning_rate': 4.974036883223798e-05, 'epoch': 33.64}
>>> 2025-09-11 17:36:32,414 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.037842996418476105, 'learning_rate': 4.896419970112499e-05, 'epoch': 33.8}
>>> 2025-09-11 17:37:02,259 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.026271037757396698, 'learning_rate': 4.819216600205254e-05, 'epoch': 33.96}
>>> 2025-09-11 17:37:13,206 - INFO - >>> {'loss': 0.0016, 'grad_norm': 0.04327289015054703, 'learning_rate': 4.742433029292855e-05, 'epoch': 34.0}
>>> 2025-09-11 17:37:44,827 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.0942545086145401, 'learning_rate': 4.6660754791497754e-05, 'epoch': 34.16}
>>> 2025-09-11 17:38:14,837 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.11229254305362701, 'learning_rate': 4.5901501370300094e-05, 'epoch': 34.32}
>>> 2025-09-11 17:38:45,457 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.023686666041612625, 'learning_rate': 4.51466315516573e-05, 'epoch': 34.48}
>>> 2025-09-11 17:39:16,561 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.16211417317390442, 'learning_rate': 4.439620650268771e-05, 'epoch': 34.64}
>>> 2025-09-11 17:39:46,522 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.03529459983110428, 'learning_rate': 4.3650287030349755e-05, 'epoch': 34.8}
>>> 2025-09-11 17:40:19,970 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.0265116635710001, 'learning_rate': 4.290893357651502e-05, 'epoch': 34.96}
>>> 2025-09-11 17:40:27,470 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.06609475612640381, 'learning_rate': 4.217220621307043e-05, 'epoch': 35.0}
>>> 2025-09-11 17:40:59,064 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.13947518169879913, 'learning_rate': 4.14401646370508e-05, 'epoch': 35.16}
>>> 2025-09-11 17:41:32,583 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.030817843973636627, 'learning_rate': 4.071286816580142e-05, 'epoch': 35.32}
>>> 2025-09-11 17:42:02,608 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.14838683605194092, 'learning_rate': 3.999037573217157e-05, 'epoch': 35.48}
>>> 2025-09-11 17:42:33,244 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.13237090408802032, 'learning_rate': 3.9272745879739346e-05, 'epoch': 35.64}
>>> 2025-09-11 17:43:04,267 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.02478335238993168, 'learning_rate': 3.8560036758067764e-05, 'epoch': 35.8}
>>> 2025-09-11 17:43:34,220 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.02398560754954815, 'learning_rate': 3.7852306117992896e-05, 'epoch': 35.96}
>>> 2025-09-11 17:43:41,705 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.057657476514577866, 'learning_rate': 3.7149611306944355e-05, 'epoch': 36.0}
>>> 2025-09-11 17:44:11,697 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.09340441972017288, 'learning_rate': 3.645200926429844e-05, 'epoch': 36.16}
>>> 2025-09-11 17:44:41,670 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.17319132387638092, 'learning_rate': 3.5759556516764205e-05, 'epoch': 36.32}
>>> 2025-09-11 17:45:15,114 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.026633245870471, 'learning_rate': 3.507230917380332e-05, 'epoch': 36.48}
>>> 2025-09-11 17:45:45,064 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.022835733368992805, 'learning_rate': 3.439032292308338e-05, 'epoch': 36.64}
>>> 2025-09-11 17:46:17,221 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.19038087129592896, 'learning_rate': 3.3713653025965544e-05, 'epoch': 36.8}
>>> 2025-09-11 17:46:48,261 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.02001473307609558, 'learning_rate': 3.30423543130267e-05, 'epoch': 36.96}
>>> 2025-09-11 17:46:55,749 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.04525346681475639, 'learning_rate': 3.237648117961665e-05, 'epoch': 37.0}
>>> 2025-09-11 17:47:30,803 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.13631953299045563, 'learning_rate': 3.1716087581450196e-05, 'epoch': 37.16}
>>> 2025-09-11 17:48:01,803 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.020015142858028412, 'learning_rate': 3.106122703023544e-05, 'epoch': 37.32}
>>> 2025-09-11 17:48:31,752 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.09379135817289352, 'learning_rate': 3.041195258933749e-05, 'epoch': 37.48}
>>> 2025-09-11 17:49:01,692 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.01777430810034275, 'learning_rate': 2.976831686947884e-05, 'epoch': 37.64}
>>> 2025-09-11 17:49:32,154 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.17570990324020386, 'learning_rate': 2.913037202447625e-05, 'epoch': 37.8}
>>> 2025-09-11 17:50:01,988 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.02566251903772354, 'learning_rate': 2.8498169747014825e-05, 'epoch': 37.96}
>>> 2025-09-11 17:50:09,451 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.7871761264459227e-05, 'epoch': 38.0}
>>> 2025-09-11 17:50:39,315 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.14463399350643158, 'learning_rate': 2.725119733470284e-05, 'epoch': 38.16}
>>> 2025-09-11 17:51:09,204 - INFO - >>> {'loss': 0.0014, 'grad_norm': 0.0215465035289526, 'learning_rate': 2.663652824205476e-05, 'epoch': 38.32}
>>> 2025-09-11 17:51:39,052 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.025322994217276573, 'learning_rate': 2.602780379316535e-05, 'epoch': 38.48}
>>> 2025-09-11 17:52:12,393 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.02207760699093342, 'learning_rate': 2.5425073312990334e-05, 'epoch': 38.64}
>>> 2025-09-11 17:52:45,615 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.0918126180768013, 'learning_rate': 2.482838564079397e-05, 'epoch': 38.8}
>>> 2025-09-11 17:53:15,515 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.030709244310855865, 'learning_rate': 2.423778912619171e-05, 'epoch': 38.96}
>>> 2025-09-11 17:53:22,987 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.03565055504441261, 'learning_rate': 2.3653331625232367e-05, 'epoch': 39.0}
>>> 2025-09-11 17:53:53,483 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.01995268650352955, 'learning_rate': 2.3075060496520308e-05, 'epoch': 39.16}
>>> 2025-09-11 17:54:23,437 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.08933041244745255, 'learning_rate': 2.250302259737803e-05, 'epoch': 39.32}
>>> 2025-09-11 17:54:53,359 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.028882937505841255, 'learning_rate': 2.1937264280049363e-05, 'epoch': 39.48}
>>> 2025-09-11 17:55:23,305 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.1885255128145218, 'learning_rate': 2.137783138794335e-05, 'epoch': 39.64}
>>> 2025-09-11 17:55:53,275 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.024730365723371506, 'learning_rate': 2.0824769251919775e-05, 'epoch': 39.8}
>>> 2025-09-11 17:56:28,384 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.10891485214233398, 'learning_rate': 2.027812268661592e-05, 'epoch': 39.96}
>>> 2025-09-11 17:56:35,900 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.03848617896437645, 'learning_rate': 1.9737935986815205e-05, 'epoch': 40.0}
>>> 2025-09-11 17:57:07,597 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.1240956038236618, 'learning_rate': 1.9204252923858003e-05, 'epoch': 40.16}
>>> 2025-09-11 17:57:37,567 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.08672913908958435, 'learning_rate': 1.8677116742094857e-05, 'epoch': 40.32}
>>> 2025-09-11 17:58:11,099 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.015908854082226753, 'learning_rate': 1.8156570155382356e-05, 'epoch': 40.48}
>>> 2025-09-11 17:58:42,092 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.024273226037621498, 'learning_rate': 1.764265534362205e-05, 'epoch': 40.64}
>>> 2025-09-11 17:59:12,013 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.022972073405981064, 'learning_rate': 1.7135413949342704e-05, 'epoch': 40.8}
>>> 2025-09-11 17:59:41,955 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.02329392172396183, 'learning_rate': 1.6634887074325843e-05, 'epoch': 40.96}
>>> 2025-09-11 17:59:49,446 - INFO - >>> {'loss': 0.0007, 'grad_norm': 0.06524734944105148, 'learning_rate': 1.6141115276275298e-05, 'epoch': 41.0}
>>> 2025-09-11 18:00:19,340 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.027043938636779785, 'learning_rate': 1.565413856553095e-05, 'epoch': 41.16}
>>> 2025-09-11 18:00:53,274 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.021828312426805496, 'learning_rate': 1.5173996401826562e-05, 'epoch': 41.32}
>>> 2025-09-11 18:01:23,129 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.11360637843608856, 'learning_rate': 1.470072769109242e-05, 'epoch': 41.48}
>>> 2025-09-11 18:01:54,016 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.17413072288036346, 'learning_rate': 1.4234370782302742e-05, 'epoch': 41.64}
>>> 2025-09-11 18:02:25,590 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.10741022229194641, 'learning_rate': 1.3774963464368295e-05, 'epoch': 41.8}
>>> 2025-09-11 18:02:55,462 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.017007125541567802, 'learning_rate': 1.3322542963074314e-05, 'epoch': 41.96}
>>> 2025-09-11 18:03:02,956 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.028923705220222473, 'learning_rate': 1.287714593806415e-05, 'epoch': 42.0}
>>> 2025-09-11 18:03:32,795 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.02272893860936165, 'learning_rate': 1.2438808479868714e-05, 'epoch': 42.16}
>>> 2025-09-11 18:04:02,633 - INFO - >>> {'loss': 0.0013, 'grad_norm': 0.031385477632284164, 'learning_rate': 1.2007566106982049e-05, 'epoch': 42.32}
>>> 2025-09-11 18:04:33,602 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.019125917926430702, 'learning_rate': 1.1583453762983287e-05, 'epoch': 42.48}
>>> 2025-09-11 18:05:03,502 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.017673037946224213, 'learning_rate': 1.1166505813705187e-05, 'epoch': 42.64}
>>> 2025-09-11 18:05:38,473 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.16006238758563995, 'learning_rate': 1.0756756044449356e-05, 'epoch': 42.8}
>>> 2025-09-11 18:06:08,428 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.01842554286122322, 'learning_rate': 1.035423765724879e-05, 'epoch': 42.96}
>>> 2025-09-11 18:06:16,601 - INFO - >>> {'loss': 0.0059, 'grad_norm': 0.3918526768684387, 'learning_rate': 9.958983268177423e-06, 'epoch': 43.0}
>>> 2025-09-11 18:06:46,468 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.024567941203713417, 'learning_rate': 9.571024904707237e-06, 'epoch': 43.16}
>>> 2025-09-11 18:07:20,454 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.018802905455231667, 'learning_rate': 9.190394003113123e-06, 'epoch': 43.32}
>>> 2025-09-11 18:07:50,406 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.019375981763005257, 'learning_rate': 8.817121405925544e-06, 'epoch': 43.48}
>>> 2025-09-11 18:08:20,333 - INFO - >>> {'loss': 0.0008, 'grad_norm': 0.025722255930304527, 'learning_rate': 8.451237359431397e-06, 'epoch': 43.64}
>>> 2025-09-11 18:08:51,370 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.09733390808105469, 'learning_rate': 8.092771511223185e-06, 'epoch': 43.8}
>>> 2025-09-11 18:09:21,284 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.11868830770254135, 'learning_rate': 7.741752907796585e-06, 'epoch': 43.96}
>>> 2025-09-11 18:09:28,773 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.04947448521852493, 'learning_rate': 7.3982099921969135e-06, 'epoch': 44.0}
>>> 2025-09-11 18:10:02,263 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.01704218052327633, 'learning_rate': 7.0621706017143015e-06, 'epoch': 44.16}
>>> 2025-09-11 18:10:32,214 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.027589723467826843, 'learning_rate': 6.73366196562808e-06, 'epoch': 44.32}
>>> 2025-09-11 18:11:02,173 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.18267759680747986, 'learning_rate': 6.4127107030003685e-06, 'epoch': 44.48}
>>> 2025-09-11 18:11:33,744 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.17239412665367126, 'learning_rate': 6.099342820519183e-06, 'epoch': 44.64}
>>> 2025-09-11 18:12:03,678 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.1310824751853943, 'learning_rate': 5.793583710391059e-06, 'epoch': 44.8}
>>> 2025-09-11 18:12:34,671 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.017554854974150658, 'learning_rate': 5.495458148283506e-06, 'epoch': 44.96}
>>> 2025-09-11 18:12:42,153 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.05258818343281746, 'learning_rate': 5.2049902913175355e-06, 'epoch': 45.0}
>>> 2025-09-11 18:13:13,061 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.025333110243082047, 'learning_rate': 4.9222036761100595e-06, 'epoch': 45.16}
>>> 2025-09-11 18:13:42,908 - INFO - >>> {'loss': 0.0027, 'grad_norm': 0.1515537053346634, 'learning_rate': 4.647121216866857e-06, 'epoch': 45.32}
>>> 2025-09-11 18:14:12,755 - INFO - >>> {'loss': 0.0015, 'grad_norm': 0.027438482269644737, 'learning_rate': 4.379765203525754e-06, 'epoch': 45.48}
>>> 2025-09-11 18:14:42,674 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.01700044795870781, 'learning_rate': 4.1201572999505e-06, 'epoch': 45.64}
>>> 2025-09-11 18:15:13,248 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.015904275700449944, 'learning_rate': 3.868318542175331e-06, 'epoch': 45.8}
>>> 2025-09-11 18:15:46,695 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.1794549971818924, 'learning_rate': 3.6242693367004364e-06, 'epoch': 45.96}
>>> 2025-09-11 18:15:55,825 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.2721279561519623, 'learning_rate': 3.3880294588383597e-06, 'epoch': 46.0}
>>> 2025-09-11 18:16:27,430 - INFO - >>> {'loss': 0.0071, 'grad_norm': 0.3496631979942322, 'learning_rate': 3.1596180511117234e-06, 'epoch': 46.16}
>>> 2025-09-11 18:16:58,013 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.016525371000170708, 'learning_rate': 2.9390536217020147e-06, 'epoch': 46.32}
>>> 2025-09-11 18:17:29,107 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.018521057441830635, 'learning_rate': 2.726354042949875e-06, 'epoch': 46.48}
>>> 2025-09-11 18:17:59,029 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.018942764028906822, 'learning_rate': 2.5215365499069443e-06, 'epoch': 46.64}
>>> 2025-09-11 18:18:32,427 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.09858016669750214, 'learning_rate': 2.3246177389392388e-06, 'epoch': 46.8}
>>> 2025-09-11 18:19:02,354 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.021594436839222908, 'learning_rate': 2.1356135663824326e-06, 'epoch': 46.96}
>>> 2025-09-11 18:19:09,821 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.9545393472488737e-06, 'epoch': 47.0}
>>> 2025-09-11 18:19:40,744 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.025746775791049004, 'learning_rate': 1.7814097539865627e-06, 'epoch': 47.16}
>>> 2025-09-11 18:20:10,667 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.022937370464205742, 'learning_rate': 1.6162388152903495e-06, 'epoch': 47.32}
>>> 2025-09-11 18:20:43,837 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.01664501614868641, 'learning_rate': 1.4590399149650768e-06, 'epoch': 47.48}
>>> 2025-09-11 18:21:13,771 - INFO - >>> {'loss': 0.0023, 'grad_norm': 0.12233033776283264, 'learning_rate': 1.309825790841146e-06, 'epoch': 47.64}
>>> 2025-09-11 18:21:44,292 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.10676110535860062, 'learning_rate': 1.168608533742399e-06, 'epoch': 47.8}
>>> 2025-09-11 18:22:15,861 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.1415576934814453, 'learning_rate': 1.0353995865063138e-06, 'epoch': 47.96}
>>> 2025-09-11 18:22:23,346 - INFO - >>> {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.10209743056889e-07, 'epoch': 48.0}
>>> 2025-09-11 18:22:56,741 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.020955907180905342, 'learning_rate': 7.930491475299229e-07, 'epoch': 48.16}
>>> 2025-09-11 18:23:27,307 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.014748151414096355, 'learning_rate': 6.839272934511143e-07, 'epoch': 48.32}
>>> 2025-09-11 18:23:57,191 - INFO - >>> {'loss': 0.0009, 'grad_norm': 0.02324531227350235, 'learning_rate': 5.828530229667228e-07, 'epoch': 48.48}
>>> 2025-09-11 18:24:27,739 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.1401851624250412, 'learning_rate': 4.89834526127153e-07, 'epoch': 48.64}
>>> 2025-09-11 18:24:57,550 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.16803579032421112, 'learning_rate': 4.0487934022328535e-07, 'epoch': 48.8}
>>> 2025-09-11 18:25:28,996 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.1506778746843338, 'learning_rate': 3.27994349175742e-07, 'epoch': 48.96}
>>> 2025-09-11 18:25:36,463 - INFO - >>> {'loss': 0.0012, 'grad_norm': 0.04590078070759773, 'learning_rate': 2.591857829770672e-07, 'epoch': 49.0}
>>> 2025-09-11 18:26:09,764 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.027432097122073174, 'learning_rate': 1.9845921718690818e-07, 'epoch': 49.16}
>>> 2025-09-11 18:26:41,314 - INFO - >>> {'loss': 0.0011, 'grad_norm': 0.01864343136548996, 'learning_rate': 1.4581957248026578e-07, 'epoch': 49.32}
>>> 2025-09-11 18:27:08,737 - INFO - >>> {'loss': 0.002, 'grad_norm': 0.07526694238185883, 'learning_rate': 1.0127111424872438e-07, 'epoch': 49.48}
>>> 2025-09-11 18:27:38,737 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.2380543202161789, 'learning_rate': 6.481745225485059e-08, 'epoch': 49.64}
>>> 2025-09-11 18:28:10,389 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.1425171196460724, 'learning_rate': 3.6461540339682855e-08, 'epoch': 49.8}
>>> 2025-09-11 18:28:40,319 - INFO - >>> {'loss': 0.001, 'grad_norm': 0.019866645336151123, 'learning_rate': 1.6205676183411732e-08, 'epoch': 49.96}
>>> 2025-09-11 18:28:47,812 - INFO - >>> {'loss': 0.0005, 'grad_norm': 0.03557264432311058, 'learning_rate': 4.051501119162193e-09, 'epoch': 50.0}
>>> 2025-09-11 18:28:48,517 - INFO - >>> {'train_runtime': 9675.8364, 'train_samples_per_second': 0.517, 'train_steps_per_second': 0.036, 'train_loss': 0.38403095678858723, 'epoch': 50.0}
>>> 2025-09-11 18:28:48,519 - INFO - 训练成功！
>>> 2025-09-11 18:28:48,519 - INFO - 模型存放位置：./output/qwen3-8b202509111547
>>> 2025-09-11 21:04:32,299 - INFO - ========__main__  202509112104========
>>> 2025-09-11 21:04:32,300 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 21:04:32,301 - INFO - 开始进行模型测试
>>> 2025-09-11 21:04:39,754 - INFO - 已选择模型文件夹: qwen3-8b202509111547
>>> 2025-09-11 21:04:39,757 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509111547/checkpoint-350
>>> 2025-09-11 21:05:50,824 - INFO - ========__main__  202509112105========
>>> 2025-09-11 21:05:50,824 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 21:05:50,825 - INFO - 开始进行模型测试
>>> 2025-09-11 21:05:53,588 - INFO - 已选择模型文件夹: qwen3-8b202509111547
>>> 2025-09-11 21:05:53,591 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509111547/checkpoint-350
>>> 2025-09-11 21:08:49,934 - INFO - ========__main__  202509112108========
>>> 2025-09-11 21:08:49,934 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 21:08:49,935 - INFO - 开始进行模型测试
>>> 2025-09-11 21:08:53,150 - INFO - 已选择模型文件夹: qwen3-8b202509111547
>>> 2025-09-11 21:08:53,153 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509111547/checkpoint-350
>>> 2025-09-11 21:58:09,867 - INFO - ========__main__  202509112158========
>>> 2025-09-11 21:58:09,868 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-11 21:58:09,868 - INFO - 开始进行模型测试
>>> 2025-09-11 22:01:56,182 - INFO - 已选择模型文件夹: qwen3-8b202509111547
>>> 2025-09-11 22:01:56,184 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509111547/checkpoint-350
>>> 2025-09-16 17:52:09,643 - INFO - 导入包完成
>>> 2025-09-16 17:52:09,661 - INFO - ========train Qwen2ForCausalLM  202509161752========
>>> 2025-09-16 17:52:09,662 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-16 17:52:09,663 - INFO - 开始进行训练
>>> 2025-09-16 17:52:09,674 - INFO - 基础配置文件读取完成
>>> 2025-09-16 17:52:09,684 - INFO - 训练配置读取完成
>>> 2025-09-16 17:52:09,684 - INFO - 数据集路径：dataset/own/Medical_Extension.json
>>> 2025-09-16 17:52:09,685 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-09-16 17:52:10,282 - INFO - tokenizer读取完成
>>> 2025-09-16 17:52:10,629 - INFO - model dtype:torch.bfloat16
>>> 2025-09-16 17:52:10,629 - INFO - 模型导入完成
>>> 2025-09-16 17:52:10,630 - INFO - 数据读取开始
>>> 2025-09-16 17:52:11,683 - INFO - 数据下载完成，训练集大小: 600
>>> 2025-09-16 17:52:18,348 - INFO - 数据映射完成
>>> 2025-09-16 17:52:18,349 - INFO - 打印训练参数如下
>>> 2025-09-16 17:52:18,350 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-16 17:52:18,350 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-16 17:52:18,351 - INFO -   load_in_4bit >>> True
>>> 2025-09-16 17:52:18,351 - INFO -   batch_size >>> 4
>>> 2025-09-16 17:52:18,351 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-16 17:52:18,352 - INFO -   warmup_steps >>> 1
>>> 2025-09-16 17:52:18,352 - INFO -   epoch >>> 50
>>> 2025-09-16 17:52:18,352 - INFO -   eval_steps >>> 5
>>> 2025-09-16 17:52:18,353 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-16 17:52:18,353 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-16 17:52:18,353 - INFO -   max_seq_length >>> 2048
>>> 2025-09-16 17:52:18,354 - INFO -   r >>> 8
>>> 2025-09-16 17:52:18,354 - INFO -   interface_mode >>> False
>>> 2025-09-16 17:52:18,354 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-16 17:52:18,355 - INFO -   lora_alpha >>> 16
>>> 2025-09-16 17:52:18,355 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-16 17:52:18,355 - INFO -   bias >>> none
>>> 2025-09-16 17:52:18,356 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-16 17:52:18,356 - INFO -   random_state >>> 3407
>>> 2025-09-16 17:52:18,356 - INFO -   use_rslora >>> True
>>> 2025-09-16 17:52:18,357 - INFO -   loftq_config >>> None
>>> 2025-09-16 17:52:28,996 - INFO - 开始训练！
>>> 2025-09-16 17:52:52,379 - INFO - >>> {'loss': 3.1377, 'grad_norm': 1.2696826457977295, 'learning_rate': 0.0, 'epoch': 0.02666666666666667}
>>> 2025-09-16 17:53:11,308 - INFO - >>> {'loss': 3.2485, 'grad_norm': 1.3687043190002441, 'learning_rate': 0.0002, 'epoch': 0.05333333333333334}
>>> 2025-09-16 17:53:32,291 - INFO - >>> {'loss': 3.447, 'grad_norm': 1.5237585306167603, 'learning_rate': 0.0001999998631579029, 'epoch': 0.08}
>>> 2025-09-16 17:53:54,029 - INFO - >>> {'loss': 3.2208, 'grad_norm': 1.227089524269104, 'learning_rate': 0.000199999452631986, 'epoch': 0.10666666666666667}
>>> 2025-09-16 17:54:15,004 - INFO - >>> {'loss': 2.9094, 'grad_norm': 1.0880767107009888, 'learning_rate': 0.00019999876842337292, 'epoch': 0.13333333333333333}
>>> 2025-09-16 17:54:43,951 - INFO - >>> {'loss': 2.8187, 'grad_norm': 0.9457405209541321, 'learning_rate': 0.00019999781053393624, 'epoch': 0.16}
>>> 2025-09-16 17:55:03,743 - INFO - >>> {'loss': 2.6981, 'grad_norm': 0.9193099141120911, 'learning_rate': 0.0001999965789662975, 'epoch': 0.18666666666666668}
>>> 2025-09-16 17:55:21,177 - INFO - >>> {'loss': 2.5662, 'grad_norm': 0.7807024121284485, 'learning_rate': 0.00019999507372382738, 'epoch': 0.21333333333333335}
>>> 2025-09-16 17:56:30,234 - INFO - 导入包完成
>>> 2025-09-16 17:56:30,235 - INFO - ========train Qwen2ForCausalLM  202509161756========
>>> 2025-09-16 17:56:30,235 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-16 17:56:30,236 - INFO - 开始进行训练
>>> 2025-09-16 17:56:30,242 - INFO - 基础配置文件读取完成
>>> 2025-09-16 17:56:30,250 - INFO - 训练配置读取完成
>>> 2025-09-16 17:56:30,250 - INFO - 数据集路径：dataset/own/Medical_Extension.json
>>> 2025-09-16 17:56:30,250 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-09-16 17:56:30,756 - INFO - tokenizer读取完成
>>> 2025-09-16 17:56:30,914 - INFO - model dtype:torch.bfloat16
>>> 2025-09-16 17:56:30,915 - INFO - 模型导入完成
>>> 2025-09-16 17:56:30,915 - INFO - 数据读取开始
>>> 2025-09-16 17:56:31,763 - INFO - 数据下载完成，训练集大小: 600
>>> 2025-09-16 17:56:36,008 - INFO - 数据映射完成
>>> 2025-09-16 17:56:36,009 - INFO - 打印训练参数如下
>>> 2025-09-16 17:56:36,009 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-16 17:56:36,010 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-16 17:56:36,010 - INFO -   load_in_4bit >>> True
>>> 2025-09-16 17:56:36,010 - INFO -   batch_size >>> 8
>>> 2025-09-16 17:56:36,011 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-16 17:56:36,011 - INFO -   warmup_steps >>> 1
>>> 2025-09-16 17:56:36,011 - INFO -   epoch >>> 20
>>> 2025-09-16 17:56:36,012 - INFO -   eval_steps >>> 5
>>> 2025-09-16 17:56:36,012 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-16 17:56:36,012 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-16 17:56:36,013 - INFO -   max_seq_length >>> 2048
>>> 2025-09-16 17:56:36,013 - INFO -   r >>> 8
>>> 2025-09-16 17:56:36,013 - INFO -   interface_mode >>> False
>>> 2025-09-16 17:56:36,014 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-16 17:56:36,014 - INFO -   lora_alpha >>> 16
>>> 2025-09-16 17:56:36,014 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-16 17:56:36,015 - INFO -   bias >>> none
>>> 2025-09-16 17:56:36,015 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-16 17:56:36,015 - INFO -   random_state >>> 3407
>>> 2025-09-16 17:56:36,016 - INFO -   use_rslora >>> True
>>> 2025-09-16 17:56:36,016 - INFO -   loftq_config >>> None
>>> 2025-09-16 17:56:41,303 - INFO - 开始训练！
>>> 2025-09-16 17:57:29,481 - INFO - >>> {'loss': 3.1945, 'grad_norm': 1.2713738679885864, 'learning_rate': 0.0, 'epoch': 0.05333333333333334}
>>> 2025-09-16 17:58:15,542 - INFO - >>> {'loss': 3.4607, 'grad_norm': 1.4322764873504639, 'learning_rate': 0.0002, 'epoch': 0.10666666666666667}
>>> 2025-09-16 17:59:06,031 - INFO - >>> {'loss': 3.2567, 'grad_norm': 1.285992980003357, 'learning_rate': 0.00019999656450877908, 'epoch': 0.16}
>>> 2025-09-16 17:59:46,782 - INFO - >>> {'loss': 3.1967, 'grad_norm': 1.3612639904022217, 'learning_rate': 0.00019998625827116827, 'epoch': 0.21333333333333335}
>>> 2025-09-16 18:00:40,466 - INFO - >>> {'loss': 2.909, 'grad_norm': 1.0546884536743164, 'learning_rate': 0.00019996908199530736, 'epoch': 0.26666666666666666}
>>> 2025-09-16 18:01:33,588 - INFO - >>> {'loss': 2.7712, 'grad_norm': 0.9175724983215332, 'learning_rate': 0.00019994503686137524, 'epoch': 0.32}
>>> 2025-09-16 18:02:18,962 - INFO - >>> {'loss': 2.7193, 'grad_norm': 0.825046181678772, 'learning_rate': 0.0001999141245215089, 'epoch': 0.37333333333333335}
>>> 2025-09-16 18:02:59,209 - INFO - >>> {'loss': 2.577, 'grad_norm': 0.6944953799247742, 'learning_rate': 0.0001998763470996897, 'epoch': 0.4266666666666667}
>>> 2025-09-16 18:03:48,848 - INFO - >>> {'loss': 2.3954, 'grad_norm': 0.46566441655158997, 'learning_rate': 0.00019983170719159769, 'epoch': 0.48}
>>> 2025-09-16 18:04:31,941 - INFO - >>> {'loss': 2.4082, 'grad_norm': 0.4561515152454376, 'learning_rate': 0.00019978020786443312, 'epoch': 0.5333333333333333}
>>> 2025-09-16 18:05:21,631 - INFO - >>> {'loss': 2.4427, 'grad_norm': 0.6597782373428345, 'learning_rate': 0.00019972185265670572, 'epoch': 0.5866666666666667}
>>> 2025-09-16 18:06:10,681 - INFO - >>> {'loss': 2.3537, 'grad_norm': 0.6192197203636169, 'learning_rate': 0.00019965664557799163, 'epoch': 0.64}
>>> 2025-09-16 18:06:59,155 - INFO - >>> {'loss': 2.2971, 'grad_norm': 0.6701198220252991, 'learning_rate': 0.00019958459110865765, 'epoch': 0.6933333333333334}
>>> 2025-09-16 18:07:39,434 - INFO - >>> {'loss': 2.255, 'grad_norm': 0.5975915193557739, 'learning_rate': 0.0001995056941995538, 'epoch': 0.7466666666666667}
>>> 2025-09-16 18:08:23,954 - INFO - >>> {'loss': 2.2301, 'grad_norm': 0.5382068157196045, 'learning_rate': 0.00019941996027167286, 'epoch': 0.8}
>>> 2025-09-16 18:09:04,991 - INFO - >>> {'loss': 2.1636, 'grad_norm': 0.43579941987991333, 'learning_rate': 0.0001993273952157779, 'epoch': 0.8533333333333334}
>>> 2025-09-16 18:09:53,106 - INFO - >>> {'loss': 2.1886, 'grad_norm': 0.41217780113220215, 'learning_rate': 0.00019922800539199772, 'epoch': 0.9066666666666666}
>>> 2025-09-16 18:10:41,767 - INFO - >>> {'loss': 2.1705, 'grad_norm': 0.49829545617103577, 'learning_rate': 0.00019912179762938964, 'epoch': 0.96}
>>> 2025-09-16 18:11:20,773 - INFO - >>> {'loss': 2.1277, 'grad_norm': 0.42883747816085815, 'learning_rate': 0.00019900877922547034, 'epoch': 1.0}
>>> 2025-09-16 18:12:09,995 - INFO - >>> {'loss': 2.0267, 'grad_norm': 0.4647395610809326, 'learning_rate': 0.00019888895794571457, 'epoch': 1.0533333333333332}
>>> 2025-09-16 18:12:47,539 - INFO - >>> {'loss': 2.0092, 'grad_norm': 0.46555787324905396, 'learning_rate': 0.0001987623420230214, 'epoch': 1.1066666666666667}
>>> 2025-09-16 18:13:32,216 - INFO - >>> {'loss': 2.0176, 'grad_norm': 0.3900451958179474, 'learning_rate': 0.00019862894015714865, 'epoch': 1.16}
>>> 2025-09-16 18:14:27,856 - INFO - >>> {'loss': 1.9929, 'grad_norm': 0.36181047558784485, 'learning_rate': 0.00019848876151411511, 'epoch': 1.2133333333333334}
>>> 2025-09-16 18:15:10,297 - INFO - >>> {'loss': 1.8984, 'grad_norm': 0.38805490732192993, 'learning_rate': 0.00019834181572557066, 'epoch': 1.2666666666666666}
>>> 2025-09-16 18:15:56,626 - INFO - >>> {'loss': 1.9622, 'grad_norm': 0.4538368582725525, 'learning_rate': 0.00019818811288813476, 'epoch': 1.32}
>>> 2025-09-16 18:16:47,173 - INFO - >>> {'loss': 1.9812, 'grad_norm': 0.4256606101989746, 'learning_rate': 0.00019802766356270227, 'epoch': 1.3733333333333333}
>>> 2025-09-16 18:17:34,134 - INFO - >>> {'loss': 1.8586, 'grad_norm': 0.43156611919403076, 'learning_rate': 0.00019786047877371821, 'epoch': 1.4266666666666667}
>>> 2025-09-16 18:18:27,829 - INFO - >>> {'loss': 1.9576, 'grad_norm': 0.4135896861553192, 'learning_rate': 0.0001976865700084201, 'epoch': 1.48}
>>> 2025-09-16 18:19:16,211 - INFO - >>> {'loss': 1.8366, 'grad_norm': 0.41544634103775024, 'learning_rate': 0.00019750594921604862, 'epoch': 1.5333333333333332}
>>> 2025-09-16 18:20:00,860 - INFO - >>> {'loss': 1.8936, 'grad_norm': 0.41303321719169617, 'learning_rate': 0.00019731862880702675, 'epoch': 1.5866666666666667}
>>> 2025-09-16 18:20:42,296 - INFO - >>> {'loss': 1.7942, 'grad_norm': 0.48308128118515015, 'learning_rate': 0.00019712462165210684, 'epoch': 1.6400000000000001}
>>> 2025-09-16 18:21:35,230 - INFO - >>> {'loss': 1.8869, 'grad_norm': 0.42858538031578064, 'learning_rate': 0.0001969239410814865, 'epoch': 1.6933333333333334}
>>> 2025-09-16 18:22:31,097 - INFO - >>> {'loss': 1.8762, 'grad_norm': 0.42259955406188965, 'learning_rate': 0.0001967166008838925, 'epoch': 1.7466666666666666}
>>> 2025-09-16 18:23:09,565 - INFO - >>> {'loss': 1.7315, 'grad_norm': 0.5265892148017883, 'learning_rate': 0.00019650261530563336, 'epoch': 1.8}
>>> 2025-09-16 18:24:02,982 - INFO - >>> {'loss': 1.7893, 'grad_norm': 0.42110002040863037, 'learning_rate': 0.00019628199904962065, 'epoch': 1.8533333333333335}
>>> 2025-09-16 18:24:50,601 - INFO - >>> {'loss': 1.7287, 'grad_norm': 0.39674970507621765, 'learning_rate': 0.00019605476727435855, 'epoch': 1.9066666666666667}
>>> 2025-09-16 18:25:42,700 - INFO - >>> {'loss': 1.7376, 'grad_norm': 0.4230564534664154, 'learning_rate': 0.00019582093559290242, 'epoch': 1.96}
>>> 2025-09-16 18:26:21,706 - INFO - >>> {'loss': 1.5984, 'grad_norm': 0.5658843517303467, 'learning_rate': 0.0001955805200717861, 'epoch': 2.0}
>>> 2025-09-16 18:27:06,707 - INFO - >>> {'loss': 1.6455, 'grad_norm': 0.47518688440322876, 'learning_rate': 0.00019533353722991776, 'epoch': 2.0533333333333332}
>>> 2025-09-16 18:27:52,589 - INFO - >>> {'loss': 1.5696, 'grad_norm': 0.5126238465309143, 'learning_rate': 0.00019508000403744517, 'epoch': 2.1066666666666665}
>>> 2025-09-16 18:28:41,381 - INFO - >>> {'loss': 1.6107, 'grad_norm': 0.4757222533226013, 'learning_rate': 0.0001948199379145894, 'epoch': 2.16}
>>> 2025-09-16 18:29:28,145 - INFO - >>> {'loss': 1.6964, 'grad_norm': 0.543383002281189, 'learning_rate': 0.00019455335673044814, 'epoch': 2.2133333333333334}
>>> 2025-09-16 18:30:17,501 - INFO - >>> {'loss': 1.5406, 'grad_norm': 0.5878945589065552, 'learning_rate': 0.00019428027880176777, 'epoch': 2.2666666666666666}
>>> 2025-09-16 18:30:54,761 - INFO - >>> {'loss': 1.3411, 'grad_norm': 0.5875118374824524, 'learning_rate': 0.00019400072289168474, 'epoch': 2.32}
>>> 2025-09-16 18:31:45,678 - INFO - >>> {'loss': 1.4858, 'grad_norm': 0.5871815085411072, 'learning_rate': 0.0001937147082084366, 'epoch': 2.3733333333333335}
>>> 2025-09-16 18:32:35,439 - INFO - >>> {'loss': 1.4441, 'grad_norm': 0.6192195415496826, 'learning_rate': 0.000193422254404042, 'epoch': 2.4266666666666667}
>>> 2025-09-16 18:33:19,131 - INFO - >>> {'loss': 1.2837, 'grad_norm': 0.6232002377510071, 'learning_rate': 0.00019312338157295052, 'epoch': 2.48}
>>> 2025-09-16 18:34:05,703 - INFO - >>> {'loss': 1.461, 'grad_norm': 0.7872175574302673, 'learning_rate': 0.00019281811025066183, 'epoch': 2.533333333333333}
>>> 2025-09-16 18:34:49,163 - INFO - >>> {'loss': 1.2972, 'grad_norm': 0.7333154082298279, 'learning_rate': 0.00019250646141231502, 'epoch': 2.586666666666667}
>>> 2025-09-16 18:35:28,887 - INFO - >>> {'loss': 1.3111, 'grad_norm': 0.8018255829811096, 'learning_rate': 0.0001921884564712469, 'epoch': 2.64}
>>> 2025-09-16 18:36:23,595 - INFO - >>> {'loss': 1.356, 'grad_norm': 0.7734638452529907, 'learning_rate': 0.00019186411727752125, 'epoch': 2.6933333333333334}
>>> 2025-09-16 18:37:14,733 - INFO - >>> {'loss': 1.2533, 'grad_norm': 0.8703864812850952, 'learning_rate': 0.00019153346611642706, 'epoch': 2.7466666666666666}
>>> 2025-09-16 18:38:06,480 - INFO - >>> {'loss': 1.2599, 'grad_norm': 0.9884150624275208, 'learning_rate': 0.0001911965257069476, 'epoch': 2.8}
>>> 2025-09-16 18:39:01,437 - INFO - >>> {'loss': 1.2191, 'grad_norm': 0.8548523783683777, 'learning_rate': 0.00019085331920019917, 'epoch': 2.8533333333333335}
>>> 2025-09-16 18:39:51,772 - INFO - >>> {'loss': 1.2248, 'grad_norm': 0.9188365936279297, 'learning_rate': 0.0001905038701778407, 'epoch': 2.9066666666666667}
>>> 2025-09-16 18:40:40,716 - INFO - >>> {'loss': 1.1291, 'grad_norm': 0.9480258226394653, 'learning_rate': 0.00019014820265045304, 'epoch': 2.96}
>>> 2025-09-16 18:41:14,934 - INFO - >>> {'loss': 1.058, 'grad_norm': 1.0811206102371216, 'learning_rate': 0.00018978634105588961, 'epoch': 3.0}
>>> 2025-09-16 18:42:05,434 - INFO - >>> {'loss': 1.0816, 'grad_norm': 1.0341432094573975, 'learning_rate': 0.00018941831025759705, 'epoch': 3.0533333333333332}
>>> 2025-09-16 18:42:52,949 - INFO - >>> {'loss': 0.9463, 'grad_norm': 1.0326037406921387, 'learning_rate': 0.00018904413554290684, 'epoch': 3.1066666666666665}
>>> 2025-09-16 18:43:27,802 - INFO - >>> {'loss': 0.8388, 'grad_norm': 1.1825133562088013, 'learning_rate': 0.000188663842621298, 'epoch': 3.16}
>>> 2025-09-16 18:44:12,947 - INFO - >>> {'loss': 1.0069, 'grad_norm': 1.7486305236816406, 'learning_rate': 0.00018827745762263037, 'epoch': 3.2133333333333334}
>>> 2025-09-16 18:44:55,771 - INFO - >>> {'loss': 0.8578, 'grad_norm': 1.5671180486679077, 'learning_rate': 0.00018788500709534934, 'epoch': 3.2666666666666666}
>>> 2025-09-16 18:45:39,098 - INFO - >>> {'loss': 0.8225, 'grad_norm': 1.3134349584579468, 'learning_rate': 0.00018748651800466176, 'epoch': 3.32}
>>> 2025-09-16 18:46:27,121 - INFO - >>> {'loss': 0.844, 'grad_norm': 1.2944958209991455, 'learning_rate': 0.00018708201773068303, 'epoch': 3.3733333333333335}
>>> 2025-09-16 18:47:12,431 - INFO - >>> {'loss': 0.8235, 'grad_norm': 1.3913805484771729, 'learning_rate': 0.00018667153406655605, 'epoch': 3.4266666666666667}
>>> 2025-09-16 18:48:02,937 - INFO - >>> {'loss': 0.7456, 'grad_norm': 1.4096547365188599, 'learning_rate': 0.00018625509521654122, 'epoch': 3.48}
>>> 2025-09-16 18:48:58,791 - INFO - >>> {'loss': 0.6729, 'grad_norm': 1.4405970573425293, 'learning_rate': 0.00018583272979407885, 'epoch': 3.533333333333333}
>>> 2025-09-16 18:49:44,605 - INFO - >>> {'loss': 0.6892, 'grad_norm': 1.478987455368042, 'learning_rate': 0.00018540446681982294, 'epoch': 3.586666666666667}
>>> 2025-09-16 18:50:32,362 - INFO - >>> {'loss': 0.6051, 'grad_norm': 1.553824543952942, 'learning_rate': 0.00018497033571964727, 'epoch': 3.64}
>>> 2025-09-16 18:51:18,146 - INFO - >>> {'loss': 0.6596, 'grad_norm': 1.7486811876296997, 'learning_rate': 0.00018453036632262352, 'epoch': 3.6933333333333334}
>>> 2025-09-16 18:52:07,192 - INFO - >>> {'loss': 0.7189, 'grad_norm': 1.6285393238067627, 'learning_rate': 0.0001840845888589717, 'epoch': 3.7466666666666666}
>>> 2025-09-16 18:52:54,306 - INFO - >>> {'loss': 0.6631, 'grad_norm': 1.425347089767456, 'learning_rate': 0.00018363303395798304, 'epoch': 3.8}
>>> 2025-09-16 18:53:46,045 - INFO - >>> {'loss': 0.6601, 'grad_norm': 1.5426093339920044, 'learning_rate': 0.00018317573264591553, 'epoch': 3.8533333333333335}
>>> 2025-09-16 18:54:32,081 - INFO - >>> {'loss': 0.616, 'grad_norm': 1.8237885236740112, 'learning_rate': 0.000182712716343862, 'epoch': 3.9066666666666667}
>>> 2025-09-16 18:55:17,798 - INFO - >>> {'loss': 0.4117, 'grad_norm': 1.7570545673370361, 'learning_rate': 0.0001822440168655913, 'epoch': 3.96}
>>> 2025-09-16 18:55:50,388 - INFO - >>> {'loss': 0.5433, 'grad_norm': 1.6012715101242065, 'learning_rate': 0.0001817696664153623, 'epoch': 4.0}
>>> 2025-09-16 18:56:35,823 - INFO - >>> {'loss': 0.4755, 'grad_norm': 1.3760510683059692, 'learning_rate': 0.0001812896975857111, 'epoch': 4.053333333333334}
>>> 2025-09-16 18:57:20,063 - INFO - >>> {'loss': 0.3313, 'grad_norm': 1.233717918395996, 'learning_rate': 0.00018080414335521172, 'epoch': 4.1066666666666665}
>>> 2025-09-16 18:58:04,839 - INFO - >>> {'loss': 0.3889, 'grad_norm': 1.2723932266235352, 'learning_rate': 0.0001803130370862101, 'epoch': 4.16}
>>> 2025-09-16 18:58:48,567 - INFO - >>> {'loss': 0.3055, 'grad_norm': 1.3823535442352295, 'learning_rate': 0.00017981641252253177, 'epoch': 4.213333333333333}
>>> 2025-09-16 18:59:37,664 - INFO - >>> {'loss': 0.3167, 'grad_norm': 1.9110108613967896, 'learning_rate': 0.00017931430378716328, 'epoch': 4.266666666666667}
>>> 2025-09-16 19:00:32,032 - INFO - >>> {'loss': 0.3094, 'grad_norm': 1.8224190473556519, 'learning_rate': 0.0001788067453799077, 'epoch': 4.32}
>>> 2025-09-16 19:01:18,061 - INFO - >>> {'loss': 0.3193, 'grad_norm': 1.3213409185409546, 'learning_rate': 0.00017829377217501403, 'epoch': 4.373333333333333}
>>> 2025-09-16 19:02:08,082 - INFO - >>> {'loss': 0.3739, 'grad_norm': 1.5128955841064453, 'learning_rate': 0.00017777541941878114, 'epoch': 4.426666666666667}
>>> 2025-09-16 19:03:01,136 - INFO - >>> {'loss': 0.2845, 'grad_norm': 1.1855794191360474, 'learning_rate': 0.00017725172272713588, 'epoch': 4.48}
>>> 2025-09-16 19:03:41,288 - INFO - >>> {'loss': 0.2918, 'grad_norm': 1.2350633144378662, 'learning_rate': 0.00017672271808318605, 'epoch': 4.533333333333333}
>>> 2025-09-16 19:04:29,158 - INFO - >>> {'loss': 0.2736, 'grad_norm': 1.38126802444458, 'learning_rate': 0.00017618844183474774, 'epoch': 4.586666666666667}
>>> 2025-09-16 19:05:08,397 - INFO - >>> {'loss': 0.2158, 'grad_norm': 1.1966915130615234, 'learning_rate': 0.00017564893069184825, 'epoch': 4.64}
>>> 2025-09-16 19:05:59,953 - INFO - >>> {'loss': 0.2573, 'grad_norm': 1.1245026588439941, 'learning_rate': 0.00017510422172420343, 'epoch': 4.693333333333333}
>>> 2025-09-16 19:06:45,540 - INFO - >>> {'loss': 0.2181, 'grad_norm': 1.1582893133163452, 'learning_rate': 0.00017455435235867088, 'epoch': 4.746666666666667}
>>> 2025-09-16 19:07:28,003 - INFO - >>> {'loss': 0.2531, 'grad_norm': 1.2083107233047485, 'learning_rate': 0.00017399936037667808, 'epoch': 4.8}
>>> 2025-09-16 19:08:18,364 - INFO - >>> {'loss': 0.2365, 'grad_norm': 1.2397807836532593, 'learning_rate': 0.00017343928391162672, 'epoch': 4.8533333333333335}
>>> 2025-09-16 19:09:14,352 - INFO - >>> {'loss': 0.2232, 'grad_norm': 1.3412240743637085, 'learning_rate': 0.00017287416144627237, 'epoch': 4.906666666666666}
>>> 2025-09-16 19:10:00,407 - INFO - >>> {'loss': 0.2305, 'grad_norm': 1.1072720289230347, 'learning_rate': 0.00017230403181008034, 'epoch': 4.96}
>>> 2025-09-16 19:10:37,639 - INFO - >>> {'loss': 0.2376, 'grad_norm': 1.3803014755249023, 'learning_rate': 0.00017172893417655792, 'epoch': 5.0}
>>> 2025-09-16 19:11:17,292 - INFO - >>> {'loss': 0.1957, 'grad_norm': 0.9263010025024414, 'learning_rate': 0.00017114890806056243, 'epoch': 5.053333333333334}
>>> 2025-09-16 19:11:58,329 - INFO - >>> {'loss': 0.1378, 'grad_norm': 0.9393694400787354, 'learning_rate': 0.00017056399331558656, 'epoch': 5.1066666666666665}
>>> 2025-09-16 19:12:47,390 - INFO - >>> {'loss': 0.1445, 'grad_norm': 1.0305722951889038, 'learning_rate': 0.00016997423013101966, 'epoch': 5.16}
>>> 2025-09-16 19:13:41,841 - INFO - >>> {'loss': 0.165, 'grad_norm': 1.0155397653579712, 'learning_rate': 0.00016937965902938666, 'epoch': 5.213333333333333}
>>> 2025-09-16 19:14:20,281 - INFO - >>> {'loss': 0.1832, 'grad_norm': 1.2073431015014648, 'learning_rate': 0.00016878032086356352, 'epoch': 5.266666666666667}
>>> 2025-09-16 19:15:10,407 - INFO - >>> {'loss': 0.1938, 'grad_norm': 0.9347848892211914, 'learning_rate': 0.00016817625681397034, 'epoch': 5.32}
>>> 2025-09-16 19:15:51,969 - INFO - >>> {'loss': 0.1224, 'grad_norm': 0.9798992276191711, 'learning_rate': 0.00016756750838574198, 'epoch': 5.373333333333333}
>>> 2025-09-16 19:16:46,787 - INFO - >>> {'loss': 0.1031, 'grad_norm': 0.9723717570304871, 'learning_rate': 0.000166954117405876, 'epoch': 5.426666666666667}
>>> 2025-09-16 19:17:41,378 - INFO - >>> {'loss': 0.0946, 'grad_norm': 0.7132620811462402, 'learning_rate': 0.000166336126020359, 'epoch': 5.48}
>>> 2025-09-16 19:18:20,788 - INFO - >>> {'loss': 0.075, 'grad_norm': 1.0294153690338135, 'learning_rate': 0.00016571357669127048, 'epoch': 5.533333333333333}
>>> 2025-09-16 19:19:11,312 - INFO - >>> {'loss': 0.0982, 'grad_norm': 1.037392497062683, 'learning_rate': 0.0001650865121938656, 'epoch': 5.586666666666667}
>>> 2025-09-16 19:20:02,347 - INFO - >>> {'loss': 0.0648, 'grad_norm': 0.8413071632385254, 'learning_rate': 0.0001644549756136358, 'epoch': 5.64}
>>> 2025-09-16 19:20:43,838 - INFO - >>> {'loss': 0.076, 'grad_norm': 0.8926107883453369, 'learning_rate': 0.00016381901034334873, 'epoch': 5.693333333333333}
>>> 2025-09-16 19:21:28,347 - INFO - >>> {'loss': 0.116, 'grad_norm': 1.0741902589797974, 'learning_rate': 0.00016317866008006639, 'epoch': 5.746666666666667}
>>> 2025-09-16 19:22:14,338 - INFO - >>> {'loss': 0.1141, 'grad_norm': 0.8316182494163513, 'learning_rate': 0.00016253396882214292, 'epoch': 5.8}
>>> 2025-09-16 19:23:04,743 - INFO - >>> {'loss': 0.1002, 'grad_norm': 0.9711507558822632, 'learning_rate': 0.00016188498086620146, 'epoch': 5.8533333333333335}
>>> 2025-09-16 19:23:55,114 - INFO - >>> {'loss': 0.0782, 'grad_norm': 0.9419199824333191, 'learning_rate': 0.00016123174080409056, 'epoch': 5.906666666666666}
>>> 2025-09-16 19:24:44,051 - INFO - >>> {'loss': 0.1126, 'grad_norm': 1.2362377643585205, 'learning_rate': 0.00016057429351982013, 'epoch': 5.96}
>>> 2025-09-16 19:25:18,474 - INFO - >>> {'loss': 0.0997, 'grad_norm': 0.9512455463409424, 'learning_rate': 0.00015991268418647772, 'epoch': 6.0}
>>> 2025-09-16 19:26:09,293 - INFO - >>> {'loss': 0.0948, 'grad_norm': 0.5491095185279846, 'learning_rate': 0.00015924695826312435, 'epoch': 6.053333333333334}
>>> 2025-09-16 19:26:53,840 - INFO - >>> {'loss': 0.0679, 'grad_norm': 0.7865507006645203, 'learning_rate': 0.00015857716149167138, 'epoch': 6.1066666666666665}
>>> 2025-09-16 19:27:41,252 - INFO - >>> {'loss': 0.0735, 'grad_norm': 0.8486827611923218, 'learning_rate': 0.00015790333989373738, 'epoch': 6.16}
>>> 2025-09-16 19:28:28,536 - INFO - >>> {'loss': 0.0531, 'grad_norm': 0.7668244242668152, 'learning_rate': 0.00015722553976748604, 'epoch': 6.213333333333333}
>>> 2025-09-16 19:29:18,842 - INFO - >>> {'loss': 0.0944, 'grad_norm': 0.9534555673599243, 'learning_rate': 0.000156543807684445, 'epoch': 6.266666666666667}
>>> 2025-09-16 19:30:03,545 - INFO - >>> {'loss': 0.0896, 'grad_norm': 0.9409412741661072, 'learning_rate': 0.00015585819048630597, 'epoch': 6.32}
>>> 2025-09-16 19:30:53,244 - INFO - >>> {'loss': 0.0818, 'grad_norm': 0.8072831630706787, 'learning_rate': 0.0001551687352817063, 'epoch': 6.373333333333333}
>>> 2025-09-16 19:31:37,715 - INFO - >>> {'loss': 0.0643, 'grad_norm': 0.6896512508392334, 'learning_rate': 0.00015447548944299202, 'epoch': 6.426666666666667}
>>> 2025-09-16 19:32:23,363 - INFO - >>> {'loss': 0.0581, 'grad_norm': 0.6816911697387695, 'learning_rate': 0.00015377850060296298, 'epoch': 6.48}
>>> 2025-09-16 19:33:04,754 - INFO - >>> {'loss': 0.0667, 'grad_norm': 0.8535177707672119, 'learning_rate': 0.00015307781665160005, 'epoch': 6.533333333333333}
>>> 2025-09-16 19:33:56,127 - INFO - >>> {'loss': 0.0387, 'grad_norm': 0.6899988651275635, 'learning_rate': 0.0001523734857327744, 'epoch': 6.586666666666667}
>>> 2025-09-16 19:34:44,256 - INFO - >>> {'loss': 0.0639, 'grad_norm': 0.679711103439331, 'learning_rate': 0.00015166555624093986, 'epoch': 6.64}
>>> 2025-09-16 19:35:34,642 - INFO - >>> {'loss': 0.0494, 'grad_norm': 0.6263181567192078, 'learning_rate': 0.00015095407681780753, 'epoch': 6.693333333333333}
>>> 2025-09-16 19:36:17,716 - INFO - >>> {'loss': 0.0511, 'grad_norm': 0.7947373986244202, 'learning_rate': 0.00015023909634900363, 'epoch': 6.746666666666667}
>>> 2025-09-16 19:37:13,764 - INFO - >>> {'loss': 0.0367, 'grad_norm': 0.7170546650886536, 'learning_rate': 0.00014952066396071062, 'epoch': 6.8}
>>> 2025-09-16 19:37:59,855 - INFO - >>> {'loss': 0.0567, 'grad_norm': 0.49521955847740173, 'learning_rate': 0.00014879882901629182, 'epoch': 6.8533333333333335}
>>> 2025-09-16 19:38:43,593 - INFO - >>> {'loss': 0.0468, 'grad_norm': 0.7431488633155823, 'learning_rate': 0.0001480736411128994, 'epoch': 6.906666666666666}
>>> 2025-09-16 19:39:31,536 - INFO - >>> {'loss': 0.074, 'grad_norm': 0.5915595293045044, 'learning_rate': 0.00014734515007806698, 'epoch': 6.96}
>>> 2025-09-16 19:40:06,031 - INFO - >>> {'loss': 0.0471, 'grad_norm': 0.7621902823448181, 'learning_rate': 0.00014661340596628563, 'epoch': 7.0}
>>> 2025-09-16 19:40:52,093 - INFO - >>> {'loss': 0.0393, 'grad_norm': 0.7239863872528076, 'learning_rate': 0.00014587845905556478, 'epoch': 7.053333333333334}
>>> 2025-09-16 19:41:38,183 - INFO - >>> {'loss': 0.0342, 'grad_norm': 0.6788120269775391, 'learning_rate': 0.00014514035984397757, 'epoch': 7.1066666666666665}
>>> 2025-09-16 19:42:25,091 - INFO - >>> {'loss': 0.047, 'grad_norm': 0.5096445679664612, 'learning_rate': 0.00014439915904619134, 'epoch': 7.16}
>>> 2025-09-16 19:43:13,232 - INFO - >>> {'loss': 0.0453, 'grad_norm': 0.5986195206642151, 'learning_rate': 0.0001436549075899827, 'epoch': 7.213333333333333}
>>> 2025-09-16 19:44:05,190 - INFO - >>> {'loss': 0.0349, 'grad_norm': 0.4409301280975342, 'learning_rate': 0.00014290765661273847, 'epoch': 7.266666666666667}
>>> 2025-09-16 19:44:47,602 - INFO - >>> {'loss': 0.0345, 'grad_norm': 0.4791339635848999, 'learning_rate': 0.00014215745745794226, 'epoch': 7.32}
>>> 2025-09-16 19:45:33,882 - INFO - >>> {'loss': 0.0344, 'grad_norm': 0.5811378955841064, 'learning_rate': 0.0001414043616716461, 'epoch': 7.373333333333333}
>>> 2025-09-16 19:46:15,241 - INFO - >>> {'loss': 0.0307, 'grad_norm': 0.754970908164978, 'learning_rate': 0.00014064842099892935, 'epoch': 7.426666666666667}
>>> 2025-09-16 19:47:00,847 - INFO - >>> {'loss': 0.0305, 'grad_norm': 0.5368032455444336, 'learning_rate': 0.00013988968738034286, 'epoch': 7.48}
>>> 2025-09-16 19:47:48,278 - INFO - >>> {'loss': 0.0544, 'grad_norm': 0.6859675645828247, 'learning_rate': 0.00013912821294834033, 'epoch': 7.533333333333333}
>>> 2025-09-16 19:48:28,667 - INFO - >>> {'loss': 0.0297, 'grad_norm': 0.6944475173950195, 'learning_rate': 0.0001383640500236963, 'epoch': 7.586666666666667}
>>> 2025-09-16 19:49:17,499 - INFO - >>> {'loss': 0.043, 'grad_norm': 0.9474995136260986, 'learning_rate': 0.00013759725111191118, 'epoch': 7.64}
>>> 2025-09-16 19:50:05,009 - INFO - >>> {'loss': 0.0247, 'grad_norm': 0.5216375589370728, 'learning_rate': 0.00013682786889960354, 'epoch': 7.693333333333333}
>>> 2025-09-16 19:50:50,640 - INFO - >>> {'loss': 0.0706, 'grad_norm': 0.6080560088157654, 'learning_rate': 0.00013605595625089005, 'epoch': 7.746666666666667}
>>> 2025-09-16 19:51:39,746 - INFO - >>> {'loss': 0.0612, 'grad_norm': 0.6440081596374512, 'learning_rate': 0.00013528156620375335, 'epoch': 7.8}
>>> 2025-09-16 19:52:25,145 - INFO - >>> {'loss': 0.0354, 'grad_norm': 0.5070047378540039, 'learning_rate': 0.00013450475196639754, 'epoch': 7.8533333333333335}
>>> 2025-09-16 19:53:15,593 - INFO - >>> {'loss': 0.0159, 'grad_norm': 0.327947735786438, 'learning_rate': 0.0001337255669135925, 'epoch': 7.906666666666666}
>>> 2025-09-16 19:53:57,677 - INFO - >>> {'loss': 0.0363, 'grad_norm': 0.7276192903518677, 'learning_rate': 0.00013294406458300644, 'epoch': 7.96}
>>> 2025-09-16 19:54:36,671 - INFO - >>> {'loss': 0.0155, 'grad_norm': 0.3138675093650818, 'learning_rate': 0.00013216029867152724, 'epoch': 8.0}
>>> 2025-09-16 19:55:19,515 - INFO - >>> {'loss': 0.0313, 'grad_norm': 0.38903605937957764, 'learning_rate': 0.00013137432303157305, 'epoch': 8.053333333333333}
>>> 2025-09-16 19:56:06,234 - INFO - >>> {'loss': 0.0317, 'grad_norm': 0.3897743225097656, 'learning_rate': 0.0001305861916673921, 'epoch': 8.106666666666667}
>>> 2025-09-16 19:56:51,326 - INFO - >>> {'loss': 0.021, 'grad_norm': 0.49872449040412903, 'learning_rate': 0.00012979595873135205, 'epoch': 8.16}
>>> 2025-09-16 19:57:38,395 - INFO - >>> {'loss': 0.0284, 'grad_norm': 0.429305762052536, 'learning_rate': 0.0001290036785202192, 'epoch': 8.213333333333333}
>>> 2025-09-16 19:58:28,645 - INFO - >>> {'loss': 0.0255, 'grad_norm': 0.4002613425254822, 'learning_rate': 0.00012820940547142773, 'epoch': 8.266666666666667}
>>> 2025-09-16 19:59:21,411 - INFO - >>> {'loss': 0.0196, 'grad_norm': 0.5486994385719299, 'learning_rate': 0.00012741319415933934, 'epoch': 8.32}
>>> 2025-09-16 20:00:08,635 - INFO - >>> {'loss': 0.0276, 'grad_norm': 0.6042600274085999, 'learning_rate': 0.00012661509929149352, 'epoch': 8.373333333333333}
>>> 2025-09-16 20:00:54,867 - INFO - >>> {'loss': 0.0192, 'grad_norm': 0.33820146322250366, 'learning_rate': 0.0001258151757048485, 'epoch': 8.426666666666666}
>>> 2025-09-16 20:01:44,108 - INFO - >>> {'loss': 0.0372, 'grad_norm': 0.6341556906700134, 'learning_rate': 0.00012501347836201343, 'epoch': 8.48}
>>> 2025-09-16 20:02:34,910 - INFO - >>> {'loss': 0.0266, 'grad_norm': 0.5881619453430176, 'learning_rate': 0.000124210062347472, 'epoch': 8.533333333333333}
>>> 2025-09-16 20:03:21,946 - INFO - >>> {'loss': 0.0163, 'grad_norm': 0.4206300973892212, 'learning_rate': 0.00012340498286379756, 'epoch': 8.586666666666666}
>>> 2025-09-16 20:04:11,113 - INFO - >>> {'loss': 0.0122, 'grad_norm': 0.39982232451438904, 'learning_rate': 0.00012259829522786003, 'epoch': 8.64}
>>> 2025-09-16 20:04:52,101 - INFO - >>> {'loss': 0.0298, 'grad_norm': 0.3398576080799103, 'learning_rate': 0.00012179005486702517, 'epoch': 8.693333333333333}
>>> 2025-09-16 20:05:40,236 - INFO - >>> {'loss': 0.0236, 'grad_norm': 0.4223995804786682, 'learning_rate': 0.00012098031731534636, 'epoch': 8.746666666666666}
>>> 2025-09-16 20:06:27,808 - INFO - >>> {'loss': 0.0255, 'grad_norm': 0.3861705958843231, 'learning_rate': 0.00012016913820974856, 'epoch': 8.8}
>>> 2025-09-16 20:07:12,418 - INFO - >>> {'loss': 0.0248, 'grad_norm': 0.43003490567207336, 'learning_rate': 0.00011935657328620566, 'epoch': 8.853333333333333}
>>> 2025-09-16 20:07:56,299 - INFO - >>> {'loss': 0.0306, 'grad_norm': 0.4251045882701874, 'learning_rate': 0.00011854267837591095, 'epoch': 8.906666666666666}
>>> 2025-09-16 20:08:47,789 - INFO - >>> {'loss': 0.0259, 'grad_norm': 0.3546275198459625, 'learning_rate': 0.0001177275094014408, 'epoch': 8.96}
>>> 2025-09-16 20:09:29,650 - INFO - >>> {'loss': 0.0134, 'grad_norm': 0.4263966381549835, 'learning_rate': 0.00011691112237291224, 'epoch': 9.0}
>>> 2025-09-16 20:10:15,882 - INFO - >>> {'loss': 0.0149, 'grad_norm': 0.5427345037460327, 'learning_rate': 0.00011609357338413476, 'epoch': 9.053333333333333}
>>> 2025-09-16 20:11:11,091 - INFO - >>> {'loss': 0.0127, 'grad_norm': 0.40410763025283813, 'learning_rate': 0.00011527491860875575, 'epoch': 9.106666666666667}
>>> 2025-09-16 20:11:54,160 - INFO - >>> {'loss': 0.0127, 'grad_norm': 0.39957109093666077, 'learning_rate': 0.00011445521429640114, 'epoch': 9.16}
>>> 2025-09-16 20:12:47,105 - INFO - >>> {'loss': 0.0227, 'grad_norm': 0.30123215913772583, 'learning_rate': 0.0001136345167688103, 'epoch': 9.213333333333333}
>>> 2025-09-16 20:13:39,381 - INFO - >>> {'loss': 0.0093, 'grad_norm': 0.21235956251621246, 'learning_rate': 0.00011281288241596624, 'epoch': 9.266666666666667}
>>> 2025-09-16 20:14:23,462 - INFO - >>> {'loss': 0.0145, 'grad_norm': 0.3977341949939728, 'learning_rate': 0.00011199036769222105, 'epoch': 9.32}
>>> 2025-09-16 20:15:09,777 - INFO - >>> {'loss': 0.0178, 'grad_norm': 0.2871299982070923, 'learning_rate': 0.00011116702911241703, 'epoch': 9.373333333333333}
>>> 2025-09-16 20:16:05,509 - INFO - >>> {'loss': 0.0122, 'grad_norm': 0.6531747579574585, 'learning_rate': 0.00011034292324800342, 'epoch': 9.426666666666666}
>>> 2025-09-16 20:16:57,422 - INFO - >>> {'loss': 0.0306, 'grad_norm': 0.380521297454834, 'learning_rate': 0.00010951810672314946, 'epoch': 9.48}
>>> 2025-09-16 20:17:45,004 - INFO - >>> {'loss': 0.0198, 'grad_norm': 0.36951377987861633, 'learning_rate': 0.00010869263621085374, 'epoch': 9.533333333333333}
>>> 2025-09-16 20:18:28,518 - INFO - >>> {'loss': 0.0159, 'grad_norm': 0.4221446216106415, 'learning_rate': 0.00010786656842905028, 'epoch': 9.586666666666666}
>>> 2025-09-16 20:19:17,673 - INFO - >>> {'loss': 0.0161, 'grad_norm': 0.360820472240448, 'learning_rate': 0.00010703996013671124, 'epoch': 9.64}
>>> 2025-09-16 20:19:59,179 - INFO - >>> {'loss': 0.0301, 'grad_norm': 0.41250431537628174, 'learning_rate': 0.00010621286812994733, 'epoch': 9.693333333333333}
>>> 2025-09-16 20:20:47,054 - INFO - >>> {'loss': 0.0218, 'grad_norm': 0.3334675431251526, 'learning_rate': 0.00010538534923810506, 'epoch': 9.746666666666666}
>>> 2025-09-16 20:21:29,679 - INFO - >>> {'loss': 0.0183, 'grad_norm': 0.4007442891597748, 'learning_rate': 0.00010455746031986215, 'epoch': 9.8}
>>> 2025-09-16 20:22:15,568 - INFO - >>> {'loss': 0.0149, 'grad_norm': 0.4519457519054413, 'learning_rate': 0.00010372925825932093, 'epoch': 9.853333333333333}
>>> 2025-09-16 20:22:58,739 - INFO - >>> {'loss': 0.0081, 'grad_norm': 0.37359926104545593, 'learning_rate': 0.00010290079996209949, 'epoch': 9.906666666666666}
>>> 2025-09-16 20:23:48,954 - INFO - >>> {'loss': 0.0128, 'grad_norm': 0.2926159203052521, 'learning_rate': 0.00010207214235142197, 'epoch': 9.96}
>>> 2025-09-16 20:24:21,732 - INFO - >>> {'loss': 0.0131, 'grad_norm': 0.399252325296402, 'learning_rate': 0.00010124334236420734, 'epoch': 10.0}
>>> 2025-09-16 20:25:09,299 - INFO - >>> {'loss': 0.0151, 'grad_norm': 0.2263224869966507, 'learning_rate': 0.00010041445694715716, 'epoch': 10.053333333333333}
>>> 2025-09-16 20:25:50,483 - INFO - >>> {'loss': 0.0068, 'grad_norm': 0.1818862408399582, 'learning_rate': 9.958554305284289e-05, 'epoch': 10.106666666666667}
>>> 2025-09-16 20:26:37,038 - INFO - >>> {'loss': 0.0123, 'grad_norm': 0.29565706849098206, 'learning_rate': 9.875665763579269e-05, 'epoch': 10.16}
>>> 2025-09-16 20:27:27,993 - INFO - >>> {'loss': 0.01, 'grad_norm': 0.29703232645988464, 'learning_rate': 9.792785764857802e-05, 'epoch': 10.213333333333333}
>>> 2025-09-16 20:28:15,708 - INFO - >>> {'loss': 0.0127, 'grad_norm': 0.41069501638412476, 'learning_rate': 9.709920003790054e-05, 'epoch': 10.266666666666667}
>>> 2025-09-16 20:29:05,681 - INFO - >>> {'loss': 0.0182, 'grad_norm': 0.4303978979587555, 'learning_rate': 9.627074174067909e-05, 'epoch': 10.32}
>>> 2025-09-16 20:29:53,743 - INFO - >>> {'loss': 0.0086, 'grad_norm': 0.2113140970468521, 'learning_rate': 9.544253968013784e-05, 'epoch': 10.373333333333333}
>>> 2025-09-16 20:30:34,294 - INFO - >>> {'loss': 0.0085, 'grad_norm': 0.26862832903862, 'learning_rate': 9.461465076189499e-05, 'epoch': 10.426666666666666}
>>> 2025-09-16 20:31:24,261 - INFO - >>> {'loss': 0.0071, 'grad_norm': 0.23079368472099304, 'learning_rate': 9.378713187005271e-05, 'epoch': 10.48}
>>> 2025-09-16 20:32:11,105 - INFO - >>> {'loss': 0.0182, 'grad_norm': 0.31855419278144836, 'learning_rate': 9.296003986328875e-05, 'epoch': 10.533333333333333}
>>> 2025-09-16 20:33:03,738 - INFO - >>> {'loss': 0.0085, 'grad_norm': 0.25426560640335083, 'learning_rate': 9.213343157094976e-05, 'epoch': 10.586666666666666}
>>> 2025-09-16 20:33:47,825 - INFO - >>> {'loss': 0.0172, 'grad_norm': 0.4424332082271576, 'learning_rate': 9.130736378914627e-05, 'epoch': 10.64}
>>> 2025-09-16 20:34:33,584 - INFO - >>> {'loss': 0.0055, 'grad_norm': 0.16301225125789642, 'learning_rate': 9.048189327685055e-05, 'epoch': 10.693333333333333}
>>> 2025-09-16 20:35:19,244 - INFO - >>> {'loss': 0.0092, 'grad_norm': 0.40111127495765686, 'learning_rate': 8.96570767519966e-05, 'epoch': 10.746666666666666}
>>> 2025-09-16 20:36:12,397 - INFO - >>> {'loss': 0.0193, 'grad_norm': 0.36426591873168945, 'learning_rate': 8.883297088758298e-05, 'epoch': 10.8}
>>> 2025-09-16 20:36:59,214 - INFO - >>> {'loss': 0.0102, 'grad_norm': 0.19888687133789062, 'learning_rate': 8.800963230777896e-05, 'epoch': 10.853333333333333}
>>> 2025-09-16 20:37:44,763 - INFO - >>> {'loss': 0.0165, 'grad_norm': 0.2703547477722168, 'learning_rate': 8.718711758403382e-05, 'epoch': 10.906666666666666}
>>> 2025-09-16 20:38:33,265 - INFO - >>> {'loss': 0.0118, 'grad_norm': 0.23402266204357147, 'learning_rate': 8.636548323118974e-05, 'epoch': 10.96}
>>> 2025-09-16 20:39:08,024 - INFO - >>> {'loss': 0.0099, 'grad_norm': 0.2816024720668793, 'learning_rate': 8.554478570359887e-05, 'epoch': 11.0}
>>> 2025-09-16 20:39:59,592 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.14483726024627686, 'learning_rate': 8.472508139124426e-05, 'epoch': 11.053333333333333}
>>> 2025-09-16 20:40:43,749 - INFO - >>> {'loss': 0.0121, 'grad_norm': 0.2516838610172272, 'learning_rate': 8.390642661586528e-05, 'epoch': 11.106666666666667}
>>> 2025-09-16 20:41:33,966 - INFO - >>> {'loss': 0.0072, 'grad_norm': 0.17367979884147644, 'learning_rate': 8.308887762708776e-05, 'epoch': 11.16}
>>> 2025-09-16 20:42:21,013 - INFO - >>> {'loss': 0.0191, 'grad_norm': 0.2533705532550812, 'learning_rate': 8.227249059855926e-05, 'epoch': 11.213333333333333}
>>> 2025-09-16 20:43:13,787 - INFO - >>> {'loss': 0.007, 'grad_norm': 0.15716780722141266, 'learning_rate': 8.145732162408907e-05, 'epoch': 11.266666666666667}
>>> 2025-09-16 20:44:05,273 - INFO - >>> {'loss': 0.0077, 'grad_norm': 0.2050643116235733, 'learning_rate': 8.064342671379435e-05, 'epoch': 11.32}
>>> 2025-09-16 20:44:53,307 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.1618574559688568, 'learning_rate': 7.983086179025148e-05, 'epoch': 11.373333333333333}
>>> 2025-09-16 20:45:38,958 - INFO - >>> {'loss': 0.0069, 'grad_norm': 0.29920321702957153, 'learning_rate': 7.901968268465366e-05, 'epoch': 11.426666666666666}
>>> 2025-09-16 20:46:31,746 - INFO - >>> {'loss': 0.0051, 'grad_norm': 0.1377379447221756, 'learning_rate': 7.820994513297484e-05, 'epoch': 11.48}
>>> 2025-09-16 20:47:13,683 - INFO - >>> {'loss': 0.0136, 'grad_norm': 0.25087833404541016, 'learning_rate': 7.740170477214003e-05, 'epoch': 11.533333333333333}
>>> 2025-09-16 20:48:02,574 - INFO - >>> {'loss': 0.0104, 'grad_norm': 0.2073187679052353, 'learning_rate': 7.659501713620246e-05, 'epoch': 11.586666666666666}
>>> 2025-09-16 20:48:51,044 - INFO - >>> {'loss': 0.0069, 'grad_norm': 0.2597172260284424, 'learning_rate': 7.578993765252798e-05, 'epoch': 11.64}
>>> 2025-09-16 20:49:31,431 - INFO - >>> {'loss': 0.0068, 'grad_norm': 0.1827593743801117, 'learning_rate': 7.498652163798658e-05, 'epoch': 11.693333333333333}
>>> 2025-09-16 20:50:21,086 - INFO - >>> {'loss': 0.0128, 'grad_norm': 0.19027316570281982, 'learning_rate': 7.418482429515152e-05, 'epoch': 11.746666666666666}
>>> 2025-09-16 20:51:12,589 - INFO - >>> {'loss': 0.0062, 'grad_norm': 0.21395091712474823, 'learning_rate': 7.338490070850649e-05, 'epoch': 11.8}
>>> 2025-09-16 20:51:55,254 - INFO - >>> {'loss': 0.0063, 'grad_norm': 0.21613676846027374, 'learning_rate': 7.258680584066069e-05, 'epoch': 11.853333333333333}
>>> 2025-09-16 20:52:42,137 - INFO - >>> {'loss': 0.0075, 'grad_norm': 0.16372565925121307, 'learning_rate': 7.17905945285723e-05, 'epoch': 11.906666666666666}
>>> 2025-09-16 20:53:22,885 - INFO - >>> {'loss': 0.0081, 'grad_norm': 0.2870193123817444, 'learning_rate': 7.099632147978081e-05, 'epoch': 11.96}
>>> 2025-09-16 20:53:57,305 - INFO - >>> {'loss': 0.0152, 'grad_norm': 0.35367918014526367, 'learning_rate': 7.020404126864794e-05, 'epoch': 12.0}
>>> 2025-09-16 20:54:44,489 - INFO - >>> {'loss': 0.0057, 'grad_norm': 0.11904329806566238, 'learning_rate': 6.94138083326079e-05, 'epoch': 12.053333333333333}
>>> 2025-09-16 20:55:41,146 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.12739436328411102, 'learning_rate': 6.862567696842694e-05, 'epoch': 12.106666666666667}
>>> 2025-09-16 20:56:25,525 - INFO - >>> {'loss': 0.006, 'grad_norm': 0.1514028012752533, 'learning_rate': 6.78397013284728e-05, 'epoch': 12.16}
>>> 2025-09-16 20:57:12,867 - INFO - >>> {'loss': 0.0062, 'grad_norm': 0.16486352682113647, 'learning_rate': 6.705593541699358e-05, 'epoch': 12.213333333333333}
>>> 2025-09-16 20:58:02,668 - INFO - >>> {'loss': 0.0108, 'grad_norm': 0.19591598212718964, 'learning_rate': 6.62744330864075e-05, 'epoch': 12.266666666666667}
>>> 2025-09-16 20:58:44,113 - INFO - >>> {'loss': 0.0083, 'grad_norm': 0.20025573670864105, 'learning_rate': 6.549524803360248e-05, 'epoch': 12.32}
>>> 2025-09-16 20:59:33,300 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.19247359037399292, 'learning_rate': 6.471843379624669e-05, 'epoch': 12.373333333333333}
>>> 2025-09-16 21:00:19,624 - INFO - >>> {'loss': 0.0069, 'grad_norm': 0.17738261818885803, 'learning_rate': 6.394404374910996e-05, 'epoch': 12.426666666666666}
>>> 2025-09-16 21:01:04,638 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.12940587103366852, 'learning_rate': 6.317213110039651e-05, 'epoch': 12.48}
>>> 2025-09-16 21:01:51,618 - INFO - >>> {'loss': 0.0108, 'grad_norm': 0.2442290186882019, 'learning_rate': 6.240274888808883e-05, 'epoch': 12.533333333333333}
>>> 2025-09-16 21:02:43,796 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.08801150321960449, 'learning_rate': 6.163594997630369e-05, 'epoch': 12.586666666666666}
>>> 2025-09-16 21:03:27,134 - INFO - >>> {'loss': 0.0098, 'grad_norm': 0.20075614750385284, 'learning_rate': 6.087178705165969e-05, 'epoch': 12.64}
>>> 2025-09-16 21:04:11,960 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.10033942013978958, 'learning_rate': 6.011031261965716e-05, 'epoch': 12.693333333333333}
>>> 2025-09-16 21:05:03,353 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.1762104034423828, 'learning_rate': 5.9351579001070655e-05, 'epoch': 12.746666666666666}
>>> 2025-09-16 21:05:47,857 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.1413285732269287, 'learning_rate': 5.859563832835393e-05, 'epoch': 12.8}
>>> 2025-09-16 21:06:37,638 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.11576813459396362, 'learning_rate': 5.784254254205779e-05, 'epoch': 12.853333333333333}
>>> 2025-09-16 21:07:17,833 - INFO - >>> {'loss': 0.006, 'grad_norm': 0.21417589485645294, 'learning_rate': 5.709234338726149e-05, 'epoch': 12.906666666666666}
>>> 2025-09-16 21:08:02,097 - INFO - >>> {'loss': 0.0136, 'grad_norm': 0.1809288114309311, 'learning_rate': 5.6345092410017366e-05, 'epoch': 12.96}
>>> 2025-09-16 21:08:37,017 - INFO - >>> {'loss': 0.0133, 'grad_norm': 0.2385718822479248, 'learning_rate': 5.5600840953808675e-05, 'epoch': 13.0}
>>> 2025-09-16 21:09:23,152 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.12153735756874084, 'learning_rate': 5.485964015602243e-05, 'epoch': 13.053333333333333}
>>> 2025-09-16 21:10:09,433 - INFO - >>> {'loss': 0.0057, 'grad_norm': 0.15038272738456726, 'learning_rate': 5.412154094443527e-05, 'epoch': 13.106666666666667}
>>> 2025-09-16 21:10:54,981 - INFO - >>> {'loss': 0.0076, 'grad_norm': 0.1506442129611969, 'learning_rate': 5.3386594033714376e-05, 'epoch': 13.16}
>>> 2025-09-16 21:11:42,002 - INFO - >>> {'loss': 0.0103, 'grad_norm': 0.18985819816589355, 'learning_rate': 5.265484992193301e-05, 'epoch': 13.213333333333333}
>>> 2025-09-16 21:12:28,916 - INFO - >>> {'loss': 0.0061, 'grad_norm': 0.12807227671146393, 'learning_rate': 5.1926358887100604e-05, 'epoch': 13.266666666666667}
>>> 2025-09-16 21:13:19,059 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.148330956697464, 'learning_rate': 5.120117098370824e-05, 'epoch': 13.32}
>>> 2025-09-16 21:14:02,976 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.1277286857366562, 'learning_rate': 5.047933603928936e-05, 'epoch': 13.373333333333333}
>>> 2025-09-16 21:14:56,446 - INFO - >>> {'loss': 0.0067, 'grad_norm': 0.15312975645065308, 'learning_rate': 4.976090365099638e-05, 'epoch': 13.426666666666666}
>>> 2025-09-16 21:15:47,987 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.13662436604499817, 'learning_rate': 4.904592318219249e-05, 'epoch': 13.48}
>>> 2025-09-16 21:16:32,663 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.1928616166114807, 'learning_rate': 4.833444375906012e-05, 'epoch': 13.533333333333333}
>>> 2025-09-16 21:17:20,842 - INFO - >>> {'loss': 0.0139, 'grad_norm': 0.18110665678977966, 'learning_rate': 4.7626514267225654e-05, 'epoch': 13.586666666666666}
>>> 2025-09-16 21:18:02,686 - INFO - >>> {'loss': 0.0059, 'grad_norm': 0.1822425127029419, 'learning_rate': 4.6922183348399996e-05, 'epoch': 13.64}
>>> 2025-09-16 21:18:46,690 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.1321907490491867, 'learning_rate': 4.622149939703704e-05, 'epoch': 13.693333333333333}
>>> 2025-09-16 21:19:32,258 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.08728816360235214, 'learning_rate': 4.5524510557008014e-05, 'epoch': 13.746666666666666}
>>> 2025-09-16 21:20:23,761 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.09430268406867981, 'learning_rate': 4.483126471829371e-05, 'epoch': 13.8}
>>> 2025-09-16 21:21:11,396 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.1753346472978592, 'learning_rate': 4.4141809513694043e-05, 'epoch': 13.853333333333333}
>>> 2025-09-16 21:21:56,591 - INFO - >>> {'loss': 0.0088, 'grad_norm': 0.17040005326271057, 'learning_rate': 4.3456192315555034e-05, 'epoch': 13.906666666666666}
>>> 2025-09-16 21:22:43,503 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.07927437871694565, 'learning_rate': 4.2774460232514e-05, 'epoch': 13.96}
>>> 2025-09-16 21:23:20,753 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.16058269143104553, 'learning_rate': 4.209666010626262e-05, 'epoch': 14.0}
>>> 2025-09-16 21:24:17,053 - INFO - >>> {'loss': 0.0064, 'grad_norm': 0.11021295934915543, 'learning_rate': 4.142283850832862e-05, 'epoch': 14.053333333333333}
>>> 2025-09-16 21:25:05,257 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.080277219414711, 'learning_rate': 4.0753041736875675e-05, 'epoch': 14.106666666666667}
>>> 2025-09-16 21:25:57,501 - INFO - >>> {'loss': 0.0057, 'grad_norm': 0.17577090859413147, 'learning_rate': 4.0087315813522283e-05, 'epoch': 14.16}
>>> 2025-09-16 21:26:43,081 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.14354749023914337, 'learning_rate': 3.942570648017988e-05, 'epoch': 14.213333333333333}
>>> 2025-09-16 21:27:37,714 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.14331914484500885, 'learning_rate': 3.876825919590944e-05, 'epoch': 14.266666666666667}
>>> 2025-09-16 21:28:21,137 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.12215481698513031, 'learning_rate': 3.811501913379853e-05, 'epoch': 14.32}
>>> 2025-09-16 21:29:08,254 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.11804598569869995, 'learning_rate': 3.74660311778571e-05, 'epoch': 14.373333333333333}
>>> 2025-09-16 21:29:57,418 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.13991957902908325, 'learning_rate': 3.68213399199336e-05, 'epoch': 14.426666666666666}
>>> 2025-09-16 21:30:45,167 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.11852072924375534, 'learning_rate': 3.618098965665126e-05, 'epoch': 14.48}
>>> 2025-09-16 21:31:30,143 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.13586455583572388, 'learning_rate': 3.554502438636419e-05, 'epoch': 14.533333333333333}
>>> 2025-09-16 21:32:11,695 - INFO - >>> {'loss': 0.0128, 'grad_norm': 0.1966627538204193, 'learning_rate': 3.491348780613444e-05, 'epoch': 14.586666666666666}
>>> 2025-09-16 21:33:04,496 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.11536914855241776, 'learning_rate': 3.4286423308729524e-05, 'epoch': 14.64}
>>> 2025-09-16 21:33:52,960 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.11925411969423294, 'learning_rate': 3.366387397964101e-05, 'epoch': 14.693333333333333}
>>> 2025-09-16 21:34:30,765 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.13876031339168549, 'learning_rate': 3.304588259412399e-05, 'epoch': 14.746666666666666}
>>> 2025-09-16 21:35:13,028 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.16981202363967896, 'learning_rate': 3.243249161425801e-05, 'epoch': 14.8}
>>> 2025-09-16 21:36:05,421 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.11413039267063141, 'learning_rate': 3.1823743186029675e-05, 'epoch': 14.853333333333333}
>>> 2025-09-16 21:36:50,832 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.09844054281711578, 'learning_rate': 3.1219679136436494e-05, 'epoch': 14.906666666666666}
>>> 2025-09-16 21:37:36,829 - INFO - >>> {'loss': 0.0133, 'grad_norm': 0.20458391308784485, 'learning_rate': 3.0620340970613345e-05, 'epoch': 14.96}
>>> 2025-09-16 21:38:11,808 - INFO - >>> {'loss': 0.0048, 'grad_norm': 0.13874466717243195, 'learning_rate': 3.0025769868980335e-05, 'epoch': 15.0}
>>> 2025-09-16 21:38:59,560 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.11414121836423874, 'learning_rate': 2.9436006684413444e-05, 'epoch': 15.053333333333333}
>>> 2025-09-16 21:39:44,931 - INFO - >>> {'loss': 0.0108, 'grad_norm': 0.15283551812171936, 'learning_rate': 2.8851091939437602e-05, 'epoch': 15.106666666666667}
>>> 2025-09-16 21:40:36,761 - INFO - >>> {'loss': 0.0062, 'grad_norm': 0.2014033943414688, 'learning_rate': 2.8271065823442123e-05, 'epoch': 15.16}
>>> 2025-09-16 21:41:26,148 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.08436161279678345, 'learning_rate': 2.7695968189919684e-05, 'epoch': 15.213333333333333}
>>> 2025-09-16 21:42:13,122 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.075940802693367, 'learning_rate': 2.712583855372769e-05, 'epoch': 15.266666666666667}
>>> 2025-09-16 21:43:05,275 - INFO - >>> {'loss': 0.0062, 'grad_norm': 0.15048743784427643, 'learning_rate': 2.6560716088373294e-05, 'epoch': 15.32}
>>> 2025-09-16 21:43:51,405 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.12782526016235352, 'learning_rate': 2.6000639623321933e-05, 'epoch': 15.373333333333333}
>>> 2025-09-16 21:44:40,863 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.13567185401916504, 'learning_rate': 2.544564764132915e-05, 'epoch': 15.426666666666666}
>>> 2025-09-16 21:45:24,223 - INFO - >>> {'loss': 0.0022, 'grad_norm': 0.07622922956943512, 'learning_rate': 2.4895778275796587e-05, 'epoch': 15.48}
>>> 2025-09-16 21:46:05,854 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.19549313187599182, 'learning_rate': 2.4351069308151775e-05, 'epoch': 15.533333333333333}
>>> 2025-09-16 21:46:44,233 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.14494067430496216, 'learning_rate': 2.381155816525228e-05, 'epoch': 15.586666666666666}
>>> 2025-09-16 21:47:33,536 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.10980711877346039, 'learning_rate': 2.3277281916813998e-05, 'epoch': 15.64}
>>> 2025-09-16 21:48:23,378 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.09989342093467712, 'learning_rate': 2.2748277272864106e-05, 'epoch': 15.693333333333333}
>>> 2025-09-16 21:49:08,623 - INFO - >>> {'loss': 0.0065, 'grad_norm': 0.12528164684772491, 'learning_rate': 2.222458058121889e-05, 'epoch': 15.746666666666666}
>>> 2025-09-16 21:50:01,607 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.12859825789928436, 'learning_rate': 2.170622782498598e-05, 'epoch': 15.8}
>>> 2025-09-16 21:50:51,417 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.11110682785511017, 'learning_rate': 2.119325462009233e-05, 'epoch': 15.853333333333333}
>>> 2025-09-16 21:51:46,362 - INFO - >>> {'loss': 0.0039, 'grad_norm': 0.09529896825551987, 'learning_rate': 2.0685696212836737e-05, 'epoch': 15.906666666666666}
>>> 2025-09-16 21:52:31,133 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.11859980970621109, 'learning_rate': 2.0183587477468226e-05, 'epoch': 15.96}
>>> 2025-09-16 21:53:04,398 - INFO - >>> {'loss': 0.0062, 'grad_norm': 0.20289786159992218, 'learning_rate': 1.9686962913789897e-05, 'epoch': 16.0}
>>> 2025-09-16 21:53:47,015 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.1445675492286682, 'learning_rate': 1.91958566447883e-05, 'epoch': 16.053333333333335}
>>> 2025-09-16 21:54:35,223 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.10822320729494095, 'learning_rate': 1.871030241428894e-05, 'epoch': 16.106666666666666}
>>> 2025-09-16 21:55:25,185 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.11015166342258453, 'learning_rate': 1.8230333584637716e-05, 'epoch': 16.16}
>>> 2025-09-16 21:56:04,624 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.06483703851699829, 'learning_rate': 1.7755983134408703e-05, 'epoch': 16.213333333333335}
>>> 2025-09-16 21:57:00,885 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.1123749241232872, 'learning_rate': 1.728728365613801e-05, 'epoch': 16.266666666666666}
>>> 2025-09-16 21:57:48,153 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.0811694785952568, 'learning_rate': 1.682426735408448e-05, 'epoch': 16.32}
>>> 2025-09-16 21:58:39,987 - INFO - >>> {'loss': 0.0053, 'grad_norm': 0.13815759122371674, 'learning_rate': 1.6366966042016996e-05, 'epoch': 16.373333333333335}
>>> 2025-09-16 21:59:31,006 - INFO - >>> {'loss': 0.0046, 'grad_norm': 0.11003267765045166, 'learning_rate': 1.5915411141028326e-05, 'epoch': 16.426666666666666}
>>> 2025-09-16 22:00:16,779 - INFO - >>> {'loss': 0.0109, 'grad_norm': 0.18960247933864594, 'learning_rate': 1.5469633677376495e-05, 'epoch': 16.48}
>>> 2025-09-16 22:00:55,462 - INFO - >>> {'loss': 0.0033, 'grad_norm': 0.11196699738502502, 'learning_rate': 1.502966428035274e-05, 'epoch': 16.533333333333335}
>>> 2025-09-16 22:01:41,988 - INFO - >>> {'loss': 0.0082, 'grad_norm': 0.14441902935504913, 'learning_rate': 1.4595533180177057e-05, 'epoch': 16.586666666666666}
>>> 2025-09-16 22:02:27,975 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.13456958532333374, 'learning_rate': 1.4167270205921169e-05, 'epoch': 16.64}
>>> 2025-09-16 22:03:21,361 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.10059890151023865, 'learning_rate': 1.37449047834588e-05, 'epoch': 16.693333333333335}
>>> 2025-09-16 22:04:05,192 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.2035943865776062, 'learning_rate': 1.332846593344399e-05, 'epoch': 16.746666666666666}
>>> 2025-09-16 22:04:53,843 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.11991999298334122, 'learning_rate': 1.2917982269316975e-05, 'epoch': 16.8}
>>> 2025-09-16 22:05:38,876 - INFO - >>> {'loss': 0.005, 'grad_norm': 0.19017833471298218, 'learning_rate': 1.2513481995338284e-05, 'epoch': 16.85333333333333}
>>> 2025-09-16 22:06:26,531 - INFO - >>> {'loss': 0.0042, 'grad_norm': 0.10694359987974167, 'learning_rate': 1.2114992904650691e-05, 'epoch': 16.906666666666666}
>>> 2025-09-16 22:07:14,167 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.07794807851314545, 'learning_rate': 1.1722542377369639e-05, 'epoch': 16.96}
>>> 2025-09-16 22:07:50,998 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.15096229314804077, 'learning_rate': 1.1336157378702018e-05, 'epoch': 17.0}
>>> 2025-09-16 22:08:42,544 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.09913375973701477, 'learning_rate': 1.0955864457093145e-05, 'epoch': 17.053333333333335}
>>> 2025-09-16 22:09:17,597 - INFO - >>> {'loss': 0.0119, 'grad_norm': 0.15811996161937714, 'learning_rate': 1.0581689742402968e-05, 'epoch': 17.106666666666666}
>>> 2025-09-16 22:10:06,877 - INFO - >>> {'loss': 0.0077, 'grad_norm': 0.14168430864810944, 'learning_rate': 1.0213658944110404e-05, 'epoch': 17.16}
>>> 2025-09-16 22:10:57,405 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.12670765817165375, 'learning_rate': 9.851797349546976e-06, 'epoch': 17.213333333333335}
>>> 2025-09-16 22:11:47,430 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.10845870524644852, 'learning_rate': 9.496129822159338e-06, 'epoch': 17.266666666666666}
>>> 2025-09-16 22:12:32,128 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.12222813814878464, 'learning_rate': 9.146680799800834e-06, 'epoch': 17.32}
>>> 2025-09-16 22:13:28,910 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.0802212581038475, 'learning_rate': 8.803474293052438e-06, 'epoch': 17.373333333333335}
>>> 2025-09-16 22:14:19,904 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.10034848749637604, 'learning_rate': 8.466533883572947e-06, 'epoch': 17.426666666666666}
>>> 2025-09-16 22:15:02,573 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.0881512314081192, 'learning_rate': 8.135882722478772e-06, 'epoch': 17.48}
>>> 2025-09-16 22:15:45,491 - INFO - >>> {'loss': 0.0041, 'grad_norm': 0.14449697732925415, 'learning_rate': 7.811543528753106e-06, 'epoch': 17.533333333333335}
>>> 2025-09-16 22:16:31,827 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.14269353449344635, 'learning_rate': 7.4935385876850114e-06, 'epoch': 17.586666666666666}
>>> 2025-09-16 22:17:15,658 - INFO - >>> {'loss': 0.0058, 'grad_norm': 0.18608514964580536, 'learning_rate': 7.181889749338178e-06, 'epoch': 17.64}
>>> 2025-09-16 22:18:01,110 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.10999922454357147, 'learning_rate': 6.876618427049509e-06, 'epoch': 17.693333333333335}
>>> 2025-09-16 22:18:48,431 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.15647517144680023, 'learning_rate': 6.577745595958018e-06, 'epoch': 17.746666666666666}
>>> 2025-09-16 22:19:38,170 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.11444372683763504, 'learning_rate': 6.28529179156343e-06, 'epoch': 17.8}
>>> 2025-09-16 22:20:27,295 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.1437443345785141, 'learning_rate': 5.999277108315271e-06, 'epoch': 17.85333333333333}
>>> 2025-09-16 22:21:15,306 - INFO - >>> {'loss': 0.0026, 'grad_norm': 0.08588734269142151, 'learning_rate': 5.719721198232253e-06, 'epoch': 17.906666666666666}
>>> 2025-09-16 22:22:00,787 - INFO - >>> {'loss': 0.0047, 'grad_norm': 0.11938290297985077, 'learning_rate': 5.4466432695518545e-06, 'epoch': 17.96}
>>> 2025-09-16 22:22:38,199 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.11419244855642319, 'learning_rate': 5.180062085410609e-06, 'epoch': 18.0}
>>> 2025-09-16 22:23:26,760 - INFO - >>> {'loss': 0.0092, 'grad_norm': 0.13119551539421082, 'learning_rate': 4.919995962554846e-06, 'epoch': 18.053333333333335}
>>> 2025-09-16 22:24:18,834 - INFO - >>> {'loss': 0.0045, 'grad_norm': 0.10719045251607895, 'learning_rate': 4.666462770082247e-06, 'epoch': 18.106666666666666}
>>> 2025-09-16 22:25:01,598 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.08554253727197647, 'learning_rate': 4.419479928213932e-06, 'epoch': 18.16}
>>> 2025-09-16 22:25:50,033 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.10881859064102173, 'learning_rate': 4.179064407097588e-06, 'epoch': 18.213333333333335}
>>> 2025-09-16 22:26:41,511 - INFO - >>> {'loss': 0.0032, 'grad_norm': 0.09416477382183075, 'learning_rate': 3.94523272564149e-06, 'epoch': 18.266666666666666}
>>> 2025-09-16 22:27:26,156 - INFO - >>> {'loss': 0.0018, 'grad_norm': 0.04992498829960823, 'learning_rate': 3.7180009503793743e-06, 'epoch': 18.32}
>>> 2025-09-16 22:28:13,003 - INFO - >>> {'loss': 0.0069, 'grad_norm': 0.12663893401622772, 'learning_rate': 3.4973846943666568e-06, 'epoch': 18.373333333333335}
>>> 2025-09-16 22:29:00,346 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.09828555583953857, 'learning_rate': 3.283399116107533e-06, 'epoch': 18.426666666666666}
>>> 2025-09-16 22:29:45,137 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.08209692686796188, 'learning_rate': 3.0760589185135026e-06, 'epoch': 18.48}
>>> 2025-09-16 22:30:34,712 - INFO - >>> {'loss': 0.0054, 'grad_norm': 0.16713982820510864, 'learning_rate': 2.8753783478931653e-06, 'epoch': 18.533333333333335}
>>> 2025-09-16 22:31:15,916 - INFO - >>> {'loss': 0.0029, 'grad_norm': 0.08271975815296173, 'learning_rate': 2.681371192973281e-06, 'epoch': 18.586666666666666}
>>> 2025-09-16 22:32:09,982 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.09812282770872116, 'learning_rate': 2.494050783951396e-06, 'epoch': 18.64}
>>> 2025-09-16 22:32:54,845 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.13122163712978363, 'learning_rate': 2.3134299915799184e-06, 'epoch': 18.693333333333335}
>>> 2025-09-16 22:33:46,215 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.09576544165611267, 'learning_rate': 2.139521226281793e-06, 'epoch': 18.746666666666666}
>>> 2025-09-16 22:34:35,075 - INFO - >>> {'loss': 0.0038, 'grad_norm': 0.09048271179199219, 'learning_rate': 1.9723364372977394e-06, 'epoch': 18.8}
>>> 2025-09-16 22:35:29,981 - INFO - >>> {'loss': 0.0056, 'grad_norm': 0.12670953571796417, 'learning_rate': 1.8118871118652515e-06, 'epoch': 18.85333333333333}
>>> 2025-09-16 22:36:17,396 - INFO - >>> {'loss': 0.0052, 'grad_norm': 0.11652300506830215, 'learning_rate': 1.6581842744293307e-06, 'epoch': 18.906666666666666}
>>> 2025-09-16 22:37:01,257 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.14941342175006866, 'learning_rate': 1.5112384858849137e-06, 'epoch': 18.96}
>>> 2025-09-16 22:37:31,701 - INFO - >>> {'loss': 0.0031, 'grad_norm': 0.14561043679714203, 'learning_rate': 1.3710598428513633e-06, 'epoch': 19.0}
>>> 2025-09-16 22:38:27,014 - INFO - >>> {'loss': 0.0045, 'grad_norm': 0.0843210369348526, 'learning_rate': 1.2376579769786124e-06, 'epoch': 19.053333333333335}
>>> 2025-09-16 22:39:10,615 - INFO - >>> {'loss': 0.0068, 'grad_norm': 0.12856373190879822, 'learning_rate': 1.1110420542854384e-06, 'epoch': 19.106666666666666}
>>> 2025-09-16 22:40:04,448 - INFO - >>> {'loss': 0.003, 'grad_norm': 0.10719486325979233, 'learning_rate': 9.912207745296665e-07, 'epoch': 19.16}
>>> 2025-09-16 22:40:53,155 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.10627469420433044, 'learning_rate': 8.782023706103925e-07, 'epoch': 19.213333333333335}
>>> 2025-09-16 22:41:46,198 - INFO - >>> {'loss': 0.0034, 'grad_norm': 0.1409224420785904, 'learning_rate': 7.719946080023021e-07, 'epoch': 19.266666666666666}
>>> 2025-09-16 22:42:29,688 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.14359347522258759, 'learning_rate': 6.726047842221084e-07, 'epoch': 19.32}
>>> 2025-09-16 22:43:20,444 - INFO - >>> {'loss': 0.0024, 'grad_norm': 0.07392904907464981, 'learning_rate': 5.800397283271752e-07, 'epoch': 19.373333333333335}
>>> 2025-09-16 22:44:03,854 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.07361356914043427, 'learning_rate': 4.943058004462042e-07, 'epoch': 19.426666666666666}
>>> 2025-09-16 22:44:56,126 - INFO - >>> {'loss': 0.0028, 'grad_norm': 0.09555888175964355, 'learning_rate': 4.154088913423615e-07, 'epoch': 19.48}
>>> 2025-09-16 22:45:43,916 - INFO - >>> {'loss': 0.004, 'grad_norm': 0.114058718085289, 'learning_rate': 3.4335442200840173e-07, 'epoch': 19.533333333333335}
>>> 2025-09-16 22:46:36,345 - INFO - >>> {'loss': 0.0035, 'grad_norm': 0.06461193412542343, 'learning_rate': 2.7814734329426606e-07, 'epoch': 19.586666666666666}
>>> 2025-09-16 22:47:15,861 - INFO - >>> {'loss': 0.0036, 'grad_norm': 0.09618199616670609, 'learning_rate': 2.197921355668875e-07, 'epoch': 19.64}
>>> 2025-09-16 22:48:11,350 - INFO - >>> {'loss': 0.0093, 'grad_norm': 0.14390279352664948, 'learning_rate': 1.682928084023261e-07, 'epoch': 19.693333333333335}
>>> 2025-09-16 22:48:52,732 - INFO - >>> {'loss': 0.0044, 'grad_norm': 0.1489243358373642, 'learning_rate': 1.2365290031032263e-07, 'epoch': 19.746666666666666}
>>> 2025-09-16 22:49:42,896 - INFO - >>> {'loss': 0.006, 'grad_norm': 0.14872421324253082, 'learning_rate': 8.587547849112642e-08, 'epoch': 19.8}
>>> 2025-09-16 22:50:24,069 - INFO - >>> {'loss': 0.0049, 'grad_norm': 0.1490606963634491, 'learning_rate': 5.496313862476399e-08, 'epoch': 19.85333333333333}
>>> 2025-09-16 22:51:08,402 - INFO - >>> {'loss': 0.0025, 'grad_norm': 0.05873442813754082, 'learning_rate': 3.091800469264827e-08, 'epoch': 19.906666666666666}
>>> 2025-09-16 22:51:56,464 - INFO - >>> {'loss': 0.0061, 'grad_norm': 0.13849282264709473, 'learning_rate': 1.3741728831750955e-08, 'epoch': 19.96}
>>> 2025-09-16 22:52:33,185 - INFO - >>> {'loss': 0.0037, 'grad_norm': 0.1020723506808281, 'learning_rate': 3.435491220937781e-09, 'epoch': 20.0}
>>> 2025-09-16 22:52:33,887 - INFO - >>> {'train_runtime': 17752.2144, 'train_samples_per_second': 0.676, 'train_steps_per_second': 0.021, 'train_loss': 0.35995634410769284, 'epoch': 20.0}
>>> 2025-09-16 22:52:33,889 - INFO - 训练成功！
>>> 2025-09-16 22:52:33,889 - INFO - 模型存放位置：./output/qwen3-8b202509161756
>>> 2025-09-16 22:54:02,473 - INFO - ========__main__  202509162254========
>>> 2025-09-16 22:54:02,473 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-16 22:54:02,474 - INFO - 开始进行模型测试
>>> 2025-09-16 22:54:12,003 - INFO - 已选择模型文件夹: qwen3-8b202509161756
>>> 2025-09-16 22:54:12,005 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509161756/checkpoint-380
>>> 2025-09-16 22:56:14,884 - INFO - 开始进行原始模型对话测试
>>> 2025-09-16 22:56:17,290 - INFO - 导入包完成
>>> 2025-09-16 22:56:17,297 - INFO - 配置文件读取完成
>>> 2025-09-16 22:56:58,117 - INFO - 开始进行原始模型对话测试
>>> 2025-09-16 22:57:00,476 - INFO - 导入包完成
>>> 2025-09-16 22:57:00,483 - INFO - 配置文件读取完成
>>> 2025-09-16 22:59:08,749 - INFO - ========__main__  202509162259========
>>> 2025-09-16 22:59:08,749 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-16 22:59:08,750 - INFO - 开始进行模型测试
>>> 2025-09-16 22:59:10,758 - INFO - 已选择模型文件夹: qwen3-8b202509161756
>>> 2025-09-16 22:59:10,761 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509161756/checkpoint-380
>>> 2025-09-16 23:02:49,544 - INFO - 开始进行原始模型对话测试
>>> 2025-09-16 23:02:52,009 - INFO - 导入包完成
>>> 2025-09-16 23:02:52,016 - INFO - 配置文件读取完成
>>> 2025-09-16 23:04:04,336 - INFO - ========__main__  202509162304========
>>> 2025-09-16 23:04:04,337 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-16 23:04:04,337 - INFO - 开始进行模型测试
>>> 2025-09-16 23:04:06,661 - INFO - 已选择模型文件夹: qwen3-8b202509161756
>>> 2025-09-16 23:04:06,664 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509161756/checkpoint-380
>>> 2025-09-16 23:06:21,170 - INFO - 开始进行原始模型对话测试
>>> 2025-09-16 23:06:23,508 - INFO - 导入包完成
>>> 2025-09-16 23:06:23,516 - INFO - 配置文件读取完成
>>> 2025-09-16 23:08:26,318 - INFO - ========__main__  202509162308========
>>> 2025-09-16 23:08:26,319 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-16 23:08:26,320 - INFO - 开始进行模型测试
>>> 2025-09-16 23:08:28,680 - INFO - 已选择模型文件夹: qwen3-8b202509161756
>>> 2025-09-16 23:08:28,682 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509161756/checkpoint-380
>>> 2025-09-17 00:25:16,863 - INFO - 导入包完成
>>> 2025-09-17 00:25:16,863 - INFO - ========train Qwen2ForCausalLM  202509170025========
>>> 2025-09-17 00:25:16,863 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 00:25:16,864 - INFO - 开始进行训练
>>> 2025-09-17 00:25:16,870 - INFO - 基础配置文件读取完成
>>> 2025-09-17 00:25:16,878 - INFO - 训练配置读取完成
>>> 2025-09-17 00:25:16,878 - INFO - 数据集路径：dataset/own/Medical_Extension.json
>>> 2025-09-17 00:25:16,879 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-09-17 00:25:17,322 - INFO - tokenizer读取完成
>>> 2025-09-17 00:25:17,475 - INFO - model dtype:torch.bfloat16
>>> 2025-09-17 00:25:17,476 - INFO - 模型导入完成
>>> 2025-09-17 00:25:17,476 - INFO - 数据读取开始
>>> 2025-09-17 00:25:18,511 - INFO - 数据下载完成，训练集大小: 600
>>> 2025-09-17 00:25:22,614 - INFO - 数据映射完成
>>> 2025-09-17 00:25:22,614 - INFO - 打印训练参数如下
>>> 2025-09-17 00:25:22,614 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-17 00:25:22,615 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-17 00:25:22,615 - INFO -   load_in_4bit >>> True
>>> 2025-09-17 00:25:22,615 - INFO -   batch_size >>> 8
>>> 2025-09-17 00:25:22,616 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-17 00:25:22,616 - INFO -   warmup_steps >>> 1
>>> 2025-09-17 00:25:22,616 - INFO -   epoch >>> 8
>>> 2025-09-17 00:25:22,617 - INFO -   eval_steps >>> 5
>>> 2025-09-17 00:25:22,617 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-17 00:25:22,617 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-17 00:25:22,618 - INFO -   max_seq_length >>> 2048
>>> 2025-09-17 00:25:22,618 - INFO -   r >>> 8
>>> 2025-09-17 00:25:22,618 - INFO -   interface_mode >>> False
>>> 2025-09-17 00:25:22,619 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-17 00:25:22,619 - INFO -   lora_alpha >>> 16
>>> 2025-09-17 00:25:22,620 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-17 00:25:22,620 - INFO -   bias >>> none
>>> 2025-09-17 00:25:22,620 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-17 00:25:22,621 - INFO -   random_state >>> 3407
>>> 2025-09-17 00:25:22,621 - INFO -   use_rslora >>> True
>>> 2025-09-17 00:25:22,621 - INFO -   loftq_config >>> None
>>> 2025-09-17 00:25:28,675 - INFO - 开始训练！
>>> 2025-09-17 00:26:16,430 - INFO - >>> {'loss': 3.1945, 'grad_norm': 1.1681411266326904, 'learning_rate': 0.0, 'epoch': 0.05333333333333334}
>>> 2025-09-17 00:27:03,835 - INFO - >>> {'loss': 3.4607, 'grad_norm': 1.307828664779663, 'learning_rate': 0.0002, 'epoch': 0.10666666666666667}
>>> 2025-09-17 00:28:19,296 - INFO - 导入包完成
>>> 2025-09-17 00:28:19,296 - INFO - ========train Qwen2ForCausalLM  202509170028========
>>> 2025-09-17 00:28:19,297 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 00:28:19,297 - INFO - 开始进行训练
>>> 2025-09-17 00:28:19,303 - INFO - 基础配置文件读取完成
>>> 2025-09-17 00:28:19,311 - INFO - 训练配置读取完成
>>> 2025-09-17 00:28:19,311 - INFO - 数据集路径：dataset/own/Medical_Extension.json
>>> 2025-09-17 00:28:19,312 - INFO - 模型路径:/home/liangshuqiao/models/Qwen2.5-14B-Instruct
>>> 2025-09-17 00:28:19,865 - INFO - tokenizer读取完成
>>> 2025-09-17 00:28:25,520 - INFO - model dtype:torch.bfloat16
>>> 2025-09-17 00:28:25,521 - INFO - 模型导入完成
>>> 2025-09-17 00:28:25,522 - INFO - 数据读取开始
>>> 2025-09-17 00:28:26,568 - INFO - 数据下载完成，训练集大小: 600
>>> 2025-09-17 00:28:33,105 - INFO - 数据映射完成
>>> 2025-09-17 00:28:33,106 - INFO - 打印训练参数如下
>>> 2025-09-17 00:28:33,106 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-17 00:28:33,107 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-17 00:28:33,107 - INFO -   load_in_4bit >>> True
>>> 2025-09-17 00:28:33,107 - INFO -   batch_size >>> 8
>>> 2025-09-17 00:28:33,108 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-17 00:28:33,108 - INFO -   warmup_steps >>> 1
>>> 2025-09-17 00:28:33,108 - INFO -   epoch >>> 10
>>> 2025-09-17 00:28:33,109 - INFO -   eval_steps >>> 5
>>> 2025-09-17 00:28:33,109 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-17 00:28:33,109 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-17 00:28:33,110 - INFO -   max_seq_length >>> 2048
>>> 2025-09-17 00:28:33,110 - INFO -   r >>> 8
>>> 2025-09-17 00:28:33,110 - INFO -   interface_mode >>> False
>>> 2025-09-17 00:28:33,111 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-17 00:28:33,111 - INFO -   lora_alpha >>> 16
>>> 2025-09-17 00:28:33,112 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-17 00:28:33,112 - INFO -   bias >>> none
>>> 2025-09-17 00:28:33,112 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-17 00:28:33,113 - INFO -   random_state >>> 3407
>>> 2025-09-17 00:28:33,113 - INFO -   use_rslora >>> True
>>> 2025-09-17 00:28:33,113 - INFO -   loftq_config >>> None
>>> 2025-09-17 00:29:52,074 - INFO - 开始训练！
>>> 2025-09-17 00:30:01,174 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 1.52 GiB. GPU 0 has a total capacity of 31.73 GiB of which 1.13 GiB is free. Including non-PyTorch memory, this process has 30.60 GiB memory in use. Of the allocated memory 29.77 GiB is allocated by PyTorch, and 470.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-09-17 00:30:31,015 - INFO - 导入包完成
>>> 2025-09-17 00:30:31,016 - INFO - ========train Qwen2ForCausalLM  202509170030========
>>> 2025-09-17 00:30:31,016 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 00:30:31,017 - INFO - 开始进行训练
>>> 2025-09-17 00:30:31,025 - INFO - 基础配置文件读取完成
>>> 2025-09-17 00:30:31,033 - INFO - 训练配置读取完成
>>> 2025-09-17 00:30:31,033 - INFO - 数据集路径：dataset/own/Medical_Extension.json
>>> 2025-09-17 00:30:31,034 - INFO - 模型路径:/home/liangshuqiao/models/Qwen2.5-14B-Instruct
>>> 2025-09-17 00:30:31,600 - INFO - tokenizer读取完成
>>> 2025-09-17 00:30:32,873 - INFO - model dtype:torch.bfloat16
>>> 2025-09-17 00:30:32,874 - INFO - 模型导入完成
>>> 2025-09-17 00:30:32,874 - INFO - 数据读取开始
>>> 2025-09-17 00:30:33,805 - INFO - 数据下载完成，训练集大小: 600
>>> 2025-09-17 00:30:37,965 - INFO - 数据映射完成
>>> 2025-09-17 00:30:37,966 - INFO - 打印训练参数如下
>>> 2025-09-17 00:30:37,966 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-17 00:30:37,967 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-17 00:30:37,967 - INFO -   load_in_4bit >>> True
>>> 2025-09-17 00:30:37,967 - INFO -   batch_size >>> 4
>>> 2025-09-17 00:30:37,968 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-17 00:30:37,968 - INFO -   warmup_steps >>> 1
>>> 2025-09-17 00:30:37,968 - INFO -   epoch >>> 10
>>> 2025-09-17 00:30:37,969 - INFO -   eval_steps >>> 5
>>> 2025-09-17 00:30:37,969 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-17 00:30:37,969 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-17 00:30:37,970 - INFO -   max_seq_length >>> 2048
>>> 2025-09-17 00:30:37,970 - INFO -   r >>> 8
>>> 2025-09-17 00:30:37,970 - INFO -   interface_mode >>> False
>>> 2025-09-17 00:30:37,971 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-17 00:30:37,971 - INFO -   lora_alpha >>> 16
>>> 2025-09-17 00:30:37,972 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-17 00:30:37,972 - INFO -   bias >>> none
>>> 2025-09-17 00:30:37,972 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-17 00:30:37,973 - INFO -   random_state >>> 3407
>>> 2025-09-17 00:30:37,973 - INFO -   use_rslora >>> True
>>> 2025-09-17 00:30:37,973 - INFO -   loftq_config >>> None
>>> 2025-09-17 00:30:58,939 - INFO - 开始训练！
>>> 2025-09-17 00:31:24,107 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 836.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 382.19 MiB is free. Including non-PyTorch memory, this process has 31.36 GiB memory in use. Of the allocated memory 30.08 GiB is allocated by PyTorch, and 920.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-09-17 00:32:10,802 - INFO - 导入包完成
>>> 2025-09-17 00:32:10,802 - INFO - ========train Qwen2ForCausalLM  202509170032========
>>> 2025-09-17 00:32:10,803 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 00:32:10,803 - INFO - 开始进行训练
>>> 2025-09-17 00:32:10,809 - INFO - 基础配置文件读取完成
>>> 2025-09-17 00:32:10,817 - INFO - 训练配置读取完成
>>> 2025-09-17 00:32:10,817 - INFO - 数据集路径：dataset/own/Medical_Extension.json
>>> 2025-09-17 00:32:10,818 - INFO - 模型路径:/home/liangshuqiao/models/Qwen2.5-14B-Instruct
>>> 2025-09-17 00:32:11,263 - INFO - tokenizer读取完成
>>> 2025-09-17 00:32:11,460 - INFO - model dtype:torch.bfloat16
>>> 2025-09-17 00:32:11,461 - INFO - 模型导入完成
>>> 2025-09-17 00:32:11,461 - INFO - 数据读取开始
>>> 2025-09-17 00:32:12,237 - INFO - 数据下载完成，训练集大小: 600
>>> 2025-09-17 00:32:18,509 - INFO - 数据映射完成
>>> 2025-09-17 00:32:18,510 - INFO - 打印训练参数如下
>>> 2025-09-17 00:32:18,510 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-17 00:32:18,510 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-17 00:32:18,511 - INFO -   load_in_4bit >>> True
>>> 2025-09-17 00:32:18,511 - INFO -   batch_size >>> 4
>>> 2025-09-17 00:32:18,511 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-17 00:32:18,512 - INFO -   warmup_steps >>> 1
>>> 2025-09-17 00:32:18,512 - INFO -   epoch >>> 10
>>> 2025-09-17 00:32:18,512 - INFO -   eval_steps >>> 5
>>> 2025-09-17 00:32:18,513 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-17 00:32:18,513 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-17 00:32:18,513 - INFO -   max_seq_length >>> 1024
>>> 2025-09-17 00:32:18,514 - INFO -   r >>> 8
>>> 2025-09-17 00:32:18,514 - INFO -   interface_mode >>> False
>>> 2025-09-17 00:32:18,514 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-17 00:32:18,515 - INFO -   lora_alpha >>> 16
>>> 2025-09-17 00:32:18,515 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-17 00:32:18,515 - INFO -   bias >>> none
>>> 2025-09-17 00:32:18,516 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-17 00:32:18,516 - INFO -   random_state >>> 3407
>>> 2025-09-17 00:32:18,516 - INFO -   use_rslora >>> True
>>> 2025-09-17 00:32:18,517 - INFO -   loftq_config >>> None
>>> 2025-09-17 00:32:28,763 - INFO - 开始训练！
>>> 2025-09-17 00:32:53,930 - ERROR - 训练失败：CUDA out of memory. Tried to allocate 836.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 382.19 MiB is free. Including non-PyTorch memory, this process has 31.36 GiB memory in use. Of the allocated memory 30.08 GiB is allocated by PyTorch, and 920.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
>>> 2025-09-17 00:34:17,283 - INFO - 导入包完成
>>> 2025-09-17 00:34:17,283 - INFO - ========train Qwen2ForCausalLM  202509170034========
>>> 2025-09-17 00:34:17,284 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 00:34:17,284 - INFO - 开始进行训练
>>> 2025-09-17 00:34:17,290 - INFO - 基础配置文件读取完成
>>> 2025-09-17 00:34:17,298 - INFO - 训练配置读取完成
>>> 2025-09-17 00:34:17,298 - INFO - 数据集路径：dataset/own/Medical_Extension.json
>>> 2025-09-17 00:34:17,299 - INFO - 模型路径:/home/liangshuqiao/models/Qwen2-7B
>>> 2025-09-17 00:34:17,848 - INFO - tokenizer读取完成
>>> 2025-09-17 00:34:19,435 - INFO - model dtype:torch.bfloat16
>>> 2025-09-17 00:34:19,436 - INFO - 模型导入完成
>>> 2025-09-17 00:34:19,437 - INFO - 数据读取开始
>>> 2025-09-17 00:34:20,232 - INFO - 数据下载完成，训练集大小: 600
>>> 2025-09-17 00:34:26,712 - INFO - 数据映射完成
>>> 2025-09-17 00:34:26,713 - INFO - 打印训练参数如下
>>> 2025-09-17 00:34:26,714 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-17 00:34:26,714 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-17 00:34:26,714 - INFO -   load_in_4bit >>> True
>>> 2025-09-17 00:34:26,715 - INFO -   batch_size >>> 8
>>> 2025-09-17 00:34:26,715 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-17 00:34:26,715 - INFO -   warmup_steps >>> 1
>>> 2025-09-17 00:34:26,716 - INFO -   epoch >>> 10
>>> 2025-09-17 00:34:26,716 - INFO -   eval_steps >>> 5
>>> 2025-09-17 00:34:26,716 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-17 00:34:26,717 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-17 00:34:26,717 - INFO -   max_seq_length >>> 2048
>>> 2025-09-17 00:34:26,718 - INFO -   r >>> 8
>>> 2025-09-17 00:34:26,718 - INFO -   interface_mode >>> False
>>> 2025-09-17 00:34:26,718 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-17 00:34:26,719 - INFO -   lora_alpha >>> 16
>>> 2025-09-17 00:34:26,719 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-17 00:34:26,719 - INFO -   bias >>> none
>>> 2025-09-17 00:34:26,720 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-17 00:34:26,720 - INFO -   random_state >>> 3407
>>> 2025-09-17 00:34:26,721 - INFO -   use_rslora >>> True
>>> 2025-09-17 00:34:26,721 - INFO -   loftq_config >>> None
>>> 2025-09-17 00:35:02,524 - INFO - 开始训练！
>>> 2025-09-17 00:35:47,094 - INFO - >>> {'loss': 3.2098, 'grad_norm': 1.1851576566696167, 'learning_rate': 0.0, 'epoch': 0.05333333333333334}
>>> 2025-09-17 00:36:30,361 - INFO - >>> {'loss': 3.345, 'grad_norm': 1.2172256708145142, 'learning_rate': 0.0002, 'epoch': 0.10666666666666667}
>>> 2025-09-17 00:37:17,324 - INFO - >>> {'loss': 3.1508, 'grad_norm': 1.5574085712432861, 'learning_rate': 0.00019998618546911056, 'epoch': 0.16}
>>> 2025-09-17 00:37:54,808 - INFO - >>> {'loss': 3.0122, 'grad_norm': 1.8562794923782349, 'learning_rate': 0.00019994474569326757, 'epoch': 0.21333333333333335}
>>> 2025-09-17 00:38:44,531 - INFO - >>> {'loss': 2.6827, 'grad_norm': 1.76387357711792, 'learning_rate': 0.00019987569212189224, 'epoch': 0.26666666666666666}
>>> 2025-09-17 00:39:33,733 - INFO - >>> {'loss': 2.541, 'grad_norm': 1.0075381994247437, 'learning_rate': 0.0001997790438338385, 'epoch': 0.32}
>>> 2025-09-17 00:40:15,923 - INFO - >>> {'loss': 2.4651, 'grad_norm': 0.5792468786239624, 'learning_rate': 0.00019965482753212156, 'epoch': 0.37333333333333335}
>>> 2025-09-17 00:40:53,026 - INFO - >>> {'loss': 2.415, 'grad_norm': 0.6360283493995667, 'learning_rate': 0.00019950307753654017, 'epoch': 0.4266666666666667}
>>> 2025-09-17 00:41:39,036 - INFO - >>> {'loss': 2.2986, 'grad_norm': 0.5250440835952759, 'learning_rate': 0.00019932383577419432, 'epoch': 0.48}
>>> 2025-09-17 00:42:18,626 - INFO - >>> {'loss': 2.2883, 'grad_norm': 0.5710268616676331, 'learning_rate': 0.0001991171517679013, 'epoch': 0.5333333333333333}
>>> 2025-09-17 00:43:04,461 - INFO - >>> {'loss': 2.2702, 'grad_norm': 0.44064587354660034, 'learning_rate': 0.00019888308262251285, 'epoch': 0.5866666666666667}
>>> 2025-09-17 00:43:50,051 - INFO - >>> {'loss': 2.1882, 'grad_norm': 0.4002400040626526, 'learning_rate': 0.00019862169300913785, 'epoch': 0.64}
>>> 2025-09-17 00:44:34,613 - INFO - >>> {'loss': 2.167, 'grad_norm': 0.40013957023620605, 'learning_rate': 0.00019833305514727395, 'epoch': 0.6933333333333334}
>>> 2025-09-17 00:45:11,625 - INFO - >>> {'loss': 2.1349, 'grad_norm': 0.44711950421333313, 'learning_rate': 0.00019801724878485438, 'epoch': 0.7466666666666667}
>>> 2025-09-17 00:45:52,389 - INFO - >>> {'loss': 2.1069, 'grad_norm': 0.5201399922370911, 'learning_rate': 0.00019767436117621413, 'epoch': 0.8}
>>> 2025-09-17 00:46:30,472 - INFO - >>> {'loss': 2.0571, 'grad_norm': 0.4310649037361145, 'learning_rate': 0.00019730448705798239, 'epoch': 0.8533333333333334}
>>> 2025-09-17 00:47:15,077 - INFO - >>> {'loss': 2.0619, 'grad_norm': 0.4109480381011963, 'learning_rate': 0.0001969077286229078, 'epoch': 0.9066666666666666}
>>> 2025-09-17 00:48:00,238 - INFO - >>> {'loss': 2.0435, 'grad_norm': 0.4865737855434418, 'learning_rate': 0.00019648419549162348, 'epoch': 0.96}
>>> 2025-09-17 00:48:36,455 - INFO - >>> {'loss': 1.9935, 'grad_norm': 0.47409728169441223, 'learning_rate': 0.00019603400468235998, 'epoch': 1.0}
>>> 2025-09-17 00:49:22,323 - INFO - >>> {'loss': 1.9286, 'grad_norm': 0.4288175404071808, 'learning_rate': 0.0001955572805786141, 'epoch': 1.0533333333333332}
>>> 2025-09-17 00:49:57,203 - INFO - >>> {'loss': 1.9196, 'grad_norm': 0.5660965442657471, 'learning_rate': 0.0001950541548947829, 'epoch': 1.1066666666666667}
>>> 2025-09-17 00:50:38,583 - INFO - >>> {'loss': 1.91, 'grad_norm': 0.5617284774780273, 'learning_rate': 0.00019452476663977248, 'epoch': 1.16}
>>> 2025-09-17 00:51:30,412 - INFO - >>> {'loss': 1.8803, 'grad_norm': 0.5083131790161133, 'learning_rate': 0.00019396926207859084, 'epoch': 1.2133333333333334}
>>> 2025-09-17 00:52:09,925 - INFO - >>> {'loss': 1.8132, 'grad_norm': 0.5300332307815552, 'learning_rate': 0.00019338779469193639, 'epoch': 1.2666666666666666}
>>> 2025-09-17 00:52:52,991 - INFO - >>> {'loss': 1.8218, 'grad_norm': 0.5849738717079163, 'learning_rate': 0.00019278052513379255, 'epoch': 1.32}
>>> 2025-09-17 00:53:39,657 - INFO - >>> {'loss': 1.8593, 'grad_norm': 0.4740389287471771, 'learning_rate': 0.00019214762118704076, 'epoch': 1.3733333333333333}
>>> 2025-09-17 00:54:22,901 - INFO - >>> {'loss': 1.7567, 'grad_norm': 0.5678945779800415, 'learning_rate': 0.00019148925771710347, 'epoch': 1.4266666666666667}
>>> 2025-09-17 00:55:12,586 - INFO - >>> {'loss': 1.8583, 'grad_norm': 0.5512335300445557, 'learning_rate': 0.0001908056166236305, 'epoch': 1.48}
>>> 2025-09-17 00:55:57,322 - INFO - >>> {'loss': 1.7385, 'grad_norm': 0.5210752487182617, 'learning_rate': 0.0001900968867902419, 'epoch': 1.5333333333333332}
>>> 2025-09-17 00:56:38,639 - INFO - >>> {'loss': 1.7872, 'grad_norm': 0.5682405829429626, 'learning_rate': 0.00018936326403234125, 'epoch': 1.5866666666666667}
>>> 2025-09-17 00:57:16,685 - INFO - >>> {'loss': 1.6736, 'grad_norm': 0.6254159808158875, 'learning_rate': 0.00018860495104301345, 'epoch': 1.6400000000000001}
>>> 2025-09-17 00:58:06,021 - INFO - >>> {'loss': 1.7841, 'grad_norm': 0.5477131009101868, 'learning_rate': 0.00018782215733702286, 'epoch': 1.6933333333333334}
>>> 2025-09-17 00:58:57,835 - INFO - >>> {'loss': 1.7653, 'grad_norm': 0.5274367928504944, 'learning_rate': 0.00018701509919292613, 'epoch': 1.7466666666666666}
>>> 2025-09-17 00:59:33,512 - INFO - >>> {'loss': 1.6458, 'grad_norm': 0.6939604878425598, 'learning_rate': 0.0001861839995933164, 'epoch': 1.8}
>>> 2025-09-17 01:00:23,328 - INFO - >>> {'loss': 1.6707, 'grad_norm': 0.568772554397583, 'learning_rate': 0.00018532908816321558, 'epoch': 1.8533333333333335}
>>> 2025-09-17 01:01:07,457 - INFO - >>> {'loss': 1.6307, 'grad_norm': 0.5535921454429626, 'learning_rate': 0.0001844506011066308, 'epoch': 1.9066666666666667}
>>> 2025-09-17 01:01:55,710 - INFO - >>> {'loss': 1.6381, 'grad_norm': 0.6223616003990173, 'learning_rate': 0.00018354878114129367, 'epoch': 1.96}
>>> 2025-09-17 01:02:31,849 - INFO - >>> {'loss': 1.4696, 'grad_norm': 0.7903804779052734, 'learning_rate': 0.0001826238774315995, 'epoch': 2.0}
>>> 2025-09-17 01:03:13,577 - INFO - >>> {'loss': 1.5297, 'grad_norm': 0.7599114179611206, 'learning_rate': 0.00018167614551976567, 'epoch': 2.0533333333333332}
>>> 2025-09-17 01:03:55,850 - INFO - >>> {'loss': 1.4433, 'grad_norm': 0.7883747220039368, 'learning_rate': 0.00018070584725522762, 'epoch': 2.1066666666666665}
>>> 2025-09-17 01:04:41,093 - INFO - >>> {'loss': 1.4865, 'grad_norm': 0.7249885201454163, 'learning_rate': 0.00017971325072229226, 'epoch': 2.16}
>>> 2025-09-17 01:05:24,294 - INFO - >>> {'loss': 1.5579, 'grad_norm': 0.7811235189437866, 'learning_rate': 0.0001786986301660689, 'epoch': 2.2133333333333334}
>>> 2025-09-17 01:06:10,203 - INFO - >>> {'loss': 1.4298, 'grad_norm': 0.8588029742240906, 'learning_rate': 0.00017766226591669785, 'epoch': 2.2666666666666666}
>>> 2025-09-17 01:06:44,731 - INFO - >>> {'loss': 1.2371, 'grad_norm': 0.921664297580719, 'learning_rate': 0.0001766044443118978, 'epoch': 2.32}
>>> 2025-09-17 01:07:31,815 - INFO - >>> {'loss': 1.4138, 'grad_norm': 0.9299910068511963, 'learning_rate': 0.0001755254576178535, 'epoch': 2.3733333333333335}
>>> 2025-09-17 01:08:17,797 - INFO - >>> {'loss': 1.3382, 'grad_norm': 0.8792340159416199, 'learning_rate': 0.00017442560394846516, 'epoch': 2.4266666666666667}
>>> 2025-09-17 01:08:58,516 - INFO - >>> {'loss': 1.1637, 'grad_norm': 0.9462278485298157, 'learning_rate': 0.00017330518718298264, 'epoch': 2.48}
>>> 2025-09-17 01:09:41,325 - INFO - >>> {'loss': 1.3317, 'grad_norm': 1.0089926719665527, 'learning_rate': 0.0001721645168820462, 'epoch': 2.533333333333333}
>>> 2025-09-17 01:10:21,126 - INFO - >>> {'loss': 1.1903, 'grad_norm': 1.0157626867294312, 'learning_rate': 0.00017100390820215804, 'epoch': 2.586666666666667}
>>> 2025-09-17 01:10:58,060 - INFO - >>> {'loss': 1.2655, 'grad_norm': 1.2014727592468262, 'learning_rate': 0.00016982368180860728, 'epoch': 2.64}
>>> 2025-09-17 01:11:48,736 - INFO - >>> {'loss': 1.2534, 'grad_norm': 1.0394401550292969, 'learning_rate': 0.0001686241637868734, 'epoch': 2.6933333333333334}
>>> 2025-09-17 01:12:36,225 - INFO - >>> {'loss': 1.1879, 'grad_norm': 1.2136961221694946, 'learning_rate': 0.00016740568555253155, 'epoch': 2.7466666666666666}
>>> 2025-09-17 01:13:24,065 - INFO - >>> {'loss': 1.1609, 'grad_norm': 1.3171485662460327, 'learning_rate': 0.00016616858375968595, 'epoch': 2.8}
>>> 2025-09-17 01:14:15,140 - INFO - >>> {'loss': 1.1543, 'grad_norm': 1.1785653829574585, 'learning_rate': 0.0001649132002079552, 'epoch': 2.8533333333333335}
>>> 2025-09-17 01:15:01,796 - INFO - >>> {'loss': 1.1262, 'grad_norm': 1.337821364402771, 'learning_rate': 0.00016363988174803638, 'epoch': 2.9066666666666667}
>>> 2025-09-17 01:15:47,035 - INFO - >>> {'loss': 1.0136, 'grad_norm': 1.2054804563522339, 'learning_rate': 0.00016234898018587337, 'epoch': 2.96}
>>> 2025-09-17 01:16:18,745 - INFO - >>> {'loss': 0.9823, 'grad_norm': 1.539061427116394, 'learning_rate': 0.00016104085218545633, 'epoch': 3.0}
>>> 2025-09-17 01:17:05,739 - INFO - >>> {'loss': 1.0009, 'grad_norm': 1.384804606437683, 'learning_rate': 0.00015971585917027862, 'epoch': 3.0533333333333332}
>>> 2025-09-17 01:17:49,791 - INFO - >>> {'loss': 0.887, 'grad_norm': 1.6515262126922607, 'learning_rate': 0.000158374367223479, 'epoch': 3.1066666666666665}
>>> 2025-09-17 01:18:22,042 - INFO - >>> {'loss': 0.7826, 'grad_norm': 1.4656414985656738, 'learning_rate': 0.0001570167469866962, 'epoch': 3.16}
>>> 2025-09-17 01:19:03,800 - INFO - >>> {'loss': 0.9434, 'grad_norm': 1.3877084255218506, 'learning_rate': 0.00015564337355766412, 'epoch': 3.2133333333333334}
>>> 2025-09-17 01:19:43,289 - INFO - >>> {'loss': 0.829, 'grad_norm': 1.6998788118362427, 'learning_rate': 0.00015425462638657595, 'epoch': 3.2666666666666666}
>>> 2025-09-17 01:20:23,392 - INFO - >>> {'loss': 0.8196, 'grad_norm': 1.4965057373046875, 'learning_rate': 0.00015285088917124556, 'epoch': 3.32}
>>> 2025-09-17 01:21:07,633 - INFO - >>> {'loss': 0.8274, 'grad_norm': 1.5495996475219727, 'learning_rate': 0.00015143254975109538, 'epoch': 3.3733333333333335}
>>> 2025-09-17 01:21:49,324 - INFO - >>> {'loss': 0.7692, 'grad_norm': 1.7313175201416016, 'learning_rate': 0.00015000000000000001, 'epoch': 3.4266666666666667}
>>> 2025-09-17 01:22:36,278 - INFO - >>> {'loss': 0.7777, 'grad_norm': 1.841573715209961, 'learning_rate': 0.00014855363571801523, 'epoch': 3.48}
>>> 2025-09-17 01:23:27,964 - INFO - >>> {'loss': 0.7059, 'grad_norm': 1.942054033279419, 'learning_rate': 0.00014709385652202203, 'epoch': 3.533333333333333}
>>> 2025-09-17 01:24:10,203 - INFO - >>> {'loss': 0.7, 'grad_norm': 1.816428303718567, 'learning_rate': 0.0001456210657353163, 'epoch': 3.586666666666667}
>>> 2025-09-17 01:24:54,116 - INFO - >>> {'loss': 0.5786, 'grad_norm': 1.620041012763977, 'learning_rate': 0.0001441356702761744, 'epoch': 3.64}
>>> 2025-09-17 01:25:36,352 - INFO - >>> {'loss': 0.6544, 'grad_norm': 1.998386025428772, 'learning_rate': 0.0001426380805454254, 'epoch': 3.6933333333333334}
>>> 2025-09-17 01:26:21,672 - INFO - >>> {'loss': 0.7346, 'grad_norm': 1.981582760810852, 'learning_rate': 0.00014112871031306119, 'epoch': 3.7466666666666666}
>>> 2025-09-17 01:27:05,399 - INFO - >>> {'loss': 0.6767, 'grad_norm': 1.605691909790039, 'learning_rate': 0.0001396079766039157, 'epoch': 3.8}
>>> 2025-09-17 01:27:53,473 - INFO - >>> {'loss': 0.6802, 'grad_norm': 1.8240516185760498, 'learning_rate': 0.00013807629958244498, 'epoch': 3.8533333333333335}
>>> 2025-09-17 01:28:36,247 - INFO - >>> {'loss': 0.6313, 'grad_norm': 1.9296005964279175, 'learning_rate': 0.00013653410243663952, 'epoch': 3.9066666666666667}
>>> 2025-09-17 01:29:18,488 - INFO - >>> {'loss': 0.4422, 'grad_norm': 1.988322138786316, 'learning_rate': 0.0001349818112611015, 'epoch': 3.96}
>>> 2025-09-17 01:29:48,946 - INFO - >>> {'loss': 0.5683, 'grad_norm': 2.0840818881988525, 'learning_rate': 0.00013341985493931877, 'epoch': 4.0}
>>> 2025-09-17 01:30:31,022 - INFO - >>> {'loss': 0.5211, 'grad_norm': 1.7771415710449219, 'learning_rate': 0.00013184866502516845, 'epoch': 4.053333333333334}
>>> 2025-09-17 01:31:12,051 - INFO - >>> {'loss': 0.3652, 'grad_norm': 1.7693299055099487, 'learning_rate': 0.0001302686756236826, 'epoch': 4.1066666666666665}
>>> 2025-09-17 01:31:53,321 - INFO - >>> {'loss': 0.4223, 'grad_norm': 1.8381550312042236, 'learning_rate': 0.00012868032327110904, 'epoch': 4.16}
>>> 2025-09-17 01:32:33,859 - INFO - >>> {'loss': 0.3671, 'grad_norm': 2.2134616374969482, 'learning_rate': 0.00012708404681430053, 'epoch': 4.213333333333333}
>>> 2025-09-17 01:33:19,282 - INFO - >>> {'loss': 0.3498, 'grad_norm': 1.7803809642791748, 'learning_rate': 0.0001254802872894655, 'epoch': 4.266666666666667}
>>> 2025-09-17 01:34:09,549 - INFO - >>> {'loss': 0.3609, 'grad_norm': 1.700617790222168, 'learning_rate': 0.0001238694878003138, 'epoch': 4.32}
>>> 2025-09-17 01:34:51,818 - INFO - >>> {'loss': 0.3779, 'grad_norm': 1.7961028814315796, 'learning_rate': 0.00012225209339563145, 'epoch': 4.373333333333333}
>>> 2025-09-17 01:35:38,049 - INFO - >>> {'loss': 0.4268, 'grad_norm': 1.6949632167816162, 'learning_rate': 0.00012062855094631778, 'epoch': 4.426666666666667}
>>> 2025-09-17 01:36:27,163 - INFO - >>> {'loss': 0.3605, 'grad_norm': 1.4907135963439941, 'learning_rate': 0.00011899930902191902, 'epoch': 4.48}
>>> 2025-09-17 01:37:04,687 - INFO - >>> {'loss': 0.3346, 'grad_norm': 1.8290786743164062, 'learning_rate': 0.00011736481776669306, 'epoch': 4.533333333333333}
>>> 2025-09-17 01:37:49,056 - INFO - >>> {'loss': 0.3269, 'grad_norm': 1.725953221321106, 'learning_rate': 0.00011572552877523854, 'epoch': 4.586666666666667}
>>> 2025-09-17 01:38:25,280 - INFO - >>> {'loss': 0.2717, 'grad_norm': 2.654059886932373, 'learning_rate': 0.00011408189496772368, 'epoch': 4.64}
>>> 2025-09-17 01:39:13,182 - INFO - >>> {'loss': 0.3283, 'grad_norm': 1.6076499223709106, 'learning_rate': 0.00011243437046474853, 'epoch': 4.693333333333333}
>>> 2025-09-17 01:39:55,406 - INFO - >>> {'loss': 0.2967, 'grad_norm': 1.4752202033996582, 'learning_rate': 0.00011078341046187589, 'epoch': 4.746666666666667}
>>> 2025-09-17 01:40:34,768 - INFO - >>> {'loss': 0.3262, 'grad_norm': 1.573555827140808, 'learning_rate': 0.00010912947110386484, 'epoch': 4.8}
>>> 2025-09-17 01:41:21,468 - INFO - >>> {'loss': 0.2979, 'grad_norm': 1.5969414710998535, 'learning_rate': 0.00010747300935864243, 'epoch': 4.8533333333333335}
>>> 2025-09-17 01:42:13,056 - INFO - >>> {'loss': 0.279, 'grad_norm': 1.5390093326568604, 'learning_rate': 0.00010581448289104758, 'epoch': 4.906666666666666}
>>> 2025-09-17 01:42:55,456 - INFO - >>> {'loss': 0.3218, 'grad_norm': 1.4004384279251099, 'learning_rate': 0.00010415434993638269, 'epoch': 4.96}
>>> 2025-09-17 01:43:29,835 - INFO - >>> {'loss': 0.3, 'grad_norm': 2.2043850421905518, 'learning_rate': 0.0001024930691738073, 'epoch': 5.0}
>>> 2025-09-17 01:44:06,462 - INFO - >>> {'loss': 0.2505, 'grad_norm': 1.5090185403823853, 'learning_rate': 0.00010083109959960973, 'epoch': 5.053333333333334}
>>> 2025-09-17 01:44:44,132 - INFO - >>> {'loss': 0.2159, 'grad_norm': 1.5797902345657349, 'learning_rate': 9.916890040039031e-05, 'epoch': 5.1066666666666665}
>>> 2025-09-17 01:45:29,594 - INFO - >>> {'loss': 0.215, 'grad_norm': 1.455612063407898, 'learning_rate': 9.750693082619273e-05, 'epoch': 5.16}
>>> 2025-09-17 01:46:19,924 - INFO - >>> {'loss': 0.2178, 'grad_norm': 1.264656901359558, 'learning_rate': 9.584565006361734e-05, 'epoch': 5.213333333333333}
>>> 2025-09-17 01:46:55,373 - INFO - >>> {'loss': 0.2476, 'grad_norm': 1.3770869970321655, 'learning_rate': 9.418551710895243e-05, 'epoch': 5.266666666666667}
>>> 2025-09-17 01:47:41,518 - INFO - >>> {'loss': 0.2524, 'grad_norm': 1.4001457691192627, 'learning_rate': 9.252699064135758e-05, 'epoch': 5.32}
>>> 2025-09-17 01:48:19,745 - INFO - >>> {'loss': 0.1833, 'grad_norm': 1.5329548120498657, 'learning_rate': 9.087052889613518e-05, 'epoch': 5.373333333333333}
>>> 2025-09-17 01:49:10,609 - INFO - >>> {'loss': 0.1653, 'grad_norm': 1.6271917819976807, 'learning_rate': 8.921658953812415e-05, 'epoch': 5.426666666666667}
>>> 2025-09-17 01:50:01,297 - INFO - >>> {'loss': 0.1403, 'grad_norm': 1.4044512510299683, 'learning_rate': 8.756562953525152e-05, 'epoch': 5.48}
>>> 2025-09-17 01:50:37,920 - INFO - >>> {'loss': 0.1327, 'grad_norm': 1.2606735229492188, 'learning_rate': 8.591810503227635e-05, 'epoch': 5.533333333333333}
>>> 2025-09-17 01:51:24,787 - INFO - >>> {'loss': 0.1574, 'grad_norm': 1.2987821102142334, 'learning_rate': 8.427447122476148e-05, 'epoch': 5.586666666666667}
>>> 2025-09-17 01:52:12,096 - INFO - >>> {'loss': 0.1073, 'grad_norm': 1.229043960571289, 'learning_rate': 8.263518223330697e-05, 'epoch': 5.64}
>>> 2025-09-17 01:52:50,624 - INFO - >>> {'loss': 0.1337, 'grad_norm': 1.257948398590088, 'learning_rate': 8.100069097808103e-05, 'epoch': 5.693333333333333}
>>> 2025-09-17 01:53:32,122 - INFO - >>> {'loss': 0.1716, 'grad_norm': 1.4520580768585205, 'learning_rate': 7.937144905368226e-05, 'epoch': 5.746666666666667}
>>> 2025-09-17 01:54:14,791 - INFO - >>> {'loss': 0.1862, 'grad_norm': 1.160092830657959, 'learning_rate': 7.774790660436858e-05, 'epoch': 5.8}
>>> 2025-09-17 01:55:01,565 - INFO - >>> {'loss': 0.1372, 'grad_norm': 1.1528019905090332, 'learning_rate': 7.613051219968623e-05, 'epoch': 5.8533333333333335}
>>> 2025-09-17 01:55:48,133 - INFO - >>> {'loss': 0.127, 'grad_norm': 1.2942935228347778, 'learning_rate': 7.451971271053455e-05, 'epoch': 5.906666666666666}
>>> 2025-09-17 01:56:33,370 - INFO - >>> {'loss': 0.1646, 'grad_norm': 1.587117314338684, 'learning_rate': 7.291595318569951e-05, 'epoch': 5.96}
>>> 2025-09-17 01:57:05,044 - INFO - >>> {'loss': 0.1328, 'grad_norm': 1.5841689109802246, 'learning_rate': 7.131967672889101e-05, 'epoch': 6.0}
>>> 2025-09-17 01:57:51,858 - INFO - >>> {'loss': 0.1513, 'grad_norm': 1.0021305084228516, 'learning_rate': 6.973132437631742e-05, 'epoch': 6.053333333333334}
>>> 2025-09-17 01:58:33,040 - INFO - >>> {'loss': 0.1075, 'grad_norm': 1.1170355081558228, 'learning_rate': 6.815133497483157e-05, 'epoch': 6.1066666666666665}
>>> 2025-09-17 01:59:16,556 - INFO - >>> {'loss': 0.1238, 'grad_norm': 1.4084339141845703, 'learning_rate': 6.658014506068126e-05, 'epoch': 6.16}
>>> 2025-09-17 01:59:59,931 - INFO - >>> {'loss': 0.0825, 'grad_norm': 0.9395963549613953, 'learning_rate': 6.501818873889855e-05, 'epoch': 6.213333333333333}
>>> 2025-09-17 02:00:46,484 - INFO - >>> {'loss': 0.1424, 'grad_norm': 1.21894109249115, 'learning_rate': 6.34658975633605e-05, 'epoch': 6.266666666666667}
>>> 2025-09-17 02:01:27,999 - INFO - >>> {'loss': 0.1411, 'grad_norm': 1.61130690574646, 'learning_rate': 6.192370041755505e-05, 'epoch': 6.32}
>>> 2025-09-17 02:02:14,091 - INFO - >>> {'loss': 0.13, 'grad_norm': 1.1971766948699951, 'learning_rate': 6.039202339608432e-05, 'epoch': 6.373333333333333}
>>> 2025-09-17 02:02:55,540 - INFO - >>> {'loss': 0.1046, 'grad_norm': 1.0053281784057617, 'learning_rate': 5.887128968693887e-05, 'epoch': 6.426666666666667}
>>> 2025-09-17 02:03:37,848 - INFO - >>> {'loss': 0.1215, 'grad_norm': 1.2247117757797241, 'learning_rate': 5.736191945457463e-05, 'epoch': 6.48}
>>> 2025-09-17 02:04:16,143 - INFO - >>> {'loss': 0.1073, 'grad_norm': 0.9524881839752197, 'learning_rate': 5.58643297238256e-05, 'epoch': 6.533333333333333}
>>> 2025-09-17 02:05:03,834 - INFO - >>> {'loss': 0.0631, 'grad_norm': 1.0528693199157715, 'learning_rate': 5.43789342646837e-05, 'epoch': 6.586666666666667}
>>> 2025-09-17 02:05:48,492 - INFO - >>> {'loss': 0.1402, 'grad_norm': 1.1983463764190674, 'learning_rate': 5.290614347797802e-05, 'epoch': 6.64}
>>> 2025-09-17 02:06:35,337 - INFO - >>> {'loss': 0.0854, 'grad_norm': 0.9024184942245483, 'learning_rate': 5.1446364281984774e-05, 'epoch': 6.693333333333333}
>>> 2025-09-17 02:07:15,603 - INFO - >>> {'loss': 0.0849, 'grad_norm': 1.183733582496643, 'learning_rate': 5.000000000000002e-05, 'epoch': 6.746666666666667}
>>> 2025-09-17 02:08:07,592 - INFO - >>> {'loss': 0.0758, 'grad_norm': 0.8853158354759216, 'learning_rate': 4.856745024890466e-05, 'epoch': 6.8}
>>> 2025-09-17 02:08:50,135 - INFO - >>> {'loss': 0.1016, 'grad_norm': 0.818644642829895, 'learning_rate': 4.7149110828754464e-05, 'epoch': 6.8533333333333335}
>>> 2025-09-17 02:09:30,753 - INFO - >>> {'loss': 0.0938, 'grad_norm': 1.2804229259490967, 'learning_rate': 4.574537361342407e-05, 'epoch': 6.906666666666666}
>>> 2025-09-17 02:10:14,951 - INFO - >>> {'loss': 0.1458, 'grad_norm': 0.8208134770393372, 'learning_rate': 4.435662644233594e-05, 'epoch': 6.96}
>>> 2025-09-17 02:10:46,831 - INFO - >>> {'loss': 0.0752, 'grad_norm': 0.7959557175636292, 'learning_rate': 4.298325301330383e-05, 'epoch': 7.0}
>>> 2025-09-17 02:11:29,318 - INFO - >>> {'loss': 0.0957, 'grad_norm': 0.891497790813446, 'learning_rate': 4.1625632776521037e-05, 'epoch': 7.053333333333334}
>>> 2025-09-17 02:12:11,910 - INFO - >>> {'loss': 0.0843, 'grad_norm': 0.9207086563110352, 'learning_rate': 4.028414082972141e-05, 'epoch': 7.1066666666666665}
>>> 2025-09-17 02:12:55,147 - INFO - >>> {'loss': 0.1042, 'grad_norm': 0.66166752576828, 'learning_rate': 3.89591478145437e-05, 'epoch': 7.16}
>>> 2025-09-17 02:13:39,516 - INFO - >>> {'loss': 0.1129, 'grad_norm': 0.9401260018348694, 'learning_rate': 3.7651019814126654e-05, 'epoch': 7.213333333333333}
>>> 2025-09-17 02:14:27,540 - INFO - >>> {'loss': 0.0689, 'grad_norm': 0.7469666600227356, 'learning_rate': 3.6360118251963645e-05, 'epoch': 7.266666666666667}
>>> 2025-09-17 02:15:06,488 - INFO - >>> {'loss': 0.0908, 'grad_norm': 0.908118486404419, 'learning_rate': 3.508679979204481e-05, 'epoch': 7.32}
>>> 2025-09-17 02:15:49,500 - INFO - >>> {'loss': 0.0769, 'grad_norm': 0.8106365203857422, 'learning_rate': 3.383141624031408e-05, 'epoch': 7.373333333333333}
>>> 2025-09-17 02:16:27,500 - INFO - >>> {'loss': 0.0722, 'grad_norm': 0.7993370294570923, 'learning_rate': 3.259431444746846e-05, 'epoch': 7.426666666666667}
>>> 2025-09-17 02:17:09,555 - INFO - >>> {'loss': 0.0983, 'grad_norm': 0.8837637305259705, 'learning_rate': 3.137583621312665e-05, 'epoch': 7.48}
>>> 2025-09-17 02:17:53,666 - INFO - >>> {'loss': 0.1164, 'grad_norm': 0.9176973700523376, 'learning_rate': 3.0176318191392726e-05, 'epoch': 7.533333333333333}
>>> 2025-09-17 02:18:31,051 - INFO - >>> {'loss': 0.0544, 'grad_norm': 0.8322312831878662, 'learning_rate': 2.8996091797841973e-05, 'epoch': 7.586666666666667}
>>> 2025-09-17 02:19:16,085 - INFO - >>> {'loss': 0.0745, 'grad_norm': 0.8061736226081848, 'learning_rate': 2.7835483117953788e-05, 'epoch': 7.64}
>>> 2025-09-17 02:20:00,304 - INFO - >>> {'loss': 0.0624, 'grad_norm': 0.7095879912376404, 'learning_rate': 2.669481281701739e-05, 'epoch': 7.693333333333333}
>>> 2025-09-17 02:20:42,367 - INFO - >>> {'loss': 0.0945, 'grad_norm': 0.7886732220649719, 'learning_rate': 2.5574396051534832e-05, 'epoch': 7.746666666666667}
>>> 2025-09-17 02:21:27,779 - INFO - >>> {'loss': 0.0932, 'grad_norm': 0.8473408222198486, 'learning_rate': 2.4474542382146537e-05, 'epoch': 7.8}
>>> 2025-09-17 02:22:09,456 - INFO - >>> {'loss': 0.073, 'grad_norm': 0.8303295373916626, 'learning_rate': 2.339555568810221e-05, 'epoch': 7.8533333333333335}
>>> 2025-09-17 02:22:56,051 - INFO - >>> {'loss': 0.0403, 'grad_norm': 0.5895901322364807, 'learning_rate': 2.2337734083302164e-05, 'epoch': 7.906666666666666}
>>> 2025-09-17 02:23:35,114 - INFO - >>> {'loss': 0.067, 'grad_norm': 0.9215012788772583, 'learning_rate': 2.1301369833931117e-05, 'epoch': 7.96}
>>> 2025-09-17 02:24:11,072 - INFO - >>> {'loss': 0.032, 'grad_norm': 0.653584361076355, 'learning_rate': 2.0286749277707782e-05, 'epoch': 8.0}
>>> 2025-09-17 02:24:50,644 - INFO - >>> {'loss': 0.0939, 'grad_norm': 0.7082198262214661, 'learning_rate': 1.929415274477239e-05, 'epoch': 8.053333333333333}
>>> 2025-09-17 02:25:33,852 - INFO - >>> {'loss': 0.0718, 'grad_norm': 0.7366179823875427, 'learning_rate': 1.832385448023435e-05, 'epoch': 8.106666666666667}
>>> 2025-09-17 02:26:15,407 - INFO - >>> {'loss': 0.0542, 'grad_norm': 0.6686883568763733, 'learning_rate': 1.7376122568400532e-05, 'epoch': 8.16}
>>> 2025-09-17 02:26:58,703 - INFO - >>> {'loss': 0.0791, 'grad_norm': 0.934664249420166, 'learning_rate': 1.6451218858706374e-05, 'epoch': 8.213333333333333}
>>> 2025-09-17 02:27:44,961 - INFO - >>> {'loss': 0.0697, 'grad_norm': 0.7607588171958923, 'learning_rate': 1.5549398893369216e-05, 'epoch': 8.266666666666667}
>>> 2025-09-17 02:28:33,834 - INFO - >>> {'loss': 0.0576, 'grad_norm': 0.6144782900810242, 'learning_rate': 1.467091183678444e-05, 'epoch': 8.32}
>>> 2025-09-17 02:29:17,501 - INFO - >>> {'loss': 0.0691, 'grad_norm': 0.6013444066047668, 'learning_rate': 1.3816000406683604e-05, 'epoch': 8.373333333333333}
>>> 2025-09-17 02:30:00,553 - INFO - >>> {'loss': 0.0503, 'grad_norm': 0.6756085157394409, 'learning_rate': 1.2984900807073919e-05, 'epoch': 8.426666666666666}
>>> 2025-09-17 02:30:46,240 - INFO - >>> {'loss': 0.1025, 'grad_norm': 0.7406302094459534, 'learning_rate': 1.2177842662977135e-05, 'epoch': 8.48}
>>> 2025-09-17 02:31:33,375 - INFO - >>> {'loss': 0.0839, 'grad_norm': 0.7100264430046082, 'learning_rate': 1.1395048956986575e-05, 'epoch': 8.533333333333333}
>>> 2025-09-17 02:32:17,176 - INFO - >>> {'loss': 0.0631, 'grad_norm': 0.8643736839294434, 'learning_rate': 1.0636735967658784e-05, 'epoch': 8.586666666666666}
>>> 2025-09-17 02:33:02,737 - INFO - >>> {'loss': 0.0414, 'grad_norm': 0.6441077589988708, 'learning_rate': 9.903113209758096e-06, 'epoch': 8.64}
>>> 2025-09-17 02:33:40,607 - INFO - >>> {'loss': 0.054, 'grad_norm': 0.5839611887931824, 'learning_rate': 9.194383376369508e-06, 'epoch': 8.693333333333333}
>>> 2025-09-17 02:34:24,958 - INFO - >>> {'loss': 0.0576, 'grad_norm': 1.1692636013031006, 'learning_rate': 8.510742282896544e-06, 'epoch': 8.746666666666666}
>>> 2025-09-17 02:35:08,755 - INFO - >>> {'loss': 0.0885, 'grad_norm': 0.7074670195579529, 'learning_rate': 7.852378812959227e-06, 'epoch': 8.8}
>>> 2025-09-17 02:35:49,982 - INFO - >>> {'loss': 0.0484, 'grad_norm': 0.6061118841171265, 'learning_rate': 7.219474866207465e-06, 'epoch': 8.853333333333333}
>>> 2025-09-17 02:36:30,672 - INFO - >>> {'loss': 0.0696, 'grad_norm': 0.9784886240959167, 'learning_rate': 6.612205308063646e-06, 'epoch': 8.906666666666666}
>>> 2025-09-17 02:37:18,576 - INFO - >>> {'loss': 0.0697, 'grad_norm': 0.6878626942634583, 'learning_rate': 6.030737921409169e-06, 'epoch': 8.96}
>>> 2025-09-17 02:37:57,577 - INFO - >>> {'loss': 0.0376, 'grad_norm': 0.5437221527099609, 'learning_rate': 5.475233360227516e-06, 'epoch': 9.0}
>>> 2025-09-17 02:38:40,645 - INFO - >>> {'loss': 0.0581, 'grad_norm': 0.6519360542297363, 'learning_rate': 4.945845105217117e-06, 'epoch': 9.053333333333333}
>>> 2025-09-17 02:39:31,803 - INFO - >>> {'loss': 0.0394, 'grad_norm': 0.5170317888259888, 'learning_rate': 4.442719421385922e-06, 'epoch': 9.106666666666667}
>>> 2025-09-17 02:40:11,701 - INFO - >>> {'loss': 0.0502, 'grad_norm': 0.6185259819030762, 'learning_rate': 3.965995317640025e-06, 'epoch': 9.16}
>>> 2025-09-17 02:41:00,703 - INFO - >>> {'loss': 0.071, 'grad_norm': 0.5030853748321533, 'learning_rate': 3.515804508376508e-06, 'epoch': 9.213333333333333}
>>> 2025-09-17 02:41:49,097 - INFO - >>> {'loss': 0.0532, 'grad_norm': 0.5486737489700317, 'learning_rate': 3.092271377092215e-06, 'epoch': 9.266666666666667}
>>> 2025-09-17 02:42:30,028 - INFO - >>> {'loss': 0.081, 'grad_norm': 0.7333811521530151, 'learning_rate': 2.6955129420176196e-06, 'epoch': 9.32}
>>> 2025-09-17 02:43:12,983 - INFO - >>> {'loss': 0.0582, 'grad_norm': 0.5538652539253235, 'learning_rate': 2.3256388237858807e-06, 'epoch': 9.373333333333333}
>>> 2025-09-17 02:44:04,581 - INFO - >>> {'loss': 0.047, 'grad_norm': 0.7298243045806885, 'learning_rate': 1.9827512151456173e-06, 'epoch': 9.426666666666666}
>>> 2025-09-17 02:44:52,592 - INFO - >>> {'loss': 0.0971, 'grad_norm': 0.6138807535171509, 'learning_rate': 1.66694485272606e-06, 'epoch': 9.48}
>>> 2025-09-17 02:45:36,512 - INFO - >>> {'loss': 0.0672, 'grad_norm': 0.7308546304702759, 'learning_rate': 1.378306990862177e-06, 'epoch': 9.533333333333333}
>>> 2025-09-17 02:46:16,527 - INFO - >>> {'loss': 0.0487, 'grad_norm': 0.5839188098907471, 'learning_rate': 1.1169173774871478e-06, 'epoch': 9.586666666666666}
>>> 2025-09-17 02:47:01,845 - INFO - >>> {'loss': 0.0641, 'grad_norm': 0.7969377040863037, 'learning_rate': 8.828482320987319e-07, 'epoch': 9.64}
>>> 2025-09-17 02:47:40,109 - INFO - >>> {'loss': 0.1097, 'grad_norm': 0.7813429236412048, 'learning_rate': 6.761642258056978e-07, 'epoch': 9.693333333333333}
>>> 2025-09-17 02:48:24,474 - INFO - >>> {'loss': 0.0681, 'grad_norm': 0.6420316100120544, 'learning_rate': 4.969224634598591e-07, 'epoch': 9.746666666666666}
>>> 2025-09-17 02:49:04,162 - INFO - >>> {'loss': 0.0529, 'grad_norm': 0.6518102288246155, 'learning_rate': 3.451724678784518e-07, 'epoch': 9.8}
>>> 2025-09-17 02:49:46,821 - INFO - >>> {'loss': 0.0509, 'grad_norm': 0.6307452917098999, 'learning_rate': 2.2095616616150115e-07, 'epoch': 9.853333333333333}
>>> 2025-09-17 02:50:26,883 - INFO - >>> {'loss': 0.037, 'grad_norm': 0.6296672821044922, 'learning_rate': 1.2430787810776555e-07, 'epoch': 9.906666666666666}
>>> 2025-09-17 02:51:13,409 - INFO - >>> {'loss': 0.0626, 'grad_norm': 0.550273597240448, 'learning_rate': 5.5254306732444025e-08, 'epoch': 9.96}
>>> 2025-09-17 02:51:43,947 - INFO - >>> {'loss': 0.0702, 'grad_norm': 0.7793160080909729, 'learning_rate': 1.3814530889433296e-08, 'epoch': 10.0}
>>> 2025-09-17 02:51:44,569 - INFO - >>> {'train_runtime': 8201.7255, 'train_samples_per_second': 0.732, 'train_steps_per_second': 0.023, 'train_loss': 0.7066839068343765, 'epoch': 10.0}
>>> 2025-09-17 02:51:44,571 - INFO - 训练成功！
>>> 2025-09-17 02:51:44,571 - INFO - 模型存放位置：./output/qwen3-8b202509170034
>>> 2025-09-17 10:03:31,064 - INFO - 导入包完成
>>> 2025-09-17 10:03:31,065 - INFO - ========train Qwen2ForCausalLM  202509171003========
>>> 2025-09-17 10:03:31,065 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 10:03:31,066 - INFO - 开始进行训练
>>> 2025-09-17 10:03:31,072 - INFO - 基础配置文件读取完成
>>> 2025-09-17 10:03:31,080 - INFO - 训练配置读取完成
>>> 2025-09-17 10:03:31,080 - INFO - 数据集路径：dataset/own/Medical_Extension.json
>>> 2025-09-17 10:03:31,081 - INFO - 模型路径:/home/liangshuqiao/models/Qwen2-7B
>>> 2025-09-17 10:03:31,648 - INFO - tokenizer读取完成
>>> 2025-09-17 10:03:32,195 - INFO - model dtype:torch.bfloat16
>>> 2025-09-17 10:03:32,196 - INFO - 模型导入完成
>>> 2025-09-17 10:03:32,196 - INFO - 数据读取开始
>>> 2025-09-17 10:03:33,048 - INFO - 数据下载完成，训练集大小: 600
>>> 2025-09-17 10:03:37,200 - INFO - 数据映射完成
>>> 2025-09-17 10:03:37,200 - INFO - 打印训练参数如下
>>> 2025-09-17 10:03:37,201 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-17 10:03:37,201 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-17 10:03:37,202 - INFO -   load_in_4bit >>> True
>>> 2025-09-17 10:03:37,202 - INFO -   batch_size >>> 8
>>> 2025-09-17 10:03:37,202 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-17 10:03:37,203 - INFO -   warmup_steps >>> 1
>>> 2025-09-17 10:03:37,203 - INFO -   epoch >>> 10
>>> 2025-09-17 10:03:37,203 - INFO -   eval_steps >>> 5
>>> 2025-09-17 10:03:37,204 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-17 10:03:37,204 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-17 10:03:37,204 - INFO -   max_seq_length >>> 2048
>>> 2025-09-17 10:03:37,205 - INFO -   r >>> 8
>>> 2025-09-17 10:03:37,205 - INFO -   interface_mode >>> False
>>> 2025-09-17 10:03:37,205 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-17 10:03:37,206 - INFO -   lora_alpha >>> 16
>>> 2025-09-17 10:03:37,206 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-17 10:03:37,206 - INFO -   bias >>> none
>>> 2025-09-17 10:03:37,206 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-17 10:03:37,207 - INFO -   random_state >>> 3407
>>> 2025-09-17 10:03:37,207 - INFO -   use_rslora >>> True
>>> 2025-09-17 10:03:37,208 - INFO -   loftq_config >>> None
>>> 2025-09-17 10:03:43,300 - INFO - 开始训练！
>>> 2025-09-17 10:04:27,474 - INFO - >>> {'loss': 3.2098, 'grad_norm': 1.1228348016738892, 'learning_rate': 0.0, 'epoch': 0.05333333333333334}
>>> 2025-09-17 10:05:10,339 - INFO - >>> {'loss': 3.345, 'grad_norm': 1.1538177728652954, 'learning_rate': 0.0002, 'epoch': 0.10666666666666667}
>>> 2025-09-17 10:05:57,881 - INFO - >>> {'loss': 3.1459, 'grad_norm': 1.4886271953582764, 'learning_rate': 0.00019998618546911056, 'epoch': 0.16}
>>> 2025-09-17 10:06:35,362 - INFO - >>> {'loss': 3.0096, 'grad_norm': 1.7259864807128906, 'learning_rate': 0.00019994474569326757, 'epoch': 0.21333333333333335}
>>> 2025-09-17 10:07:25,099 - INFO - >>> {'loss': 2.6832, 'grad_norm': 1.6115527153015137, 'learning_rate': 0.00019987569212189224, 'epoch': 0.26666666666666666}
>>> 2025-09-17 10:08:14,191 - INFO - >>> {'loss': 2.5515, 'grad_norm': 0.8969265818595886, 'learning_rate': 0.0001997790438338385, 'epoch': 0.32}
>>> 2025-09-17 10:08:56,343 - INFO - >>> {'loss': 2.4826, 'grad_norm': 0.5703991651535034, 'learning_rate': 0.00019965482753212156, 'epoch': 0.37333333333333335}
>>> 2025-09-17 10:09:33,434 - INFO - >>> {'loss': 2.436, 'grad_norm': 0.653343677520752, 'learning_rate': 0.00019950307753654017, 'epoch': 0.4266666666666667}
>>> 2025-09-17 10:10:19,440 - INFO - >>> {'loss': 2.3178, 'grad_norm': 0.5415937900543213, 'learning_rate': 0.00019932383577419432, 'epoch': 0.48}
>>> 2025-09-17 10:10:59,129 - INFO - >>> {'loss': 2.3066, 'grad_norm': 0.5599008202552795, 'learning_rate': 0.0001991171517679013, 'epoch': 0.5333333333333333}
>>> 2025-09-17 10:11:45,163 - INFO - >>> {'loss': 2.2835, 'grad_norm': 0.4786462187767029, 'learning_rate': 0.00019888308262251285, 'epoch': 0.5866666666666667}
>>> 2025-09-17 10:12:30,932 - INFO - >>> {'loss': 2.1988, 'grad_norm': 0.41841718554496765, 'learning_rate': 0.00019862169300913785, 'epoch': 0.64}
>>> 2025-09-17 10:13:15,675 - INFO - >>> {'loss': 2.1785, 'grad_norm': 0.3868618309497833, 'learning_rate': 0.00019833305514727395, 'epoch': 0.6933333333333334}
>>> 2025-09-17 10:13:52,747 - INFO - >>> {'loss': 2.1446, 'grad_norm': 0.436647891998291, 'learning_rate': 0.00019801724878485438, 'epoch': 0.7466666666666667}
>>> 2025-09-17 10:14:33,470 - INFO - >>> {'loss': 2.1174, 'grad_norm': 0.5136386752128601, 'learning_rate': 0.00019767436117621413, 'epoch': 0.8}
>>> 2025-09-17 10:15:11,481 - INFO - >>> {'loss': 2.0666, 'grad_norm': 0.43063440918922424, 'learning_rate': 0.00019730448705798239, 'epoch': 0.8533333333333334}
>>> 2025-09-17 10:15:55,952 - INFO - >>> {'loss': 2.0684, 'grad_norm': 0.4103136360645294, 'learning_rate': 0.0001969077286229078, 'epoch': 0.9066666666666666}
>>> 2025-09-17 10:16:41,052 - INFO - >>> {'loss': 2.0537, 'grad_norm': 0.4840904772281647, 'learning_rate': 0.00019648419549162348, 'epoch': 0.96}
>>> 2025-09-17 10:17:17,290 - INFO - >>> {'loss': 2.0066, 'grad_norm': 0.479196697473526, 'learning_rate': 0.00019603400468235998, 'epoch': 1.0}
>>> 2025-09-17 10:18:03,151 - INFO - >>> {'loss': 1.9355, 'grad_norm': 0.4280603528022766, 'learning_rate': 0.0001955572805786141, 'epoch': 1.0533333333333332}
>>> 2025-09-17 10:18:37,986 - INFO - >>> {'loss': 1.9295, 'grad_norm': 0.5560854077339172, 'learning_rate': 0.0001950541548947829, 'epoch': 1.1066666666666667}
>>> 2025-09-17 10:19:19,463 - INFO - >>> {'loss': 1.919, 'grad_norm': 0.5528420805931091, 'learning_rate': 0.00019452476663977248, 'epoch': 1.16}
>>> 2025-09-17 10:20:11,352 - INFO - >>> {'loss': 1.8836, 'grad_norm': 0.4971434772014618, 'learning_rate': 0.00019396926207859084, 'epoch': 1.2133333333333334}
>>> 2025-09-17 10:20:50,952 - INFO - >>> {'loss': 1.822, 'grad_norm': 0.5341041684150696, 'learning_rate': 0.00019338779469193639, 'epoch': 1.2666666666666666}
>>> 2025-09-17 10:21:33,988 - INFO - >>> {'loss': 1.831, 'grad_norm': 0.5820502638816833, 'learning_rate': 0.00019278052513379255, 'epoch': 1.32}
>>> 2025-09-17 10:22:20,770 - INFO - >>> {'loss': 1.8684, 'grad_norm': 0.4806746244430542, 'learning_rate': 0.00019214762118704076, 'epoch': 1.3733333333333333}
>>> 2025-09-17 10:23:04,262 - INFO - >>> {'loss': 1.7703, 'grad_norm': 0.5813559889793396, 'learning_rate': 0.00019148925771710347, 'epoch': 1.4266666666666667}
>>> 2025-09-17 10:23:54,257 - INFO - >>> {'loss': 1.8696, 'grad_norm': 0.5624849796295166, 'learning_rate': 0.0001908056166236305, 'epoch': 1.48}
>>> 2025-09-17 10:24:39,051 - INFO - >>> {'loss': 1.7447, 'grad_norm': 0.5334211587905884, 'learning_rate': 0.0001900968867902419, 'epoch': 1.5333333333333332}
>>> 2025-09-17 10:25:20,345 - INFO - >>> {'loss': 1.7934, 'grad_norm': 0.5782760381698608, 'learning_rate': 0.00018936326403234125, 'epoch': 1.5866666666666667}
>>> 2025-09-17 10:25:58,204 - INFO - >>> {'loss': 1.6848, 'grad_norm': 0.6356834769248962, 'learning_rate': 0.00018860495104301345, 'epoch': 1.6400000000000001}
>>> 2025-09-17 10:26:47,240 - INFO - >>> {'loss': 1.7879, 'grad_norm': 0.5556405186653137, 'learning_rate': 0.00018782215733702286, 'epoch': 1.6933333333333334}
>>> 2025-09-17 10:27:39,029 - INFO - >>> {'loss': 1.7745, 'grad_norm': 0.5264396071434021, 'learning_rate': 0.00018701509919292613, 'epoch': 1.7466666666666666}
>>> 2025-09-17 10:28:14,827 - INFO - >>> {'loss': 1.6544, 'grad_norm': 0.6911022663116455, 'learning_rate': 0.0001861839995933164, 'epoch': 1.8}
>>> 2025-09-17 10:29:04,957 - INFO - >>> {'loss': 1.6823, 'grad_norm': 0.5663737058639526, 'learning_rate': 0.00018532908816321558, 'epoch': 1.8533333333333335}
>>> 2025-09-17 10:29:49,149 - INFO - >>> {'loss': 1.6421, 'grad_norm': 0.5557414293289185, 'learning_rate': 0.0001844506011066308, 'epoch': 1.9066666666666667}
>>> 2025-09-17 10:30:37,374 - INFO - >>> {'loss': 1.6449, 'grad_norm': 0.6149543523788452, 'learning_rate': 0.00018354878114129367, 'epoch': 1.96}
>>> 2025-09-17 10:31:13,455 - INFO - >>> {'loss': 1.4837, 'grad_norm': 0.8109298348426819, 'learning_rate': 0.0001826238774315995, 'epoch': 2.0}
>>> 2025-09-17 10:31:55,152 - INFO - >>> {'loss': 1.5401, 'grad_norm': 0.7081228494644165, 'learning_rate': 0.00018167614551976567, 'epoch': 2.0533333333333332}
>>> 2025-09-17 10:32:37,466 - INFO - >>> {'loss': 1.4602, 'grad_norm': 0.814817488193512, 'learning_rate': 0.00018070584725522762, 'epoch': 2.1066666666666665}
>>> 2025-09-17 10:33:22,790 - INFO - >>> {'loss': 1.5019, 'grad_norm': 0.9078448414802551, 'learning_rate': 0.00017971325072229226, 'epoch': 2.16}
>>> 2025-09-17 10:34:06,056 - INFO - >>> {'loss': 1.5696, 'grad_norm': 0.7955490946769714, 'learning_rate': 0.0001786986301660689, 'epoch': 2.2133333333333334}
>>> 2025-09-17 10:34:51,956 - INFO - >>> {'loss': 1.44, 'grad_norm': 0.839860200881958, 'learning_rate': 0.00017766226591669785, 'epoch': 2.2666666666666666}
>>> 2025-09-17 10:35:26,490 - INFO - >>> {'loss': 1.257, 'grad_norm': 0.9547579884529114, 'learning_rate': 0.0001766044443118978, 'epoch': 2.32}
>>> 2025-09-17 10:36:13,665 - INFO - >>> {'loss': 1.4266, 'grad_norm': 0.8844656944274902, 'learning_rate': 0.0001755254576178535, 'epoch': 2.3733333333333335}
>>> 2025-09-17 10:36:59,759 - INFO - >>> {'loss': 1.3547, 'grad_norm': 0.8703917264938354, 'learning_rate': 0.00017442560394846516, 'epoch': 2.4266666666666667}
>>> 2025-09-17 10:37:40,687 - INFO - >>> {'loss': 1.1805, 'grad_norm': 1.1692850589752197, 'learning_rate': 0.00017330518718298264, 'epoch': 2.48}
>>> 2025-09-17 10:38:23,783 - INFO - >>> {'loss': 1.3582, 'grad_norm': 1.1951308250427246, 'learning_rate': 0.0001721645168820462, 'epoch': 2.533333333333333}
>>> 2025-09-17 10:39:03,826 - INFO - >>> {'loss': 1.2032, 'grad_norm': 1.0154832601547241, 'learning_rate': 0.00017100390820215804, 'epoch': 2.586666666666667}
>>> 2025-09-17 10:39:40,894 - INFO - >>> {'loss': 1.2767, 'grad_norm': 1.0688416957855225, 'learning_rate': 0.00016982368180860728, 'epoch': 2.64}
>>> 2025-09-17 10:40:31,848 - INFO - >>> {'loss': 1.2754, 'grad_norm': 1.0408165454864502, 'learning_rate': 0.0001686241637868734, 'epoch': 2.6933333333333334}
>>> 2025-09-17 10:41:19,541 - INFO - >>> {'loss': 1.2056, 'grad_norm': 1.277244210243225, 'learning_rate': 0.00016740568555253155, 'epoch': 2.7466666666666666}
>>> 2025-09-17 10:42:07,353 - INFO - >>> {'loss': 1.1759, 'grad_norm': 1.3633956909179688, 'learning_rate': 0.00016616858375968595, 'epoch': 2.8}
>>> 2025-09-17 10:42:58,347 - INFO - >>> {'loss': 1.1802, 'grad_norm': 1.1895692348480225, 'learning_rate': 0.0001649132002079552, 'epoch': 2.8533333333333335}
>>> 2025-09-17 10:43:44,916 - INFO - >>> {'loss': 1.1488, 'grad_norm': 1.0478274822235107, 'learning_rate': 0.00016363988174803638, 'epoch': 2.9066666666666667}
>>> 2025-09-17 10:44:30,130 - INFO - >>> {'loss': 1.0435, 'grad_norm': 1.195280909538269, 'learning_rate': 0.00016234898018587337, 'epoch': 2.96}
>>> 2025-09-17 10:45:01,705 - INFO - >>> {'loss': 0.9947, 'grad_norm': 1.5274609327316284, 'learning_rate': 0.00016104085218545633, 'epoch': 3.0}
>>> 2025-09-17 10:45:48,421 - INFO - >>> {'loss': 1.0219, 'grad_norm': 1.276258111000061, 'learning_rate': 0.00015971585917027862, 'epoch': 3.0533333333333332}
>>> 2025-09-17 10:46:32,229 - INFO - >>> {'loss': 0.9162, 'grad_norm': 1.701690435409546, 'learning_rate': 0.000158374367223479, 'epoch': 3.1066666666666665}
>>> 2025-09-17 10:47:04,397 - INFO - >>> {'loss': 0.8218, 'grad_norm': 1.5614817142486572, 'learning_rate': 0.0001570167469866962, 'epoch': 3.16}
>>> 2025-09-17 10:47:46,178 - INFO - >>> {'loss': 0.9743, 'grad_norm': 1.3698444366455078, 'learning_rate': 0.00015564337355766412, 'epoch': 3.2133333333333334}
>>> 2025-09-17 10:48:25,747 - INFO - >>> {'loss': 0.859, 'grad_norm': 1.5388296842575073, 'learning_rate': 0.00015425462638657595, 'epoch': 3.2666666666666666}
>>> 2025-09-17 10:49:06,030 - INFO - >>> {'loss': 0.8557, 'grad_norm': 1.6919947862625122, 'learning_rate': 0.00015285088917124556, 'epoch': 3.32}
>>> 2025-09-17 10:49:50,589 - INFO - >>> {'loss': 0.8399, 'grad_norm': 1.578452706336975, 'learning_rate': 0.00015143254975109538, 'epoch': 3.3733333333333335}
>>> 2025-09-17 10:50:32,529 - INFO - >>> {'loss': 0.7984, 'grad_norm': 1.5013188123703003, 'learning_rate': 0.00015000000000000001, 'epoch': 3.4266666666666667}
>>> 2025-09-17 10:51:19,743 - INFO - >>> {'loss': 0.79, 'grad_norm': 1.7907253503799438, 'learning_rate': 0.00014855363571801523, 'epoch': 3.48}
>>> 2025-09-17 10:52:11,714 - INFO - >>> {'loss': 0.7294, 'grad_norm': 1.8770430088043213, 'learning_rate': 0.00014709385652202203, 'epoch': 3.533333333333333}
>>> 2025-09-17 10:52:54,090 - INFO - >>> {'loss': 0.7235, 'grad_norm': 2.0396969318389893, 'learning_rate': 0.0001456210657353163, 'epoch': 3.586666666666667}
>>> 2025-09-17 10:53:38,259 - INFO - >>> {'loss': 0.6059, 'grad_norm': 1.7328546047210693, 'learning_rate': 0.0001441356702761744, 'epoch': 3.64}
>>> 2025-09-17 10:54:20,622 - INFO - >>> {'loss': 0.6671, 'grad_norm': 2.0063328742980957, 'learning_rate': 0.0001426380805454254, 'epoch': 3.6933333333333334}
>>> 2025-09-17 10:55:05,958 - INFO - >>> {'loss': 0.7625, 'grad_norm': 1.9242299795150757, 'learning_rate': 0.00014112871031306119, 'epoch': 3.7466666666666666}
>>> 2025-09-17 10:55:49,735 - INFO - >>> {'loss': 0.6952, 'grad_norm': 1.7831525802612305, 'learning_rate': 0.0001396079766039157, 'epoch': 3.8}
>>> 2025-09-17 10:56:37,496 - INFO - >>> {'loss': 0.6953, 'grad_norm': 1.9820996522903442, 'learning_rate': 0.00013807629958244498, 'epoch': 3.8533333333333335}
>>> 2025-09-17 10:57:20,065 - INFO - >>> {'loss': 0.6502, 'grad_norm': 1.9395594596862793, 'learning_rate': 0.00013653410243663952, 'epoch': 3.9066666666666667}
>>> 2025-09-17 10:58:02,099 - INFO - >>> {'loss': 0.4604, 'grad_norm': 1.8825703859329224, 'learning_rate': 0.0001349818112611015, 'epoch': 3.96}
>>> 2025-09-17 10:58:32,444 - INFO - >>> {'loss': 0.5921, 'grad_norm': 2.205077886581421, 'learning_rate': 0.00013341985493931877, 'epoch': 4.0}
>>> 2025-09-17 10:59:14,435 - INFO - >>> {'loss': 0.5392, 'grad_norm': 1.7002149820327759, 'learning_rate': 0.00013184866502516845, 'epoch': 4.053333333333334}
>>> 2025-09-17 10:59:55,457 - INFO - >>> {'loss': 0.3878, 'grad_norm': 1.718388557434082, 'learning_rate': 0.0001302686756236826, 'epoch': 4.1066666666666665}
>>> 2025-09-17 11:00:36,809 - INFO - >>> {'loss': 0.4339, 'grad_norm': 1.9686111211776733, 'learning_rate': 0.00012868032327110904, 'epoch': 4.16}
>>> 2025-09-17 11:01:17,533 - INFO - >>> {'loss': 0.3782, 'grad_norm': 2.5981929302215576, 'learning_rate': 0.00012708404681430053, 'epoch': 4.213333333333333}
>>> 2025-09-17 11:02:03,297 - INFO - >>> {'loss': 0.365, 'grad_norm': 1.7952687740325928, 'learning_rate': 0.0001254802872894655, 'epoch': 4.266666666666667}
>>> 2025-09-17 11:02:54,023 - INFO - >>> {'loss': 0.3756, 'grad_norm': 1.9472033977508545, 'learning_rate': 0.0001238694878003138, 'epoch': 4.32}
>>> 2025-09-17 11:03:36,583 - INFO - >>> {'loss': 0.4081, 'grad_norm': 1.831390380859375, 'learning_rate': 0.00012225209339563145, 'epoch': 4.373333333333333}
>>> 2025-09-17 11:04:23,161 - INFO - >>> {'loss': 0.4366, 'grad_norm': 1.7511770725250244, 'learning_rate': 0.00012062855094631778, 'epoch': 4.426666666666667}
>>> 2025-09-17 11:05:12,372 - INFO - >>> {'loss': 0.3703, 'grad_norm': 1.3885337114334106, 'learning_rate': 0.00011899930902191902, 'epoch': 4.48}
>>> 2025-09-17 11:05:49,942 - INFO - >>> {'loss': 0.3377, 'grad_norm': 1.6329188346862793, 'learning_rate': 0.00011736481776669306, 'epoch': 4.533333333333333}
>>> 2025-09-17 11:06:34,289 - INFO - >>> {'loss': 0.3406, 'grad_norm': 1.7927892208099365, 'learning_rate': 0.00011572552877523854, 'epoch': 4.586666666666667}
>>> 2025-09-17 11:07:10,503 - INFO - >>> {'loss': 0.2805, 'grad_norm': 2.0012664794921875, 'learning_rate': 0.00011408189496772368, 'epoch': 4.64}
>>> 2025-09-17 11:07:58,269 - INFO - >>> {'loss': 0.3409, 'grad_norm': 1.5316259860992432, 'learning_rate': 0.00011243437046474853, 'epoch': 4.693333333333333}
>>> 2025-09-17 11:08:40,387 - INFO - >>> {'loss': 0.2955, 'grad_norm': 1.6848211288452148, 'learning_rate': 0.00011078341046187589, 'epoch': 4.746666666666667}
>>> 2025-09-17 11:09:19,703 - INFO - >>> {'loss': 0.3215, 'grad_norm': 1.7851494550704956, 'learning_rate': 0.00010912947110386484, 'epoch': 4.8}
>>> 2025-09-17 11:10:06,443 - INFO - >>> {'loss': 0.3014, 'grad_norm': 1.888532280921936, 'learning_rate': 0.00010747300935864243, 'epoch': 4.8533333333333335}
>>> 2025-09-17 11:10:58,122 - INFO - >>> {'loss': 0.2873, 'grad_norm': 1.5083439350128174, 'learning_rate': 0.00010581448289104758, 'epoch': 4.906666666666666}
>>> 2025-09-17 11:11:40,868 - INFO - >>> {'loss': 0.3243, 'grad_norm': 1.5228060483932495, 'learning_rate': 0.00010415434993638269, 'epoch': 4.96}
>>> 2025-09-17 11:12:15,431 - INFO - >>> {'loss': 0.3022, 'grad_norm': 2.0861685276031494, 'learning_rate': 0.0001024930691738073, 'epoch': 5.0}
>>> 2025-09-17 11:12:52,247 - INFO - >>> {'loss': 0.2557, 'grad_norm': 1.5099549293518066, 'learning_rate': 0.00010083109959960973, 'epoch': 5.053333333333334}
>>> 2025-09-17 11:13:30,066 - INFO - >>> {'loss': 0.2125, 'grad_norm': 1.4717931747436523, 'learning_rate': 9.916890040039031e-05, 'epoch': 5.1066666666666665}
>>> 2025-09-17 11:14:15,653 - INFO - >>> {'loss': 0.2209, 'grad_norm': 1.5760353803634644, 'learning_rate': 9.750693082619273e-05, 'epoch': 5.16}
>>> 2025-09-17 11:15:06,175 - INFO - >>> {'loss': 0.2266, 'grad_norm': 1.8804419040679932, 'learning_rate': 9.584565006361734e-05, 'epoch': 5.213333333333333}
>>> 2025-09-17 11:15:41,665 - INFO - >>> {'loss': 0.2502, 'grad_norm': 1.3198264837265015, 'learning_rate': 9.418551710895243e-05, 'epoch': 5.266666666666667}
>>> 2025-09-17 11:16:28,014 - INFO - >>> {'loss': 0.262, 'grad_norm': 1.3656939268112183, 'learning_rate': 9.252699064135758e-05, 'epoch': 5.32}
>>> 2025-09-17 11:17:06,232 - INFO - >>> {'loss': 0.1851, 'grad_norm': 1.314020037651062, 'learning_rate': 9.087052889613518e-05, 'epoch': 5.373333333333333}
>>> 2025-09-17 11:17:57,139 - INFO - >>> {'loss': 0.1544, 'grad_norm': 1.3985553979873657, 'learning_rate': 8.921658953812415e-05, 'epoch': 5.426666666666667}
>>> 2025-09-17 11:18:47,802 - INFO - >>> {'loss': 0.142, 'grad_norm': 1.5681240558624268, 'learning_rate': 8.756562953525152e-05, 'epoch': 5.48}
>>> 2025-09-17 11:19:24,339 - INFO - >>> {'loss': 0.1412, 'grad_norm': 1.5476665496826172, 'learning_rate': 8.591810503227635e-05, 'epoch': 5.533333333333333}
>>> 2025-09-17 11:20:11,051 - INFO - >>> {'loss': 0.159, 'grad_norm': 1.4871487617492676, 'learning_rate': 8.427447122476148e-05, 'epoch': 5.586666666666667}
>>> 2025-09-17 11:20:58,130 - INFO - >>> {'loss': 0.1076, 'grad_norm': 1.1398860216140747, 'learning_rate': 8.263518223330697e-05, 'epoch': 5.64}
>>> 2025-09-17 11:21:36,566 - INFO - >>> {'loss': 0.1279, 'grad_norm': 1.3057957887649536, 'learning_rate': 8.100069097808103e-05, 'epoch': 5.693333333333333}
>>> 2025-09-17 11:22:17,936 - INFO - >>> {'loss': 0.1682, 'grad_norm': 1.3239617347717285, 'learning_rate': 7.937144905368226e-05, 'epoch': 5.746666666666667}
>>> 2025-09-17 11:23:00,521 - INFO - >>> {'loss': 0.1884, 'grad_norm': 1.1489163637161255, 'learning_rate': 7.774790660436858e-05, 'epoch': 5.8}
>>> 2025-09-17 11:23:47,242 - INFO - >>> {'loss': 0.1474, 'grad_norm': 1.3194983005523682, 'learning_rate': 7.613051219968623e-05, 'epoch': 5.8533333333333335}
>>> 2025-09-17 11:24:33,885 - INFO - >>> {'loss': 0.1305, 'grad_norm': 1.2841213941574097, 'learning_rate': 7.451971271053455e-05, 'epoch': 5.906666666666666}
>>> 2025-09-17 11:25:19,422 - INFO - >>> {'loss': 0.1659, 'grad_norm': 1.4063677787780762, 'learning_rate': 7.291595318569951e-05, 'epoch': 5.96}
>>> 2025-09-17 11:25:51,177 - INFO - >>> {'loss': 0.1333, 'grad_norm': 1.3986778259277344, 'learning_rate': 7.131967672889101e-05, 'epoch': 6.0}
>>> 2025-09-17 11:26:38,207 - INFO - >>> {'loss': 0.1556, 'grad_norm': 0.9613018035888672, 'learning_rate': 6.973132437631742e-05, 'epoch': 6.053333333333334}
>>> 2025-09-17 11:27:19,497 - INFO - >>> {'loss': 0.1075, 'grad_norm': 1.0725117921829224, 'learning_rate': 6.815133497483157e-05, 'epoch': 6.1066666666666665}
>>> 2025-09-17 11:28:03,162 - INFO - >>> {'loss': 0.1234, 'grad_norm': 1.1120108366012573, 'learning_rate': 6.658014506068126e-05, 'epoch': 6.16}
>>> 2025-09-17 11:28:46,663 - INFO - >>> {'loss': 0.0805, 'grad_norm': 0.9098801016807556, 'learning_rate': 6.501818873889855e-05, 'epoch': 6.213333333333333}
>>> 2025-09-17 11:29:33,265 - INFO - >>> {'loss': 0.1453, 'grad_norm': 1.366913914680481, 'learning_rate': 6.34658975633605e-05, 'epoch': 6.266666666666667}
>>> 2025-09-17 11:30:14,802 - INFO - >>> {'loss': 0.1468, 'grad_norm': 1.2048288583755493, 'learning_rate': 6.192370041755505e-05, 'epoch': 6.32}
>>> 2025-09-17 11:31:00,865 - INFO - >>> {'loss': 0.1371, 'grad_norm': 1.4769772291183472, 'learning_rate': 6.039202339608432e-05, 'epoch': 6.373333333333333}
>>> 2025-09-17 11:31:42,250 - INFO - >>> {'loss': 0.104, 'grad_norm': 0.7441111207008362, 'learning_rate': 5.887128968693887e-05, 'epoch': 6.426666666666667}
>>> 2025-09-17 11:32:24,491 - INFO - >>> {'loss': 0.1175, 'grad_norm': 1.2461236715316772, 'learning_rate': 5.736191945457463e-05, 'epoch': 6.48}
>>> 2025-09-17 11:33:02,686 - INFO - >>> {'loss': 0.1121, 'grad_norm': 1.0649610757827759, 'learning_rate': 5.58643297238256e-05, 'epoch': 6.533333333333333}
>>> 2025-09-17 11:33:50,337 - INFO - >>> {'loss': 0.0581, 'grad_norm': 1.013692855834961, 'learning_rate': 5.43789342646837e-05, 'epoch': 6.586666666666667}
>>> 2025-09-17 11:34:34,984 - INFO - >>> {'loss': 0.1418, 'grad_norm': 1.243205189704895, 'learning_rate': 5.290614347797802e-05, 'epoch': 6.64}
>>> 2025-09-17 11:35:21,877 - INFO - >>> {'loss': 0.0899, 'grad_norm': 0.9842283725738525, 'learning_rate': 5.1446364281984774e-05, 'epoch': 6.693333333333333}
>>> 2025-09-17 11:36:02,031 - INFO - >>> {'loss': 0.0804, 'grad_norm': 1.1888595819473267, 'learning_rate': 5.000000000000002e-05, 'epoch': 6.746666666666667}
>>> 2025-09-17 11:36:54,248 - INFO - >>> {'loss': 0.0746, 'grad_norm': 1.0334113836288452, 'learning_rate': 4.856745024890466e-05, 'epoch': 6.8}
>>> 2025-09-17 11:37:37,152 - INFO - >>> {'loss': 0.1051, 'grad_norm': 0.967803955078125, 'learning_rate': 4.7149110828754464e-05, 'epoch': 6.8533333333333335}
>>> 2025-09-17 11:38:17,901 - INFO - >>> {'loss': 0.0999, 'grad_norm': 1.3368995189666748, 'learning_rate': 4.574537361342407e-05, 'epoch': 6.906666666666666}
>>> 2025-09-17 11:39:02,203 - INFO - >>> {'loss': 0.1487, 'grad_norm': 0.9413387179374695, 'learning_rate': 4.435662644233594e-05, 'epoch': 6.96}
>>> 2025-09-17 11:39:34,205 - INFO - >>> {'loss': 0.0691, 'grad_norm': 0.9347810745239258, 'learning_rate': 4.298325301330383e-05, 'epoch': 7.0}
>>> 2025-09-17 11:40:16,802 - INFO - >>> {'loss': 0.0987, 'grad_norm': 0.8605627417564392, 'learning_rate': 4.1625632776521037e-05, 'epoch': 7.053333333333334}
>>> 2025-09-17 11:40:59,523 - INFO - >>> {'loss': 0.0868, 'grad_norm': 1.0561848878860474, 'learning_rate': 4.028414082972141e-05, 'epoch': 7.1066666666666665}
>>> 2025-09-17 11:41:42,804 - INFO - >>> {'loss': 0.1034, 'grad_norm': 0.738635241985321, 'learning_rate': 3.89591478145437e-05, 'epoch': 7.16}
>>> 2025-09-17 11:42:27,172 - INFO - >>> {'loss': 0.1214, 'grad_norm': 1.054399013519287, 'learning_rate': 3.7651019814126654e-05, 'epoch': 7.213333333333333}
>>> 2025-09-17 11:43:15,144 - INFO - >>> {'loss': 0.0672, 'grad_norm': 0.7190641760826111, 'learning_rate': 3.6360118251963645e-05, 'epoch': 7.266666666666667}
>>> 2025-09-17 11:43:54,023 - INFO - >>> {'loss': 0.097, 'grad_norm': 0.8600294589996338, 'learning_rate': 3.508679979204481e-05, 'epoch': 7.32}
>>> 2025-09-17 11:44:36,884 - INFO - >>> {'loss': 0.0785, 'grad_norm': 0.8070163726806641, 'learning_rate': 3.383141624031408e-05, 'epoch': 7.373333333333333}
>>> 2025-09-17 11:45:14,836 - INFO - >>> {'loss': 0.0757, 'grad_norm': 1.0710564851760864, 'learning_rate': 3.259431444746846e-05, 'epoch': 7.426666666666667}
>>> 2025-09-17 11:45:56,948 - INFO - >>> {'loss': 0.096, 'grad_norm': 0.8069015145301819, 'learning_rate': 3.137583621312665e-05, 'epoch': 7.48}
>>> 2025-09-17 11:46:41,085 - INFO - >>> {'loss': 0.1183, 'grad_norm': 0.9762640595436096, 'learning_rate': 3.0176318191392726e-05, 'epoch': 7.533333333333333}
>>> 2025-09-17 11:47:18,493 - INFO - >>> {'loss': 0.0595, 'grad_norm': 1.0468580722808838, 'learning_rate': 2.8996091797841973e-05, 'epoch': 7.586666666666667}
>>> 2025-09-17 11:48:03,521 - INFO - >>> {'loss': 0.076, 'grad_norm': 1.0808310508728027, 'learning_rate': 2.7835483117953788e-05, 'epoch': 7.64}
>>> 2025-09-17 11:48:47,863 - INFO - >>> {'loss': 0.0609, 'grad_norm': 0.6863098740577698, 'learning_rate': 2.669481281701739e-05, 'epoch': 7.693333333333333}
>>> 2025-09-17 11:49:30,042 - INFO - >>> {'loss': 0.0964, 'grad_norm': 0.8357698321342468, 'learning_rate': 2.5574396051534832e-05, 'epoch': 7.746666666666667}
>>> 2025-09-17 11:50:15,618 - INFO - >>> {'loss': 0.0963, 'grad_norm': 0.7884922623634338, 'learning_rate': 2.4474542382146537e-05, 'epoch': 7.8}
>>> 2025-09-17 11:50:57,451 - INFO - >>> {'loss': 0.0697, 'grad_norm': 0.7576599717140198, 'learning_rate': 2.339555568810221e-05, 'epoch': 7.8533333333333335}
>>> 2025-09-17 11:51:44,420 - INFO - >>> {'loss': 0.044, 'grad_norm': 0.6711922287940979, 'learning_rate': 2.2337734083302164e-05, 'epoch': 7.906666666666666}
>>> 2025-09-17 11:52:23,630 - INFO - >>> {'loss': 0.0705, 'grad_norm': 1.0622286796569824, 'learning_rate': 2.1301369833931117e-05, 'epoch': 7.96}
>>> 2025-09-17 11:52:59,709 - INFO - >>> {'loss': 0.034, 'grad_norm': 0.5544217228889465, 'learning_rate': 2.0286749277707782e-05, 'epoch': 8.0}
>>> 2025-09-17 11:53:39,326 - INFO - >>> {'loss': 0.1003, 'grad_norm': 0.7005309462547302, 'learning_rate': 1.929415274477239e-05, 'epoch': 8.053333333333333}
>>> 2025-09-17 11:54:22,563 - INFO - >>> {'loss': 0.0736, 'grad_norm': 0.7168897986412048, 'learning_rate': 1.832385448023435e-05, 'epoch': 8.106666666666667}
>>> 2025-09-17 11:55:04,112 - INFO - >>> {'loss': 0.0552, 'grad_norm': 0.7327539920806885, 'learning_rate': 1.7376122568400532e-05, 'epoch': 8.16}
>>> 2025-09-17 11:55:47,380 - INFO - >>> {'loss': 0.0777, 'grad_norm': 0.7670560479164124, 'learning_rate': 1.6451218858706374e-05, 'epoch': 8.213333333333333}
>>> 2025-09-17 11:56:33,583 - INFO - >>> {'loss': 0.0714, 'grad_norm': 0.7413648366928101, 'learning_rate': 1.5549398893369216e-05, 'epoch': 8.266666666666667}
>>> 2025-09-17 11:57:22,295 - INFO - >>> {'loss': 0.0594, 'grad_norm': 0.6017998456954956, 'learning_rate': 1.467091183678444e-05, 'epoch': 8.32}
>>> 2025-09-17 11:58:05,959 - INFO - >>> {'loss': 0.0718, 'grad_norm': 0.8459993600845337, 'learning_rate': 1.3816000406683604e-05, 'epoch': 8.373333333333333}
>>> 2025-09-17 11:58:48,859 - INFO - >>> {'loss': 0.047, 'grad_norm': 0.6928629279136658, 'learning_rate': 1.2984900807073919e-05, 'epoch': 8.426666666666666}
>>> 2025-09-17 11:59:34,443 - INFO - >>> {'loss': 0.1073, 'grad_norm': 0.7526877522468567, 'learning_rate': 1.2177842662977135e-05, 'epoch': 8.48}
>>> 2025-09-17 12:00:21,557 - INFO - >>> {'loss': 0.0879, 'grad_norm': 0.7436404228210449, 'learning_rate': 1.1395048956986575e-05, 'epoch': 8.533333333333333}
>>> 2025-09-17 12:01:05,386 - INFO - >>> {'loss': 0.0662, 'grad_norm': 0.7788972854614258, 'learning_rate': 1.0636735967658784e-05, 'epoch': 8.586666666666666}
>>> 2025-09-17 12:01:51,090 - INFO - >>> {'loss': 0.0441, 'grad_norm': 0.7180589437484741, 'learning_rate': 9.903113209758096e-06, 'epoch': 8.64}
>>> 2025-09-17 12:02:28,951 - INFO - >>> {'loss': 0.0526, 'grad_norm': 0.6581723093986511, 'learning_rate': 9.194383376369508e-06, 'epoch': 8.693333333333333}
>>> 2025-09-17 12:03:13,471 - INFO - >>> {'loss': 0.0534, 'grad_norm': 0.7801278829574585, 'learning_rate': 8.510742282896544e-06, 'epoch': 8.746666666666666}
>>> 2025-09-17 12:03:57,362 - INFO - >>> {'loss': 0.0901, 'grad_norm': 0.7551811933517456, 'learning_rate': 7.852378812959227e-06, 'epoch': 8.8}
>>> 2025-09-17 12:04:38,671 - INFO - >>> {'loss': 0.0489, 'grad_norm': 0.516453206539154, 'learning_rate': 7.219474866207465e-06, 'epoch': 8.853333333333333}
>>> 2025-09-17 12:05:19,371 - INFO - >>> {'loss': 0.0697, 'grad_norm': 0.6557802557945251, 'learning_rate': 6.612205308063646e-06, 'epoch': 8.906666666666666}
>>> 2025-09-17 12:06:07,049 - INFO - >>> {'loss': 0.0779, 'grad_norm': 0.6229143142700195, 'learning_rate': 6.030737921409169e-06, 'epoch': 8.96}
>>> 2025-09-17 12:06:45,851 - INFO - >>> {'loss': 0.0388, 'grad_norm': 0.556659996509552, 'learning_rate': 5.475233360227516e-06, 'epoch': 9.0}
>>> 2025-09-17 12:07:28,820 - INFO - >>> {'loss': 0.0575, 'grad_norm': 0.7026550769805908, 'learning_rate': 4.945845105217117e-06, 'epoch': 9.053333333333333}
>>> 2025-09-17 12:08:19,857 - INFO - >>> {'loss': 0.0434, 'grad_norm': 0.5183476209640503, 'learning_rate': 4.442719421385922e-06, 'epoch': 9.106666666666667}
>>> 2025-09-17 12:08:59,719 - INFO - >>> {'loss': 0.0539, 'grad_norm': 0.6208259463310242, 'learning_rate': 3.965995317640025e-06, 'epoch': 9.16}
>>> 2025-09-17 12:09:48,677 - INFO - >>> {'loss': 0.0748, 'grad_norm': 0.5136739611625671, 'learning_rate': 3.515804508376508e-06, 'epoch': 9.213333333333333}
>>> 2025-09-17 12:10:37,097 - INFO - >>> {'loss': 0.0564, 'grad_norm': 0.5238223671913147, 'learning_rate': 3.092271377092215e-06, 'epoch': 9.266666666666667}
>>> 2025-09-17 12:11:18,025 - INFO - >>> {'loss': 0.0833, 'grad_norm': 0.8060004115104675, 'learning_rate': 2.6955129420176196e-06, 'epoch': 9.32}
>>> 2025-09-17 12:12:01,033 - INFO - >>> {'loss': 0.059, 'grad_norm': 0.5576592087745667, 'learning_rate': 2.3256388237858807e-06, 'epoch': 9.373333333333333}
>>> 2025-09-17 12:12:52,748 - INFO - >>> {'loss': 0.0507, 'grad_norm': 0.6680997610092163, 'learning_rate': 1.9827512151456173e-06, 'epoch': 9.426666666666666}
>>> 2025-09-17 12:13:40,949 - INFO - >>> {'loss': 0.105, 'grad_norm': 0.6324778199195862, 'learning_rate': 1.66694485272606e-06, 'epoch': 9.48}
>>> 2025-09-17 12:14:25,026 - INFO - >>> {'loss': 0.0673, 'grad_norm': 0.6704407334327698, 'learning_rate': 1.378306990862177e-06, 'epoch': 9.533333333333333}
>>> 2025-09-17 12:15:05,187 - INFO - >>> {'loss': 0.0491, 'grad_norm': 0.6450456976890564, 'learning_rate': 1.1169173774871478e-06, 'epoch': 9.586666666666666}
>>> 2025-09-17 12:15:50,599 - INFO - >>> {'loss': 0.0641, 'grad_norm': 0.7230213284492493, 'learning_rate': 8.828482320987319e-07, 'epoch': 9.64}
>>> 2025-09-17 12:16:28,825 - INFO - >>> {'loss': 0.1086, 'grad_norm': 0.7709915041923523, 'learning_rate': 6.761642258056978e-07, 'epoch': 9.693333333333333}
>>> 2025-09-17 12:17:13,035 - INFO - >>> {'loss': 0.0699, 'grad_norm': 0.6537250876426697, 'learning_rate': 4.969224634598591e-07, 'epoch': 9.746666666666666}
>>> 2025-09-17 12:17:52,601 - INFO - >>> {'loss': 0.0545, 'grad_norm': 0.6365748643875122, 'learning_rate': 3.451724678784518e-07, 'epoch': 9.8}
>>> 2025-09-17 12:18:35,101 - INFO - >>> {'loss': 0.0498, 'grad_norm': 0.6883111596107483, 'learning_rate': 2.2095616616150115e-07, 'epoch': 9.853333333333333}
>>> 2025-09-17 12:19:15,024 - INFO - >>> {'loss': 0.0396, 'grad_norm': 0.6890707612037659, 'learning_rate': 1.2430787810776555e-07, 'epoch': 9.906666666666666}
>>> 2025-09-17 12:20:01,491 - INFO - >>> {'loss': 0.0655, 'grad_norm': 0.567171037197113, 'learning_rate': 5.5254306732444025e-08, 'epoch': 9.96}
>>> 2025-09-17 12:20:31,981 - INFO - >>> {'loss': 0.0663, 'grad_norm': 0.8530696034431458, 'learning_rate': 1.3814530889433296e-08, 'epoch': 10.0}
>>> 2025-09-17 12:20:32,604 - INFO - >>> {'train_runtime': 8209.0072, 'train_samples_per_second': 0.731, 'train_steps_per_second': 0.023, 'train_loss': 0.7144448551300325, 'epoch': 10.0}
>>> 2025-09-17 12:20:32,606 - INFO - 训练成功！
>>> 2025-09-17 12:20:32,607 - INFO - 模型存放位置：./output/qwen3-8b202509171003
>>> 2025-09-17 16:17:15,696 - INFO - ========__main__  202509171617========
>>> 2025-09-17 16:17:15,696 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 16:17:15,697 - INFO - 开始进行模型测试
>>> 2025-09-17 16:17:24,053 - INFO - 已选择模型文件夹: qwen3-8b202509171003
>>> 2025-09-17 16:17:24,056 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171003/checkpoint-190
>>> 2025-09-17 16:21:29,641 - INFO - ========__main__  202509171621========
>>> 2025-09-17 16:21:29,641 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 16:21:29,642 - INFO - 开始进行模型测试
>>> 2025-09-17 16:21:35,620 - INFO - 已选择模型文件夹: qwen3-8b202509171003
>>> 2025-09-17 16:21:35,622 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171003/checkpoint-190
>>> 2025-09-17 16:25:12,801 - INFO - ========__main__  202509171625========
>>> 2025-09-17 16:25:12,801 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 16:25:12,802 - INFO - 开始进行模型测试
>>> 2025-09-17 16:25:15,675 - INFO - 已选择模型文件夹: qwen3-8b202509171003
>>> 2025-09-17 16:25:15,678 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171003/checkpoint-190
>>> 2025-09-17 16:25:55,043 - INFO - ========__main__  202509171625========
>>> 2025-09-17 16:25:55,044 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 16:25:55,044 - INFO - 开始进行模型测试
>>> 2025-09-17 16:25:57,780 - INFO - 已选择模型文件夹: qwen3-8b202509170034
>>> 2025-09-17 16:25:57,783 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509170034/checkpoint-190
>>> 2025-09-17 16:26:32,165 - INFO - ========__main__  202509171626========
>>> 2025-09-17 16:26:32,166 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 16:26:32,166 - INFO - 开始进行模型测试
>>> 2025-09-17 16:26:48,903 - INFO - 已选择模型文件夹: qwen3-8b202509161756
>>> 2025-09-17 16:26:48,905 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509161756/checkpoint-380
>>> 2025-09-17 16:27:23,365 - INFO - ========__main__  202509171627========
>>> 2025-09-17 16:27:23,365 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 16:27:23,366 - INFO - 开始进行模型测试
>>> 2025-09-17 16:27:27,445 - INFO - 已选择模型文件夹: qwen3-8b202509171003
>>> 2025-09-17 16:27:27,447 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171003/checkpoint-190
>>> 2025-09-17 16:32:29,399 - INFO - ========__main__  202509171632========
>>> 2025-09-17 16:32:29,400 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 16:32:29,400 - INFO - 开始进行模型测试
>>> 2025-09-17 16:32:35,321 - INFO - 已选择模型文件夹: qwen3-8b202509171003
>>> 2025-09-17 16:32:35,324 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171003/checkpoint-190
>>> 2025-09-17 16:33:06,954 - INFO - ========__main__  202509171633========
>>> 2025-09-17 16:33:06,955 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 16:33:06,955 - INFO - 开始进行模型测试
>>> 2025-09-17 16:33:09,487 - INFO - 已选择模型文件夹: qwen3-8b202509171003
>>> 2025-09-17 16:33:09,489 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171003/checkpoint-190
>>> 2025-09-17 16:34:47,450 - INFO - ========__main__  202509171634========
>>> 2025-09-17 16:34:47,450 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 16:34:47,451 - INFO - 开始进行模型测试
>>> 2025-09-17 16:34:49,694 - INFO - 已选择模型文件夹: qwen3-8b202509171003
>>> 2025-09-17 16:34:49,696 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171003/checkpoint-190
>>> 2025-09-17 16:36:14,082 - INFO - ========__main__  202509171636========
>>> 2025-09-17 16:36:14,082 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 16:36:14,083 - INFO - 开始进行模型测试
>>> 2025-09-17 16:37:14,325 - INFO - 已选择模型文件夹: qwen3-8b202509171003
>>> 2025-09-17 16:37:14,329 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171003/checkpoint-190
>>> 2025-09-17 16:46:32,638 - INFO - 导入包完成
>>> 2025-09-17 16:46:32,639 - INFO - ========train Qwen2ForCausalLM  202509171646========
>>> 2025-09-17 16:46:32,639 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 16:46:32,640 - INFO - 开始进行训练
>>> 2025-09-17 16:46:32,646 - INFO - 基础配置文件读取完成
>>> 2025-09-17 16:46:32,653 - INFO - 训练配置读取完成
>>> 2025-09-17 16:46:32,654 - INFO - 数据集路径：dataset/own/Medical_Extension.json
>>> 2025-09-17 16:46:32,654 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-09-17 16:46:33,163 - INFO - tokenizer读取完成
>>> 2025-09-17 16:46:33,503 - INFO - model dtype:torch.bfloat16
>>> 2025-09-17 16:46:33,504 - INFO - 模型导入完成
>>> 2025-09-17 16:46:33,504 - INFO - 数据读取开始
>>> 2025-09-17 16:46:34,523 - INFO - 数据下载完成，训练集大小: 600
>>> 2025-09-17 16:46:38,724 - INFO - 数据映射完成
>>> 2025-09-17 16:46:38,724 - INFO - 打印训练参数如下
>>> 2025-09-17 16:46:38,725 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-17 16:46:38,725 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-17 16:46:38,725 - INFO -   load_in_4bit >>> True
>>> 2025-09-17 16:46:38,726 - INFO -   batch_size >>> 8
>>> 2025-09-17 16:46:38,726 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-17 16:46:38,726 - INFO -   warmup_steps >>> 1
>>> 2025-09-17 16:46:38,727 - INFO -   epoch >>> 10
>>> 2025-09-17 16:46:38,727 - INFO -   eval_steps >>> 5
>>> 2025-09-17 16:46:38,727 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-17 16:46:38,728 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-17 16:46:38,728 - INFO -   max_seq_length >>> 2048
>>> 2025-09-17 16:46:38,728 - INFO -   r >>> 8
>>> 2025-09-17 16:46:38,729 - INFO -   interface_mode >>> False
>>> 2025-09-17 16:46:38,729 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-17 16:46:38,729 - INFO -   lora_alpha >>> 16
>>> 2025-09-17 16:46:38,730 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-17 16:46:38,730 - INFO -   bias >>> none
>>> 2025-09-17 16:46:38,730 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-17 16:46:38,731 - INFO -   random_state >>> 3407
>>> 2025-09-17 16:46:38,731 - INFO -   use_rslora >>> True
>>> 2025-09-17 16:46:38,731 - INFO -   loftq_config >>> None
>>> 2025-09-17 16:46:46,697 - INFO - 开始训练！
>>> 2025-09-17 16:47:12,075 - INFO - 导入包完成
>>> 2025-09-17 16:47:12,076 - INFO - ========train Qwen2ForCausalLM  202509171647========
>>> 2025-09-17 16:47:12,076 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 16:47:12,077 - INFO - 开始进行训练
>>> 2025-09-17 16:47:12,083 - INFO - 基础配置文件读取完成
>>> 2025-09-17 16:47:12,090 - INFO - 训练配置读取完成
>>> 2025-09-17 16:47:12,091 - INFO - 数据集路径：dataset/own/Medical_Extension.json
>>> 2025-09-17 16:47:12,091 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-09-17 16:47:12,533 - INFO - tokenizer读取完成
>>> 2025-09-17 16:47:12,687 - INFO - model dtype:torch.bfloat16
>>> 2025-09-17 16:47:12,687 - INFO - 模型导入完成
>>> 2025-09-17 16:47:12,687 - INFO - 数据读取开始
>>> 2025-09-17 16:47:13,505 - INFO - 数据下载完成，训练集大小: 600
>>> 2025-09-17 16:47:17,575 - INFO - 数据映射完成
>>> 2025-09-17 16:47:17,576 - INFO - 打印训练参数如下
>>> 2025-09-17 16:47:17,576 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-17 16:47:17,577 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-17 16:47:17,577 - INFO -   load_in_4bit >>> True
>>> 2025-09-17 16:47:17,577 - INFO -   batch_size >>> 8
>>> 2025-09-17 16:47:17,578 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-17 16:47:17,578 - INFO -   warmup_steps >>> 1
>>> 2025-09-17 16:47:17,578 - INFO -   epoch >>> 8
>>> 2025-09-17 16:47:17,578 - INFO -   eval_steps >>> 5
>>> 2025-09-17 16:47:17,579 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-17 16:47:17,579 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-17 16:47:17,580 - INFO -   max_seq_length >>> 2048
>>> 2025-09-17 16:47:17,580 - INFO -   r >>> 8
>>> 2025-09-17 16:47:17,580 - INFO -   interface_mode >>> False
>>> 2025-09-17 16:47:17,580 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-17 16:47:17,581 - INFO -   lora_alpha >>> 16
>>> 2025-09-17 16:47:17,581 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-17 16:47:17,582 - INFO -   bias >>> none
>>> 2025-09-17 16:47:17,582 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-17 16:47:17,582 - INFO -   random_state >>> 3407
>>> 2025-09-17 16:47:17,582 - INFO -   use_rslora >>> True
>>> 2025-09-17 16:47:17,583 - INFO -   loftq_config >>> None
>>> 2025-09-17 16:47:23,640 - INFO - 开始训练！
>>> 2025-09-17 16:51:25,475 - INFO - 导入包完成
>>> 2025-09-17 16:51:25,475 - INFO - ========train Qwen2ForCausalLM  202509171651========
>>> 2025-09-17 16:51:25,476 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 16:51:25,476 - INFO - 开始进行训练
>>> 2025-09-17 16:51:25,482 - INFO - 基础配置文件读取完成
>>> 2025-09-17 16:51:25,490 - INFO - 训练配置读取完成
>>> 2025-09-17 16:51:25,490 - INFO - 数据集路径：dataset/own/Medical_Extension.json
>>> 2025-09-17 16:51:25,491 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-09-17 16:51:25,931 - INFO - tokenizer读取完成
>>> 2025-09-17 16:51:26,086 - INFO - model dtype:torch.bfloat16
>>> 2025-09-17 16:51:26,087 - INFO - 模型导入完成
>>> 2025-09-17 16:51:26,087 - INFO - 数据读取开始
>>> 2025-09-17 16:51:26,864 - INFO - 数据下载完成，训练集大小: 600
>>> 2025-09-17 16:51:31,097 - INFO - 数据映射完成
>>> 2025-09-17 16:51:31,098 - INFO - 打印训练参数如下
>>> 2025-09-17 16:51:31,098 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-17 16:51:31,099 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-17 16:51:31,099 - INFO -   load_in_4bit >>> True
>>> 2025-09-17 16:51:31,099 - INFO -   batch_size >>> 8
>>> 2025-09-17 16:51:31,100 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-17 16:51:31,100 - INFO -   warmup_steps >>> 1
>>> 2025-09-17 16:51:31,100 - INFO -   epoch >>> 8
>>> 2025-09-17 16:51:31,101 - INFO -   eval_steps >>> 5
>>> 2025-09-17 16:51:31,101 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-17 16:51:31,101 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-17 16:51:31,102 - INFO -   max_seq_length >>> 2048
>>> 2025-09-17 16:51:31,102 - INFO -   r >>> 8
>>> 2025-09-17 16:51:31,102 - INFO -   interface_mode >>> False
>>> 2025-09-17 16:51:31,103 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-17 16:51:31,103 - INFO -   lora_alpha >>> 16
>>> 2025-09-17 16:51:31,103 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-17 16:51:31,104 - INFO -   bias >>> none
>>> 2025-09-17 16:51:31,104 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-17 16:51:31,104 - INFO -   random_state >>> 3407
>>> 2025-09-17 16:51:31,105 - INFO -   use_rslora >>> True
>>> 2025-09-17 16:51:31,105 - INFO -   loftq_config >>> None
>>> 2025-09-17 16:51:37,253 - INFO - 开始训练！
>>> 2025-09-17 16:52:25,288 - INFO - >>> {'loss': 3.1945, 'grad_norm': 1.283014178276062, 'learning_rate': 0.0, 'epoch': 0.05333333333333334}
>>> 2025-09-17 16:53:13,355 - INFO - >>> {'loss': 3.4607, 'grad_norm': 1.442548394203186, 'learning_rate': 0.0002, 'epoch': 0.10666666666666667}
>>> 2025-09-17 16:54:04,121 - INFO - >>> {'loss': 3.2565, 'grad_norm': 1.299869179725647, 'learning_rate': 0.0001999783578606323, 'epoch': 0.16}
>>> 2025-09-17 16:54:44,865 - INFO - >>> {'loss': 3.1959, 'grad_norm': 1.3669037818908691, 'learning_rate': 0.0001999134408101731, 'epoch': 0.21333333333333335}
>>> 2025-09-17 16:55:38,403 - INFO - >>> {'loss': 2.9088, 'grad_norm': 1.0503047704696655, 'learning_rate': 0.00019980527694749952, 'epoch': 0.26666666666666666}
>>> 2025-09-17 16:56:31,426 - INFO - >>> {'loss': 2.7727, 'grad_norm': 0.918912410736084, 'learning_rate': 0.0001996539130905593, 'epoch': 0.32}
>>> 2025-09-17 16:57:16,700 - INFO - >>> {'loss': 2.7213, 'grad_norm': 0.8242613673210144, 'learning_rate': 0.00019945941475610623, 'epoch': 0.37333333333333335}
>>> 2025-09-17 16:57:56,956 - INFO - >>> {'loss': 2.5784, 'grad_norm': 0.6963813304901123, 'learning_rate': 0.0001992218661313415, 'epoch': 0.4266666666666667}
>>> 2025-09-17 16:58:46,581 - INFO - >>> {'loss': 2.394, 'grad_norm': 0.47033295035362244, 'learning_rate': 0.00019894137003747403, 'epoch': 0.48}
>>> 2025-09-17 16:59:29,690 - INFO - >>> {'loss': 2.4055, 'grad_norm': 0.45367753505706787, 'learning_rate': 0.00019861804788521493, 'epoch': 0.5333333333333333}
>>> 2025-09-17 17:00:19,461 - INFO - >>> {'loss': 2.438, 'grad_norm': 0.6431912779808044, 'learning_rate': 0.00019825203962222572, 'epoch': 0.5866666666666667}
>>> 2025-09-17 17:01:08,584 - INFO - >>> {'loss': 2.3522, 'grad_norm': 0.6107193827629089, 'learning_rate': 0.00019784350367254322, 'epoch': 0.64}
>>> 2025-09-17 17:01:57,054 - INFO - >>> {'loss': 2.2944, 'grad_norm': 0.6656967997550964, 'learning_rate': 0.0001973926168680066, 'epoch': 0.6933333333333334}
>>> 2025-09-17 17:02:37,231 - INFO - >>> {'loss': 2.2554, 'grad_norm': 0.6038021445274353, 'learning_rate': 0.0001968995743717171, 'epoch': 0.7466666666666667}
>>> 2025-09-17 17:03:21,433 - INFO - >>> {'loss': 2.2316, 'grad_norm': 0.5452008247375488, 'learning_rate': 0.00019636458959356316, 'epoch': 0.8}
>>> 2025-09-17 17:04:02,340 - INFO - >>> {'loss': 2.1643, 'grad_norm': 0.44119876623153687, 'learning_rate': 0.00019578789409784727, 'epoch': 0.8533333333333334}
>>> 2025-09-17 17:04:50,436 - INFO - >>> {'loss': 2.1898, 'grad_norm': 0.4169826805591583, 'learning_rate': 0.00019516973750305532, 'epoch': 0.9066666666666666}
>>> 2025-09-17 17:05:39,112 - INFO - >>> {'loss': 2.1714, 'grad_norm': 0.5045390725135803, 'learning_rate': 0.00019451038737381077, 'epoch': 0.96}
>>> 2025-09-17 17:06:18,129 - INFO - >>> {'loss': 2.1306, 'grad_norm': 0.4363549053668976, 'learning_rate': 0.00019381012910506146, 'epoch': 1.0}
>>> 2025-09-17 17:07:07,352 - INFO - >>> {'loss': 2.0291, 'grad_norm': 0.47379523515701294, 'learning_rate': 0.00019306926579854821, 'epoch': 1.0533333333333332}
>>> 2025-09-17 17:07:44,905 - INFO - >>> {'loss': 2.0105, 'grad_norm': 0.47186213731765747, 'learning_rate': 0.0001922881181316097, 'epoch': 1.1066666666666667}
>>> 2025-09-17 17:08:29,640 - INFO - >>> {'loss': 2.0215, 'grad_norm': 0.3967180550098419, 'learning_rate': 0.0001914670242183795, 'epoch': 1.16}
>>> 2025-09-17 17:09:25,407 - INFO - >>> {'loss': 1.9948, 'grad_norm': 0.3613409399986267, 'learning_rate': 0.0001906063394634356, 'epoch': 1.2133333333333334}
>>> 2025-09-17 17:10:07,879 - INFO - >>> {'loss': 1.8998, 'grad_norm': 0.3818657398223877, 'learning_rate': 0.00018970643640796642, 'epoch': 1.2666666666666666}
>>> 2025-09-17 17:10:54,252 - INFO - >>> {'loss': 1.9652, 'grad_norm': 0.44700437784194946, 'learning_rate': 0.00018876770456851877, 'epoch': 1.32}
>>> 2025-09-17 17:11:44,823 - INFO - >>> {'loss': 1.985, 'grad_norm': 0.42107954621315, 'learning_rate': 0.00018779055026839868, 'epoch': 1.3733333333333333}
>>> 2025-09-17 17:12:31,847 - INFO - >>> {'loss': 1.8625, 'grad_norm': 0.43830981850624084, 'learning_rate': 0.00018677539646179707, 'epoch': 1.4266666666666667}
>>> 2025-09-17 17:13:25,632 - INFO - >>> {'loss': 1.962, 'grad_norm': 0.41394567489624023, 'learning_rate': 0.00018572268255071718, 'epoch': 1.48}
>>> 2025-09-17 17:14:14,091 - INFO - >>> {'loss': 1.8417, 'grad_norm': 0.40344950556755066, 'learning_rate': 0.00018463286419478255, 'epoch': 1.5333333333333332}
>>> 2025-09-17 17:14:58,719 - INFO - >>> {'loss': 1.8986, 'grad_norm': 0.40740737318992615, 'learning_rate': 0.00018350641311400812, 'epoch': 1.5866666666666667}
>>> 2025-09-17 17:15:40,096 - INFO - >>> {'loss': 1.8015, 'grad_norm': 0.47357961535453796, 'learning_rate': 0.00018234381688461942, 'epoch': 1.6400000000000001}
>>> 2025-09-17 17:16:32,951 - INFO - >>> {'loss': 1.8943, 'grad_norm': 0.42457419633865356, 'learning_rate': 0.00018114557872800905, 'epoch': 1.6933333333333334}
>>> 2025-09-17 17:17:28,664 - INFO - >>> {'loss': 1.8841, 'grad_norm': 0.42078956961631775, 'learning_rate': 0.0001799122172929206, 'epoch': 1.7466666666666666}
>>> 2025-09-17 17:18:07,131 - INFO - >>> {'loss': 1.7429, 'grad_norm': 0.5239028334617615, 'learning_rate': 0.0001786442664309554, 'epoch': 1.8}
>>> 2025-09-17 17:19:00,540 - INFO - >>> {'loss': 1.8029, 'grad_norm': 0.42019984126091003, 'learning_rate': 0.0001773422749654988, 'epoch': 1.8533333333333335}
>>> 2025-09-17 17:19:48,254 - INFO - >>> {'loss': 1.7445, 'grad_norm': 0.3994405269622803, 'learning_rate': 0.00017600680645416583, 'epoch': 1.9066666666666667}
>>> 2025-09-17 17:20:40,373 - INFO - >>> {'loss': 1.7517, 'grad_norm': 0.41819509863853455, 'learning_rate': 0.00017463843894486937, 'epoch': 1.96}
>>> 2025-09-17 17:21:19,407 - INFO - >>> {'loss': 1.6157, 'grad_norm': 0.5470454096794128, 'learning_rate': 0.00017323776472561627, 'epoch': 2.0}
>>> 2025-09-17 17:22:04,332 - INFO - >>> {'loss': 1.6667, 'grad_norm': 0.4670405089855194, 'learning_rate': 0.0001718053900681397, 'epoch': 2.0533333333333332}
>>> 2025-09-17 17:22:50,231 - INFO - >>> {'loss': 1.5956, 'grad_norm': 0.5113934278488159, 'learning_rate': 0.00017034193496547902, 'epoch': 2.1066666666666665}
>>> 2025-09-17 17:23:39,069 - INFO - >>> {'loss': 1.6405, 'grad_norm': 0.4546184837818146, 'learning_rate': 0.00016884803286362, 'epoch': 2.16}
>>> 2025-09-17 17:24:25,919 - INFO - >>> {'loss': 1.7291, 'grad_norm': 0.5264373421669006, 'learning_rate': 0.00016732433038731242, 'epoch': 2.2133333333333334}
>>> 2025-09-17 17:25:15,294 - INFO - >>> {'loss': 1.5748, 'grad_norm': 0.5545191764831543, 'learning_rate': 0.00016577148706018328, 'epoch': 2.2666666666666666}
>>> 2025-09-17 17:25:52,560 - INFO - >>> {'loss': 1.3801, 'grad_norm': 0.5634082555770874, 'learning_rate': 0.00016419017501926656, 'epoch': 2.32}
>>> 2025-09-17 17:26:43,467 - INFO - >>> {'loss': 1.5257, 'grad_norm': 0.5700486898422241, 'learning_rate': 0.00016258107872407375, 'epoch': 2.3733333333333335}
>>> 2025-09-17 17:27:33,176 - INFO - >>> {'loss': 1.4866, 'grad_norm': 0.546782374382019, 'learning_rate': 0.00016094489466033043, 'epoch': 2.4266666666666667}
>>> 2025-09-17 17:28:16,770 - INFO - >>> {'loss': 1.3345, 'grad_norm': 0.5566708445549011, 'learning_rate': 0.0001592823310385073, 'epoch': 2.48}
>>> 2025-09-17 17:29:03,331 - INFO - >>> {'loss': 1.5096, 'grad_norm': 0.6856558322906494, 'learning_rate': 0.00015759410748727662, 'epoch': 2.533333333333333}
>>> 2025-09-17 17:29:46,763 - INFO - >>> {'loss': 1.355, 'grad_norm': 0.6522203087806702, 'learning_rate': 0.00015588095474202595, 'epoch': 2.586666666666667}
>>> 2025-09-17 17:30:26,494 - INFO - >>> {'loss': 1.363, 'grad_norm': 0.7216989398002625, 'learning_rate': 0.00015414361432856475, 'epoch': 2.64}
>>> 2025-09-17 17:31:21,246 - INFO - >>> {'loss': 1.4261, 'grad_norm': 0.7048042416572571, 'learning_rate': 0.00015238283824216015, 'epoch': 2.6933333333333334}
>>> 2025-09-17 17:32:12,377 - INFO - >>> {'loss': 1.3257, 'grad_norm': 0.7704112529754639, 'learning_rate': 0.00015059938862204127, 'epoch': 2.7466666666666666}
>>> 2025-09-17 17:33:03,999 - INFO - >>> {'loss': 1.3276, 'grad_norm': 0.896447479724884, 'learning_rate': 0.00014879403742151283, 'epoch': 2.8}
>>> 2025-09-17 17:33:58,891 - INFO - >>> {'loss': 1.305, 'grad_norm': 0.7701235413551331, 'learning_rate': 0.0001469675660738206, 'epoch': 2.8533333333333335}
>>> 2025-09-17 17:34:49,154 - INFO - >>> {'loss': 1.3116, 'grad_norm': 0.7891995906829834, 'learning_rate': 0.00014512076515391375, 'epoch': 2.9066666666666667}
>>> 2025-09-17 17:35:37,966 - INFO - >>> {'loss': 1.2212, 'grad_norm': 0.8150668144226074, 'learning_rate': 0.0001432544340362501, 'epoch': 2.96}
>>> 2025-09-17 17:36:12,164 - INFO - >>> {'loss': 1.1545, 'grad_norm': 0.9492076635360718, 'learning_rate': 0.00014136938054879283, 'epoch': 3.0}
>>> 2025-09-17 17:37:02,633 - INFO - >>> {'loss': 1.2003, 'grad_norm': 0.833314061164856, 'learning_rate': 0.00013946642062334766, 'epoch': 3.0533333333333332}
>>> 2025-09-17 17:37:50,091 - INFO - >>> {'loss': 1.0751, 'grad_norm': 0.8466523885726929, 'learning_rate': 0.000137546377942393, 'epoch': 3.1066666666666665}
>>> 2025-09-17 17:38:24,915 - INFO - >>> {'loss': 0.9708, 'grad_norm': 0.9811726808547974, 'learning_rate': 0.00013561008358255468, 'epoch': 3.16}
>>> 2025-09-17 17:39:10,034 - INFO - >>> {'loss': 1.1368, 'grad_norm': 1.0354968309402466, 'learning_rate': 0.00013365837565488064, 'epoch': 3.2133333333333334}
>>> 2025-09-17 17:39:52,878 - INFO - >>> {'loss': 1.0031, 'grad_norm': 1.2537909746170044, 'learning_rate': 0.0001316920989420703, 'epoch': 3.2666666666666666}
>>> 2025-09-17 17:40:36,251 - INFO - >>> {'loss': 0.9898, 'grad_norm': 1.1754266023635864, 'learning_rate': 0.00012971210453281674, 'epoch': 3.32}
>>> 2025-09-17 17:41:24,372 - INFO - >>> {'loss': 0.9928, 'grad_norm': 1.0718703269958496, 'learning_rate': 0.00012771924945341906, 'epoch': 3.3733333333333335}
>>> 2025-09-17 17:42:09,753 - INFO - >>> {'loss': 0.975, 'grad_norm': 1.117959976196289, 'learning_rate': 0.0001257143962968246, 'epoch': 3.4266666666666667}
>>> 2025-09-17 17:43:00,360 - INFO - >>> {'loss': 0.8906, 'grad_norm': 1.3088898658752441, 'learning_rate': 0.00012369841284926188, 'epoch': 3.48}
>>> 2025-09-17 17:43:56,177 - INFO - >>> {'loss': 0.8415, 'grad_norm': 1.3038935661315918, 'learning_rate': 0.00012167217171462566, 'epoch': 3.533333333333333}
>>> 2025-09-17 17:44:41,860 - INFO - >>> {'loss': 0.8347, 'grad_norm': 1.269192099571228, 'learning_rate': 0.00011963654993677645, 'epoch': 3.586666666666667}
>>> 2025-09-17 17:45:29,469 - INFO - >>> {'loss': 0.7423, 'grad_norm': 1.433568000793457, 'learning_rate': 0.00011759242861991855, 'epoch': 3.64}
>>> 2025-09-17 17:46:15,106 - INFO - >>> {'loss': 0.8183, 'grad_norm': 1.7269617319107056, 'learning_rate': 0.00011554069254722051, 'epoch': 3.6933333333333334}
>>> 2025-09-17 17:47:03,885 - INFO - >>> {'loss': 0.8911, 'grad_norm': 1.4196851253509521, 'learning_rate': 0.00011348222979784289, 'epoch': 3.7466666666666666}
>>> 2025-09-17 17:47:50,814 - INFO - >>> {'loss': 0.8293, 'grad_norm': 1.2500580549240112, 'learning_rate': 0.00011141793136253986, 'epoch': 3.8}
>>> 2025-09-17 17:48:42,384 - INFO - >>> {'loss': 0.835, 'grad_norm': 1.3503977060317993, 'learning_rate': 0.000109348690758, 'epoch': 3.8533333333333335}
>>> 2025-09-17 17:49:28,492 - INFO - >>> {'loss': 0.7952, 'grad_norm': 1.4225103855133057, 'learning_rate': 0.0001072754036400944, 'epoch': 3.9066666666666667}
>>> 2025-09-17 17:50:14,288 - INFO - >>> {'loss': 0.5664, 'grad_norm': 1.4003279209136963, 'learning_rate': 0.00010519896741619803, 'epoch': 3.96}
>>> 2025-09-17 17:50:47,041 - INFO - >>> {'loss': 0.7094, 'grad_norm': 1.6279823780059814, 'learning_rate': 0.00010312028085675391, 'epoch': 4.0}
>>> 2025-09-17 17:51:32,607 - INFO - >>> {'loss': 0.6772, 'grad_norm': 1.3617029190063477, 'learning_rate': 0.00010104024370624644, 'epoch': 4.053333333333334}
>>> 2025-09-17 17:52:16,976 - INFO - >>> {'loss': 0.5002, 'grad_norm': 1.4563500881195068, 'learning_rate': 9.895975629375359e-05, 'epoch': 4.1066666666666665}
>>> 2025-09-17 17:53:01,805 - INFO - >>> {'loss': 0.5852, 'grad_norm': 1.4032976627349854, 'learning_rate': 9.687971914324607e-05, 'epoch': 4.16}
>>> 2025-09-17 17:53:45,517 - INFO - >>> {'loss': 0.4796, 'grad_norm': 1.5032106637954712, 'learning_rate': 9.480103258380198e-05, 'epoch': 4.213333333333333}
>>> 2025-09-17 17:54:34,497 - INFO - >>> {'loss': 0.5056, 'grad_norm': 1.5204825401306152, 'learning_rate': 9.272459635990562e-05, 'epoch': 4.266666666666667}
>>> 2025-09-17 17:55:28,701 - INFO - >>> {'loss': 0.4935, 'grad_norm': 1.4979016780853271, 'learning_rate': 9.065130924199998e-05, 'epoch': 4.32}
>>> 2025-09-17 17:56:14,712 - INFO - >>> {'loss': 0.4934, 'grad_norm': 1.5837745666503906, 'learning_rate': 8.858206863746018e-05, 'epoch': 4.373333333333333}
>>> 2025-09-17 17:57:04,746 - INFO - >>> {'loss': 0.5529, 'grad_norm': 1.5370526313781738, 'learning_rate': 8.651777020215712e-05, 'epoch': 4.426666666666667}
>>> 2025-09-17 17:57:57,689 - INFO - >>> {'loss': 0.4652, 'grad_norm': 1.1388044357299805, 'learning_rate': 8.445930745277953e-05, 'epoch': 4.48}
>>> 2025-09-17 17:58:37,752 - INFO - >>> {'loss': 0.4838, 'grad_norm': 1.4385279417037964, 'learning_rate': 8.240757138008149e-05, 'epoch': 4.533333333333333}
>>> 2025-09-17 17:59:25,536 - INFO - >>> {'loss': 0.4333, 'grad_norm': 1.337553858757019, 'learning_rate': 8.036345006322359e-05, 'epoch': 4.586666666666667}
>>> 2025-09-17 18:00:04,768 - INFO - >>> {'loss': 0.3865, 'grad_norm': 1.6215635538101196, 'learning_rate': 7.832782828537437e-05, 'epoch': 4.64}
>>> 2025-09-17 18:00:56,345 - INFO - >>> {'loss': 0.4528, 'grad_norm': 1.4120862483978271, 'learning_rate': 7.630158715073813e-05, 'epoch': 4.693333333333333}
>>> 2025-09-17 18:01:42,006 - INFO - >>> {'loss': 0.395, 'grad_norm': 1.4054802656173706, 'learning_rate': 7.428560370317542e-05, 'epoch': 4.746666666666667}
>>> 2025-09-17 18:02:24,563 - INFO - >>> {'loss': 0.4197, 'grad_norm': 1.2978028059005737, 'learning_rate': 7.228075054658096e-05, 'epoch': 4.8}
>>> 2025-09-17 18:03:14,985 - INFO - >>> {'loss': 0.4245, 'grad_norm': 1.5441111326217651, 'learning_rate': 7.028789546718326e-05, 'epoch': 4.8533333333333335}
>>> 2025-09-17 18:04:10,826 - INFO - >>> {'loss': 0.3807, 'grad_norm': 1.2820929288864136, 'learning_rate': 6.830790105792973e-05, 'epoch': 4.906666666666666}
>>> 2025-09-17 18:04:56,873 - INFO - >>> {'loss': 0.4054, 'grad_norm': 1.2194290161132812, 'learning_rate': 6.63416243451194e-05, 'epoch': 4.96}
>>> 2025-09-17 18:05:34,185 - INFO - >>> {'loss': 0.3925, 'grad_norm': 1.7242063283920288, 'learning_rate': 6.43899164174453e-05, 'epoch': 5.0}
>>> 2025-09-17 18:06:13,927 - INFO - >>> {'loss': 0.3502, 'grad_norm': 1.1818784475326538, 'learning_rate': 6.245362205760704e-05, 'epoch': 5.053333333333334}
>>> 2025-09-17 18:06:54,985 - INFO - >>> {'loss': 0.3084, 'grad_norm': 1.1893846988677979, 'learning_rate': 6.053357937665237e-05, 'epoch': 5.1066666666666665}
>>> 2025-09-17 18:07:44,105 - INFO - >>> {'loss': 0.3313, 'grad_norm': 1.2068290710449219, 'learning_rate': 5.863061945120719e-05, 'epoch': 5.16}
>>> 2025-09-17 18:08:38,491 - INFO - >>> {'loss': 0.3076, 'grad_norm': 1.0663522481918335, 'learning_rate': 5.6745565963749925e-05, 'epoch': 5.213333333333333}
>>> 2025-09-17 18:09:16,904 - INFO - >>> {'loss': 0.328, 'grad_norm': 1.2990559339523315, 'learning_rate': 5.487923484608629e-05, 'epoch': 5.266666666666667}
>>> 2025-09-17 18:10:06,914 - INFO - >>> {'loss': 0.3965, 'grad_norm': 1.2765499353408813, 'learning_rate': 5.3032433926179395e-05, 'epoch': 5.32}
>>> 2025-09-17 18:10:48,397 - INFO - >>> {'loss': 0.2827, 'grad_norm': 1.2834081649780273, 'learning_rate': 5.1205962578487155e-05, 'epoch': 5.373333333333333}
>>> 2025-09-17 18:11:43,159 - INFO - >>> {'loss': 0.2516, 'grad_norm': 1.2807739973068237, 'learning_rate': 4.940061137795876e-05, 'epoch': 5.426666666666667}
>>> 2025-09-17 18:12:37,859 - INFO - >>> {'loss': 0.249, 'grad_norm': 1.1554280519485474, 'learning_rate': 4.761716175783989e-05, 'epoch': 5.48}
>>> 2025-09-17 18:13:17,356 - INFO - >>> {'loss': 0.1977, 'grad_norm': 1.2676373720169067, 'learning_rate': 4.585638567143529e-05, 'epoch': 5.533333333333333}
>>> 2025-09-17 18:14:08,064 - INFO - >>> {'loss': 0.2456, 'grad_norm': 1.2514816522598267, 'learning_rate': 4.411904525797408e-05, 'epoch': 5.586666666666667}
>>> 2025-09-17 18:14:59,292 - INFO - >>> {'loss': 0.1919, 'grad_norm': 1.0635100603103638, 'learning_rate': 4.240589251272342e-05, 'epoch': 5.64}
>>> 2025-09-17 18:15:40,885 - INFO - >>> {'loss': 0.1903, 'grad_norm': 1.123306393623352, 'learning_rate': 4.071766896149273e-05, 'epoch': 5.693333333333333}
>>> 2025-09-17 18:16:25,580 - INFO - >>> {'loss': 0.2565, 'grad_norm': 1.2994009256362915, 'learning_rate': 3.9055105339669595e-05, 'epoch': 5.746666666666667}
>>> 2025-09-17 18:17:11,660 - INFO - >>> {'loss': 0.2753, 'grad_norm': 1.0990592241287231, 'learning_rate': 3.741892127592625e-05, 'epoch': 5.8}
>>> 2025-09-17 18:18:02,204 - INFO - >>> {'loss': 0.2452, 'grad_norm': 1.0564205646514893, 'learning_rate': 3.580982498073344e-05, 'epoch': 5.8533333333333335}
>>> 2025-09-17 18:18:52,600 - INFO - >>> {'loss': 0.1845, 'grad_norm': 1.0358339548110962, 'learning_rate': 3.422851293981676e-05, 'epoch': 5.906666666666666}
>>> 2025-09-17 18:19:41,471 - INFO - >>> {'loss': 0.217, 'grad_norm': 1.1892437934875488, 'learning_rate': 3.2675669612687565e-05, 'epoch': 5.96}
>>> 2025-09-17 18:20:15,809 - INFO - >>> {'loss': 0.2177, 'grad_norm': 1.5109195709228516, 'learning_rate': 3.115196713638e-05, 'epoch': 6.0}
>>> 2025-09-17 18:21:06,428 - INFO - >>> {'loss': 0.2148, 'grad_norm': 0.8272771835327148, 'learning_rate': 2.9658065034520978e-05, 'epoch': 6.053333333333334}
>>> 2025-09-17 18:21:50,826 - INFO - >>> {'loss': 0.1669, 'grad_norm': 0.9602981209754944, 'learning_rate': 2.8194609931860316e-05, 'epoch': 6.1066666666666665}
>>> 2025-09-17 18:22:37,989 - INFO - >>> {'loss': 0.2069, 'grad_norm': 1.0653148889541626, 'learning_rate': 2.6762235274383772e-05, 'epoch': 6.16}
>>> 2025-09-17 18:23:24,987 - INFO - >>> {'loss': 0.1792, 'grad_norm': 0.8523702025413513, 'learning_rate': 2.536156105513062e-05, 'epoch': 6.213333333333333}
>>> 2025-09-17 18:24:15,348 - INFO - >>> {'loss': 0.2308, 'grad_norm': 1.0326659679412842, 'learning_rate': 2.399319354583418e-05, 'epoch': 6.266666666666667}
>>> 2025-09-17 18:25:00,323 - INFO - >>> {'loss': 0.2457, 'grad_norm': 1.0222078561782837, 'learning_rate': 2.265772503450122e-05, 'epoch': 6.32}
>>> 2025-09-17 18:25:50,418 - INFO - >>> {'loss': 0.1961, 'grad_norm': 0.9255421757698059, 'learning_rate': 2.1355733569044635e-05, 'epoch': 6.373333333333333}
>>> 2025-09-17 18:26:35,004 - INFO - >>> {'loss': 0.1916, 'grad_norm': 0.774269700050354, 'learning_rate': 2.008778270707944e-05, 'epoch': 6.426666666666667}
>>> 2025-09-17 18:27:20,725 - INFO - >>> {'loss': 0.1836, 'grad_norm': 0.9484257102012634, 'learning_rate': 1.8854421271990964e-05, 'epoch': 6.48}
>>> 2025-09-17 18:28:02,197 - INFO - >>> {'loss': 0.18, 'grad_norm': 0.9226539731025696, 'learning_rate': 1.7656183115380577e-05, 'epoch': 6.533333333333333}
>>> 2025-09-17 18:28:53,623 - INFO - >>> {'loss': 0.1267, 'grad_norm': 0.8531612753868103, 'learning_rate': 1.649358688599191e-05, 'epoch': 6.586666666666667}
>>> 2025-09-17 18:29:41,778 - INFO - >>> {'loss': 0.2074, 'grad_norm': 1.0094162225723267, 'learning_rate': 1.5367135805217458e-05, 'epoch': 6.64}
>>> 2025-09-17 18:30:32,233 - INFO - >>> {'loss': 0.161, 'grad_norm': 0.9085741639137268, 'learning_rate': 1.4277317449282834e-05, 'epoch': 6.693333333333333}
>>> 2025-09-17 18:31:15,329 - INFO - >>> {'loss': 0.1696, 'grad_norm': 0.9244050979614258, 'learning_rate': 1.3224603538202929e-05, 'epoch': 6.746666666666667}
>>> 2025-09-17 18:32:11,281 - INFO - >>> {'loss': 0.1535, 'grad_norm': 0.8840281963348389, 'learning_rate': 1.220944973160133e-05, 'epoch': 6.8}
>>> 2025-09-17 18:32:57,280 - INFO - >>> {'loss': 0.1717, 'grad_norm': 0.8293298482894897, 'learning_rate': 1.1232295431481222e-05, 'epoch': 6.8533333333333335}
>>> 2025-09-17 18:33:40,895 - INFO - >>> {'loss': 0.1802, 'grad_norm': 1.1421566009521484, 'learning_rate': 1.0293563592033595e-05, 'epoch': 6.906666666666666}
>>> 2025-09-17 18:34:28,761 - INFO - >>> {'loss': 0.2181, 'grad_norm': 0.7771543860435486, 'learning_rate': 9.393660536564408e-06, 'epoch': 6.96}
>>> 2025-09-17 18:35:03,203 - INFO - >>> {'loss': 0.1326, 'grad_norm': 0.9104354381561279, 'learning_rate': 8.532975781620512e-06, 'epoch': 7.0}
>>> 2025-09-17 18:35:49,227 - INFO - >>> {'loss': 0.1715, 'grad_norm': 0.8170737028121948, 'learning_rate': 7.711881868390291e-06, 'epoch': 7.053333333333334}
>>> 2025-09-17 18:36:35,391 - INFO - >>> {'loss': 0.1484, 'grad_norm': 0.8988967537879944, 'learning_rate': 6.930734201451816e-06, 'epoch': 7.1066666666666665}
>>> 2025-09-17 18:37:22,517 - INFO - >>> {'loss': 0.1802, 'grad_norm': 0.8104085922241211, 'learning_rate': 6.189870894938587e-06, 'epoch': 7.16}
>>> 2025-09-17 18:38:10,789 - INFO - >>> {'loss': 0.2093, 'grad_norm': 0.9498206973075867, 'learning_rate': 5.489612626189245e-06, 'epoch': 7.213333333333333}
>>> 2025-09-17 18:39:02,793 - INFO - >>> {'loss': 0.1406, 'grad_norm': 0.7244024872779846, 'learning_rate': 4.830262496944693e-06, 'epoch': 7.266666666666667}
>>> 2025-09-17 18:39:45,208 - INFO - >>> {'loss': 0.1732, 'grad_norm': 0.8559492826461792, 'learning_rate': 4.21210590215273e-06, 'epoch': 7.32}
>>> 2025-09-17 18:40:31,477 - INFO - >>> {'loss': 0.1832, 'grad_norm': 0.8717175126075745, 'learning_rate': 3.6354104064368566e-06, 'epoch': 7.373333333333333}
>>> 2025-09-17 18:41:12,730 - INFO - >>> {'loss': 0.1497, 'grad_norm': 0.8337476253509521, 'learning_rate': 3.100425628282899e-06, 'epoch': 7.426666666666667}
>>> 2025-09-17 18:41:58,308 - INFO - >>> {'loss': 0.1834, 'grad_norm': 0.9057604670524597, 'learning_rate': 2.607383131993424e-06, 'epoch': 7.48}
>>> 2025-09-17 18:42:45,754 - INFO - >>> {'loss': 0.1993, 'grad_norm': 0.9332695007324219, 'learning_rate': 2.1564963274568027e-06, 'epoch': 7.533333333333333}
>>> 2025-09-17 18:43:26,159 - INFO - >>> {'loss': 0.1262, 'grad_norm': 0.8192704916000366, 'learning_rate': 1.7479603777742938e-06, 'epoch': 7.586666666666667}
>>> 2025-09-17 18:44:14,828 - INFO - >>> {'loss': 0.1795, 'grad_norm': 0.914505660533905, 'learning_rate': 1.3819521147851123e-06, 'epoch': 7.64}
>>> 2025-09-17 18:45:02,238 - INFO - >>> {'loss': 0.1215, 'grad_norm': 0.7569186687469482, 'learning_rate': 1.05862996252597e-06, 'epoch': 7.693333333333333}
>>> 2025-09-17 18:45:47,716 - INFO - >>> {'loss': 0.1938, 'grad_norm': 0.9385307431221008, 'learning_rate': 7.781338686584927e-07, 'epoch': 7.746666666666667}
>>> 2025-09-17 18:46:36,749 - INFO - >>> {'loss': 0.1802, 'grad_norm': 0.8065782785415649, 'learning_rate': 5.405852438937764e-07, 'epoch': 7.8}
>>> 2025-09-17 18:47:21,995 - INFO - >>> {'loss': 0.1666, 'grad_norm': 0.8273832201957703, 'learning_rate': 3.4608690944071263e-07, 'epoch': 7.8533333333333335}
>>> 2025-09-17 18:48:12,408 - INFO - >>> {'loss': 0.1262, 'grad_norm': 0.8100235462188721, 'learning_rate': 1.947230525005006e-07, 'epoch': 7.906666666666666}
>>> 2025-09-17 18:48:54,490 - INFO - >>> {'loss': 0.1359, 'grad_norm': 0.8897238969802856, 'learning_rate': 8.655918982689581e-08, 'epoch': 7.96}
>>> 2025-09-17 18:49:33,532 - INFO - >>> {'loss': 0.1043, 'grad_norm': 0.7620895504951477, 'learning_rate': 2.164213936770576e-08, 'epoch': 8.0}
>>> 2025-09-17 18:49:34,261 - INFO - >>> {'train_runtime': 7076.6788, 'train_samples_per_second': 0.678, 'train_steps_per_second': 0.021, 'train_loss': 0.9848553333431482, 'epoch': 8.0}
>>> 2025-09-17 18:49:34,263 - INFO - 训练成功！
>>> 2025-09-17 18:49:34,263 - INFO - 模型存放位置：./output/qwen3-8b202509171651
>>> 2025-09-17 19:54:43,177 - INFO - ========__main__  202509171954========
>>> 2025-09-17 19:54:43,178 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 19:54:43,178 - INFO - 开始进行模型测试
>>> 2025-09-17 19:55:06,334 - INFO - 已选择模型文件夹: qwen3-8b202509171651
>>> 2025-09-17 19:55:06,337 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171651/checkpoint-152
>>> 2025-09-17 23:08:14,835 - INFO - ========__main__  202509172308========
>>> 2025-09-17 23:08:14,836 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 23:08:14,836 - INFO - 开始进行模型测试
>>> 2025-09-17 23:08:17,869 - INFO - 已选择模型文件夹: qwen3-8b202509171651
>>> 2025-09-17 23:08:17,871 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171651/checkpoint-152
>>> 2025-09-17 23:09:26,380 - INFO - ========__main__  202509172309========
>>> 2025-09-17 23:09:26,380 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 23:09:26,381 - INFO - 开始进行模型测试
>>> 2025-09-17 23:09:28,592 - INFO - 已选择模型文件夹: qwen202508191333
>>> 2025-09-17 23:09:28,594 - INFO - 最新的 LoRA checkpoint 路径:output/qwen202508191333/checkpoint-100
>>> 2025-09-17 23:09:39,614 - INFO - ========__main__  202509172309========
>>> 2025-09-17 23:09:39,615 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 23:09:39,615 - INFO - 开始进行模型测试
>>> 2025-09-17 23:09:41,289 - INFO - 已选择模型文件夹: qwen3-8b202509171651
>>> 2025-09-17 23:09:41,291 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171651/checkpoint-152
>>> 2025-09-17 23:10:56,823 - INFO - ========__main__  202509172310========
>>> 2025-09-17 23:10:56,824 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 23:10:56,824 - INFO - 开始进行模型测试
>>> 2025-09-17 23:10:59,099 - INFO - 已选择模型文件夹: qwen3-8b202509171651
>>> 2025-09-17 23:10:59,102 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171651/checkpoint-152
>>> 2025-09-17 23:20:42,349 - INFO - ========__main__  202509172320========
>>> 2025-09-17 23:20:42,349 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-17 23:20:42,350 - INFO - 开始进行模型测试
>>> 2025-09-17 23:20:44,854 - INFO - 已选择模型文件夹: qwen3-8b202509171651
>>> 2025-09-17 23:20:44,857 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171651/checkpoint-152
>>> 2025-09-18 10:26:39,255 - INFO - 已选择模型文件夹: qwen3-8b202509171651
>>> 2025-09-18 10:26:39,258 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171651/checkpoint-152
>>> 2025-09-18 10:33:17,866 - INFO - 已选择模型文件夹: qwen3-8b202509171651
>>> 2025-09-18 10:33:17,868 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171651/checkpoint-152
>>> 2025-09-18 10:33:17,870 - INFO - 正在加载基础模型...
>>> 2025-09-18 10:33:23,842 - INFO - 正在加载LoRA权重...
>>> 2025-09-18 10:33:24,426 - INFO - 正在合并模型...
>>> 2025-09-18 10:33:24,549 - INFO - 正在加载tokenizer...
>>> 2025-09-18 10:33:24,989 - INFO - 正在保存合并后的模型...
>>> 2025-09-18 10:33:56,332 - INFO - 模型合并完成，已保存至: /home/liangshuqiao/models/qwen_v2
>>> 2025-09-18 11:20:21,515 - INFO - ========__main__  202509181120========
>>> 2025-09-18 11:20:21,532 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-18 11:20:21,544 - INFO - 开始进行模型测试
>>> 2025-09-18 11:20:25,812 - INFO - 已选择模型文件夹: qwen3-8b202509171651
>>> 2025-09-18 11:20:25,838 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171651/checkpoint-152
>>> 2025-09-18 11:24:09,104 - INFO - 开始进行原始模型对话测试
>>> 2025-09-18 11:24:11,436 - INFO - 导入包完成
>>> 2025-09-18 11:24:11,444 - INFO - 配置文件读取完成
>>> 2025-09-18 11:30:20,925 - INFO - 开始进行原始模型对话测试
>>> 2025-09-18 11:30:23,278 - INFO - 导入包完成
>>> 2025-09-18 11:30:23,285 - INFO - 配置文件读取完成
>>> 2025-09-18 11:31:09,794 - INFO - ========__main__  202509181131========
>>> 2025-09-18 11:31:09,795 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-18 11:31:09,796 - INFO - 开始进行模型测试
>>> 2025-09-18 11:31:12,198 - INFO - 已选择模型文件夹: qwen3-8b202509171651
>>> 2025-09-18 11:31:12,201 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171651/checkpoint-152
>>> 2025-09-18 11:31:54,253 - INFO - 导入包完成
>>> 2025-09-18 11:31:54,253 - INFO - ========train Qwen2ForCausalLM  202509181131========
>>> 2025-09-18 11:31:54,254 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-18 11:31:54,254 - INFO - 开始进行训练
>>> 2025-09-18 11:31:54,261 - INFO - 基础配置文件读取完成
>>> 2025-09-18 11:31:54,268 - INFO - 训练配置读取完成
>>> 2025-09-18 11:31:54,269 - INFO - 数据集路径：dataset/own/Medical_Extension.json
>>> 2025-09-18 11:31:54,269 - INFO - 模型路径:/home/liangshuqiao/models/qwen3-8b
>>> 2025-09-18 11:31:54,704 - INFO - tokenizer读取完成
>>> 2025-09-18 11:31:54,858 - INFO - model dtype:torch.bfloat16
>>> 2025-09-18 11:31:54,858 - INFO - 模型导入完成
>>> 2025-09-18 11:31:54,858 - INFO - 数据读取开始
>>> 2025-09-18 11:31:55,754 - INFO - 数据下载完成，训练集大小: 600
>>> 2025-09-18 11:31:59,914 - INFO - 数据映射完成
>>> 2025-09-18 11:31:59,915 - INFO - 打印训练参数如下
>>> 2025-09-18 11:31:59,915 - INFO -   task_type >>> CAUSAL_LM
>>> 2025-09-18 11:31:59,915 - INFO -   dtype >>> torch.bfloat16
>>> 2025-09-18 11:31:59,916 - INFO -   load_in_4bit >>> True
>>> 2025-09-18 11:31:59,916 - INFO -   batch_size >>> 8
>>> 2025-09-18 11:31:59,916 - INFO -   gradient_accumulator_steps >>> 4
>>> 2025-09-18 11:31:59,917 - INFO -   warmup_steps >>> 1
>>> 2025-09-18 11:31:59,917 - INFO -   epoch >>> 8
>>> 2025-09-18 11:31:59,918 - INFO -   eval_steps >>> 5
>>> 2025-09-18 11:31:59,918 - INFO -   learning_rate >>> 0.0002
>>> 2025-09-18 11:31:59,918 - INFO -   lr_scheduler_type >>> cosine
>>> 2025-09-18 11:31:59,919 - INFO -   max_seq_length >>> 2048
>>> 2025-09-18 11:31:59,919 - INFO -   r >>> 8
>>> 2025-09-18 11:31:59,919 - INFO -   interface_mode >>> False
>>> 2025-09-18 11:31:59,920 - INFO -   target_modules >>> ['q_proj', 'k_proj', 'v_proj', 'o_proj']
>>> 2025-09-18 11:31:59,920 - INFO -   lora_alpha >>> 16
>>> 2025-09-18 11:31:59,920 - INFO -   lora_dropout >>> 0.1
>>> 2025-09-18 11:31:59,921 - INFO -   bias >>> none
>>> 2025-09-18 11:31:59,921 - INFO -   use_gradient_checkpointing >>> unsloth
>>> 2025-09-18 11:31:59,922 - INFO -   random_state >>> 3407
>>> 2025-09-18 11:31:59,922 - INFO -   use_rslora >>> True
>>> 2025-09-18 11:31:59,922 - INFO -   loftq_config >>> None
>>> 2025-09-18 11:32:06,022 - INFO - 开始训练！
>>> 2025-09-18 11:41:34,170 - INFO - ========__main__  202509181141========
>>> 2025-09-18 11:41:34,171 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-18 11:41:34,171 - INFO - 开始进行模型测试
>>> 2025-09-18 11:41:45,298 - INFO - 已选择模型文件夹: qwen3-8b202509171651
>>> 2025-09-18 11:41:45,301 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171651/checkpoint-152
>>> 2025-09-18 11:48:39,252 - INFO - ========__main__  202509181148========
>>> 2025-09-18 11:48:39,253 - INFO - 当前环境：/home/liangshuqiao/agent_trainer/agent-fine-tuning-trainer/clean_env/bin/python
>>> 2025-09-18 11:48:39,254 - INFO - 开始进行模型测试
>>> 2025-09-18 11:48:42,424 - INFO - 已选择模型文件夹: qwen3-8b202509171651
>>> 2025-09-18 11:48:42,427 - INFO - 最新的 LoRA checkpoint 路径:output/qwen3-8b202509171651/checkpoint-152
